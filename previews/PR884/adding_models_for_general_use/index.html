<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Adding Models for General Use · MLJ</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.svg" alt="MLJ logo"/></a><div class="docs-package-name"><span class="docs-autofit">MLJ</span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../about_mlj/">About MLJ</a></li><li><a class="tocitem" href="../getting_started/">Getting Started</a></li><li><a class="tocitem" href="../common_mlj_workflows/">Common MLJ Workflows</a></li><li><a class="tocitem" href="../working_with_categorical_data/">Working with Categorical Data</a></li><li><a class="tocitem" href="../model_search/">Model Search</a></li><li><a class="tocitem" href="../loading_model_code/">Loading Model Code</a></li><li><a class="tocitem" href="../machines/">Machines</a></li><li><a class="tocitem" href="../evaluating_model_performance/">Evaluating Model Performance</a></li><li><a class="tocitem" href="../performance_measures/">Performance Measures</a></li><li><a class="tocitem" href="../weights/">Weights</a></li><li><a class="tocitem" href="../tuning_models/">Tuning Models</a></li><li><a class="tocitem" href="../learning_curves/">Learning Curves</a></li><li><a class="tocitem" href="../preparing_data/">Preparing Data</a></li><li><a class="tocitem" href="../transformers/">Transformers and Other Unsupervised models</a></li><li><a class="tocitem" href="../more_on_probabilistic_predictors/">More on Probablistic Predictors</a></li><li><a class="tocitem" href="../composing_models/">Composing Models</a></li><li><a class="tocitem" href="../linear_pipelines/">Linear Pipelines</a></li><li><a class="tocitem" href="../target_transformations/">Target Transformations</a></li><li><a class="tocitem" href="../homogeneous_ensembles/">Homogeneous Ensembles</a></li><li><a class="tocitem" href="../model_stacking/">Model Stacking</a></li><li><a class="tocitem" href="../controlling_iterative_models/">Controlling Iterative Models</a></li><li><a class="tocitem" href="../generating_synthetic_data/">Generating Synthetic Data</a></li><li><a class="tocitem" href="../openml_integration/">OpenML Integration</a></li><li><a class="tocitem" href="../acceleration_and_parallelism/">Acceleration and Parallelism</a></li><li><a class="tocitem" href="../simple_user_defined_models/">Simple User Defined Models</a></li><li><a class="tocitem" href="../quick_start_guide_to_adding_models/">Quick-Start Guide to Adding Models</a></li><li class="is-active"><a class="tocitem" href>Adding Models for General Use</a><ul class="internal"><li><a class="tocitem" href="#Overview"><span>Overview</span></a></li><li><a class="tocitem" href="#New-model-type-declarations-and-optional-clean!-method"><span>New model type declarations and optional clean! method</span></a></li><li><a class="tocitem" href="#Supervised-models"><span>Supervised models</span></a></li><li><a class="tocitem" href="#Models-that-learn-a-probability-distribution"><span>Models that learn a probability distribution</span></a></li><li><a class="tocitem" href="#Unsupervised-models"><span>Unsupervised models</span></a></li><li><a class="tocitem" href="#Convenience-methods"><span>Convenience methods</span></a></li></ul></li><li><a class="tocitem" href="../modifying_behavior/">Modifying Behavior</a></li><li><a class="tocitem" href="../internals/">Internals</a></li><li><a class="tocitem" href="../list_of_supported_models/">List of Supported Models</a></li><li><a class="tocitem" href="../third_party_packages/">Third Party Packages</a></li><li><a class="tocitem" href="../glossary/">Glossary</a></li><li><a class="tocitem" href="../mlj_cheatsheet/">MLJ Cheatsheet</a></li><li><a class="tocitem" href="../known_issues/">Known Issues</a></li><li><a class="tocitem" href="../frequently_asked_questions/">FAQ</a></li><li><a class="tocitem" href="../julia_blogpost/">Julia BlogPost</a></li><li><a class="tocitem" href="../api/">Index of Methods</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Adding Models for General Use</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Adding Models for General Use</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/alan-turing-institute/MLJ.jl/blob/master/docs/src/adding_models_for_general_use.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Adding-Models-for-General-Use"><a class="docs-heading-anchor" href="#Adding-Models-for-General-Use">Adding Models for General Use</a><a id="Adding-Models-for-General-Use-1"></a><a class="docs-heading-anchor-permalink" href="#Adding-Models-for-General-Use" title="Permalink"></a></h1><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>Models implementing the MLJ model interface according to the instructions given here should import MLJModelInterface version 1.0.0 or higher. This is enforced with a statement such as <code>MLJModelInterface = &quot;^1&quot;</code> under <code>[compat]</code> in the Project.toml file of the package containing the implementation.</p></div></div><p>This guide outlines the specification of the MLJ model interface and provides detailed guidelines for implementing the interface for models intended for general use. See also the more condensed <a href="../quick_start_guide_to_adding_models/#Quick-Start-Guide-to-Adding-Models">Quick-Start Guide to Adding Models</a>.</p><p>For sample implementations, see <a href="https://github.com/JuliaAI/MLJModels.jl/tree/master/src/builtins">MLJModels/src</a>.</p><p>Interface code can be hosted by the package providing the core machine learning algorithm, or by a stand-alone &quot;interface-only&quot; package, using the template <a href="https://github.com/JuliaAI/MLJExampleInterface.jl">MLJExampleInterface.jl</a> (see <a href="#Where-to-place-code-implementing-new-models">Where to place code implementing new models</a> below).</p><p>The machine learning tools provided by MLJ can be applied to the models in any package that imports the package <a href="https://github.com/JuliaAI/MLJModelInterface.jl">MLJModelInterface</a> and implements the API defined there, as outlined below. For a quick-and-dirty implementation of user-defined models see <a href="../simple_user_defined_models/">Simple User Defined Models</a>.  To make new models available to all MLJ users, see <a href="#Where-to-place-code-implementing-new-models">Where to place code implementing new models</a>.</p><h4 id="Important"><a class="docs-heading-anchor" href="#Important">Important</a><a id="Important-1"></a><a class="docs-heading-anchor-permalink" href="#Important" title="Permalink"></a></h4><p><a href="https://github.com/JuliaAI/MLJModelInterface.jl">MLJModelInterface</a> is a very light-weight interface allowing you to <em>define</em> your interface, but does not provide the functionality required to use or test your interface; this requires <a href="https://github.com/JuliaAI/MLJBase.jl">MLJBase</a>.  So, while you only need to add <code>MLJModelInterface</code> to your project&#39;s [deps], for testing purposes you need to add <a href="https://github.com/JuliaAI/MLJBase.jl">MLJBase</a> to your project&#39;s [extras] and [targets]. In testing, simply use <code>MLJBase</code> in place of <code>MLJModelInterface</code>.</p><p>It is assumed the reader has read <a href="../">Getting Started</a>. To implement the API described here, some familiarity with the following packages is also helpful:</p><ul><li><p><a href="https://github.com/JuliaAI/ScientificTypes.jl">ScientificTypes.jl</a> (for specifying model requirements of data)</p></li><li><p><a href="https://github.com/JuliaStats/Distributions.jl">Distributions.jl</a> (for probabilistic predictions)</p></li><li><p><a href="https://github.com/JuliaData/CategoricalArrays.jl">CategoricalArrays.jl</a> (essential if you are implementing a model handling data of <code>Multiclass</code> or <code>OrderedFactor</code> scitype; familiarity with <code>CategoricalPool</code> objects required)</p></li><li><p><a href="https://github.com/JuliaData/Tables.jl">Tables.jl</a> (if your algorithm needs input data in a novel format).</p></li></ul><p>In MLJ, the basic interface exposed to the user, built atop the model interface described here, is the <em>machine interface</em>. After a first reading of this document, the reader may wish to refer to <a href="../internals/">MLJ Internals</a> for context.</p><h2 id="Overview"><a class="docs-heading-anchor" href="#Overview">Overview</a><a id="Overview-1"></a><a class="docs-heading-anchor-permalink" href="#Overview" title="Permalink"></a></h2><p>A <em>model</em> is an object storing hyperparameters associated with some machine learning algorithm, and that is all. In MLJ, hyperparameters include configuration parameters, like the number of threads, and special instructions, such as &quot;compute feature rankings&quot;, which may or may not affect the final learning outcome.  However, the logging level (<code>verbosity</code> below) is excluded. <em>Learned parameters</em> (such as the coefficients in a linear model) have no place in the model struct.</p><p>The name of the Julia type associated with a model indicates the associated algorithm (e.g., <code>DecisionTreeClassifier</code>). The outcome of training a learning algorithm is called a <em>fitresult</em>. For ordinary multivariate regression, for example, this would be the coefficients and intercept. For a general supervised model, it is the (generally minimal) information needed to make new predictions.</p><p>The ultimate supertype of all models is <code>MLJModelInterface.Model</code>, which has two abstract subtypes:</p><pre><code class="language-julia">abstract type Supervised &lt;: Model end
abstract type Unsupervised &lt;: Model end</code></pre><p><code>Supervised</code> models are further divided according to whether they are able to furnish probabilistic predictions of the target (which they will then do by default) or directly predict &quot;point&quot; estimates, for each new input pattern:</p><pre><code class="language-julia">abstract type Probabilistic &lt;: Supervised end
abstract type Deterministic &lt;: Supervised end</code></pre><p>Further division of model types is realized through <a href="#Trait-declarations">Trait declarations</a>.</p><p>Associated with every concrete subtype of <code>Model</code> there must be a <code>fit</code> method, which implements the associated algorithm to produce the fitresult. Additionally, every <code>Supervised</code> model has a <code>predict</code> method, while <code>Unsupervised</code> models must have a <code>transform</code> method. More generally, methods such as these, that are dispatched on a model instance and a fitresult (plus other data), are called <em>operations</em>. <code>Probabilistic</code> supervised models optionally implement a <code>predict_mode</code> operation (in the case of classifiers) or a <code>predict_mean</code> and/or <code>predict_median</code> operations (in the case of regressors) although MLJModelInterface also provides fallbacks that will suffice in most cases. <code>Unsupervised</code> models may implement an <code>inverse_transform</code> operation.</p><h2 id="New-model-type-declarations-and-optional-clean!-method"><a class="docs-heading-anchor" href="#New-model-type-declarations-and-optional-clean!-method">New model type declarations and optional clean! method</a><a id="New-model-type-declarations-and-optional-clean!-method-1"></a><a class="docs-heading-anchor-permalink" href="#New-model-type-declarations-and-optional-clean!-method" title="Permalink"></a></h2><p>Here is an example of a concrete supervised model type declaration, for a model with a single hyper-parameter:</p><pre><code class="language-julia">import MLJModelInterface
const MMI = MLJModelInterface

mutable struct RidgeRegressor &lt;: MMI.Deterministic
    lambda::Float64
end</code></pre><p>Models (which are mutable) should not be given internal constructors. It is recommended that they be given an external lazy keyword constructor of the same name. This constructor defines default values for every field, and optionally corrects invalid field values by calling a <code>clean!</code> method (whose fallback returns an empty message string):</p><pre><code class="language-julia">function MMI.clean!(model::RidgeRegressor)
    warning = &quot;&quot;
    if model.lambda &lt; 0
        warning *= &quot;Need lambda ≥ 0. Resetting lambda=0. &quot;
        model.lambda = 0
    end
    return warning
end

# keyword constructor
function RidgeRegressor(; lambda=0.0)
    model = RidgeRegressor(lambda)
    message = MMI.clean!(model)
    isempty(message) || @warn message
    return model
end</code></pre><p><em>Important.</em> The clean method must have the property that <code>clean!(clean!(model)) == clean!(model)</code> for any instance <code>model</code>.</p><p>Although not essential, try to avoid <code>Union</code> types for model fields. For example, a field declaration <code>features::Vector{Symbol}</code> with a default of <code>Symbol[]</code> (detected with <code>isempty</code> method) is preferred to <code>features::Union{Vector{Symbol}, Nothing}</code> with a default of <code>nothing</code>.</p><h3 id="Hyper-parameters-for-parellizatioin-options"><a class="docs-heading-anchor" href="#Hyper-parameters-for-parellizatioin-options">Hyper-parameters for parellizatioin options</a><a id="Hyper-parameters-for-parellizatioin-options-1"></a><a class="docs-heading-anchor-permalink" href="#Hyper-parameters-for-parellizatioin-options" title="Permalink"></a></h3><p>The section <a href="../acceleration_and_parallelism/#Acceleration-and-Parallelism">Acceleration and Parallelism</a> indicates how MLJ models specify an option to run an algorithm using distributed processing or multi-threading. A hyper-parameter specifying such an option should be called <code>acceleration</code>. Its value <code>a</code> should satisfy  <code>a isa AbstractResource</code> where <code>AbstractResource</code> is defined in the ComputationalResources.jl package. An option to run on a GPU is ordinarily indicated with the <code>CUDALibs()</code> resource.</p><h3 id="Hyper-parameter-access-and-mutation"><a class="docs-heading-anchor" href="#Hyper-parameter-access-and-mutation">Hyper-parameter access and mutation</a><a id="Hyper-parameter-access-and-mutation-1"></a><a class="docs-heading-anchor-permalink" href="#Hyper-parameter-access-and-mutation" title="Permalink"></a></h3><p>To support hyper-parameter optimization (see <a href="../tuning_models/#Tuning-Models">Tuning Models</a>) any hyper-parameter to be individually controlled must be:</p><ul><li>property-accessible; nested property access allowed, as in <code>model.detector.K</code> </li><li>mutable </li></ul><p>For an un-nested hyper-parameter, the requirement is that <code>getproperty(model, :param_name)</code> and <code>setproperty!(model, :param_name, value)</code> have the expected behavior. (In hyper-parameter tuning, recursive access is implemented using <a href="#MLJBase.recursive_getproperty"><code>MLJBase.recursive_getproperty</code></a><code>and [</code>MLJBase.recursively_setproperty!`](@ref).)</p><p>Combining hyper-parameters in a named tuple does not generally work, because, although property-accessible (with nesting), an individual value cannot be mutated. </p><p>For a suggested way to deal with hyper-parameters varying in number, see the <a href="https://github.com/JuliaAI/MLJBase.jl/blob/dev/src/composition/models/stacking.jl">implementation</a> of <code>Stack</code>, where the model struct stores a varying number of base models internally as a vector, but components are named at construction and accessed by overloading <code>getproperty/setproperty!</code> appropriately.</p><h3 id="Macro-shortcut"><a class="docs-heading-anchor" href="#Macro-shortcut">Macro shortcut</a><a id="Macro-shortcut-1"></a><a class="docs-heading-anchor-permalink" href="#Macro-shortcut" title="Permalink"></a></h3><p>An alternative to declaring the model struct, clean! method and keyword constructor, is to use the <code>@mlj_model</code> macro, as in the following example:</p><pre><code class="language-julia">@mlj_model mutable struct YourModel &lt;: MMI.Deterministic
    a::Float64 = 0.5::(_ &gt; 0)
    b::String  = &quot;svd&quot;::(_ in (&quot;svd&quot;,&quot;qr&quot;))
end</code></pre><p>This declaration specifies:</p><ul><li>A keyword constructor (here <code>YourModel(; a=..., b=...)</code>),</li><li>Default values for the hyperparameters,</li><li>Constraints on the hyperparameters where <code>_</code> refers to a value passed.</li></ul><p>For example, <code>a::Float64 = 0.5::(_ &gt; 0)</code> indicates that the field <code>a</code> is a <code>Float64</code>, takes <code>0.5</code> as default value, and expects its value to be positive.</p><p>You cannot use the <code>@mlj_model</code> macro if your model struct has type parameters.</p><h4 id="Known-issue-with-@mlj_macro"><a class="docs-heading-anchor" href="#Known-issue-with-@mlj_macro">Known issue with @mlj_macro</a><a id="Known-issue-with-@mlj_macro-1"></a><a class="docs-heading-anchor-permalink" href="#Known-issue-with-@mlj_macro" title="Permalink"></a></h4><p>Defaults with negative values can trip up the <code>@mlj_macro</code> (see <a href="https://github.com/JuliaAI/MLJBase.jl/issues/68">this issue</a>). So, for example, this does not work:</p><pre><code class="language-julia">@mlj_model mutable struct Bar
    a::Int = -1::(_ &gt; -2)
end</code></pre><p>But this does:</p><pre><code class="language-julia">@mlj_model mutable struct Bar
    a::Int = (-)(1)::(_ &gt; -2)
end</code></pre><h2 id="Supervised-models"><a class="docs-heading-anchor" href="#Supervised-models">Supervised models</a><a id="Supervised-models-1"></a><a class="docs-heading-anchor-permalink" href="#Supervised-models" title="Permalink"></a></h2><h3 id="Mathematical-assumptions"><a class="docs-heading-anchor" href="#Mathematical-assumptions">Mathematical assumptions</a><a id="Mathematical-assumptions-1"></a><a class="docs-heading-anchor-permalink" href="#Mathematical-assumptions" title="Permalink"></a></h3><p>At present, MLJ&#39;s performance estimate functionality (resampling using <code>evaluate</code>/<code>evaluate!</code>) tacitly assumes that feature-label pairs of observations <code>(X1, y1), (X2, y2), (X2, y2), ...</code> are being modelled as identically independent random variables (i.i.d.), and constructs some kind of representation of an estimate of the conditional probablility <code>p(y | X)</code> (<code>y</code> and <code>X</code> single observations). It may be that a model implementing the MLJ interface has the potential to make predictions under weaker assumptions (e.g., time series forecasting models). However the output of the compulsory <code>predict</code> method described below should be the output of the model under the i.i.d assumption.</p><p>In the future newer methods may be introduced to handle weaker assumptions (see, e.g., <a href="#The-predict_joint-method">The predict_joint method</a> below).</p><h3 id="Summary-of-methods"><a class="docs-heading-anchor" href="#Summary-of-methods">Summary of methods</a><a id="Summary-of-methods-1"></a><a class="docs-heading-anchor-permalink" href="#Summary-of-methods" title="Permalink"></a></h3><p>The compulsory and optional methods to be implemented for each concrete type <code>SomeSupervisedModel &lt;: MMI.Supervised</code> are summarized below. </p><p>An <code>=</code> indicates the return value for a fallback version of the method.</p><p>Compulsory:</p><pre><code class="language-julia">MMI.fit(model::SomeSupervisedModel, verbosity, X, y) -&gt; fitresult, cache, report
MMI.predict(model::SomeSupervisedModel, fitresult, Xnew) -&gt; yhat</code></pre><p>Optional, to check and correct invalid hyperparameter values:</p><pre><code class="language-julia">MMI.clean!(model::SomeSupervisedModel) = &quot;&quot;</code></pre><p>Optional, to return user-friendly form of fitted parameters:</p><pre><code class="language-julia">MMI.fitted_params(model::SomeSupervisedModel, fitresult) = fitresult</code></pre><p>Optional, to avoid redundant calculations when re-fitting machines associated with a model:</p><pre><code class="language-julia">MMI.update(model::SomeSupervisedModel, verbosity, old_fitresult, old_cache, X, y) =
   MMI.fit(model, verbosity, X, y)</code></pre><p>Optional, to specify default hyperparameter ranges (for use in tuning):</p><pre><code class="language-julia">MMI.hyperparameter_ranges(T::Type) = Tuple(fill(nothing, length(fieldnames(T))))</code></pre><p>Optional, if <code>SomeSupervisedModel &lt;: Probabilistic</code>:</p><pre><code class="language-julia">MMI.predict_mode(model::SomeSupervisedModel, fitresult, Xnew) =
    mode.(predict(model, fitresult, Xnew))
MMI.predict_mean(model::SomeSupervisedModel, fitresult, Xnew) =
    mean.(predict(model, fitresult, Xnew))
MMI.predict_median(model::SomeSupervisedModel, fitresult, Xnew) =
    median.(predict(model, fitresult, Xnew))</code></pre><p>Required, if the model is to be registered (findable by general users):</p><pre><code class="language-julia">MMI.load_path(::Type{&lt;:SomeSupervisedModel})    = &quot;&quot;
MMI.package_name(::Type{&lt;:SomeSupervisedModel}) = &quot;Unknown&quot;
MMI.package_uuid(::Type{&lt;:SomeSupervisedModel}) = &quot;Unknown&quot;</code></pre><pre><code class="language-julia">MMI.input_scitype(::Type{&lt;:SomeSupervisedModel}) = Unknown</code></pre><p>Strongly recommended, to constrain the form of target data passed to fit:</p><pre><code class="language-julia">MMI.target_scitype(::Type{&lt;:SomeSupervisedModel}) = Unknown</code></pre><p>Optional but recommended:</p><pre><code class="language-julia">MMI.package_url(::Type{&lt;:SomeSupervisedModel})  = &quot;unknown&quot;
MMI.is_pure_julia(::Type{&lt;:SomeSupervisedModel}) = false
MMI.package_license(::Type{&lt;:SomeSupervisedModel}) = &quot;unknown&quot;</code></pre><p>If <code>SomeSupervisedModel</code> supports sample weights or class weights, then instead of the <code>fit</code> above, one implements</p><pre><code class="language-julia">MMI.fit(model::SomeSupervisedModel, verbosity, X, y, w=nothing) -&gt; fitresult, cache, report</code></pre><p>and, if appropriate</p><pre><code class="language-julia">MMI.update(model::SomeSupervisedModel, verbosity, old_fitresult, old_cache, X, y, w=nothing) =
   MMI.fit(model, verbosity, X, y, w)</code></pre><p>Additionally, if <code>SomeSupervisedModel</code> supports sample weights, one must declare</p><pre><code class="language-julia">MMI.supports_weights(model::Type{&lt;:SomeSupervisedModel}) = true</code></pre><p>Optionally, an implemenation may add a data front-end, for transforming user data (such as a table) into some model-specific format (such as a matrix), and for adding methods to specify how said format is resampled. (This alters the meaning of <code>X</code>, <code>y</code> and <code>w</code> in the signatures of <code>fit</code>, <code>update</code>, <code>predict</code>, etc; see <a href="#Implementing-a-data-front-end">Implementing a data front-end</a> for details). This can provide the MLJ user certain performance advantages when fitting a machine.</p><pre><code class="language-julia">MLJModelInterface.reformat(model::SomeSupervisedModel, args...) = args
MLJModelInterface.selectrows(model::SomeSupervisedModel, I, data...) = data</code></pre><p>Optionally, to customized support for serialization of machines (see <a href="#Serialization">Serialization</a>), overload</p><pre><code class="language-julia">MMI.save(filename, model::SomeModel, fitresult; kwargs...) = fitresult</code></pre><p>and possibly</p><pre><code class="language-julia">MMI.restore(filename, model::SomeModel, serializable_fitresult) -&gt; serializable_fitresult</code></pre><p>These last two are unlikely to be needed if wrapping pure Julia code.</p><h3 id="The-form-of-data-for-fitting-and-predicting"><a class="docs-heading-anchor" href="#The-form-of-data-for-fitting-and-predicting">The form of data for fitting and predicting</a><a id="The-form-of-data-for-fitting-and-predicting-1"></a><a class="docs-heading-anchor-permalink" href="#The-form-of-data-for-fitting-and-predicting" title="Permalink"></a></h3><p>The model implementer does not have absolute control over the types of data <code>X</code>, <code>y</code> and <code>Xnew</code> appearing in the <code>fit</code> and <code>predict</code> methods they must implement. Rather, they can specify the <em>scientific type</em> of this data by making appropriate declarations of the traits <code>input_scitype</code> and <code>target_scitype</code> discussed later under <a href="#Trait-declarations">Trait declarations</a>.</p><p><em>Important Note.</em> Unless it genuinely makes little sense to do so, the MLJ recommendation is to specify a <code>Table</code> scientific type for <code>X</code> (and hence <code>Xnew</code>) and an <code>AbstractVector</code> scientific type (e.g., <code>AbstractVector{Continuous}</code>) for targets <code>y</code>. Algorithms requiring matrix input can coerce their inputs appropriately; see below.</p><h4 id="Additional-type-coercions"><a class="docs-heading-anchor" href="#Additional-type-coercions">Additional type coercions</a><a id="Additional-type-coercions-1"></a><a class="docs-heading-anchor-permalink" href="#Additional-type-coercions" title="Permalink"></a></h4><p>If the core algorithm being wrapped requires data in a different or more specific form, then <code>fit</code> will need to coerce the table into the form desired (and the same coercions applied to <code>X</code> will have to be repeated for <code>Xnew</code> in <code>predict</code>). To assist with common cases, MLJ provides the convenience method <a href="#MLJModelInterface.matrix"><code>MMI.matrix</code></a>. <code>MMI.matrix(Xtable)</code> has type <code>Matrix{T}</code> where <code>T</code> is the tightest common type of elements of <code>Xtable</code>, and <code>Xtable</code> is any table. (If <code>Xtable</code> is itself just a wrapped matrix, <code>Xtable=Tables.table(A)</code>, then <code>A=MMI.table(Xtable)</code> will be returned without any copying.)</p><p>Alternatively, a more performant option is to implement a data front-end for your model; see <a href="#Implementing-a-data-front-end">Implementing a data front-end</a>.</p><p>Other auxiliary methods provided by MLJModelInterface for handling tabular data are: <code>selectrows</code>, <code>selectcols</code>, <code>select</code> and <code>schema</code> (for extracting the size, names and eltypes of a table&#39;s columns). See <a href="#Convenience-methods">Convenience methods</a> below for details.</p><h4 id="Important-convention"><a class="docs-heading-anchor" href="#Important-convention">Important convention</a><a id="Important-convention-1"></a><a class="docs-heading-anchor-permalink" href="#Important-convention" title="Permalink"></a></h4><p>It is to be understood that the columns of the table <code>X</code> correspond to features and the rows to observations. So, for example, the predict method for a linear regression model might look like <code>predict(model, w, Xnew) = MMI.matrix(Xnew)*w</code>, where <code>w</code> is the vector of learned coefficients.</p><h3 id="The-fit-method"><a class="docs-heading-anchor" href="#The-fit-method">The fit method</a><a id="The-fit-method-1"></a><a class="docs-heading-anchor-permalink" href="#The-fit-method" title="Permalink"></a></h3><p>A compulsory <code>fit</code> method returns three objects:</p><pre><code class="language-julia">MMI.fit(model::SomeSupervisedModel, verbosity, X, y) -&gt; fitresult, cache, report</code></pre><ol><li><p><code>fitresult</code> is the fitresult in the sense above (which becomes an  argument for <code>predict</code> discussed below).</p></li><li><p><code>report</code> is a (possibly empty) <code>NamedTuple</code>, for example,  <code>report=(deviance=..., dof_residual=..., stderror=..., vcov=...)</code>.  Any training-related statistics, such as internal estimates of the  generalization error, and feature rankings, should be returned in  the <code>report</code> tuple. How, or if, these are generated should be  controlled by hyperparameters (the fields of <code>model</code>). Fitted  parameters, such as the coefficients of a linear model, do not go  in the report as they will be extractable from <code>fitresult</code> (and  accessible to MLJ through the <code>fitted_params</code> method described below).</p></li></ol><p>3.	The value of <code>cache</code> can be <code>nothing</code>, unless one is also defining     an <code>update</code> method (see below). The Julia type of <code>cache</code> is not     presently restricted.</p><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>The  <code>fit</code> (and <code>update</code>) methods should not mutate the <code>model</code>. If necessary, <code>fit</code> can create a <code>deepcopy</code> of <code>model</code> first. </p></div></div><p>It is not necessary for <code>fit</code> to provide type or dimension checks on <code>X</code> or <code>y</code> or to call <code>clean!</code> on the model; MLJ will carry out such checks. </p><p>The types of <code>X</code> and <code>y</code> are constrained by the <code>input_scitype</code> and <code>target_scitype</code> trait declarations; see <a href="#Trait-declarations">Trait declarations</a> below. (That is, unless a data front-end is implemented, in which case these traits refer instead to the arguments of the overloaded <code>reformat</code> method, and the types of <code>X</code> and <code>y</code> are determined by the output of <code>reformat</code>.)</p><p>The method <code>fit</code> should never alter hyperparameter values, the sole exception being fields of type <code>&lt;:AbstractRNG</code>. If the package is able to suggest better hyperparameters, as a byproduct of training, return these in the report field.</p><p>The <code>verbosity</code> level (0 for silent) is for passing to learning algorithm itself. A <code>fit</code> method wrapping such an algorithm should generally avoid doing any of its own logging.</p><p><em>Sample weight support.</em> If <code>supports_weights(::Type{&lt;:SomeSupervisedModel})</code> has been declared <code>true</code>, then one instead implements the following variation on the above <code>fit</code>:</p><pre><code class="language-julia">MMI.fit(model::SomeSupervisedModel, verbosity, X, y, w=nothing) -&gt; fitresult, cache, report</code></pre><h3 id="The-fitted_params-method"><a class="docs-heading-anchor" href="#The-fitted_params-method">The fitted_params method</a><a id="The-fitted_params-method-1"></a><a class="docs-heading-anchor-permalink" href="#The-fitted_params-method" title="Permalink"></a></h3><p>A <code>fitted_params</code> method may be optionally overloaded. It&#39;s purpose is to provide MLJ access to a user-friendly representation of the learned parameters of the model (as opposed to the hyperparameters). They must be extractable from <code>fitresult</code>.</p><pre><code class="language-julia">MMI.fitted_params(model::SomeSupervisedModel, fitresult) -&gt; friendly_fitresult::NamedTuple</code></pre><p>For a linear model, for example, one might declare something like <code>friendly_fitresult=(coefs=[...], bias=...)</code>.</p><p>The fallback is to return <code>(fitresult=fitresult,)</code>.</p><h3 id="The-predict-method"><a class="docs-heading-anchor" href="#The-predict-method">The predict method</a><a id="The-predict-method-1"></a><a class="docs-heading-anchor-permalink" href="#The-predict-method" title="Permalink"></a></h3><p>A compulsory <code>predict</code> method has the form</p><pre><code class="language-julia">MMI.predict(model::SomeSupervisedModel, fitresult, Xnew) -&gt; yhat</code></pre><p>Here <code>Xnew</code> will have the same form as the <code>X</code> passed to <code>fit</code>. </p><p>Note that while <code>Xnew</code> generally consists of multiple observations (e.g., has multiple rows in the case of a table) it is assumed, in view of the i.i.d assumption recalled above, that calling <code>predict(..., Xnew)</code> is equivalent to broadcasting some method <code>predict_one(..., x)</code> over the individual observations <code>x</code> in <code>Xnew</code> (a method implementing the probablility distribution <code>p(X |y)</code> above).</p><h4 id="Prediction-types-for-deterministic-responses."><a class="docs-heading-anchor" href="#Prediction-types-for-deterministic-responses.">Prediction types for deterministic responses.</a><a id="Prediction-types-for-deterministic-responses.-1"></a><a class="docs-heading-anchor-permalink" href="#Prediction-types-for-deterministic-responses." title="Permalink"></a></h4><p>In the case of <code>Deterministic</code> models, <code>yhat</code> should have the same scitype as the <code>y</code> passed to <code>fit</code> (see above). If <code>y</code> is a <code>CategoricalVector</code> (classification) then elements of the predition <code>yhat</code> <strong>must have a pool == to the pool of the target <code>y</code> presented in training</strong>, even if not all levels appear in the training data or prediction itself.</p><p>Unfortunately, code not written with the preservation of categorical levels in mind poses special problems. To help with this, MLJModelInterface provides some utilities: <a href="#MLJModelInterface.int"><code>MLJModelInterface.int</code></a> (for converting a <code>CategoricalValue</code> into an integer, the ordering of these integers being consistent with that of the pool) and <code>MLJModelInterface.decoder</code> (for constructing a callable object that decodes the integers back into <code>CategoricalValue</code> objects). Refer to <a href="#Convenience-methods">Convenience methods</a> below for important details.</p><p>Note that a decoder created during <code>fit</code> may need to be bundled with <code>fitresult</code> to make it available to <code>predict</code> during re-encoding. So, for example, if the core algorithm being wrapped by <code>fit</code> expects a nominal target <code>yint</code> of type <code>Vector{&lt;:Integer}</code> then a <code>fit</code> method may look something like this:</p><pre><code class="language-julia">function MMI.fit(model::SomeSupervisedModel, verbosity, X, y)
    yint = MMI.int(y)
    a_target_element = y[1]                # a CategoricalValue/String
    decode = MMI.decoder(a_target_element) # can be called on integers

    core_fitresult = SomePackage.fit(X, yint, verbosity=verbosity)

    fitresult = (decode, core_fitresult)
    cache = nothing
    report = nothing
    return fitresult, cache, report
end</code></pre><p>while a corresponding deterministic <code>predict</code> operation might look like this:</p><pre><code class="language-julia">function MMI.predict(model::SomeSupervisedModel, fitresult, Xnew)
    decode, core_fitresult = fitresult
    yhat = SomePackage.predict(core_fitresult, Xnew)
    return decode.(yhat)
end</code></pre><p>For a concrete example, refer to the <a href="https://github.com/JuliaAI/MLJModels.jl/blob/master/src/ScikitLearn.jl">code</a> for <code>SVMClassifier</code>.</p><p>Of course, if you are coding a learning algorithm from scratch, rather than wrapping an existing one, these extra measures may be unnecessary.</p><h4 id="Prediction-types-for-probabilistic-responses"><a class="docs-heading-anchor" href="#Prediction-types-for-probabilistic-responses">Prediction types for probabilistic responses</a><a id="Prediction-types-for-probabilistic-responses-1"></a><a class="docs-heading-anchor-permalink" href="#Prediction-types-for-probabilistic-responses" title="Permalink"></a></h4><p>In the case of <code>Probabilistic</code> models with univariate targets, <code>yhat</code> must be an <code>AbstractVector</code> or table whose elements are distributions. In the common case of a vector (single target), this means one distribution per row of <code>Xnew</code>.</p><p>A <em>distribution</em> is some object that, at the least, implements <code>Base.rng</code> (i.e., is something that can be sampled).  Currently, all performance measures (metrics) defined in MLJBase.jl additionally assume that a distribution is either:</p><ul><li>An instance of some subtype of <code>Distributions.Distribution</code>, an abstract type defined in the <a href="https://juliastats.org/Distributions.jl/stable/"><code>Distributions.jl</code></a> package; or</li><li>An instance of <code>CategoricalDistributions.UnivariateFinite</code>, from the <a href="https://github.com/JuliaAI/CategoricalDistributions.jl">CategoricalDistributions.jl</a> package, <em>which should be used for all probabilistic classifiers</em>, i.e., for predictors whose target has scientific type <code>&lt;:AbstractVector{&lt;:Finite}</code>.</li></ul><p>All such distributions implement the probability mass or density function <code>Distributions.pdf</code>. If your model&#39;s predictions cannot be predict objects of this form, then you will need to implement appropriate performance measures to buy into MLJ&#39;s performance evaluation apparatus.</p><p>An implementation can avoid CategoricalDistributions.jl as a dependency by using the &quot;dummy&quot; constructor <code>MLJModelInterface.UnivariateFinite</code>, which is bound to the true one when MLJBase.jl is loaded.</p><p>For efficiency, one should not construct <code>UnivariateFinite</code> instances one at a time. Rather, once a probability vector, matrix, or dictionary is known, construct an instance of <code>UnivariateFiniteVector &lt;: AbstractArray{&lt;:UnivariateFinite},1}</code> to return. Both <code>UnivariateFinite</code> and <code>UnivariateFiniteVector</code> objects are constructed using the single <code>UnivariateFinite</code> function.</p><p>For example, suppose the target <code>y</code> arrives as a subsample of some <code>ybig</code> and is missing some classes:</p><pre><code class="language-julia">ybig = categorical([:a, :b, :a, :a, :b, :a, :rare, :a, :b])
y = ybig[1:6]</code></pre><p>Your fit method has bundled the first element of <code>y</code> with the <code>fitresult</code> to make it available to <code>predict</code> for purposes of tracking the complete pool of classes. Let&#39;s call this <code>an_element = y[1]</code>. Then, supposing the corresponding probabilities of the observed classes <code>[:a, :b]</code> are in an <code>n x 2</code> matrix <code>probs</code> (where <code>n</code> the number of rows of <code>Xnew</code>) then you return</p><pre><code class="language-julia">yhat = MLJModelInterface.UnivariateFinite([:a, :b], probs, pool=an_element)</code></pre><p>This object automatically assigns zero-probability to the unseen class <code>:rare</code> (i.e., <code>pdf.(yhat, :rare)</code> works and returns a zero vector). If you would like to assign <code>:rare</code> non-zero probabilities, simply add it to the first vector (the <em>support</em>) and supply a larger <code>probs</code> matrix.</p><p>In a binary classification problem it suffices to specify a single vector of probabilities, provided you specify <code>augment=true</code>, as in the following example, <em>and note carefully that these probablities are associated with the</em> <strong>last</strong> <em>(second) class you specify in the constructor:</em></p><pre><code class="language-julia">y = categorical([:TRUE, :FALSE, :FALSE, :TRUE, :TRUE])
an_element = y[1]
probs = rand(10)
yhat = MLJModelInterface.UnivariateFinite([:FALSE, :TRUE], probs, augment=true, pool=an_element)</code></pre><p>The constructor has a lot of options, including passing a dictionary instead of vectors. See <code>CategoricalDistributions.UnivariateFinite</code>](@ref) for details.</p><p>See <a href="https://github.com/JuliaAI/MLJModels.jl/blob/master/src/GLM.jl">LinearBinaryClassifier</a> for an example of a Probabilistic classifier implementation.</p><p><em>Important note on binary classifiers.</em> There is no &quot;Binary&quot; scitype distinct from <code>Multiclass{2}</code> or <code>OrderedFactor{2}</code>; <code>Binary</code> is just an alias for <code>Union{Multiclass{2},OrderedFactor{2}}</code>. The <code>target_scitype</code> of a binary classifier will generally be <code>AbstractVector{&lt;:Binary}</code> and according to the <em>mlj</em> scitype convention, elements of <code>y</code> have type <code>CategoricalValue</code>, and <em>not</em> <code>Bool</code>. See <a href="https://github.com/JuliaAI/MLJModels.jl/blob/master/src/GLM.jl">BinaryClassifier</a> for an example.</p><h3 id="The-predict_joint-method"><a class="docs-heading-anchor" href="#The-predict_joint-method">The predict_joint method</a><a id="The-predict_joint-method-1"></a><a class="docs-heading-anchor-permalink" href="#The-predict_joint-method" title="Permalink"></a></h3><div class="admonition is-warning"><header class="admonition-header">Experimental</header><div class="admonition-body"><p>The following API is experimental. It is subject to breaking changes during minor or major releases without warning.</p></div></div><pre><code class="language-julia">MMI.predict_joint(model::SomeSupervisedModel, fitresult, Xnew) -&gt; yhat</code></pre><p>Any <code>Probabilistic</code> model type <code>SomeModel</code>may optionally implement a <code>predict_joint</code> method, which has the same signature as <code>predict</code>, but whose predictions are a single distribution (rather than a vector of per-observation distributions). </p><p>Specifically, the output <code>yhat</code> of <code>predict_joint</code> should be an instance of <code>Distributions.Sampleable{&lt;:Multivariate,V}</code>, where <code>scitype(V) = target_scitype(SomeModel)</code> and samples have length <code>n</code>, where <code>n</code> is the number of observations in <code>Xnew</code>.</p><p>If a new model type subtypes <code>JointProbablistic &lt;: Probabilistic</code> then implementation of <code>predict_joint</code> is compulsory.</p><h3 id="Trait-declarations"><a class="docs-heading-anchor" href="#Trait-declarations">Trait declarations</a><a id="Trait-declarations-1"></a><a class="docs-heading-anchor-permalink" href="#Trait-declarations" title="Permalink"></a></h3><p>Two trait functions allow the implementer to restrict the types of data <code>X</code>, <code>y</code> and <code>Xnew</code> discussed above. The MLJ task interface uses these traits for data type checks but also for model search. If they are omitted (and your model is registered) then a general user may attempt to use your model with inappropriately typed data.</p><p>The trait functions <code>input_scitype</code> and <code>target_scitype</code> take scientific data types as values. We assume here familiarity with <a href="https://github.com/JuliaAI/ScientificTypes.jl">ScientificTypes.jl</a> (see <a href="../">Getting Started</a> for the basics).</p><p>For example, to ensure that the <code>X</code> presented to the <code>DecisionTreeClassifier</code> <code>fit</code> method is a table whose columns all have <code>Continuous</code> element type (and hence <code>AbstractFloat</code> machine type), one declares</p><pre><code class="language-julia">MMI.input_scitype(::Type{&lt;:DecisionTreeClassifier}) = MMI.Table(MMI.Continuous)</code></pre><p>or, equivalently,</p><pre><code class="language-julia">MMI.input_scitype(::Type{&lt;:DecisionTreeClassifier}) = Table(Continuous)</code></pre><p>If, instead, columns were allowed to have either: (i) a mixture of <code>Continuous</code> and <code>Missing</code> values, or (ii) <code>Count</code> (i.e., integer) values, then the declaration would be</p><pre><code class="language-julia">MMI.input_scitype(::Type{&lt;:DecisionTreeClassifier}) = Table(Union{Continuous,Missing},Count)</code></pre><p>Similarly, to ensure the target is an AbstractVector whose elements have <code>Finite</code> scitype (and hence <code>CategoricalValue</code> machine type) we declare</p><pre><code class="language-julia">MMI.target_scitype(::Type{&lt;:DecisionTreeClassifier}) = AbstractVector{&lt;:Finite}</code></pre><h4 id="Multivariate-targets"><a class="docs-heading-anchor" href="#Multivariate-targets">Multivariate targets</a><a id="Multivariate-targets-1"></a><a class="docs-heading-anchor-permalink" href="#Multivariate-targets" title="Permalink"></a></h4><p>The above remarks continue to hold unchanged for the case multivariate targets.  For example, if we declare</p><pre><code class="language-julia">target_scitype(SomeSupervisedModel) = Table(Continuous)</code></pre><p>then this constrains the target to be any table whose columns have <code>Continous</code> element scitype (i.e., <code>AbstractFloat</code>), while</p><pre><code class="language-julia">target_scitype(SomeSupervisedModel) = Table(Continuous, Finite{2})</code></pre><p>restricts to tables with continuous or binary (ordered or unordered) columns.</p><p>For predicting variable length sequences of, say, binary values (<code>CategoricalValue</code>s) with some common size-two pool) we declare</p><pre><code class="language-julia">target_scitype(SomeSupervisedModel) = AbstractVector{&lt;:NTuple{&lt;:Finite{2}}}</code></pre><p>The trait functions controlling the form of data are summarized as follows:</p><table><tr><th style="text-align: right">method</th><th style="text-align: right">return type</th><th style="text-align: right">declarable return values</th><th style="text-align: right">fallback value</th></tr><tr><td style="text-align: right"><code>input_scitype</code></td><td style="text-align: right"><code>Type</code></td><td style="text-align: right">some scientfic type</td><td style="text-align: right"><code>Unknown</code></td></tr><tr><td style="text-align: right"><code>target_scitype</code></td><td style="text-align: right"><code>Type</code></td><td style="text-align: right">some scientific type</td><td style="text-align: right"><code>Unknown</code></td></tr></table><p>Additional trait functions tell MLJ&#39;s <code>@load</code> macro how to find your model if it is registered, and provide other self-explanatory metadata about the model:</p><table><tr><th style="text-align: right">method</th><th style="text-align: right">return type</th><th style="text-align: right">declarable return values</th><th style="text-align: right">fallback value</th></tr><tr><td style="text-align: right"><code>load_path</code></td><td style="text-align: right"><code>String</code></td><td style="text-align: right">unrestricted</td><td style="text-align: right">&quot;unknown&quot;</td></tr><tr><td style="text-align: right"><code>package_name</code></td><td style="text-align: right"><code>String</code></td><td style="text-align: right">unrestricted</td><td style="text-align: right">&quot;unknown&quot;</td></tr><tr><td style="text-align: right"><code>package_uuid</code></td><td style="text-align: right"><code>String</code></td><td style="text-align: right">unrestricted</td><td style="text-align: right">&quot;unknown&quot;</td></tr><tr><td style="text-align: right"><code>package_url</code></td><td style="text-align: right"><code>String</code></td><td style="text-align: right">unrestricted</td><td style="text-align: right">&quot;unknown&quot;</td></tr><tr><td style="text-align: right"><code>package_license</code></td><td style="text-align: right"><code>String</code></td><td style="text-align: right">unrestricted</td><td style="text-align: right">&quot;unknown&quot;</td></tr><tr><td style="text-align: right"><code>is_pure_julia</code></td><td style="text-align: right"><code>Bool</code></td><td style="text-align: right"><code>true</code> or <code>false</code></td><td style="text-align: right"><code>false</code></td></tr><tr><td style="text-align: right"><code>supports_weights</code></td><td style="text-align: right"><code>Bool</code></td><td style="text-align: right"><code>true</code> or <code>false</code></td><td style="text-align: right"><code>false</code></td></tr></table><p>Here is the complete list of trait function declarations for <code>DecisionTreeClassifier</code>, whose core algorithms are provided by DecisionTree.jl, but whose interface actually lives at <a href="https://github.com/JuliaAI/MLJDecisionTreeInterface.jl">MLJDecisionTreeInterface.jl</a>.</p><pre><code class="language-julia">MMI.input_scitype(::Type{&lt;:DecisionTreeClassifier}) = MMI.Table(MMI.Continuous)
MMI.target_scitype(::Type{&lt;:DecisionTreeClassifier}) = AbstractVector{&lt;:MMI.Finite}
MMI.load_path(::Type{&lt;:DecisionTreeClassifier}) = &quot;MLJDecisionTreeInterface.DecisionTreeClassifier&quot;
MMI.package_name(::Type{&lt;:DecisionTreeClassifier}) = &quot;DecisionTree&quot;
MMI.package_uuid(::Type{&lt;:DecisionTreeClassifier}) = &quot;7806a523-6efd-50cb-b5f6-3fa6f1930dbb&quot;
MMI.package_url(::Type{&lt;:DecisionTreeClassifier}) = &quot;https://github.com/bensadeghi/DecisionTree.jl&quot;
MMI.is_pure_julia(::Type{&lt;:DecisionTreeClassifier}) = true</code></pre><p>Alternatively these traits can also be declared using <code>MMI.metadata_pkg</code> and <code>MMI.metadata_model</code> helper functions as:</p><pre><code class="language-julia">MMI.metadata_pkg(DecisionTreeClassifier,name=&quot;DecisionTree&quot;,
                     packge_uuid=&quot;7806a523-6efd-50cb-b5f6-3fa6f1930dbb&quot;,
                     package_url=&quot;https://github.com/bensadeghi/DecisionTree.jl&quot;,
                     is_pure_julia=true)

MMI.metadata_model(DecisionTreeClassifier,
                        input_scitype=MMI.Table(MMI.Continuous),
                        target_scitype=AbstractVector{&lt;:MMI.Finite},
                        load_path=&quot;MLJDecisionTreeInterface.DecisionTreeClassifier&quot;)</code></pre><p><em>Important.</em> Do not omit the <code>load_path</code> specification. If unsure what it should be, post an issue at <a href="https://github.com/alan-turing-institute/MLJ.jl/issues">MLJ</a>.</p><article class="docstring"><header><a class="docstring-binding" id="MLJModelInterface.metadata_pkg" href="#MLJModelInterface.metadata_pkg"><code>MLJModelInterface.metadata_pkg</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">metadata_pkg(T; args...)</code></pre><p>Helper function to write the metadata for a package providing model <code>T</code>. Use it with broadcasting to define the metadata of the package providing a series of models.</p><p><strong>Keywords</strong></p><ul><li><code>package_name=&quot;unknown&quot;</code>   : package name</li><li><code>package_uuid=&quot;unknown&quot;</code>   : package uuid</li><li><code>package_url=&quot;unknown&quot;</code>    : package url</li><li><code>is_pure_julia=missing</code>    : whether the package is pure julia</li><li><code>package_license=&quot;unknown&quot;</code>: package license</li><li><code>is_wrapper=false</code> : whether the package is a wrapper</li></ul><p><strong>Example</strong></p><pre><code class="language-julia">metadata_pkg.((KNNRegressor, KNNClassifier),
    package_name=&quot;NearestNeighbors&quot;,
    package_uuid=&quot;b8a86587-4115-5ab1-83bc-aa920d37bbce&quot;,
    package_url=&quot;https://github.com/KristofferC/NearestNeighbors.jl&quot;,
    is_pure_julia=true,
    package_license=&quot;MIT&quot;,
    is_wrapper=false)</code></pre></div></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJModelInterface.metadata_model" href="#MLJModelInterface.metadata_model"><code>MLJModelInterface.metadata_model</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">metadata_model(`T`; args...)</code></pre><p>Helper function to write the metadata for a model <code>T</code>.</p><p><strong>Keywords</strong></p><ul><li><code>input_scitype=Unknown</code> : allowed scientific type of the input data</li><li><code>target_scitype=Unknown</code>: allowed sc. type of the target (supervised)</li><li><code>output_scitype=Unknown</code>: allowed sc. type of the transformed data (unsupervised)</li><li><code>supports_weights=false</code> : whether the model supports sample weights</li><li><code>docstring=&quot;&quot;</code> : short description of the model</li><li><code>load_path=&quot;&quot;</code> : where the model is (usually <code>PackageName.ModelName</code>)</li></ul><p><strong>Example</strong></p><pre><code class="language-julia">metadata_model(KNNRegressor,
    input_scitype=MLJModelInterface.Table(MLJModelInterface.Continuous),
    target_scitype=AbstractVector{MLJModelInterface.Continuous},
    supports_weights=true,
    docstring=&quot;K-Nearest Neighbors classifier: ...&quot;,
    load_path=&quot;NearestNeighbors.KNNRegressor&quot;)</code></pre></div></section></article><h3 id="Iterative-models-and-the-update!-method"><a class="docs-heading-anchor" href="#Iterative-models-and-the-update!-method">Iterative models and the update! method</a><a id="Iterative-models-and-the-update!-method-1"></a><a class="docs-heading-anchor-permalink" href="#Iterative-models-and-the-update!-method" title="Permalink"></a></h3><p>An <code>update</code> method may be optionally overloaded to enable a call by MLJ to retrain a model (on the same training data) to avoid repeating computations unnecessarily.</p><pre><code class="language-julia">MMI.update(model::SomeSupervisedModel, verbosity, old_fitresult, old_cache, X, y) -&gt; fit
result, cache, report
MMI.update(model::SomeSupervisedModel, verbosity, old_fitresult, old_cache, X, y, w=nothing) -&gt; fit
result, cache, report</code></pre><p>Here the second variation applies if <code>SomeSupervisedModel</code> supports sample weights.</p><p>If an MLJ <code>Machine</code> is being <code>fit!</code> and it is not the first time, then <code>update</code> is called instead of <code>fit</code>, unless the machine <code>fit!</code> has been called with a new <code>rows</code> keyword argument. However, <code>MLJModelInterface</code> defines a fallback for <code>update</code> which just calls <code>fit</code>. For context, see <a href="../internals/">MLJ Internals</a>.</p><p>Learning networks wrapped as models constitute one use-case (see <a href="../">Composing Models</a>): one would like each component model to be retrained only when hyperparameter changes &quot;upstream&quot; make this necessary. In this case MLJ provides a fallback (specifically, the fallback is for any subtype of <code>SupervisedNetwork = Union{DeterministicNetwork,ProbabilisticNetwork}</code>). A second more generally relevant use-case is iterative models, where calls to increase the number of iterations only restarts the iterative procedure if other hyperparameters have also changed. (A useful method for inspecting model changes in such cases is <code>MLJModelInterface.is_same_except</code>. ) For an example, see <a href="https://github.com/JuliaAI/MLJEnsembles.jl">MLJEnsembles.jl</a>.</p><p>A third use-case is to avoid repeating time-consuming preprocessing of <code>X</code> and <code>y</code> required by some models.</p><p>In the event that the argument <code>fitresult</code> (returned by a preceding call to <code>fit</code>) is not sufficient for performing an update, the author can arrange for <code>fit</code> to output in its <code>cache</code> return value any additional information required (for example, pre-processed versions of <code>X</code> and <code>y</code>), as this is also passed as an argument to the <code>update</code> method.</p><h3 id="Implementing-a-data-front-end"><a class="docs-heading-anchor" href="#Implementing-a-data-front-end">Implementing a data front-end</a><a id="Implementing-a-data-front-end-1"></a><a class="docs-heading-anchor-permalink" href="#Implementing-a-data-front-end" title="Permalink"></a></h3><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>It is suggested that packages implementing MLJ&#39;s model API, that later implement a data front-end, should tag their changes in a breaking release. (The changes will not break use of models for the ordinary MLJ user, who interacts with models exlusively through the machine interface. However, it will break usage for some external packages that have chosen to depend directly on the model API.)</p></div></div><pre><code class="language-julia">MLJModelInterface.reformat(model, args...) -&gt; data
MLJModelInterface.selectrows(::Model, I, data...) -&gt; sampled_data</code></pre><p>Models optionally overload <code>reformat</code> to define transformations of user-supplied data into some model-specific representation (e.g., from a table to a matrix). Computational overheads associated with multiple <code>fit!</code>/<code>predict</code>/<code>transform</code> calls (on MLJ machines) are then avoided, when memory resources allow. The fallback returns <code>args</code> (no transformation). </p><p>The <code>selectrows(::Model, I, data...)</code> method is overloaded to specify how the model-specific data is to be subsampled, for some observation indices <code>I</code> (a colon, <code>:</code>, or instance of <code>AbstractVector{&lt;:Integer}</code>). In this way, implementing a data front-end also allow more efficient resampling of data (in user calls to <code>evaluate!</code>).</p><p>After detailing formal requirments for implementing a data front-end, we give a <a href="#Sample-implementation">Sample implementation</a>. A simple implementation <a href="https://github.com/Evovest/EvoTrees.jl/blob/94b58faf3042009bd609c9a5155a2e95486c2f0e/src/MLJ.jl#L23">implementation</a> also appears in the EvoTrees.jl package.</p><p>Here &quot;user-supplied data&quot; is what the MLJ user supplies when constructing a machine, as in <code>machine(models, args...)</code>, which coincides with the arguments expected by <code>fit(model, verbosity, args...)</code> when <code>reformat</code> is not overloaded.</p><p>Implementing a <code>reformat</code> data front-end is permitted for any <code>Model</code> subtype, except for subtypes of <code>Static</code>. Here is a complete list of responsibilities for such an implementation, for some <code>model::SomeModelType</code> (a sample implementation follows after):</p><ul><li><p>A <code>reformat(model::SomeModelType, args...) -&gt; data</code> method must be implemented for each form of <code>args...</code> appearing in a valid machine construction <code>machine(model, args...)</code> (there will be one for each possible signature of <code>fit(::SomeModelType, ...)</code>).</p></li><li><p>Additionally, if not included above, there must be a single argument form of reformat, <code>reformat(model::SommeModelType, arg) -&gt; (data,)</code>, serving as a data front-end for operations like <code>predict</code>. It must always hold that <code>reformat(model, args...)[1] = reformat(model, args[1])</code>.</p></li></ul><p><em>Important.</em> <code>reformat(model::SomeModelType, args...)</code> must always   return a tuple of the same length as <code>args</code>, even if this is one.</p><ul><li><p><code>fit(model::SomeModelType, verbosity, data...)</code> should be implemented as if <code>data</code> is the output of <code>reformat(model, args...)</code>, where <code>args</code> is the data an MLJ user has bound to <code>model</code> in some machine. The same applies to any overloading of <code>update</code>.</p></li><li><p>Each implemented operation, such as <code>predict</code> and <code>transform</code> - but excluding <code>inverse_transform</code> - must be defined as if its data arguments are <code>reformat</code>ed versions of user-supplied data. For example, in the supervised case, <code>data_new</code> in <code>predict(model::SomeModelType, fitresult, data_new)</code> is <code>reformat(model, Xnew)</code>, where <code>Xnew</code> is the data provided by the MLJ user in a call <code>predict(mach, Xnew)</code> (<code>mach.model == model</code>).</p></li><li><p>To specify how the model-specific representation of data is to be resampled, implement <code>selectrows(model::SomeModelType, I, data...) -&gt; resampled_data</code> for each overloading of <code>reformat(model::SomeModel, args...) -&gt; data</code> above. Here <code>I</code> is an arbitrary abstract integer vector or <code>:</code> (type <code>Colon</code>).</p></li></ul><p><em>Important.</em> <code>selectrows(model::SomeModelType, I, args...)</code> must always return a tuple of the same length as <code>args</code>, even if this is one.</p><p>The fallback for <code>selectrows</code> is described at <a href="#MLJModelInterface.selectrows"><code>selectrows</code></a>.</p><h4 id="Sample-implementation"><a class="docs-heading-anchor" href="#Sample-implementation">Sample implementation</a><a id="Sample-implementation-1"></a><a class="docs-heading-anchor-permalink" href="#Sample-implementation" title="Permalink"></a></h4><p>Suppose a supervised model type <code>SomeSupervised</code> supports sample weights, leading to two different <code>fit</code> signatures, and that it has a single operation <code>predict</code>:</p><pre><code class="language-none">fit(model::SomeSupervised, verbosity, X, y)
fit(model::SomeSupervised, verbosity, X, y, w)

predict(model::SomeSupervised, fitresult, Xnew)</code></pre><p>Without a data front-end implemented, suppose <code>X</code> is expected to be a table and <code>y</code> a vector, but suppose the core algorithm always converts <code>X</code> to a matrix with features as rows (features corresponding to columns in the table).  Then a new data-front end might look like this:</p><pre><code class="language-none">constant MMI = MLJModelInterface

# for fit:
MMI.reformat(::SomeSupervised, X, y) = (MMI.matrix(X, transpose=true), y)
MMI.reformat(::SomeSupervised, X, y, w) = (MMI.matrix(X, transpose=true), y, w)
MMI.selectrows(::SomeSupervised, I, Xmatrix, y) =
    (view(Xmatrix, :, I), view(y, I))
MMI.selectrows(::SomeSupervised, I, Xmatrix, y, w) =
    (view(Xmatrix, :, I), view(y, I), view(w, I))

# for predict:
MMI.reformat(::SomeSupervised, X) = (MMI.matrix(X, transpose=true),)
MMI.selectrows(::SomeSupervised, I, Xmatrix) = view(Xmatrix, I)</code></pre><p>With these additions, <code>fit</code> and <code>predict</code> are refactored, so that <code>X</code> and <code>Xnew</code> represent matrices with features as rows.</p><h3 id="Supervised-models-with-a-transform-method"><a class="docs-heading-anchor" href="#Supervised-models-with-a-transform-method">Supervised models with a <code>transform</code> method</a><a id="Supervised-models-with-a-transform-method-1"></a><a class="docs-heading-anchor-permalink" href="#Supervised-models-with-a-transform-method" title="Permalink"></a></h3><p>A supervised model may optionally implement a <code>transform</code> method, whose signature is the same as <code>predict</code>. In that case the implementation should define a value for the <code>output_scitype</code> trait. A declaration</p><pre><code class="language-julia">output_scitype(::Type{&lt;:SomeSupervisedModel}) = T</code></pre><p>is an assurance that <code>scitype(transform(model, fitresult, Xnew)) &lt;: T</code> always holds, for any <code>model</code> of type <code>SomeSupervisedModel</code>.</p><p>A use-case for a <code>transform</code> method for a supervised model is a neural network that learns <em>feature embeddings</em> for categorical input features as part of overall training. Such a model becomes a transformer that other supervised models can use to transform the categorical features (instead of applying the higher-dimensional one-hot encoding representations).</p><h2 id="Models-that-learn-a-probability-distribution"><a class="docs-heading-anchor" href="#Models-that-learn-a-probability-distribution">Models that learn a probability distribution</a><a id="Models-that-learn-a-probability-distribution-1"></a><a class="docs-heading-anchor-permalink" href="#Models-that-learn-a-probability-distribution" title="Permalink"></a></h2><div class="admonition is-warning"><header class="admonition-header">Experimental</header><div class="admonition-body"><p>The following API is experimental. It is subject to breaking changes during minor or major releases without warning. Models implementing this interface will not work with MLJBase versions earlier than 0.17.5.</p></div></div><p>Models that fit a probability distribution to some <code>data</code> should be regarded as <code>Probablisitic &lt;: Supervised</code> models with target <code>y = data</code> and <code>X = nothing</code>. </p><p>The <code>predict</code> method should return a single distribution. </p><p>A working implementation of a model that fits a <code>UnivariateFinite</code> distribution to some categorical data using <a href="https://en.wikipedia.org/wiki/Additive_smoothing">Laplace smoothing</a> controlled by a hyper-parameter <code>alpha</code> is given <a href="https://github.com/JuliaAI/MLJBase.jl/blob/d377bee1198ec179a4ade191c11fef583854af4a/test/interface/model_api.jl#L36">here</a>.</p><h3 id="Serialization"><a class="docs-heading-anchor" href="#Serialization">Serialization</a><a id="Serialization-1"></a><a class="docs-heading-anchor-permalink" href="#Serialization" title="Permalink"></a></h3><div class="admonition is-warning"><header class="admonition-header">Experimental</header><div class="admonition-body"><p>The following API is experimental. It is subject to breaking changes during minor or major releases without warning.</p></div></div><p>The MLJ user can serialize and deserialize a <em>machine</em>, which means serializing/deserializing:</p><ul><li>the associated <code>Model</code> object (storing hyperparameters)</li><li>the <code>fitresult</code> (learned parameters)</li><li>the <code>report</code> generating during training</li></ul><p>These are bundled into a single file or <code>IO</code> stream specified by the user using the package <code>JLSO</code>. There are two scenarios in which a new MLJ model API implementation will want to overload two additional methods <code>save</code> and <code>restore</code> to support serialization:</p><ol><li>The algorithm-providing package already has it&#39;s own serialization format for learned parameters and/or hyper-parameters, which users may want to access. In that case <em>the implementation overloads</em> <code>save</code>.</li></ol><ol><li>The <code>fitresult</code> is not a sufficiently persistent object; for example, it is a pointer passed from wrapped C code. In that case <em>the implementation overloads</em> <code>save</code> <em>and</em> <code>restore</code>.</li></ol><p>In case 2, 1 presumably applies also, for otherwise MLJ serialization is probably not going to be possible without changes to the algorithm-providing package. An example is given below.</p><p>Note that in case 1, MLJ will continue to create it&#39;s own self-contained serialization of the machine. Below <code>filename</code> refers to the corresponding serialization file name, as specified by the user, but with any final extension (e.g., &quot;.jlso&quot;, &quot;.gz&quot;) removed. If the user has alternatively specified an <code>IO</code> object for serialization, then <code>filename</code> is a randomly generated numeric string.</p><h4 id="The-save-method"><a class="docs-heading-anchor" href="#The-save-method">The save method</a><a id="The-save-method-1"></a><a class="docs-heading-anchor-permalink" href="#The-save-method" title="Permalink"></a></h4><pre><code class="language-julia">MMI.save(filename, model::SomeModel, fitresult; kwargs...) -&gt; serializable_fitresult</code></pre><p>Implement this method to serialize using a format specific to models of type <code>SomeModel</code>. The <code>fitresult</code> is the first return value of <code>MMI.fit</code> for such model types; <code>kwargs</code> is a list of keyword arguments specified by the user and understood to relate to a some model-specific serialization (cannot be <code>format=...</code> or <code>compression=...</code>). The value of <code>serializable_fitresult</code> should be a persistent representation of <code>fitresult</code>, from which a correct and valid <code>fitresult</code> can be reconstructed using <code>restore</code> (see below). </p><p>The fallback of <code>save</code> performs no action and returns <code>fitresult</code>.</p><h4 id="The-restore-method"><a class="docs-heading-anchor" href="#The-restore-method">The restore method</a><a id="The-restore-method-1"></a><a class="docs-heading-anchor-permalink" href="#The-restore-method" title="Permalink"></a></h4><pre><code class="language-julia">MMI.restore(filename, model::SomeModel, serializable_fitresult) -&gt; fitresult</code></pre><p>Implement this method to reconstruct a <code>fitresult</code> (as returned by <code>MMI.fit</code>) from a persistent representation constructed using <code>MMI.save</code> as described above. </p><p>The fallback of <code>restore</code> returns <code>serializable_fitresult</code>.</p><h4 id="Example"><a class="docs-heading-anchor" href="#Example">Example</a><a id="Example-1"></a><a class="docs-heading-anchor-permalink" href="#Example" title="Permalink"></a></h4><p>Below is an example drawn from MLJ&#39;s XGBoost wrapper. In this example the <code>fitresult</code> returned by <code>MMI.fit</code> is a tuple <code>(booster, a_target_element)</code> where <code>booster</code> is the <code>XGBoost.jl</code> object storing the learned parameters (essentially a pointer to some object created by C code) and <code>a_target_element</code> is an ordinary <code>CategoricalValue</code> used to track the target classes (a persistent object, requiring no special treatment).</p><pre><code class="language-julia">function MLJModelInterface.save(filename,
                                ::XGBoostClassifier,
                                fitresult;
                                kwargs...)
    booster, a_target_element = fitresult

    xgb_filename = string(filename, &quot;.xgboost.model&quot;)
    XGBoost.save(booster, xgb_filename)
    persistent_booster = read(xgb_filename)
    @info &quot;Additional XGBoost serialization file \&quot;$xgb_filename\&quot; generated. &quot;
    return (persistent_booster, a_target_element)
end

function MLJModelInterface.restore(filename,
                                   ::XGBoostClassifier,
                                   serializable_fitresult)
    persistent_booster, a_target_element = serializable_fitresult

    xgb_filename = string(filename, &quot;.tmp&quot;)
    open(xgb_filename, &quot;w&quot;) do file
        write(file, persistent_booster)
    end
    booster = XGBoost.Booster(model_file=xgb_filename)
    rm(xgb_filename)
    fitresult = (booster, a_target_element)
    return fitresult
end</code></pre><h2 id="Unsupervised-models"><a class="docs-heading-anchor" href="#Unsupervised-models">Unsupervised models</a><a id="Unsupervised-models-1"></a><a class="docs-heading-anchor-permalink" href="#Unsupervised-models" title="Permalink"></a></h2><p>Unsupervised models implement the MLJ model interface in a very similar fashion. The main differences are:</p><ul><li><p>The <code>fit</code> method has only one training argument <code>X</code>, as in <code>MLJModelInterface.fit(model, verbosity, X)</code>. However, it has the same return value <code>(fitresult, cache, report)</code>. An <code>update</code> method (e.g., for iterative models) can be optionally implemented in the same way.</p></li><li><p>A <code>transform</code> method is compulsory and has the same signature as <code>predict</code>, as in <code>MLJModelInterface.transform(model, fitresult, Xnew)</code>.</p></li><li><p>Instead of defining the <code>target_scitype</code> trait, one declares an <code>output_scitype</code> trait (see above for the meaning).</p></li><li><p>An <code>inverse_transform</code> can be optionally implemented. The signature is the same as <code>transform</code>, as in <code>MLJModelInterface.inverse_transform(model, fitresult, Xout)</code>, which:</p><ul><li><p>must make sense for any <code>Xout</code> for which <code>scitype(Xout) &lt;: output_scitype(SomeSupervisedModel)</code> (see below); and</p></li><li><p>must return an object <code>Xin</code> satisfying <code>scitype(Xin) &lt;: input_scitype(SomeSupervisedModel)</code>.</p></li></ul></li><li><p>A <code>predict</code> method may be optionally implemented, and has the same signature as for supervised models, as in <code>MLJModelInterface.predict(model, fitresult, Xnew)</code>. A use-case is clustering algorithms that <code>predict</code> labels and <code>transform</code> new input features into a space of lower-dimension. See <a href="../transformers/#Transformers-that-also-predict">Transformers that also predict</a> for an example.</p></li></ul><h2 id="Convenience-methods"><a class="docs-heading-anchor" href="#Convenience-methods">Convenience methods</a><a id="Convenience-methods-1"></a><a class="docs-heading-anchor-permalink" href="#Convenience-methods" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="MLJModelInterface.table" href="#MLJModelInterface.table"><code>MLJModelInterface.table</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">table(columntable; prototype=nothing)</code></pre><p>Convert a named tuple of vectors or tuples <code>columntable</code>, into a table of the &quot;preferred sink type&quot; of <code>prototype</code>. This is often the type of <code>prototype</code> itself, when <code>prototype</code> is a sink; see the Tables.jl documentation. If <code>prototype</code> is not specified, then a named tuple of vectors is returned.</p><pre><code class="language-none">table(A::AbstractMatrix; names=nothing, prototype=nothing)</code></pre><p>Wrap an abstract matrix <code>A</code> as a Tables.jl compatible table with the specified column <code>names</code> (a tuple of symbols). If <code>names</code> are not specified, <code>names=(:x1, :x2, ..., :xn)</code> is used, where <code>n=size(A, 2)</code>.</p><p>If a <code>prototype</code> is specified, then the matrix is materialized as a table of the preferred sink type of <code>prototype</code>, rather than wrapped. Note that if <code>prototype</code> is <em>not</em> specified, then <code>matrix(table(A))</code> is essentially a no-op.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJModelInterface.matrix" href="#MLJModelInterface.matrix"><code>MLJModelInterface.matrix</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">matrix(X; transpose=false)</code></pre><p>If <code>X isa AbstractMatrix</code>, return <code>X</code> or <code>permutedims(X)</code> if <code>transpose=true</code>. Otherwise if <code>X</code> is a Tables.jl compatible table source, convert <code>X</code> into a <code>Matrix</code>.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJModelInterface.int" href="#MLJModelInterface.int"><code>MLJModelInterface.int</code></a> — <span class="docstring-category">Function</span></header><section><div><p>int(x; type=nothing)</p><p>The positional integer of the <code>CategoricalString</code> or <code>CategoricalValue</code> <code>x</code>, in the ordering defined by the pool of <code>x</code>. The type of <code>int(x)</code> is the reference type of <code>x</code>.</p><p>Not to be confused with <code>x.ref</code>, which is unchanged by reordering of the pool of <code>x</code>, but has the same type.</p><pre><code class="language-none">int(X::CategoricalArray)
int(W::Array{&lt;:CategoricalString})
int(W::Array{&lt;:CategoricalValue})</code></pre><p>Broadcasted versions of <code>int</code>.</p><pre><code class="language-julia">julia&gt; v = categorical([&quot;c&quot;, &quot;b&quot;, &quot;c&quot;, &quot;a&quot;])
4-element CategoricalArrays.CategoricalArray{String,1,UInt32}:
 &quot;c&quot;
 &quot;b&quot;
 &quot;c&quot;
 &quot;a&quot;

julia&gt; levels(v)
3-element Vector{String}:
 &quot;a&quot;
 &quot;b&quot;
 &quot;c&quot;

julia&gt; int(v)
4-element Vector{UInt32}:
 0x00000003
 0x00000002
 0x00000003
 0x00000001</code></pre><p>See also: <a href="#MLJModelInterface.decoder"><code>decoder</code></a>.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="CategoricalDistributions.UnivariateFinite" href="#CategoricalDistributions.UnivariateFinite"><code>CategoricalDistributions.UnivariateFinite</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">UnivariateFinite(support,
                 probs;
                 pool=nothing,
                 augmented=false,
                 ordered=false)</code></pre><p>Construct a discrete univariate distribution whose finite support is the elements of the vector <code>support</code>, and whose corresponding probabilities are elements of the vector <code>probs</code>. Alternatively, construct an abstract <em>array</em> of <code>UnivariateFinite</code> distributions by choosing <code>probs</code> to be an array of one higher dimension than the array generated.</p><p>Here the word &quot;probabilities&quot; is an abuse of terminology as there is no requirement that the that probabilities actually sum to one. The only requirement is that the probabilities have a common type <code>T</code> for which <code>zero(T)</code> is defined. In particular, <code>UnivariateFinite</code> objects implement arbitrary non-negative, signed, or complex measures over finite sets of labelled points. A <code>UnivariateDistribution</code> will be a bona fide probability measure when constructed using the <code>augment=true</code> option (see below) or when <code>fit</code> to data. And the probabilities of a <code>UnivariateFinite</code> object <code>d</code> must be non-negative, with a non-zero sum, for <code>rand(d)</code> to be defined and interpretable.</p><p>Unless <code>pool</code> is specified, <code>support</code> should have type  <code>AbstractVector{&lt;:CategoricalValue}</code> and all elements are assumed to  share the same categorical pool, which may be larger than <code>support</code>.</p><p><em>Important.</em> All levels of the common pool have associated probabilities, not just those in the specified <code>support</code>. However, these probabilities are always zero (see example below).</p><p>If <code>probs</code> is a matrix, it should have a column for each class in <code>support</code> (or one less, if <code>augment=true</code>). More generally, <code>probs</code> will be an array whose size is of the form <code>(n1, n2, ..., nk, c)</code>, where <code>c = length(support)</code> (or one less, if <code>augment=true</code>) and the constructor then returns an array of <code>UnivariateFinite</code> distributions of size <code>(n1, n2, ..., nk)</code>.</p><pre><code class="language-none">using CategoricalDistributions, CategoricalArrays, Distributions
samples = categorical([&#39;x&#39;, &#39;x&#39;, &#39;y&#39;, &#39;x&#39;, &#39;z&#39;])
julia&gt; Distributions.fit(UnivariateFinite, samples)
           UnivariateFinite{Multiclass{3}}
     ┌                                        ┐
   x ┤■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■ 0.6
   y ┤■■■■■■■■■■■■ 0.2
   z ┤■■■■■■■■■■■■ 0.2
     └                                        ┘

julia&gt; d = UnivariateFinite([samples[1], samples[end]], [0.1, 0.9])
UnivariateFinite{Multiclass{3}(x=&gt;0.1, z=&gt;0.9)
           UnivariateFinite{Multiclass{3}}
     ┌                                        ┐
   x ┤■■■■ 0.1
   z ┤■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■ 0.9
     └                                        ┘

julia&gt; rand(d, 3)
3-element Array{Any,1}:
 CategoricalValue{Symbol,UInt32} &#39;z&#39;
 CategoricalValue{Symbol,UInt32} &#39;z&#39;
 CategoricalValue{Symbol,UInt32} &#39;z&#39;

julia&gt; levels(samples)
3-element Array{Symbol,1}:
 &#39;x&#39;
 &#39;y&#39;
 &#39;z&#39;

julia&gt; pdf(d, &#39;y&#39;)
0.0</code></pre><p><strong>Specifying a pool</strong></p><p>Alternatively, <code>support</code> may be a list of raw (non-categorical) elements if <code>pool</code> is:</p><ul><li><p>some <code>CategoricalArray</code>, <code>CategoricalValue</code> or <code>CategoricalPool</code>, such that <code>support</code> is a subset of <code>levels(pool)</code></p></li><li><p><code>missing</code>, in which case a new categorical pool is created which has <code>support</code> as its only levels.</p></li></ul><p>In the last case, specify <code>ordered=true</code> if the pool is to be considered ordered.</p><pre><code class="language-none">julia&gt; UnivariateFinite([&#39;x&#39;, &#39;z&#39;], [0.1, 0.9], pool=missing, ordered=true)
         UnivariateFinite{OrderedFactor{2}}
     ┌                                        ┐
   x ┤■■■■ 0.1
   z ┤■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■ 0.9
     └                                        ┘

samples = categorical([&#39;x&#39;, &#39;x&#39;, &#39;y&#39;, &#39;x&#39;, &#39;z&#39;])
julia&gt; d = UnivariateFinite([&#39;x&#39;, &#39;z&#39;], [0.1, 0.9], pool=samples)
     ┌                                        ┐
   x ┤■■■■ 0.1
   z ┤■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■ 0.9
     └                                        ┘

julia&gt; pdf(d, &#39;y&#39;) # allowed as `&#39;y&#39; in levels(samples)`
0.0

v = categorical([&#39;x&#39;, &#39;x&#39;, &#39;y&#39;, &#39;x&#39;, &#39;z&#39;, &#39;w&#39;])
probs = rand(100, 3)
probs = probs ./ sum(probs, dims=2)
julia&gt; d1 = UnivariateFinite([&#39;x&#39;, &#39;y&#39;, &#39;z&#39;], probs, pool=v)
100-element UnivariateFiniteVector{Multiclass{4},Symbol,UInt32,Float64}:
 UnivariateFinite{Multiclass{4}}(x=&gt;0.194, y=&gt;0.3, z=&gt;0.505)
 UnivariateFinite{Multiclass{4}}(x=&gt;0.727, y=&gt;0.234, z=&gt;0.0391)
 UnivariateFinite{Multiclass{4}}(x=&gt;0.674, y=&gt;0.00535, z=&gt;0.321)
   ⋮
 UnivariateFinite{Multiclass{4}}(x=&gt;0.292, y=&gt;0.339, z=&gt;0.369)</code></pre><p><strong>Probability augmentation</strong></p><p>If <code>augment=true</code> the provided array is augmented by inserting appropriate elements <em>ahead</em> of those provided, along the last dimension of the array. This means the user only provides probabilities for the classes <code>c2, c3, ..., cn</code>. The class <code>c1</code> probabilities are chosen so that each <code>UnivariateFinite</code> distribution in the returned array is a bona fide probability distribution.</p><pre><code class="language-julia">julia&gt; UnivariateFinite([0.1, 0.2, 0.3], augment=true, pool=missing)
3-element UnivariateFiniteArray{Multiclass{2}, String, UInt8, Float64, 1}:
 UnivariateFinite{Multiclass{2}}(class_1=&gt;0.9, class_2=&gt;0.1)
 UnivariateFinite{Multiclass{2}}(class_1=&gt;0.8, class_2=&gt;0.2)
 UnivariateFinite{Multiclass{2}}(class_1=&gt;0.7, class_2=&gt;0.3)

d2 = UnivariateFinite([&#39;x&#39;, &#39;y&#39;, &#39;z&#39;], probs[:, 2:end], augment=true, pool=v)
julia&gt; pdf(d1, levels(v)) ≈ pdf(d2, levels(v))
true</code></pre><hr/><pre><code class="language-none">UnivariateFinite(prob_given_class; pool=nothing, ordered=false)</code></pre><p>Construct a discrete univariate distribution whose finite support is the set of keys of the provided dictionary, <code>prob_given_class</code>, and whose values specify the corresponding probabilities.</p><p>The type requirements on the keys of the dictionary are the same as the elements of <code>support</code> given above with this exception: if non-categorical elements (raw labels) are used as keys, then <code>pool=...</code> must be specified and cannot be <code>missing</code>.</p><p>If the values (probabilities) are arrays instead of scalars, then an abstract array of <code>UnivariateFinite</code> elements is created, with the same size as the array.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="CategoricalDistributions.classes" href="#CategoricalDistributions.classes"><code>CategoricalDistributions.classes</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">classes(x)</code></pre><p>Return, as a <code>CategoricalVector</code>, all the categorical elements with the same pool as <code>CategoricalValue</code> <code>x</code> (including <code>x</code>), with an ordering consistent with the pool. Note that <code>x in classes(x)</code> is always true.</p><p>Not to be confused with <code>levels(x.pool)</code>. See the example below.</p><p>Also, overloaded for <code>x</code> a <code>CategoricalArray</code>, <code>CategoricalPool</code>, and for views of <code>CategoricalArray</code>.</p><p>**Private method.*</p><pre><code class="language-none">julia&gt;  v = categorical([:c, :b, :c, :a])
4-element CategoricalArrays.CategoricalArray{Symbol,1,UInt32}:
 :c
 :b
 :c
 :a

julia&gt; levels(v)
3-element Array{Symbol,1}:
 :a
 :b
 :c

julia&gt; x = v[4]
CategoricalArrays.CategoricalValue{Symbol,UInt32} :a

julia&gt; classes(x)
3-element CategoricalArrays.CategoricalArray{Symbol,1,UInt32}:
 :a
 :b
 :c

julia&gt; levels(x.pool)
3-element Array{Symbol,1}:
 :a
 :b
 :c</code></pre></div></section><section><div><pre><code class="language-none">classes(d::UnivariateFinite)
classes(d::UnivariateFiniteArray)</code></pre><p>A list of categorial elements in the common pool of classes used to construct <code>d</code>.</p><pre><code class="language-none">v = categorical([&quot;yes&quot;, &quot;maybe&quot;, &quot;no&quot;, &quot;yes&quot;])
d = UnivariateFinite(v[1:2], [0.3, 0.7])
classes(d) # CategoricalArray{String,1,UInt32}[&quot;maybe&quot;, &quot;no&quot;, &quot;yes&quot;]</code></pre></div></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJModelInterface.decoder" href="#MLJModelInterface.decoder"><code>MLJModelInterface.decoder</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">d = decoder(x)</code></pre><p>A callable object for decoding the integer representation of a <code>CategoricalString</code> or <code>CategoricalValue</code> sharing the same pool as <code>x</code>. (Here <code>x</code> is of one of these two types.) Specifically, one has <code>d(int(y)) == y</code> for all <code>y in classes(x)</code>. One can also call <code>d</code> on integer arrays, in which case <code>d</code> is broadcast over all elements.</p><pre><code class="language-julia">julia&gt; v = categorical([&quot;c&quot;, &quot;b&quot;, &quot;c&quot;, &quot;a&quot;])
4-element CategoricalArrays.CategoricalArray{String,1,UInt32}:
 &quot;c&quot;
 &quot;b&quot;
 &quot;c&quot;
 &quot;a&quot;

julia&gt; int(v)
4-element Vector{UInt32}:
 0x00000003
 0x00000002
 0x00000003
 0x00000001

julia&gt; d = decoder(v[3]);

julia&gt; d(int(v)) == v
true</code></pre><p><strong>Warning:</strong></p><p>It is <em>not</em> true that <code>int(d(u)) == u</code> always holds.</p><p>See also: <a href="#MLJModelInterface.int"><code>int</code></a>, <a href="@ref"><code>classes</code></a>.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJModelInterface.select" href="#MLJModelInterface.select"><code>MLJModelInterface.select</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">select(X, r, c)</code></pre><p>Select element(s) of a table or matrix at row(s) <code>r</code> and column(s) <code>c</code>. An object of the sink type of <code>X</code> (or a matrix) is returned unless <code>c</code> is a single integer or symbol. In that case a vector is returned, unless <code>r</code> is a single integer, in which case a single element is returned.</p><p>See also: <a href="#MLJModelInterface.selectrows"><code>selectrows</code></a>, <a href="#MLJModelInterface.selectcols"><code>selectcols</code></a>.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJModelInterface.selectrows" href="#MLJModelInterface.selectrows"><code>MLJModelInterface.selectrows</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">selectrows(X, r)</code></pre><p>Select single or multiple rows from a table, abstract vector or matrix <code>X</code>. If <code>X</code> is tabular, the object returned is a table of the preferred sink type of <code>typeof(X)</code>, even if only a single row is selected.</p><p>If the object is neither a table, abstract vector or matrix, <code>X</code> is returned and <code>r</code> is ignored.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJModelInterface.selectcols" href="#MLJModelInterface.selectcols"><code>MLJModelInterface.selectcols</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">selectcols(X, c)</code></pre><p>Select single or multiple columns from a matrix or table <code>X</code>. If <code>c</code> is an abstract vector of integers or symbols, then the object returned is a table of the preferred sink type of <code>typeof(X)</code>. If <code>c</code> is a <em>single</em> integer or column, then an <code>AbstractVector</code> is returned.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.recursive_getproperty" href="#MLJBase.recursive_getproperty"><code>MLJBase.recursive_getproperty</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">recursive_getproperty(object, nested_name::Expr)</code></pre><p>Call getproperty recursively on <code>object</code> to extract the value of some nested property, as in the following example:</p><pre><code class="language-none">julia&gt; object = (X = (x = 1, y = 2), Y = 3)
julia&gt; recursive_getproperty(object, :(X.y))
2</code></pre></div></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.recursive_setproperty!" href="#MLJBase.recursive_setproperty!"><code>MLJBase.recursive_setproperty!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">recursively_setproperty!(object, nested_name::Expr, value)</code></pre><p>Set a nested property of an <code>object</code> to <code>value</code>, as in the following example:</p><pre><code class="language-none">julia&gt; mutable struct Foo
           X
           Y
       end

julia&gt; mutable struct Bar
           x
           y
       end

julia&gt; object = Foo(Bar(1, 2), 3)
Foo(Bar(1, 2), 3)

julia&gt; recursively_setproperty!(object, :(X.y), 42)
42

julia&gt; object
Foo(Bar(1, 42), 3)</code></pre></div></section></article><h3 id="Where-to-place-code-implementing-new-models"><a class="docs-heading-anchor" href="#Where-to-place-code-implementing-new-models">Where to place code implementing new models</a><a id="Where-to-place-code-implementing-new-models-1"></a><a class="docs-heading-anchor-permalink" href="#Where-to-place-code-implementing-new-models" title="Permalink"></a></h3><p>Note that different packages can implement models having the same name without causing conflicts, although an MLJ user cannot simultaneously <em>load</em> two such models.</p><p>There are two options for making a new model implementation available to all MLJ users:</p><ol><li><p><strong>Native implementations</strong> (preferred option). The implementation code lives in the same package that contains the learning algorithms implementing the interface. An example is <a href="https://github.com/Evovest/EvoTrees.jl/blob/master/src/MLJ.jl"><code>EvoTrees.jl</code></a>. In this case, it is sufficient to open an issue at <a href="https://github.com/alan-turing-institute/MLJ.jl">MLJ</a> requesting the package to be registered with MLJ. Registering a package allows the MLJ user to access its models&#39; metadata and to selectively load them.</p></li><li><p><strong>Separate interface package</strong>. Implementation code lives in a separate <em>interface package</em>, which has the algorithm providing package as a dependency. See the template repository <a href="https://github.com/JuliaAI/MLJExampleInterface.jl">MLJExampleInterface.jl</a>.</p></li></ol><p>Additionally, one needs to ensure that the implementation code defines the <code>package_name</code> and <code>load_path</code> model traits appropriately, so that <code>MLJ</code>&#39;s <code>@load</code> macro can find the necessary code (see <a href="https://github.com/JuliaAI/MLJModels.jl/tree/master/src">MLJModels/src</a> for examples).</p><h3 id="How-to-add-models-to-the-MLJ-model-registry?"><a class="docs-heading-anchor" href="#How-to-add-models-to-the-MLJ-model-registry?">How to add models to the MLJ model registry?</a><a id="How-to-add-models-to-the-MLJ-model-registry?-1"></a><a class="docs-heading-anchor-permalink" href="#How-to-add-models-to-the-MLJ-model-registry?" title="Permalink"></a></h3><p>The MLJ model registry is located in the <a href="https://github.com/JuliaAI/MLJModels.jl">MLJModels.jl repository</a>. To add a model, you need to follow these steps</p><ul><li><p>Ensure your model conforms to the interface defined above</p></li><li><p>Raise an issue at <a href="https://github.com/JuliaAI/MLJModels.jl/issues">MLJModels.jl</a> and point out where the MLJ-interface implementation is, e.g. by providing a link to the code.</p></li><li><p>An administrator will then review your implementation and work with you to add the model to the registry</p></li></ul></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../quick_start_guide_to_adding_models/">« Quick-Start Guide to Adding Models</a><a class="docs-footer-nextpage" href="../modifying_behavior/">Modifying Behavior »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Monday 3 January 2022 20:33">Monday 3 January 2022</span>. Using Julia version 1.7.1.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
