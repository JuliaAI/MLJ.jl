var documenterSearchIndex = {"docs":
[{"location":"common_mlj_workflows/#Common-MLJ-Workflows-1","page":"Common MLJ Workflows","title":"Common MLJ Workflows","text":"","category":"section"},{"location":"common_mlj_workflows/#Data-ingestion-1","page":"Common MLJ Workflows","title":"Data ingestion","text":"","category":"section"},{"location":"common_mlj_workflows/#","page":"Common MLJ Workflows","title":"Common MLJ Workflows","text":"using MLJ; color_off() #hide\nimport RDatasets\nchanning = RDatasets.dataset(\"boot\", \"channing\")\nfirst(channing, 4)","category":"page"},{"location":"common_mlj_workflows/#","page":"Common MLJ Workflows","title":"Common MLJ Workflows","text":"Inspecting metadata, including column scientific types:","category":"page"},{"location":"common_mlj_workflows/#","page":"Common MLJ Workflows","title":"Common MLJ Workflows","text":"schema(channing)","category":"page"},{"location":"common_mlj_workflows/#","page":"Common MLJ Workflows","title":"Common MLJ Workflows","text":"Unpacking data and correcting for wrong scitypes:","category":"page"},{"location":"common_mlj_workflows/#","page":"Common MLJ Workflows","title":"Common MLJ Workflows","text":"y, X =  unpack(channing,\n               ==(:Exit),            # y is the :Exit column\n               !=(:Time);            # X is the rest, except :Time\n               :Exit=>Continuous,\n               :Entry=>Continuous,\n               :Cens=>Multiclass)\nfirst(X, 4)","category":"page"},{"location":"common_mlj_workflows/#","page":"Common MLJ Workflows","title":"Common MLJ Workflows","text":"Note: Before julia 1.2, replace !=(:Time) with col -> col != :Time.","category":"page"},{"location":"common_mlj_workflows/#","page":"Common MLJ Workflows","title":"Common MLJ Workflows","text":"y[1:4]","category":"page"},{"location":"common_mlj_workflows/#","page":"Common MLJ Workflows","title":"Common MLJ Workflows","text":"Loading a built-in supervised dataset:","category":"page"},{"location":"common_mlj_workflows/#","page":"Common MLJ Workflows","title":"Common MLJ Workflows","text":"X, y = @load_iris;\nselectrows(X, 1:4) # selectrows works for any Tables.jl table","category":"page"},{"location":"common_mlj_workflows/#","page":"Common MLJ Workflows","title":"Common MLJ Workflows","text":"y[1:4]","category":"page"},{"location":"common_mlj_workflows/#Model-search-1","page":"Common MLJ Workflows","title":"Model search","text":"","category":"section"},{"location":"common_mlj_workflows/#","page":"Common MLJ Workflows","title":"Common MLJ Workflows","text":"Reference:   Model Search","category":"page"},{"location":"common_mlj_workflows/#","page":"Common MLJ Workflows","title":"Common MLJ Workflows","text":"Searching for a supervised model:","category":"page"},{"location":"common_mlj_workflows/#","page":"Common MLJ Workflows","title":"Common MLJ Workflows","text":"X, y = @load_boston\nmodels(matching(X, y))","category":"page"},{"location":"common_mlj_workflows/#","page":"Common MLJ Workflows","title":"Common MLJ Workflows","text":"models(matching(X, y))[6]","category":"page"},{"location":"common_mlj_workflows/#","page":"Common MLJ Workflows","title":"Common MLJ Workflows","text":"More refined searches:","category":"page"},{"location":"common_mlj_workflows/#","page":"Common MLJ Workflows","title":"Common MLJ Workflows","text":"models() do model\n    matching(model, X, y) &&\n    model.prediction_type == :deterministic &&\n    model.is_pure_julia\nend","category":"page"},{"location":"common_mlj_workflows/#","page":"Common MLJ Workflows","title":"Common MLJ Workflows","text":"Searching for an unsupervised model:","category":"page"},{"location":"common_mlj_workflows/#","page":"Common MLJ Workflows","title":"Common MLJ Workflows","text":"models(matching(X))","category":"page"},{"location":"common_mlj_workflows/#","page":"Common MLJ Workflows","title":"Common MLJ Workflows","text":"Getting the metadata entry for a given model type:","category":"page"},{"location":"common_mlj_workflows/#","page":"Common MLJ Workflows","title":"Common MLJ Workflows","text":"info(\"PCA\")\ninfo(\"RidgeRegressor\", pkg=\"MultivariateStats\") # a model type in multiple packages","category":"page"},{"location":"common_mlj_workflows/#Instantiating-a-model-1","page":"Common MLJ Workflows","title":"Instantiating a model","text":"","category":"section"},{"location":"common_mlj_workflows/#","page":"Common MLJ Workflows","title":"Common MLJ Workflows","text":"Reference:   Getting Started","category":"page"},{"location":"common_mlj_workflows/#","page":"Common MLJ Workflows","title":"Common MLJ Workflows","text":"@load DecisionTreeClassifier\nmodel = DecisionTreeClassifier(min_samples_split=5, max_depth=4)","category":"page"},{"location":"common_mlj_workflows/#","page":"Common MLJ Workflows","title":"Common MLJ Workflows","text":"or","category":"page"},{"location":"common_mlj_workflows/#","page":"Common MLJ Workflows","title":"Common MLJ Workflows","text":"model = @load DecisionTreeClassifier\nmodel.min_samples_split = 5\nmodel.max_depth = 4","category":"page"},{"location":"common_mlj_workflows/#Evaluating-a-model-1","page":"Common MLJ Workflows","title":"Evaluating a model","text":"","category":"section"},{"location":"common_mlj_workflows/#","page":"Common MLJ Workflows","title":"Common MLJ Workflows","text":"Reference:   Evaluating Model Performance","category":"page"},{"location":"common_mlj_workflows/#","page":"Common MLJ Workflows","title":"Common MLJ Workflows","text":"X, y = @load_boston\nmodel = @load KNNRegressor\nevaluate(model, X, y, resampling=CV(nfolds=5), measure=[rms, mav])","category":"page"},{"location":"common_mlj_workflows/#Basic-fit/evaluate/predict-by-hand:-1","page":"Common MLJ Workflows","title":"Basic fit/evaluate/predict by hand:","text":"","category":"section"},{"location":"common_mlj_workflows/#","page":"Common MLJ Workflows","title":"Common MLJ Workflows","text":"Reference:   Getting Started, Machines, Evaluating Model Performance, Performance Measures","category":"page"},{"location":"common_mlj_workflows/#","page":"Common MLJ Workflows","title":"Common MLJ Workflows","text":"import RDatasets\nvaso = RDatasets.dataset(\"robustbase\", \"vaso\"); # a DataFrame\nfirst(vaso, 3)","category":"page"},{"location":"common_mlj_workflows/#","page":"Common MLJ Workflows","title":"Common MLJ Workflows","text":"y, X = unpack(vaso, ==(:Y), c -> true; :Y => Multiclass)\n\ntree_model = @load DecisionTreeClassifier\ntree_model.max_depth=2; nothing # hide","category":"page"},{"location":"common_mlj_workflows/#","page":"Common MLJ Workflows","title":"Common MLJ Workflows","text":"Bind the model and data together in a machine , which will additionally store the learned parameters (fitresults) when fit:","category":"page"},{"location":"common_mlj_workflows/#","page":"Common MLJ Workflows","title":"Common MLJ Workflows","text":"tree = machine(tree_model, X, y)","category":"page"},{"location":"common_mlj_workflows/#","page":"Common MLJ Workflows","title":"Common MLJ Workflows","text":"Split row indices into training and evaluation rows:","category":"page"},{"location":"common_mlj_workflows/#","page":"Common MLJ Workflows","title":"Common MLJ Workflows","text":"train, test = partition(eachindex(y), 0.7, shuffle=true, rng=1234); # 70:30 split","category":"page"},{"location":"common_mlj_workflows/#","page":"Common MLJ Workflows","title":"Common MLJ Workflows","text":"Fit on train and evaluate on test:","category":"page"},{"location":"common_mlj_workflows/#","page":"Common MLJ Workflows","title":"Common MLJ Workflows","text":"fit!(tree, rows=train)\nyhat = predict(tree, X[test,:])\nmean(cross_entropy(yhat, y[test]))","category":"page"},{"location":"common_mlj_workflows/#","page":"Common MLJ Workflows","title":"Common MLJ Workflows","text":"Predict on new data:","category":"page"},{"location":"common_mlj_workflows/#","page":"Common MLJ Workflows","title":"Common MLJ Workflows","text":"Xnew = (Volume=3*rand(3), Rate=3*rand(3))\npredict(tree, Xnew)      # a vector of distributions","category":"page"},{"location":"common_mlj_workflows/#","page":"Common MLJ Workflows","title":"Common MLJ Workflows","text":"predict_mode(tree, Xnew) # a vector of point-predictions","category":"page"},{"location":"common_mlj_workflows/#More-performance-evaluation-examples-1","page":"Common MLJ Workflows","title":"More performance evaluation examples","text":"","category":"section"},{"location":"common_mlj_workflows/#","page":"Common MLJ Workflows","title":"Common MLJ Workflows","text":"import LossFunctions.ZeroOneLoss","category":"page"},{"location":"common_mlj_workflows/#","page":"Common MLJ Workflows","title":"Common MLJ Workflows","text":"Evaluating model + data directly:","category":"page"},{"location":"common_mlj_workflows/#","page":"Common MLJ Workflows","title":"Common MLJ Workflows","text":"evaluate(tree_model, X, y,\n         resampling=Holdout(fraction_train=0.7, shuffle=true, rng=1234),\n         measure=[cross_entropy, ZeroOneLoss()])","category":"page"},{"location":"common_mlj_workflows/#","page":"Common MLJ Workflows","title":"Common MLJ Workflows","text":"If a machine is already defined, as above:","category":"page"},{"location":"common_mlj_workflows/#","page":"Common MLJ Workflows","title":"Common MLJ Workflows","text":"evaluate!(tree,\n          resampling=Holdout(fraction_train=0.7, shuffle=true, rng=1234),\n          measure=[cross_entropy, ZeroOneLoss()])","category":"page"},{"location":"common_mlj_workflows/#","page":"Common MLJ Workflows","title":"Common MLJ Workflows","text":"Using cross-validation:","category":"page"},{"location":"common_mlj_workflows/#","page":"Common MLJ Workflows","title":"Common MLJ Workflows","text":"evaluate!(tree, resampling=CV(nfolds=5, shuffle=true, rng=1234),\n          measure=[cross_entropy, ZeroOneLoss()])","category":"page"},{"location":"common_mlj_workflows/#","page":"Common MLJ Workflows","title":"Common MLJ Workflows","text":"With user-specified train/test pairs of row indices:","category":"page"},{"location":"common_mlj_workflows/#","page":"Common MLJ Workflows","title":"Common MLJ Workflows","text":"f1, f2, f3 = 1:13, 14:26, 27:36\npairs = [(f1, vcat(f2, f3)), (f2, vcat(f3, f1)), (f3, vcat(f1, f2))];\nevaluate!(tree,\n          resampling=pairs,\n          measure=[cross_entropy, ZeroOneLoss()])","category":"page"},{"location":"common_mlj_workflows/#","page":"Common MLJ Workflows","title":"Common MLJ Workflows","text":"Changing a hyperparameter and re-evaluating:","category":"page"},{"location":"common_mlj_workflows/#","page":"Common MLJ Workflows","title":"Common MLJ Workflows","text":"tree_model.max_depth = 3\nevaluate!(tree,\n          resampling=CV(nfolds=5, shuffle=true, rng=1234),\n          measure=[cross_entropy, ZeroOneLoss()])","category":"page"},{"location":"common_mlj_workflows/#Inspecting-training-results-1","page":"Common MLJ Workflows","title":"Inspecting training results","text":"","category":"section"},{"location":"common_mlj_workflows/#","page":"Common MLJ Workflows","title":"Common MLJ Workflows","text":"Fit a ordinary least square model to some synthetic data:","category":"page"},{"location":"common_mlj_workflows/#","page":"Common MLJ Workflows","title":"Common MLJ Workflows","text":"x1 = rand(100)\nx2 = rand(100)\n\nX = (x1=x1, x2=x2)\ny = x1 - 2x2 + 0.1*rand(100);\n\nols_model = @load LinearRegressor pkg=GLM\nols =  machine(ols_model, X, y)\nfit!(ols)","category":"page"},{"location":"common_mlj_workflows/#","page":"Common MLJ Workflows","title":"Common MLJ Workflows","text":"Get a named tuple representing the learned parameters, human-readable if appropriate:","category":"page"},{"location":"common_mlj_workflows/#","page":"Common MLJ Workflows","title":"Common MLJ Workflows","text":"fitted_params(ols)","category":"page"},{"location":"common_mlj_workflows/#","page":"Common MLJ Workflows","title":"Common MLJ Workflows","text":"Get other training-related information:","category":"page"},{"location":"common_mlj_workflows/#","page":"Common MLJ Workflows","title":"Common MLJ Workflows","text":"report(ols)","category":"page"},{"location":"common_mlj_workflows/#Basic-fit/transform-for-unsupervised-models-1","page":"Common MLJ Workflows","title":"Basic fit/transform for unsupervised models","text":"","category":"section"},{"location":"common_mlj_workflows/#","page":"Common MLJ Workflows","title":"Common MLJ Workflows","text":"Load data:","category":"page"},{"location":"common_mlj_workflows/#","page":"Common MLJ Workflows","title":"Common MLJ Workflows","text":"X, y = @load_iris\ntrain, test = partition(eachindex(y), 0.97, shuffle=true, rng=123)","category":"page"},{"location":"common_mlj_workflows/#","page":"Common MLJ Workflows","title":"Common MLJ Workflows","text":"Instantiate and fit the model/machine:","category":"page"},{"location":"common_mlj_workflows/#","page":"Common MLJ Workflows","title":"Common MLJ Workflows","text":"@load PCA\npca_model = PCA(maxoutdim=2)\npca = machine(pca_model, X)\nfit!(pca, rows=train)","category":"page"},{"location":"common_mlj_workflows/#","page":"Common MLJ Workflows","title":"Common MLJ Workflows","text":"Transform selected data bound to the machine:","category":"page"},{"location":"common_mlj_workflows/#","page":"Common MLJ Workflows","title":"Common MLJ Workflows","text":"transform(pca, rows=test);","category":"page"},{"location":"common_mlj_workflows/#","page":"Common MLJ Workflows","title":"Common MLJ Workflows","text":"Transform new data:","category":"page"},{"location":"common_mlj_workflows/#","page":"Common MLJ Workflows","title":"Common MLJ Workflows","text":"Xnew = (sepal_length=rand(3), sepal_width=rand(3),\n        petal_length=rand(3), petal_width=rand(3));\ntransform(pca, Xnew)","category":"page"},{"location":"common_mlj_workflows/#Inverting-learned-transformations-1","page":"Common MLJ Workflows","title":"Inverting learned transformations","text":"","category":"section"},{"location":"common_mlj_workflows/#","page":"Common MLJ Workflows","title":"Common MLJ Workflows","text":"y = rand(100);\nstand_model = UnivariateStandardizer()\nstand = machine(stand_model, y)\nfit!(stand)\nz = transform(stand, y);\n@assert inverse_transform(stand, z) ≈ y # true","category":"page"},{"location":"common_mlj_workflows/#Nested-hyperparameter-tuning-1","page":"Common MLJ Workflows","title":"Nested hyperparameter tuning","text":"","category":"section"},{"location":"common_mlj_workflows/#","page":"Common MLJ Workflows","title":"Common MLJ Workflows","text":"Reference:   Tuning Models","category":"page"},{"location":"common_mlj_workflows/#","page":"Common MLJ Workflows","title":"Common MLJ Workflows","text":"X, y = @load_iris; nothing # hide","category":"page"},{"location":"common_mlj_workflows/#","page":"Common MLJ Workflows","title":"Common MLJ Workflows","text":"Define a model with nested hyperparameters:","category":"page"},{"location":"common_mlj_workflows/#","page":"Common MLJ Workflows","title":"Common MLJ Workflows","text":"tree_model = @load DecisionTreeClassifier\nforest_model = EnsembleModel(atom=tree_model, n=300)","category":"page"},{"location":"common_mlj_workflows/#","page":"Common MLJ Workflows","title":"Common MLJ Workflows","text":"Inspect all hyperparameters, even nested ones (returns nested named tuple):","category":"page"},{"location":"common_mlj_workflows/#","page":"Common MLJ Workflows","title":"Common MLJ Workflows","text":"params(forest_model)","category":"page"},{"location":"common_mlj_workflows/#","page":"Common MLJ Workflows","title":"Common MLJ Workflows","text":"Define ranges for hyperparameters to be tuned:","category":"page"},{"location":"common_mlj_workflows/#","page":"Common MLJ Workflows","title":"Common MLJ Workflows","text":"r1 = range(forest_model, :bagging_fraction, lower=0.5, upper=1.0, scale=:log10)","category":"page"},{"location":"common_mlj_workflows/#","page":"Common MLJ Workflows","title":"Common MLJ Workflows","text":"r2 = range(forest_model, :(atom.n_subfeatures), lower=1, upper=4) # nested","category":"page"},{"location":"common_mlj_workflows/#","page":"Common MLJ Workflows","title":"Common MLJ Workflows","text":"Wrap the model in a tuning strategy:","category":"page"},{"location":"common_mlj_workflows/#","page":"Common MLJ Workflows","title":"Common MLJ Workflows","text":"tuned_forest = TunedModel(model=forest_model,\n                          tuning=Grid(resolution=12),\n                          resampling=CV(nfolds=6),\n                          ranges=[r1, r2],\n                          measure=cross_entropy)","category":"page"},{"location":"common_mlj_workflows/#","page":"Common MLJ Workflows","title":"Common MLJ Workflows","text":"Bound the wrapped model to data:","category":"page"},{"location":"common_mlj_workflows/#","page":"Common MLJ Workflows","title":"Common MLJ Workflows","text":"tuned = machine(tuned_forest, X, y)","category":"page"},{"location":"common_mlj_workflows/#","page":"Common MLJ Workflows","title":"Common MLJ Workflows","text":"Fitting the resultant machine optimizes the hyperparameters specified in range, using the specified tuning and resampling strategies and performance measure (possibly a vector of measures), and retrains on all data bound to the machine:","category":"page"},{"location":"common_mlj_workflows/#","page":"Common MLJ Workflows","title":"Common MLJ Workflows","text":"fit!(tuned)","category":"page"},{"location":"common_mlj_workflows/#","page":"Common MLJ Workflows","title":"Common MLJ Workflows","text":"Inspecting the optimal model:","category":"page"},{"location":"common_mlj_workflows/#","page":"Common MLJ Workflows","title":"Common MLJ Workflows","text":"F = fitted_params(tuned)","category":"page"},{"location":"common_mlj_workflows/#","page":"Common MLJ Workflows","title":"Common MLJ Workflows","text":"F.best_model","category":"page"},{"location":"common_mlj_workflows/#","page":"Common MLJ Workflows","title":"Common MLJ Workflows","text":"Inspecting details of tuning procedure:","category":"page"},{"location":"common_mlj_workflows/#","page":"Common MLJ Workflows","title":"Common MLJ Workflows","text":"report(tuned)","category":"page"},{"location":"common_mlj_workflows/#","page":"Common MLJ Workflows","title":"Common MLJ Workflows","text":"Visualizing these results:","category":"page"},{"location":"common_mlj_workflows/#","page":"Common MLJ Workflows","title":"Common MLJ Workflows","text":"using Plots\nplot(tuned)","category":"page"},{"location":"common_mlj_workflows/#","page":"Common MLJ Workflows","title":"Common MLJ Workflows","text":"(Image: )","category":"page"},{"location":"common_mlj_workflows/#","page":"Common MLJ Workflows","title":"Common MLJ Workflows","text":"Predicting on new data using the optimized model:","category":"page"},{"location":"common_mlj_workflows/#","page":"Common MLJ Workflows","title":"Common MLJ Workflows","text":"predict(tuned, Xnew)","category":"page"},{"location":"common_mlj_workflows/#Constructing-a-linear-pipeline-1","page":"Common MLJ Workflows","title":"Constructing a linear pipeline","text":"","category":"section"},{"location":"common_mlj_workflows/#","page":"Common MLJ Workflows","title":"Common MLJ Workflows","text":"Reference:   Composing Models","category":"page"},{"location":"common_mlj_workflows/#","page":"Common MLJ Workflows","title":"Common MLJ Workflows","text":"Constructing a linear (unbranching) pipeline with a learned target transformation/inverse transformation:","category":"page"},{"location":"common_mlj_workflows/#","page":"Common MLJ Workflows","title":"Common MLJ Workflows","text":"X, y = @load_reduced_ames\n@load KNNRegressor\npipe = @pipeline(X -> coerce(X, :age=>Continuous),\n                 OneHotEncoder,\n                 KNNRegressor(K=3),\n                 target = UnivariateStandardizer)","category":"page"},{"location":"common_mlj_workflows/#","page":"Common MLJ Workflows","title":"Common MLJ Workflows","text":"Evaluating the pipeline (just as you would any other model):","category":"page"},{"location":"common_mlj_workflows/#","page":"Common MLJ Workflows","title":"Common MLJ Workflows","text":"pipe.knn_regressor.K = 2\npipe.one_hot_encoder.drop_last = true\nevaluate(pipe, X, y, resampling=Holdout(), measure=rms, verbosity=2)","category":"page"},{"location":"common_mlj_workflows/#","page":"Common MLJ Workflows","title":"Common MLJ Workflows","text":"Inspecting the learned parameters in a pipeline:","category":"page"},{"location":"common_mlj_workflows/#","page":"Common MLJ Workflows","title":"Common MLJ Workflows","text":"mach = machine(pipe, X, y) |> fit!\nF = fitted_params(mach)\nF.one_hot_encoder","category":"page"},{"location":"common_mlj_workflows/#","page":"Common MLJ Workflows","title":"Common MLJ Workflows","text":"Constructing a linear (unbranching) pipeline with a static (unlearned) target transformation/inverse transformation:","category":"page"},{"location":"common_mlj_workflows/#","page":"Common MLJ Workflows","title":"Common MLJ Workflows","text":"@load DecisionTreeRegressor\npipe2 = @pipeline(X -> coerce(X, :age=>Continuous),\n                  OneHotEncoder,\n                  DecisionTreeRegressor(max_depth=4),\n                  target = y -> log.(y),\n                  inverse = z -> exp.(z))","category":"page"},{"location":"common_mlj_workflows/#Creating-a-homogeneous-ensemble-of-models-1","page":"Common MLJ Workflows","title":"Creating a homogeneous ensemble of models","text":"","category":"section"},{"location":"common_mlj_workflows/#","page":"Common MLJ Workflows","title":"Common MLJ Workflows","text":"Reference: Homogeneous Ensembles","category":"page"},{"location":"common_mlj_workflows/#","page":"Common MLJ Workflows","title":"Common MLJ Workflows","text":"X, y = @load_iris\ntree_model = @load DecisionTreeClassifier\nforest_model = EnsembleModel(atom=tree_model, bagging_fraction=0.8, n=300)\nforest = machine(forest_model, X, y)\nevaluate!(forest, measure=cross_entropy)","category":"page"},{"location":"common_mlj_workflows/#Performance-curves-1","page":"Common MLJ Workflows","title":"Performance curves","text":"","category":"section"},{"location":"common_mlj_workflows/#","page":"Common MLJ Workflows","title":"Common MLJ Workflows","text":"Generate a plot of performance, as a function of some hyperparameter (building on the preceding example)","category":"page"},{"location":"common_mlj_workflows/#","page":"Common MLJ Workflows","title":"Common MLJ Workflows","text":"Single performance curve:","category":"page"},{"location":"common_mlj_workflows/#","page":"Common MLJ Workflows","title":"Common MLJ Workflows","text":"r = range(forest_model, :n, lower=1, upper=1000, scale=:log10)\ncurve = learning_curve(forest,\n                            range=r,\n                            resampling=Holdout(),\n                            resolution=50,\n                            measure=cross_entropy,\n                            verbosity=0)","category":"page"},{"location":"common_mlj_workflows/#","page":"Common MLJ Workflows","title":"Common MLJ Workflows","text":"using Plots\nplot(curve.parameter_values, curve.measurements, xlab=curve.parameter_name, xscale=curve.parameter_scale)","category":"page"},{"location":"common_mlj_workflows/#","page":"Common MLJ Workflows","title":"Common MLJ Workflows","text":"(Image: )","category":"page"},{"location":"common_mlj_workflows/#","page":"Common MLJ Workflows","title":"Common MLJ Workflows","text":"Multiple curves:","category":"page"},{"location":"common_mlj_workflows/#","page":"Common MLJ Workflows","title":"Common MLJ Workflows","text":"curve = learning_curve(forest,\n                       range=r,\n                       resampling=Holdout(),\n                       measure=cross_entropy,\n                       resolution=50,\n                       rng_name=:rng,\n                       rngs=4,\n                       verbosity=0)","category":"page"},{"location":"common_mlj_workflows/#","page":"Common MLJ Workflows","title":"Common MLJ Workflows","text":"plot(curve.parameter_values, curve.measurements,\nxlab=curve.parameter_name, xscale=curve.parameter_scale)","category":"page"},{"location":"common_mlj_workflows/#","page":"Common MLJ Workflows","title":"Common MLJ Workflows","text":"(Image: )","category":"page"},{"location":"performance_measures/#Performance-Measures-1","page":"Performance Measures","title":"Performance Measures","text":"","category":"section"},{"location":"performance_measures/#","page":"Performance Measures","title":"Performance Measures","text":"In MLJ loss functions, scoring rules, sensitivities, and so on, are collectively referred to as measures. These include re-exported loss functions from the LossFunctions.jl library, overloaded to behave the same way as the built-in measures.","category":"page"},{"location":"performance_measures/#","page":"Performance Measures","title":"Performance Measures","text":"To see list all measures, run measures().  Further measures for probabilistic predictors, such as proper scoring rules, and for constructing multi-target product measures, are planned.  If you'd like to see measure added to MLJ, post a comment here","category":"page"},{"location":"performance_measures/#","page":"Performance Measures","title":"Performance Measures","text":"Note for developers: The measures interface and the built-in measures described here are defined in MLJBase, but will ultimately live in a separate package.","category":"page"},{"location":"performance_measures/#Using-built-in-measures-1","page":"Performance Measures","title":"Using built-in measures","text":"","category":"section"},{"location":"performance_measures/#","page":"Performance Measures","title":"Performance Measures","text":"These measures all have the common calling syntax","category":"page"},{"location":"performance_measures/#","page":"Performance Measures","title":"Performance Measures","text":"measure(ŷ, y)","category":"page"},{"location":"performance_measures/#","page":"Performance Measures","title":"Performance Measures","text":"or","category":"page"},{"location":"performance_measures/#","page":"Performance Measures","title":"Performance Measures","text":"measure(ŷ, y, w)","category":"page"},{"location":"performance_measures/#","page":"Performance Measures","title":"Performance Measures","text":"where y iterates over observations of some target variable, and ŷ iterates over predictions (Distribution or Sampler objects in the probabilistic case). Here w is an optional vector of sample weights, or a dictionary of class weights, when these are supported by the measure.","category":"page"},{"location":"performance_measures/#","page":"Performance Measures","title":"Performance Measures","text":"using MLJ\ny = [1, 2, 3, 4];\nŷ = [2, 3, 3, 3];\nw = [1, 2, 2, 1];\nrms(ŷ, y) # reports an aggregrate loss\nl2(ŷ, y, w) # reports per observation losses\ny = coerce([\"male\", \"female\", \"female\"], Multiclass)\nd = UnivariateFinite([\"male\", \"female\"], [0.55, 0.45], pool=y);\nŷ = [d, d, d];\nlog_loss(ŷ, y)","category":"page"},{"location":"performance_measures/#","page":"Performance Measures","title":"Performance Measures","text":"The measures rms, l2 and log_loss illustrated here are actually \tinstances of measure types. For, example, l2 = LPLoss(p=2) and log_loss = LogLoss() = LogLoss(tol=eps()). Common aliases are provided:","category":"page"},{"location":"performance_measures/#","page":"Performance Measures","title":"Performance Measures","text":"cross_entropy","category":"page"},{"location":"performance_measures/#Traits-and-custom-measures-1","page":"Performance Measures","title":"Traits and custom measures","text":"","category":"section"},{"location":"performance_measures/#","page":"Performance Measures","title":"Performance Measures","text":"Notice that l1 reports per-sample evaluations, while rms only reports an aggregated result. This and other behavior can be gleaned from measure traits which are summarized by the info method:","category":"page"},{"location":"performance_measures/#","page":"Performance Measures","title":"Performance Measures","text":"info(l1)","category":"page"},{"location":"performance_measures/#","page":"Performance Measures","title":"Performance Measures","text":"Query the doc-string for a measure using the name of its type:","category":"page"},{"location":"performance_measures/#","page":"Performance Measures","title":"Performance Measures","text":"rms\n@doc RootMeanSquaredError # same as `?RootMeanSqauredError","category":"page"},{"location":"performance_measures/#","page":"Performance Measures","title":"Performance Measures","text":"Use measures() to list all measures, and measures(conditions...) to search for measures with given traits (as you would query models). The trait instances list the actual callable instances of a given measure type (typically aliases for the default instance).","category":"page"},{"location":"performance_measures/#","page":"Performance Measures","title":"Performance Measures","text":"measures(conditions...)","category":"page"},{"location":"performance_measures/#MLJBase.measures-Tuple","page":"Performance Measures","title":"MLJBase.measures","text":"measures()\n\nList all measures as named-tuples keyed on measure traits.\n\nmeasures(filters...)\n\nList all measures m for which filter(m) is true, for each filter in filters.\n\nmeasures(matching(y))\n\nList all measures compatible with the target y.\n\nmeasures(needle::Union{AbstractString,Regex}\n\nList all measures with needle in a measure's name or docstring.\n\nExample\n\nFind all classification measures supporting sample weights:\n\nmeasures(m -> m.target_scitype <: AbstractVector{<:Finite} &&\n              m.supports_weights)\n\nFind all classification measures where the number of classes is three:\n\ny  = categorical(1:3)\nmeasures(matching(y))\n\nFind all measures in the rms family:\n\nmeasures(\"rms\")\n\n\n\n\n\n","category":"method"},{"location":"performance_measures/#","page":"Performance Measures","title":"Performance Measures","text":"A user-defined measure in MLJ can be passed to the evaluate! method, and elsewhere in MLJ, provided it is a function or callable object conforming to the above syntactic conventions. By default, a custom measure is understood to:","category":"page"},{"location":"performance_measures/#","page":"Performance Measures","title":"Performance Measures","text":"be a loss function (rather than a score)\nreport an aggregated value (rather than per-sample evaluations)\nbe feature-independent","category":"page"},{"location":"performance_measures/#","page":"Performance Measures","title":"Performance Measures","text":"To override this behaviour one simply overloads the appropriate trait, as shown in the following examples:","category":"page"},{"location":"performance_measures/#","page":"Performance Measures","title":"Performance Measures","text":"y = [1, 2, 3, 4];\nŷ = [2, 3, 3, 3];\nw = [1, 2, 2, 1];\nmy_loss(ŷ, y) = maximum((ŷ - y).^2);\nmy_loss(ŷ, y)\nmy_per_sample_loss(ŷ, y) = abs.(ŷ - y);\nMLJ.reports_each_observation(::typeof(my_per_sample_loss)) = true;\nmy_per_sample_loss(ŷ, y)\nmy_weighted_score(ŷ, y) = 1/mean(abs.(ŷ - y));\nmy_weighted_score(ŷ, y, w) = 1/mean(abs.((ŷ - y).^w));\nMLJ.supports_weights(::typeof(my_weighted_score)) = true;\nMLJ.orientation(::typeof(my_weighted_score)) = :score;\nmy_weighted_score(ŷ, y)\nX = (x=rand(4), penalty=[1, 2, 3, 4]);\nmy_feature_dependent_loss(ŷ, X, y) = sum(abs.(ŷ - y) .* X.penalty)/sum(X.penalty);\nMLJ.is_feature_dependent(::typeof(my_feature_dependent_loss)) = true\nmy_feature_dependent_loss(ŷ, X, y)","category":"page"},{"location":"performance_measures/#","page":"Performance Measures","title":"Performance Measures","text":"The possible signatures for custom measures are: measure(ŷ, y), measure(ŷ, y, w), measure(ŷ, X, y) and measure(ŷ, X, y, w), each measure implementing one non-weighted version, and possibly a second weighted version.","category":"page"},{"location":"performance_measures/#","page":"Performance Measures","title":"Performance Measures","text":"Implementation detail: Internally, every measure is evaluated using the syntax","category":"page"},{"location":"performance_measures/#","page":"Performance Measures","title":"Performance Measures","text":"MLJ.value(measure, ŷ, X, y, w)","category":"page"},{"location":"performance_measures/#","page":"Performance Measures","title":"Performance Measures","text":"and the traits determine what can be ignored and how measure is actually called. If w=nothing then the non-weighted form of measure is dispatched.","category":"page"},{"location":"performance_measures/#Using-measures-from-LossFunctions.jl-1","page":"Performance Measures","title":"Using measures from LossFunctions.jl","text":"","category":"section"},{"location":"performance_measures/#","page":"Performance Measures","title":"Performance Measures","text":"The LossFunctions.jl package includes \"distance loss\" functions for Continuous targets, and \"marginal loss\" functions for Finite{2} (binary) targets. While the LossFunctions.jl interface differs from the present one (for, example binary observations must be +1 or -1), MLJ has overloaded instances of the LossFunctions.jl types to behave the same as the built-in types.","category":"page"},{"location":"performance_measures/#","page":"Performance Measures","title":"Performance Measures","text":"Note that the \"distance losses\" in the package apply to deterministic predictions, while the \"marginal losses\" apply to probabilistic predictions.","category":"page"},{"location":"performance_measures/#List-of-measures-1","page":"Performance Measures","title":"List of measures","text":"","category":"section"},{"location":"performance_measures/#","page":"Performance Measures","title":"Performance Measures","text":"using DataFrames","category":"page"},{"location":"performance_measures/#","page":"Performance Measures","title":"Performance Measures","text":"ms = measures()\ntypes = map(ms) do m m.name end\ninstance = map(ms) do m m.instances end\nt = (type=types, instances=instance)\nDataFrame(t)","category":"page"},{"location":"performance_measures/#Other-performance-related-tools-1","page":"Performance Measures","title":"Other performance related tools","text":"","category":"section"},{"location":"performance_measures/#","page":"Performance Measures","title":"Performance Measures","text":"In MLJ one computes a confusion matrix by calling an instance of the ConfusionMatrix measure type on the data:","category":"page"},{"location":"performance_measures/#","page":"Performance Measures","title":"Performance Measures","text":"ConfusionMatrix","category":"page"},{"location":"performance_measures/#MLJBase.ConfusionMatrix","page":"Performance Measures","title":"MLJBase.ConfusionMatrix","text":"MLJBase.ConfusionMatrix\n\nA measure type for confusion matrix, which includes the instance(s), confusion_matrix, confmat.\n\nConfusionMatrix()(ŷ, y)\n\nEvaluate the default instance of ConfusionMatrix on observations ŷ, given ground truth values y. \n\nIf r is the return value, then the raw confusion matrix is r.mat, whose rows correspond to predictions, and columns to ground truth. The ordering follows that of levels(y).\n\nUse ConfusionMatrix(perm=[2, 1]) to reverse the class order for binary data. For more than two classes, specify an appropriate permutation, as in ConfusionMatrix(perm=[2, 3, 1]).\n\nRequires scitype(y) to be a subtype of AbstractArray{<:OrderedFactor{2}} (binary classification where choice of \"true\" effects the measure); ŷ must be a deterministic prediction. \n\nFor more information, run info(ConfusionMatrix). \n\n\n\n\n\n","category":"type"},{"location":"performance_measures/#","page":"Performance Measures","title":"Performance Measures","text":"roc_curve","category":"page"},{"location":"performance_measures/#MLJBase.roc_curve","page":"Performance Measures","title":"MLJBase.roc_curve","text":"fprs, tprs, ts = roc_curve(ŷ, y) = roc(ŷ, y)\n\nReturn the ROC curve for a two-class probabilistic prediction ŷ given the ground  truth y. The true positive rates, false positive rates over a range of thresholds ts are returned. Note that if there are k unique scores, there are correspondingly  k thresholds and k+1 \"bins\" over which the FPR and TPR are constant:\n\n[0.0 - thresh[1]]\n[thresh[1] - thresh[2]]\n...\n[thresh[k] - 1]\n\nconsequently, tprs and fprs are of length k+1 if ts is of length k.\n\nTo draw the curve using your favorite plotting backend, do plot(fprs, tprs).\n\n\n\n\n\n","category":"function"},{"location":"quick_start_guide_to_adding_models/#Quick-Start-Guide-to-Adding-Models-1","page":"Quick-Start Guide to Adding Models","title":"Quick-Start Guide to Adding Models","text":"","category":"section"},{"location":"quick_start_guide_to_adding_models/#","page":"Quick-Start Guide to Adding Models","title":"Quick-Start Guide to Adding Models","text":"The definitive specification of the MLJ model interface is given in Adding Models for General Use. In the more informal and condensed instructions below, we assume: (i) you have a Julia registered package YourPackage.jl implementing some machine learning models; (ii) that you would like to interface and register these models with MLJ; and (iii) that you have a rough understanding of how things work with MLJ.  In particular you are familiar with:","category":"page"},{"location":"quick_start_guide_to_adding_models/#","page":"Quick-Start Guide to Adding Models","title":"Quick-Start Guide to Adding Models","text":"what scientific types are\nwhat Probabilistic, Deterministic and Unsupervised models are\nthe fact that MLJ generally works with tables rather than bare bone matrices. Here a table is a container satisfying the Tables.jl API (e.g., DataFrame, JuliaDB table, CSV file, named tuple of equi-length vectors)\nCategoricalArrays.jl (if working with finite discrete data, e.g., doing classification)","category":"page"},{"location":"quick_start_guide_to_adding_models/#","page":"Quick-Start Guide to Adding Models","title":"Quick-Start Guide to Adding Models","text":"If you're not familiar with any one of these points, please refer to relevant sections of this manual, and in particular Getting Started and Adding Models for General Use.","category":"page"},{"location":"quick_start_guide_to_adding_models/#","page":"Quick-Start Guide to Adding Models","title":"Quick-Start Guide to Adding Models","text":"But tables don't make sense for my model! If a case can be made that tabular input does not make sense for your particular model, then MLJ can still handle this; you just need to define a non-tabular input_scitype trait. However, you should probably open an issue to clarify the appropriate declaration. The discussion below assumes input data is tabular.","category":"page"},{"location":"quick_start_guide_to_adding_models/#Overview-1","page":"Quick-Start Guide to Adding Models","title":"Overview","text":"","category":"section"},{"location":"quick_start_guide_to_adding_models/#","page":"Quick-Start Guide to Adding Models","title":"Quick-Start Guide to Adding Models","text":"To write an interface create a file or a module in your package which includes:","category":"page"},{"location":"quick_start_guide_to_adding_models/#","page":"Quick-Start Guide to Adding Models","title":"Quick-Start Guide to Adding Models","text":"a using MLJModelInterface or import MLJModelInterface statement\nMLJ-compatible model types and constructors,\nimplementation of fit, predict/transform and optionally fitted_params for your models,\nmetadata for your package and for each of your models","category":"page"},{"location":"quick_start_guide_to_adding_models/#Important-1","page":"Quick-Start Guide to Adding Models","title":"Important","text":"","category":"section"},{"location":"quick_start_guide_to_adding_models/#","page":"Quick-Start Guide to Adding Models","title":"Quick-Start Guide to Adding Models","text":"MLJModelInterface is a very light-weight interface allowing you to define your interface, but does not provide the functionality required to use or test your interface; this requires MLJBase. So, while you only need to add MLJModelInterface to your project's [deps], for testing purposes you need to add MLJBase to your project's [extras] and [targets]. In testing, simply use MLJBase in place of MLJModelInterface.","category":"page"},{"location":"quick_start_guide_to_adding_models/#","page":"Quick-Start Guide to Adding Models","title":"Quick-Start Guide to Adding Models","text":"We give some details for each step below with, each time, a few examples that you can mimic.  The instructions are intentionally brief.","category":"page"},{"location":"quick_start_guide_to_adding_models/#Model-type-and-constructor-1","page":"Quick-Start Guide to Adding Models","title":"Model type and constructor","text":"","category":"section"},{"location":"quick_start_guide_to_adding_models/#","page":"Quick-Start Guide to Adding Models","title":"Quick-Start Guide to Adding Models","text":"MLJ-compatible constructors for your models need to meet the following requirements:","category":"page"},{"location":"quick_start_guide_to_adding_models/#","page":"Quick-Start Guide to Adding Models","title":"Quick-Start Guide to Adding Models","text":"be mutable struct,\nbe subtypes of MLJModelInterface.Probabilistic or MLJModelInterface.Deterministic or MLJModelInterface.Unsupervised,\nhave fields corresponding exclusively to hyperparameters,\nhave a keyword constructor assigning default values to all hyperparameters.","category":"page"},{"location":"quick_start_guide_to_adding_models/#","page":"Quick-Start Guide to Adding Models","title":"Quick-Start Guide to Adding Models","text":"You may use the @mlj_model macro from MLJModelInterface to declare a (non parametric) model type:","category":"page"},{"location":"quick_start_guide_to_adding_models/#","page":"Quick-Start Guide to Adding Models","title":"Quick-Start Guide to Adding Models","text":"MLJModelInterface.@mlj_model mutable struct YourModel <: MLJModelInterface.Deterministic\n    a::Float64 = 0.5::(_ > 0)\n    b::String  = \"svd\"::(_ in (\"svd\",\"qr\"))\nend","category":"page"},{"location":"quick_start_guide_to_adding_models/#","page":"Quick-Start Guide to Adding Models","title":"Quick-Start Guide to Adding Models","text":"That macro specifies:","category":"page"},{"location":"quick_start_guide_to_adding_models/#","page":"Quick-Start Guide to Adding Models","title":"Quick-Start Guide to Adding Models","text":"A keyword constructor (here YourModel(; a=..., b=...)),\nDefault values for the hyperparameters,\nConstraints on the hyperparameters where _ refers to a value passed.","category":"page"},{"location":"quick_start_guide_to_adding_models/#","page":"Quick-Start Guide to Adding Models","title":"Quick-Start Guide to Adding Models","text":"Further to the last point, a::Float64 = 0.5::(_ > 0) indicates that the field a is a Float64, takes 0.5 as default value, and expects its value to be positive.","category":"page"},{"location":"quick_start_guide_to_adding_models/#","page":"Quick-Start Guide to Adding Models","title":"Quick-Start Guide to Adding Models","text":"Please see this issue for a known issue and workaround relating to the use of @mlj_model with negative defaults.","category":"page"},{"location":"quick_start_guide_to_adding_models/#","page":"Quick-Start Guide to Adding Models","title":"Quick-Start Guide to Adding Models","text":"If you decide not to use the @mlj_model macro (e.g. in the case of a parametric type), you will need to write a keyword constructor and a clean! method:","category":"page"},{"location":"quick_start_guide_to_adding_models/#","page":"Quick-Start Guide to Adding Models","title":"Quick-Start Guide to Adding Models","text":"mutable struct YourModel <: MLJModelInterface.Deterministic\n    a::Float64\nend\nfunction YourModel(; a=0.5)\n    model   = YourModel(a)\n    message = MLJModelInterface.clean!(model)\n    isempty(message) || @warn message\n    return model\nend\nfunction MLJModelInterface.clean!(m::YourModel)\n    warning = \"\"\n    if m.a <= 0\n        warning *= \"Parameter `a` expected to be positive, resetting to 0.5\"\n        m.a = 0.5\n    end\n    return warning\nend","category":"page"},{"location":"quick_start_guide_to_adding_models/#","page":"Quick-Start Guide to Adding Models","title":"Quick-Start Guide to Adding Models","text":"Additional notes:","category":"page"},{"location":"quick_start_guide_to_adding_models/#","page":"Quick-Start Guide to Adding Models","title":"Quick-Start Guide to Adding Models","text":"Please annotate all fields with concrete types, if possible, using type parameters if necessary.\nPlease prefer Symbol over String if you can (e.g. to pass the name of a solver).\nPlease add constraints to your fields even if they seem obvious to you.\nYour model may have 0 fields, that's fine.\nAlthough not essential, try to avoid Union types for model fields. For example, a field declaration features::Vector{Symbol} with a default of Symbol[] (detected with the isempty method) is preferred to features::Union{Vector{Symbol}, Nothing} with a default of nothing.","category":"page"},{"location":"quick_start_guide_to_adding_models/#","page":"Quick-Start Guide to Adding Models","title":"Quick-Start Guide to Adding Models","text":"Examples:","category":"page"},{"location":"quick_start_guide_to_adding_models/#","page":"Quick-Start Guide to Adding Models","title":"Quick-Start Guide to Adding Models","text":"KNNClassifier which uses @mlj_model,\nXGBoostRegressor which does not.","category":"page"},{"location":"quick_start_guide_to_adding_models/#Fit-1","page":"Quick-Start Guide to Adding Models","title":"Fit","text":"","category":"section"},{"location":"quick_start_guide_to_adding_models/#","page":"Quick-Start Guide to Adding Models","title":"Quick-Start Guide to Adding Models","text":"The implementation of fit will look like","category":"page"},{"location":"quick_start_guide_to_adding_models/#","page":"Quick-Start Guide to Adding Models","title":"Quick-Start Guide to Adding Models","text":"function MLJModelInterface.fit(m::YourModel, verbosity, X, y, w=nothing)\n    # body ...\n    return (fitresult, cache, report)\nend","category":"page"},{"location":"quick_start_guide_to_adding_models/#","page":"Quick-Start Guide to Adding Models","title":"Quick-Start Guide to Adding Models","text":"where y should only be there for a supervised model and w for a supervised model that supports sample weights.  You must type verbosity to Int and you must not type X, y and w (MLJ handles that).","category":"page"},{"location":"quick_start_guide_to_adding_models/#Regressor-1","page":"Quick-Start Guide to Adding Models","title":"Regressor","text":"","category":"section"},{"location":"quick_start_guide_to_adding_models/#","page":"Quick-Start Guide to Adding Models","title":"Quick-Start Guide to Adding Models","text":"In the body of the fit function, you should assume that X is a table and that y is an AbstractVector (for multitask regression it may be a table).","category":"page"},{"location":"quick_start_guide_to_adding_models/#","page":"Quick-Start Guide to Adding Models","title":"Quick-Start Guide to Adding Models","text":"Typical steps in the body of the fit function will be:","category":"page"},{"location":"quick_start_guide_to_adding_models/#","page":"Quick-Start Guide to Adding Models","title":"Quick-Start Guide to Adding Models","text":"forming a matrix-view of the data, possibly transposed if your model expects a p x n formalism (MLJ assumes columns are features by default i.e. n x p), use MLJModelInterface.matrix for this,\npassing the data to your model,\nreturning the results as a tuple (fitresult, cache, report).","category":"page"},{"location":"quick_start_guide_to_adding_models/#","page":"Quick-Start Guide to Adding Models","title":"Quick-Start Guide to Adding Models","text":"The fitresult part should contain everything that is needed at the predict or transform step, it should not be expected to be accessed by users.  The cache should be left to nothing for now. The report should be a NamedTuple with any auxiliary useful information that a user would want to know about the fit (e.g., feature rankings). See more on this below.","category":"page"},{"location":"quick_start_guide_to_adding_models/#","page":"Quick-Start Guide to Adding Models","title":"Quick-Start Guide to Adding Models","text":"Example: GLM's LinearRegressor","category":"page"},{"location":"quick_start_guide_to_adding_models/#Classifier-1","page":"Quick-Start Guide to Adding Models","title":"Classifier","text":"","category":"section"},{"location":"quick_start_guide_to_adding_models/#","page":"Quick-Start Guide to Adding Models","title":"Quick-Start Guide to Adding Models","text":"For a classifier, the steps are fairly similar to a regressor with these differences:","category":"page"},{"location":"quick_start_guide_to_adding_models/#","page":"Quick-Start Guide to Adding Models","title":"Quick-Start Guide to Adding Models","text":"y will be a categorical vector and you will typically want to use the integer encoding of y instead of CategoricalValues; use MLJModelInterface.int for this.\nYou will need to pass the full pool of target labels (not just those observed in the training data) and additionally, in the Deterministic case, the encoding, to make these available to predict. A simple way to do this is to pass y[1] in the fitresult, for then MLJModelInterface.classes(y[1]) is a complete list of possible categorical elements, and d = MLJModelInterface.decoder(y[1]) is a method for recovering categorical elements from their integer representations (e.g., d(2) is the categorical element with 2 as encoding).\nIn the case of a probabilistic classifier you should pass all probabilities simultaneously to the UnivariateFinite constructor to get an abstract UnivariateFinite vector (type UnivariateFiniteArray) rather than use comprehension or broadcasting to get a vanilla vector. This is for performance reasons.","category":"page"},{"location":"quick_start_guide_to_adding_models/#","page":"Quick-Start Guide to Adding Models","title":"Quick-Start Guide to Adding Models","text":"If implementing a classifier, you should probably consult the more detailed instructions at The predict method.","category":"page"},{"location":"quick_start_guide_to_adding_models/#","page":"Quick-Start Guide to Adding Models","title":"Quick-Start Guide to Adding Models","text":"Examples:","category":"page"},{"location":"quick_start_guide_to_adding_models/#","page":"Quick-Start Guide to Adding Models","title":"Quick-Start Guide to Adding Models","text":"GLM's BinaryClassifier (Probabilistic)\nLIBSVM's SVC (Deterministic)","category":"page"},{"location":"quick_start_guide_to_adding_models/#Transformer-1","page":"Quick-Start Guide to Adding Models","title":"Transformer","text":"","category":"section"},{"location":"quick_start_guide_to_adding_models/#","page":"Quick-Start Guide to Adding Models","title":"Quick-Start Guide to Adding Models","text":"Nothing special for a transformer.","category":"page"},{"location":"quick_start_guide_to_adding_models/#","page":"Quick-Start Guide to Adding Models","title":"Quick-Start Guide to Adding Models","text":"Example: FillImputer","category":"page"},{"location":"quick_start_guide_to_adding_models/#Fitted-parameters-1","page":"Quick-Start Guide to Adding Models","title":"Fitted parameters","text":"","category":"section"},{"location":"quick_start_guide_to_adding_models/#","page":"Quick-Start Guide to Adding Models","title":"Quick-Start Guide to Adding Models","text":"There is a function you can optionally implement which will return the learned parameters of your model for purposes of user-inspection. For instance, in the case of a linear regression, the user may want to get direct access to the coefficients and intercept. This should be as human and machine readable as practical (not a graphical representation) and the information should be combined in the form of a named tuple.","category":"page"},{"location":"quick_start_guide_to_adding_models/#","page":"Quick-Start Guide to Adding Models","title":"Quick-Start Guide to Adding Models","text":"The function will always look like:","category":"page"},{"location":"quick_start_guide_to_adding_models/#","page":"Quick-Start Guide to Adding Models","title":"Quick-Start Guide to Adding Models","text":"function MLJModelInterface.fitted_params(model::YourModel, fitresult)\n    # extract what's relevant from `fitresult`\n    # ...\n    # then return as a NamedTuple\n    return (learned_param1 = ..., learned_param2 = ...)\nend","category":"page"},{"location":"quick_start_guide_to_adding_models/#","page":"Quick-Start Guide to Adding Models","title":"Quick-Start Guide to Adding Models","text":"Example: for GLM models","category":"page"},{"location":"quick_start_guide_to_adding_models/#Summary-of-user-interface-points-(or,-What-to-put-where?)-1","page":"Quick-Start Guide to Adding Models","title":"Summary of user interface points (or, What to put where?)","text":"","category":"section"},{"location":"quick_start_guide_to_adding_models/#","page":"Quick-Start Guide to Adding Models","title":"Quick-Start Guide to Adding Models","text":"Recall that the fitresult returned as part of fit represents everything needed by predict (or transform) to make new predictions. It is not intended to be directly inspected by the user. Here is a summary of the interface points for users that your implementation creates:","category":"page"},{"location":"quick_start_guide_to_adding_models/#","page":"Quick-Start Guide to Adding Models","title":"Quick-Start Guide to Adding Models","text":"Use fitted_params to expose learned parameters, such as linear coefficients, to the user in a machine and human readable form (for re-use in another model, for example).\nUse the fields of your model struct for hyperparameters, i.e., those parameters declared by the user ahead of time that generally affect the outcome of training. It is okay to add \"control\" parameters (such a specifying an acceleration parameter specifying computational resources, as here).\nUse report to return everything else, including model-specific methods (or other callable objects). This includes: feature rankings, decision boundaries, SVM support vectors, clustering centres, methods for visualizing training outcomes, methods for saving learned parameters in a custom format, degrees of freedom, deviance, etc. If there is a performance cost to extra functionality you want to expose, the functionality can be toggled on/off through a hyperparameter, but this should otherwise be avoided. For, example, in a decision tree model report.print_tree(depth) might generate a pretty tree representation of the learned tree, up to the specified depth.","category":"page"},{"location":"quick_start_guide_to_adding_models/#Predict/Transform-1","page":"Quick-Start Guide to Adding Models","title":"Predict/Transform","text":"","category":"section"},{"location":"quick_start_guide_to_adding_models/#","page":"Quick-Start Guide to Adding Models","title":"Quick-Start Guide to Adding Models","text":"The implementation of predict (for a supervised model) or transform (for an unsupervised one) will look like:","category":"page"},{"location":"quick_start_guide_to_adding_models/#","page":"Quick-Start Guide to Adding Models","title":"Quick-Start Guide to Adding Models","text":"function MLJModelInterface.predict(m::YourModel, fitresult, Xnew)\n    # ...\nend","category":"page"},{"location":"quick_start_guide_to_adding_models/#","page":"Quick-Start Guide to Adding Models","title":"Quick-Start Guide to Adding Models","text":"Here Xnew is expected to be a table and part of the logic in predict or transform may be similar to that in fit.","category":"page"},{"location":"quick_start_guide_to_adding_models/#","page":"Quick-Start Guide to Adding Models","title":"Quick-Start Guide to Adding Models","text":"The values returned should be:","category":"page"},{"location":"quick_start_guide_to_adding_models/#","page":"Quick-Start Guide to Adding Models","title":"Quick-Start Guide to Adding Models","text":"model subtype return value of predict/transform\nDeterministic vector of values (or table if multi-target)\nProbabilistic vector of Distribution objects, for classifiers in particular, a vector of UnivariateFinite\nUnsupervised table","category":"page"},{"location":"quick_start_guide_to_adding_models/#","page":"Quick-Start Guide to Adding Models","title":"Quick-Start Guide to Adding Models","text":"In the case of a Probabilistic model, you may further want to implement a predict_mean or a predict_mode. However, MLJModelInterface provides fallbacks, defined in terms of predict, whose performance may suffice.","category":"page"},{"location":"quick_start_guide_to_adding_models/#","page":"Quick-Start Guide to Adding Models","title":"Quick-Start Guide to Adding Models","text":"Examples","category":"page"},{"location":"quick_start_guide_to_adding_models/#","page":"Quick-Start Guide to Adding Models","title":"Quick-Start Guide to Adding Models","text":"Deterministic regression: KNNRegressor\nProbabilistic regression: LinearRegressor and the predict_mean\nProbabilistic classification: LogisticClassifier","category":"page"},{"location":"quick_start_guide_to_adding_models/#Metadata-1","page":"Quick-Start Guide to Adding Models","title":"Metadata","text":"","category":"section"},{"location":"quick_start_guide_to_adding_models/#","page":"Quick-Start Guide to Adding Models","title":"Quick-Start Guide to Adding Models","text":"Adding metadata for your model(s) is crucial for the discoverability of your package and its models and to make sure your model is used with data it can handle.  You can individually overload a number of trait functions that encode this metadata by following the instuctions in Adding Models for General Use), which also explains these traits in more detail. However, your most convenient option is to use metadata_model and metadata_pkg functionalities from MLJModelInterface to do this:","category":"page"},{"location":"quick_start_guide_to_adding_models/#","page":"Quick-Start Guide to Adding Models","title":"Quick-Start Guide to Adding Models","text":"const ALL_MODELS = Union{YourModel1, YourModel2, ...}\n\nMLJModelInterface.metadata_pkg.(ALL_MODELS\n    name = \"YourPackage\",\n    uuid = \"6ee0df7b-...\", # see your Project.toml\n    url  = \"https://...\",  # URL to your package repo\n    julia = true,          # is it written entirely in Julia?\n    license = \"MIT\",       # your package license\n    is_wrapper = false,    # does it wrap around some other package?\n)\n\n# Then for each model,\nMLJModelInterface.metadata_model(YourModel1,\n    input   = MLJModelInterface.Table(MLJModelInterface.Continuous),  # what input data is supported?\n    target  = AbstractVector{MLJModelInterface.Continuous},           # for a supervised model, what target?\n    output  = MLJModelInterface.Table(MLJModelInterface.Continuous),  # for an unsupervised, what output?\n    weights = false,                                                  # does the model support sample weights?\n    descr   = \"A short description of your model\"\n\tpath    = \"YourPackage.SubModuleContainingModelStructDefinition.YourModel1\"\n    )","category":"page"},{"location":"quick_start_guide_to_adding_models/#","page":"Quick-Start Guide to Adding Models","title":"Quick-Start Guide to Adding Models","text":"Important. Do not omit the path specification. ","category":"page"},{"location":"quick_start_guide_to_adding_models/#","page":"Quick-Start Guide to Adding Models","title":"Quick-Start Guide to Adding Models","text":"Examples:","category":"page"},{"location":"quick_start_guide_to_adding_models/#","page":"Quick-Start Guide to Adding Models","title":"Quick-Start Guide to Adding Models","text":"package metadata\nGLM\nMLJLinearModels\nmodel metadata\nLinearRegressor\nDecisionTree\nA series of regressors","category":"page"},{"location":"quick_start_guide_to_adding_models/#Adding-a-model-to-the-model-registry-1","page":"Quick-Start Guide to Adding Models","title":"Adding a model to the model registry","text":"","category":"section"},{"location":"quick_start_guide_to_adding_models/#","page":"Quick-Start Guide to Adding Models","title":"Quick-Start Guide to Adding Models","text":"See here.","category":"page"},{"location":"quick_start_guide_to_adding_models/#Does-it-work?-1","page":"Quick-Start Guide to Adding Models","title":"Does it work?","text":"","category":"section"},{"location":"quick_start_guide_to_adding_models/#","page":"Quick-Start Guide to Adding Models","title":"Quick-Start Guide to Adding Models","text":"In new julia environment add MLJ and YourPackage\nrun using MLJModels; @load YourModel pkg=YourPackage","category":"page"},{"location":"learning_curves/#Learning-Curves-1","page":"Learning Curves","title":"Learning Curves","text":"","category":"section"},{"location":"learning_curves/#","page":"Learning Curves","title":"Learning Curves","text":"A learning curve in MLJ is a plot of some performance estimate, as a function of some model hyperparameter. This can be useful when tuning a single model hyperparameter, or when deciding how many iterations are required for some iterative model. The learning_curve method does not actually generate a plot, but generates the data needed to do so.","category":"page"},{"location":"learning_curves/#","page":"Learning Curves","title":"Learning Curves","text":"To generate learning curves you can bind data to a model by instantiating a machine. You can choose to supply all available data, as performance estimates are computed using a resampling strategy, defaulting to Holdout(fraction_train=0.7).","category":"page"},{"location":"learning_curves/#","page":"Learning Curves","title":"Learning Curves","text":"X, y = @load_boston;\n\natom = @load RidgeRegressor pkg=MultivariateStats\nensemble = EnsembleModel(atom=atom, n=1000)\nmach = machine(ensemble, X, y)\n\nr_lambda = range(ensemble, :(atom.lambda), lower=10, upper=500, scale=:log10)\ncurve = MLJ.learning_curve(mach;\n                           range=r_lambda,\n                           resampling=CV(nfolds=3),\n                           measure=mav)\nusing Plots\nplot(curve.parameter_values,\n     curve.measurements,\n     xlab=curve.parameter_name,\n     xscale=curve.parameter_scale,\n     ylab = \"CV estimate of RMS error\")","category":"page"},{"location":"learning_curves/#","page":"Learning Curves","title":"Learning Curves","text":"(Image: )","category":"page"},{"location":"learning_curves/#","page":"Learning Curves","title":"Learning Curves","text":"In the case the range hyperparameter is the number of iterations in some iterative model, learning_curve will not restart the training from scratch for each new value, unless a non-holdout resampling strategy is specified (and provided the model implements an appropriate update method). To obtain multiple curves (that are distinct) you will need to pass the name of the model random number generator, rng_name, and specify the random number generators to be used using rngs=... (an integer automatically generates the number specified):","category":"page"},{"location":"learning_curves/#","page":"Learning Curves","title":"Learning Curves","text":"atom.lambda=200\nr_n = range(ensemble, :n, lower=1, upper=50)\ncurves = MLJ.learning_curve(mach;\n                             range=r_n,\n                             verbosity=0,\n                             rng_name=:rng,\n                             rngs=4)\nplot(curves.parameter_values,\n     curves.measurements,\n     xlab=curves.parameter_name,\n     ylab=\"Holdout estimate of RMS error\")","category":"page"},{"location":"learning_curves/#","page":"Learning Curves","title":"Learning Curves","text":"(Image: )","category":"page"},{"location":"learning_curves/#API-reference-1","page":"Learning Curves","title":"API reference","text":"","category":"section"},{"location":"learning_curves/#","page":"Learning Curves","title":"Learning Curves","text":"MLJTuning.learning_curve","category":"page"},{"location":"learning_curves/#MLJTuning.learning_curve","page":"Learning Curves","title":"MLJTuning.learning_curve","text":"curve = learning_curve(mach; resolution=30,\n                             resampling=Holdout(),\n                             repeats=1,\n                             measure=default_measure(machine.model),\n                             rows=nothing,\n                             weights=nothing,\n                             operation=predict,\n                             range=nothing,\n                             acceleration=default_resource(),\n                             acceleration_grid=CPU1(),\n                             rngs=nothing,\n                             rng_name=nothing)\n\nGiven a supervised machine mach, returns a named tuple of objects suitable for generating a plot of performance estimates, as a function of the single hyperparameter specified in range. The tuple curve has the following keys: :parameter_name, :parameter_scale, :parameter_values, :measurements.\n\nTo generate multiple curves for a model with a random number generator (RNG) as a hyperparameter, specify the name, rng_name, of the (possibly nested) RNG field, and a vector rngs of RNG's, one for each curve. Alternatively, set rngs to the number of curves desired, in which case RNG's are automatically generated. The individual curve computations can be distributed across multiple processes using acceleration=CPUProcesses() or acceleration=CPUThreads(). See the second example below for a demonstration.\n\nX, y = @load_boston;\natom = @load RidgeRegressor pkg=MultivariateStats\nensemble = EnsembleModel(atom=atom, n=1000)\nmach = machine(ensemble, X, y)\nr_lambda = range(ensemble, :(atom.lambda), lower=10, upper=500, scale=:log10)\ncurve = learning_curve(mach; range=r_lambda, resampling=CV(), measure=mav)\nusing Plots\nplot(curve.parameter_values,\n     curve.measurements,\n     xlab=curve.parameter_name,\n     xscale=curve.parameter_scale,\n     ylab = \"CV estimate of RMS error\")\n\nIf using a Holdout() resampling strategy (with no shuffling) and if the specified hyperparameter is the number of iterations in some iterative model (and that model has an appropriately overloaded MLJModelInterface.update method) then training is not restarted from scratch for each increment of the parameter, ie the model is trained progressively.\n\natom.lambda=200\nr_n = range(ensemble, :n, lower=1, upper=250)\ncurves = learning_curve(mach; range=r_n, verbosity=0, rng_name=:rng, rngs=3)\nplot!(curves.parameter_values,\n     curves.measurements,\n     xlab=curves.parameter_name,\n     ylab=\"Holdout estimate of RMS error\")\n\n\n\nlearning_curve(model::Supervised, X, y; kwargs...)\nlearning_curve(model::Supervised, X, y, w; kwargs...)\n\nPlot a learning curve (or curves) directly, without first constructing a machine.\n\nSummary of key-word options\n\nresolution - number of points generated from range (number model evaluations); default is 30\nresampling - resampling strategy; default is Holdout(fraction_train=0.7)\nrepeats - set to more than 1 for repeated (Monte Carlo) resampling\nmeasure - performance measure (metric); automatically inferred from model by default when possible\nrows - row indices to which resampling should be restricted; default is all rows\nweights - sample weights used by measure where supported\noperation - operation, such as predict, to be used in evaluations. If prediction_type(mach.model) == :probabilistic but prediction_type(measure) == :deterministic consider ,predictmode,predictmodeorpredict_median; default ispredict`.\nrange - object constructed using range(model, ...) or range(type, ...) representing one-dimensional hyper-parameter range.\nacceleration - parallelization option for passing to evaluate!; an instance of CPU1, CPUProcesses or CPUThreads from the ComputationalResources.jl; default is default_resource()\nacceleration_grid - parallelization option for distributing each performancde evaluation\nrngs - for specifying random number generator(s) to be passed to the model (see above)\nrng_name - name of the model hyper-parameter representing a random number generator (see above); possibly nested\n\n\n\n\n\n","category":"function"},{"location":"working_with_categorical_data/#Working-with-Categorical-Data-1","page":"Working with Categorical Data","title":"Working with Categorical Data","text":"","category":"section"},{"location":"working_with_categorical_data/#Scientific-types-for-discrete-data-1","page":"Working with Categorical Data","title":"Scientific types for discrete data","text":"","category":"section"},{"location":"working_with_categorical_data/#","page":"Working with Categorical Data","title":"Working with Categorical Data","text":"Recall that models articulate their data requirements using scientific types (see Getting Started or the MLJScientificTypes.jl documentation). There are three scientific types discrete data can have: Count, OrderedFactor and Multiclass.","category":"page"},{"location":"working_with_categorical_data/#Count-data-1","page":"Working with Categorical Data","title":"Count data","text":"","category":"section"},{"location":"working_with_categorical_data/#","page":"Working with Categorical Data","title":"Working with Categorical Data","text":"In MLJ you cannot use integers to represent (finite) categorical data. Integers are reserved for discrete data you want interpreted as Count <: Infinite:","category":"page"},{"location":"working_with_categorical_data/#","page":"Working with Categorical Data","title":"Working with Categorical Data","text":"using MLJ # hide\nscitype([1, 4, 5, 6])","category":"page"},{"location":"working_with_categorical_data/#","page":"Working with Categorical Data","title":"Working with Categorical Data","text":"The Count scientific type includes things like the number of phone calls, or city populations, and other \"frequency\" data of a generally unbounded nature.","category":"page"},{"location":"working_with_categorical_data/#","page":"Working with Categorical Data","title":"Working with Categorical Data","text":"That said, you may have data that is theoretically Count, but which you coerce to OrderedFactor to enable the use of more models, trusting to your knowledge of how those models work to inform an appropriate interpretation.","category":"page"},{"location":"working_with_categorical_data/#OrderedFactor-and-Multiclass-data-1","page":"Working with Categorical Data","title":"OrderedFactor and Multiclass data","text":"","category":"section"},{"location":"working_with_categorical_data/#","page":"Working with Categorical Data","title":"Working with Categorical Data","text":"Other integer data, such as the number of an animal's legs, or number of rooms of homes, are generally coerced to OrderedFactor <: Finite. The other categorical scientific type is Multiclass <: Finite, which is for unordered categorical data. Coercing data to one of these two forms is discussed under  Detecting and coercing improperly represented categorical data below.","category":"page"},{"location":"working_with_categorical_data/#Binary-data-1","page":"Working with Categorical Data","title":"Binary data","text":"","category":"section"},{"location":"working_with_categorical_data/#","page":"Working with Categorical Data","title":"Working with Categorical Data","text":"There is no separate scientific type for binary data. Binary data is either OrderedFactor{2} if ordered, and Multiclass{2} otherwise. Data with type OrderedFactor{2} is considered to have an intrinsic \"positive\" class, e.g., the outcome of a medical test, and the \"pass/fail\" outcome of an exam. MLJ measures, such as true_positive assume the second class in the ordering is the \"positive\" class. Inspecting and changing order is discussed in the next section.","category":"page"},{"location":"working_with_categorical_data/#","page":"Working with Categorical Data","title":"Working with Categorical Data","text":"If data has type Bool it is considered Count data (as Bool <: Integer) and generally users will want to coerce such data to Multiclass or OrderedFactor.","category":"page"},{"location":"working_with_categorical_data/#Detecting-and-coercing-improperly-represented-categorical-data-1","page":"Working with Categorical Data","title":"Detecting and coercing improperly represented categorical data","text":"","category":"section"},{"location":"working_with_categorical_data/#","page":"Working with Categorical Data","title":"Working with Categorical Data","text":"One inspects the scientific type of data using scitype as shown above. To inspect all column scientific types in a table simultaneously, use schema. (The scitype(X) of a table X contains a condensed form of this information used in type dispatch; see here.)","category":"page"},{"location":"working_with_categorical_data/#","page":"Working with Categorical Data","title":"Working with Categorical Data","text":"import DataFrames.DataFrame\nX = DataFrame(\n                 name       = [\"Siri\", \"Robo\", \"Alexa\", \"Cortana\"],\n                 gender     = [\"male\", \"male\", \"Female\", \"female\"],\n                 likes_soup = [true, false, false, true],\n                 height     = [152, missing, 148, 163],\n                 rating     = [2, 5, 2, 1],\n                 outcome    = [\"rejected\", \"accepted\", \"accepted\", \"rejected\"])\nschema(X)","category":"page"},{"location":"working_with_categorical_data/#","page":"Working with Categorical Data","title":"Working with Categorical Data","text":"Coercing a single column:","category":"page"},{"location":"working_with_categorical_data/#","page":"Working with Categorical Data","title":"Working with Categorical Data","text":"X.outcome = coerce(X.outcome, OrderedFactor)","category":"page"},{"location":"working_with_categorical_data/#","page":"Working with Categorical Data","title":"Working with Categorical Data","text":"The machine type of the result is a CategoricalArray. For more on this type see Under the hood: CategoricalValue and CategoricalArray below.","category":"page"},{"location":"working_with_categorical_data/#","page":"Working with Categorical Data","title":"Working with Categorical Data","text":"Inspecting the order of the levels:","category":"page"},{"location":"working_with_categorical_data/#","page":"Working with Categorical Data","title":"Working with Categorical Data","text":"levels(X.outcome)","category":"page"},{"location":"working_with_categorical_data/#","page":"Working with Categorical Data","title":"Working with Categorical Data","text":"Since we wish to regard \"accepted\" as the positive class, it should appear second, which we correct with the levels! function:","category":"page"},{"location":"working_with_categorical_data/#","page":"Working with Categorical Data","title":"Working with Categorical Data","text":"levels!(X.outcome, [\"rejected\", \"accepted\"])\nlevels(X.outcome)","category":"page"},{"location":"working_with_categorical_data/#","page":"Working with Categorical Data","title":"Working with Categorical Data","text":"warning: Changing levels of categorical data\nThe order of levels should generally be changed early in your data science work-flow and then not again. Similar remarks apply to adding levels (which is possible; see the CategorialArrays.jl documentation). MLJ supervised and unsupervised models assume levels and their order do not change.","category":"page"},{"location":"working_with_categorical_data/#","page":"Working with Categorical Data","title":"Working with Categorical Data","text":"Coercing all remaining types simultaneously:","category":"page"},{"location":"working_with_categorical_data/#","page":"Working with Categorical Data","title":"Working with Categorical Data","text":"Xnew = coerce(X, :gender     => Multiclass,\n                 :likes_soup => OrderedFactor,\n                 :height     => Continuous,\n                 :rating     => OrderedFactor)\nschema(Xnew)","category":"page"},{"location":"working_with_categorical_data/#","page":"Working with Categorical Data","title":"Working with Categorical Data","text":"For DataFrames there is also in-place coercion, using coerce!.","category":"page"},{"location":"working_with_categorical_data/#Tracking-all-levels-1","page":"Working with Categorical Data","title":"Tracking all levels","text":"","category":"section"},{"location":"working_with_categorical_data/#","page":"Working with Categorical Data","title":"Working with Categorical Data","text":"The key property of vectors of scientific type OrderedFactor and  Multiclass is that the pool of all levels is not lost when separating out one or more elements:","category":"page"},{"location":"working_with_categorical_data/#","page":"Working with Categorical Data","title":"Working with Categorical Data","text":"v = Xnew.rating","category":"page"},{"location":"working_with_categorical_data/#","page":"Working with Categorical Data","title":"Working with Categorical Data","text":"levels(v)","category":"page"},{"location":"working_with_categorical_data/#","page":"Working with Categorical Data","title":"Working with Categorical Data","text":"levels(v[1:2])","category":"page"},{"location":"working_with_categorical_data/#","page":"Working with Categorical Data","title":"Working with Categorical Data","text":"levels(v[2])","category":"page"},{"location":"working_with_categorical_data/#","page":"Working with Categorical Data","title":"Working with Categorical Data","text":"By tracking all classes in this way, MLJ avoids common pain points around categorical data, such as evaluating models on an evaluation set, only to crash your code because classes appear there which were not seen during training.","category":"page"},{"location":"working_with_categorical_data/#","page":"Working with Categorical Data","title":"Working with Categorical Data","text":"By drawing test, validation and training data from a common data structure (as described in Getting Started, for example) one ensures that all possible classes of categorical variables are tracked at all times. However, this does not mitigate problems with new production data, if categorical features there are missing classes or contain previously unseen classes.","category":"page"},{"location":"working_with_categorical_data/#New-or-missing-levels-in-production-data-1","page":"Working with Categorical Data","title":"New or missing levels in production data","text":"","category":"section"},{"location":"working_with_categorical_data/#","page":"Working with Categorical Data","title":"Working with Categorical Data","text":"warning: Warning\nUnpredictable behaviour may result whenever Finite categorical data presents in a production set with different classes (levels) from those presented in training","category":"page"},{"location":"working_with_categorical_data/#","page":"Working with Categorical Data","title":"Working with Categorical Data","text":"Consider, for example, the following naive workflow:","category":"page"},{"location":"working_with_categorical_data/#","page":"Working with Categorical Data","title":"Working with Categorical Data","text":"# train a one-hot encoder on some data:\nx = coerce([\"black\", \"white\", \"white\", \"black\"], Multiclass)\nX = DataFrame(x=x)\n\nmodel = OneHotEncoder()\nmach = machine(model, X) |> fit!\n\n# one-hot encode new data with missing classes:\nxproduction = coerce([\"white\", \"white\"], Multiclass)\nXproduction = DataFrame(x=xproduction)\nXproduction == X[2:3,:] # true\ntransform(mach, Xproduction) == transform(mach, X[2:3,:])","category":"page"},{"location":"working_with_categorical_data/#","page":"Working with Categorical Data","title":"Working with Categorical Data","text":"The problem here is that levels(X.x) and levels(Xproduction.x) are different:","category":"page"},{"location":"working_with_categorical_data/#","page":"Working with Categorical Data","title":"Working with Categorical Data","text":"levels(X.x)","category":"page"},{"location":"working_with_categorical_data/#","page":"Working with Categorical Data","title":"Working with Categorical Data","text":"levels(Xproduction.x)","category":"page"},{"location":"working_with_categorical_data/#","page":"Working with Categorical Data","title":"Working with Categorical Data","text":"This could be anticipated by the fact that the training and production data have different schema:","category":"page"},{"location":"working_with_categorical_data/#","page":"Working with Categorical Data","title":"Working with Categorical Data","text":"schema(X)","category":"page"},{"location":"working_with_categorical_data/#","page":"Working with Categorical Data","title":"Working with Categorical Data","text":"schema(Xproduction)","category":"page"},{"location":"working_with_categorical_data/#","page":"Working with Categorical Data","title":"Working with Categorical Data","text":"One fix is to manually correct the levels of the production data:","category":"page"},{"location":"working_with_categorical_data/#","page":"Working with Categorical Data","title":"Working with Categorical Data","text":"levels!(Xproduction.x, levels(x))\ntransform(mach, Xproduction) == transform(mach, X[2:3,:])","category":"page"},{"location":"working_with_categorical_data/#","page":"Working with Categorical Data","title":"Working with Categorical Data","text":"Another solution is to pack all production data with dummy rows based on the training data (subsequently dropped) to ensure there are no missing classes. Currently MLJ contains no general tooling to check and fix categorical levels in production data (although one can check that training data and production data have the same schema, to ensure the number of classes in categorical data is consistent).","category":"page"},{"location":"working_with_categorical_data/#Extracting-an-integer-representation-of-Finite-data-1","page":"Working with Categorical Data","title":"Extracting an integer representation of Finite data","text":"","category":"section"},{"location":"working_with_categorical_data/#","page":"Working with Categorical Data","title":"Working with Categorical Data","text":"Occasionally, you may really want an integer representation of data that currently has scitype Finite. For example, you are developer wrapping an algorithm from an external package for use in MLJ, and that algorithm uses integer representations. Use the int method for this purpose, and use decoder to construct decoders for reversing the transformation:","category":"page"},{"location":"working_with_categorical_data/#","page":"Working with Categorical Data","title":"Working with Categorical Data","text":"v = coerce([\"one\", \"two\", \"three\", \"one\"], OrderedFactor);\nlevels!(v, [\"one\", \"two\", \"three\"]);\nv_int = int(v)","category":"page"},{"location":"working_with_categorical_data/#","page":"Working with Categorical Data","title":"Working with Categorical Data","text":"d = decoder(v); # or decoder(v[1])\nd.(v_int)","category":"page"},{"location":"working_with_categorical_data/#Under-the-hood:-CategoricalValue-and-CategoricalArray-1","page":"Working with Categorical Data","title":"Under the hood: CategoricalValue and CategoricalArray","text":"","category":"section"},{"location":"working_with_categorical_data/#","page":"Working with Categorical Data","title":"Working with Categorical Data","text":"In MLJ the objects with OrderedFactor or Multiclass scientific type have machine type CategoricalValue, from the CategoricalArrays.jl package. In some sense CategoricalValues are an implementation detail users can ignore for the most part, as shown above. However, you may want some basic understanding of these types, and those implementing MLJ's model interface for new algorithms will have to understand them. For the complete API, see the CategoricalArrays.jl documentation. Here are the basics:","category":"page"},{"location":"working_with_categorical_data/#","page":"Working with Categorical Data","title":"Working with Categorical Data","text":"To construct an OrderedFactor or Multiclass vector directly from raw labels, one uses categorical:","category":"page"},{"location":"working_with_categorical_data/#","page":"Working with Categorical Data","title":"Working with Categorical Data","text":"using CategoricalArrays # hide\nv = categorical(['A', 'B', 'A', 'A', 'C'])\ntypeof(v)","category":"page"},{"location":"working_with_categorical_data/#","page":"Working with Categorical Data","title":"Working with Categorical Data","text":"(Equivalent to the idiomatically MLJ v = coerce(['A', 'B', 'A', 'A', 'C']), Multiclass).)","category":"page"},{"location":"working_with_categorical_data/#","page":"Working with Categorical Data","title":"Working with Categorical Data","text":"scitype(v)","category":"page"},{"location":"working_with_categorical_data/#","page":"Working with Categorical Data","title":"Working with Categorical Data","text":"v = categorical(['A', 'B', 'A', 'A', 'C'], ordered=true, compress=true)","category":"page"},{"location":"working_with_categorical_data/#","page":"Working with Categorical Data","title":"Working with Categorical Data","text":"scitype(v)","category":"page"},{"location":"working_with_categorical_data/#","page":"Working with Categorical Data","title":"Working with Categorical Data","text":"When you index a CategoricalVector you don't get a raw label, but instead an instance of CategoricalValue. As explained above, this value knows the complete pool of levels from vector from which it came. Use get(val) to extract the raw label from a value val.","category":"page"},{"location":"working_with_categorical_data/#","page":"Working with Categorical Data","title":"Working with Categorical Data","text":"Despite the distinction that exists between a value (element) and a label, the two are the same, from the point of == and in:","category":"page"},{"location":"working_with_categorical_data/#","page":"Working with Categorical Data","title":"Working with Categorical Data","text":"v[1] == 'A' # true\n'A' in v    # true","category":"page"},{"location":"working_with_categorical_data/#Probabilistic-predictions-of-categorical-data-1","page":"Working with Categorical Data","title":"Probabilistic predictions of categorical data","text":"","category":"section"},{"location":"working_with_categorical_data/#","page":"Working with Categorical Data","title":"Working with Categorical Data","text":"Recall from Getting Started that probabilistic classfiers ordinarily predict UnivariateFinite distributions, not raw probabilities (which are instead accessed using the pdf method.) Here's how to construct such a distribution yourself:","category":"page"},{"location":"working_with_categorical_data/#","page":"Working with Categorical Data","title":"Working with Categorical Data","text":"v = coerce([\"yes\", \"no\", \"yes\", \"yes\", \"maybe\"], Multiclass)\nd = UnivariateFinite([v[1], v[2]], [0.9, 0.1])","category":"page"},{"location":"working_with_categorical_data/#","page":"Working with Categorical Data","title":"Working with Categorical Data","text":"Or, equivalently,","category":"page"},{"location":"working_with_categorical_data/#","page":"Working with Categorical Data","title":"Working with Categorical Data","text":"d = UnivariateFinite([\"no\", \"yes\"], [0.9, 0.1], pool=v)","category":"page"},{"location":"working_with_categorical_data/#","page":"Working with Categorical Data","title":"Working with Categorical Data","text":"This distribution tracks all levels, not just the ones to which you have assigned probabilities:","category":"page"},{"location":"working_with_categorical_data/#","page":"Working with Categorical Data","title":"Working with Categorical Data","text":"pdf(d, \"maybe\")","category":"page"},{"location":"working_with_categorical_data/#","page":"Working with Categorical Data","title":"Working with Categorical Data","text":"However, pdf(d, \"dunno\") will throw an error.","category":"page"},{"location":"working_with_categorical_data/#","page":"Working with Categorical Data","title":"Working with Categorical Data","text":"You can declare pool=missing, but then \"maybe\" will not be tracked:","category":"page"},{"location":"working_with_categorical_data/#","page":"Working with Categorical Data","title":"Working with Categorical Data","text":"d = UnivariateFinite([\"no\", \"yes\"], [0.9, 0.1], pool=missing)\nlevels(d)","category":"page"},{"location":"working_with_categorical_data/#","page":"Working with Categorical Data","title":"Working with Categorical Data","text":"To construct a whole vector of UnivariateFinite distributions, simply give the constructor a matrix of probabilities:","category":"page"},{"location":"working_with_categorical_data/#","page":"Working with Categorical Data","title":"Working with Categorical Data","text":"yes_probs = rand(5)\nprobs = hcat(1 .- yes_probs, yes_probs)\nd_vec = UnivariateFinite([\"no\", \"yes\"], probs, pool=v)","category":"page"},{"location":"working_with_categorical_data/#","page":"Working with Categorical Data","title":"Working with Categorical Data","text":"Or, equivalently:","category":"page"},{"location":"working_with_categorical_data/#","page":"Working with Categorical Data","title":"Working with Categorical Data","text":"d_vec = UnivariateFinite([\"no\", \"yes\"], yes_probs, augment=true, pool=v)","category":"page"},{"location":"working_with_categorical_data/#","page":"Working with Categorical Data","title":"Working with Categorical Data","text":"For more options, see UnivariateFinite.","category":"page"},{"location":"model_search/#Model-Search-1","page":"Model Search","title":"Model Search","text":"","category":"section"},{"location":"model_search/#","page":"Model Search","title":"Model Search","text":"MLJ has a model registry, allowing the user to search models and their properties, without loading all the packages containing model code. In turn, this allows one to efficiently find all models solving a given machine learning task. The task itself is specified with the help of the matching method, and the search executed with the models methods, as detailed below. ","category":"page"},{"location":"model_search/#","page":"Model Search","title":"Model Search","text":"A table of all models is also given at List of Supported Models.","category":"page"},{"location":"model_search/#Model-metadata-1","page":"Model Search","title":"Model metadata","text":"","category":"section"},{"location":"model_search/#","page":"Model Search","title":"Model Search","text":"Terminology. In this section the word \"model\" refers to a metadata entry in the model registry, as opposed to an actual model struct that such an entry represents. One can obtain such an entry with the info command:","category":"page"},{"location":"model_search/#","page":"Model Search","title":"Model Search","text":"using MLJ\nMLJ.color_off()","category":"page"},{"location":"model_search/#","page":"Model Search","title":"Model Search","text":"info(\"PCA\")","category":"page"},{"location":"model_search/#","page":"Model Search","title":"Model Search","text":"So a \"model\" in the present context is just a named tuple containing metadata, and not an actual model type or instance. If two models with the same name occur in different packages, the package name must be specified, as in info(\"LinearRegressor\", pkg=\"GLM\").","category":"page"},{"location":"model_search/#General-model-queries-1","page":"Model Search","title":"General model queries","text":"","category":"section"},{"location":"model_search/#","page":"Model Search","title":"Model Search","text":"We list all models (named tuples) using models(), and list the models for which code is  already loaded with localmodels():","category":"page"},{"location":"model_search/#","page":"Model Search","title":"Model Search","text":"localmodels()\nlocalmodels()[2]","category":"page"},{"location":"model_search/#","page":"Model Search","title":"Model Search","text":"One can search for models containing specified strings or regular expressions in their docstring attributes, as in","category":"page"},{"location":"model_search/#","page":"Model Search","title":"Model Search","text":"models(\"forest\")","category":"page"},{"location":"model_search/#","page":"Model Search","title":"Model Search","text":"or by specifying a filter (Bool-valued function):","category":"page"},{"location":"model_search/#","page":"Model Search","title":"Model Search","text":"filter(model) = model.is_supervised &&\n                model.input_scitype >: MLJ.Table(Continuous) &&\n                model.target_scitype >: AbstractVector{<:Multiclass{3}} &&\n                model.prediction_type == :deterministic\nmodels(filter)","category":"page"},{"location":"model_search/#","page":"Model Search","title":"Model Search","text":"Multiple test arguments may be passed to models, which are applied conjunctively.","category":"page"},{"location":"model_search/#Matching-models-to-data-1","page":"Model Search","title":"Matching models to data","text":"","category":"section"},{"location":"model_search/#","page":"Model Search","title":"Model Search","text":"Common searches are streamlined with the help of the matching command, defined as follows:","category":"page"},{"location":"model_search/#","page":"Model Search","title":"Model Search","text":"matching(model, X, y) == true exactly when model is supervised  and admits inputs and targets with the scientific types of X and  y, respectively\nmatching(model, X) == true exactly when model is unsupervised  and admits inputs with the scientific types of X.","category":"page"},{"location":"model_search/#","page":"Model Search","title":"Model Search","text":"So, to search for all supervised probabilistic models handling input X and target y, one can define the testing function task by","category":"page"},{"location":"model_search/#","page":"Model Search","title":"Model Search","text":"task(model) = matching(model, X, y) && model.prediction_type == :probabilistic","category":"page"},{"location":"model_search/#","page":"Model Search","title":"Model Search","text":"And execute the search with","category":"page"},{"location":"model_search/#","page":"Model Search","title":"Model Search","text":"models(task)","category":"page"},{"location":"model_search/#","page":"Model Search","title":"Model Search","text":"Also defined are Bool-valued callable objects matching(model), matching(X, y) and matching(X), with obvious behaviour. For example, matching(X, y)(model) = matching(model, X, y).","category":"page"},{"location":"model_search/#","page":"Model Search","title":"Model Search","text":"So, to search for all models compatible with input X and target y, for example, one executes","category":"page"},{"location":"model_search/#","page":"Model Search","title":"Model Search","text":"models(matching(X, y))","category":"page"},{"location":"model_search/#","page":"Model Search","title":"Model Search","text":"while the preceding search can also be written","category":"page"},{"location":"model_search/#","page":"Model Search","title":"Model Search","text":"models() do model\n    matching(model, X, y) &&\n    model.prediction_type == :probabilistic\nend","category":"page"},{"location":"model_search/#API-1","page":"Model Search","title":"API","text":"","category":"section"},{"location":"model_search/#","page":"Model Search","title":"Model Search","text":"models\nlocalmodels","category":"page"},{"location":"model_search/#MLJBase.models","page":"Model Search","title":"MLJBase.models","text":"models(N::AbstractNode)\n\nA vector of all models referenced by a node N, each model appearing exactly once.\n\n\n\n\n\nmodels()\n\nList all models in the MLJ registry. Here and below model means the registry metadata entry for a genuine model type (a proxy for types whose defining code may not be loaded).\n\nmodels(filters..)\n\nList all models m for which filter(m) is true, for each filter in filters.\n\nmodels(matching(X, y))\n\nList all supervised models compatible with training data X, y.\n\nmodels(matching(X))\n\nList all unsupervised models compatible with training data X.\n\nExcluded in the listings are the built-in model-wraps, like EnsembleModel, TunedModel, and IteratedModel.\n\nExample\n\nIf\n\ntask(model) = model.is_supervised && model.is_probabilistic\n\nthen models(task) lists all supervised models making probabilistic predictions.\n\nSee also: localmodels.\n\n\n\n\n\nmodels(needle::Union{AbstractString,Regex})\n\nList all models whole name or docstring matches a given needle.\n\n\n\n\n\n","category":"function"},{"location":"model_search/#MLJModels.localmodels","page":"Model Search","title":"MLJModels.localmodels","text":"localmodels(; modl=Main)\nlocalmodels(conditions...; modl=Main)\nlocalmodels(needle::Union{AbstractString,Regex}; modl=Main)\n\nList all models whose names are in the namespace of the specified module modl, or meeting the conditions, if specified. Here a condition is a Bool-valued function on models.\n\nSee also models\n\n\n\n\n\n","category":"function"},{"location":"simple_user_defined_models/#Simple-User-Defined-Models-1","page":"Simple User Defined Models","title":"Simple User Defined Models","text":"","category":"section"},{"location":"simple_user_defined_models/#","page":"Simple User Defined Models","title":"Simple User Defined Models","text":"To quickly implement a new supervised model in MLJ, it suffices to:","category":"page"},{"location":"simple_user_defined_models/#","page":"Simple User Defined Models","title":"Simple User Defined Models","text":"Define a mutable struct to store hyperparameters. This is either a subtype of Probabilistic or Deterministic, depending on whether probabilistic or ordinary point predictions are intended. This struct is the model.\nDefine a fit method, dispatched on the model, returning learned parameters, also known as the fitresult.\nDefine a predict method, dispatched on the model, and the fitresult, to return predictions on new patterns.","category":"page"},{"location":"simple_user_defined_models/#","page":"Simple User Defined Models","title":"Simple User Defined Models","text":"In the examples below, the training input X of fit, and the new input Xnew passed to predict, are tables. Each training target y is a AbstractVector.","category":"page"},{"location":"simple_user_defined_models/#","page":"Simple User Defined Models","title":"Simple User Defined Models","text":"The predictions returned by predict have the same form as y for deterministic models, but are Vectors of distributions for probabilistic models.","category":"page"},{"location":"simple_user_defined_models/#","page":"Simple User Defined Models","title":"Simple User Defined Models","text":"Advanced model functionality not addressed here includes: (i) optional update method to avoid redundant calculations when calling fit! on machines a second time; (ii) reporting extra training-related statistics; (iii) exposing model-specific functionality; (iv) checking the scientific type of data passed to your model in machine construction; and (iv) checking validity of hyperparameter values. All this is described in Adding Models for General Use.","category":"page"},{"location":"simple_user_defined_models/#","page":"Simple User Defined Models","title":"Simple User Defined Models","text":"For an unsupervised model, implement transform and, optionally, inverse_transform using the same signature at predict below.","category":"page"},{"location":"simple_user_defined_models/#A-simple-deterministic-regressor-1","page":"Simple User Defined Models","title":"A simple deterministic regressor","text":"","category":"section"},{"location":"simple_user_defined_models/#","page":"Simple User Defined Models","title":"Simple User Defined Models","text":"Here's a quick-and-dirty implementation of a ridge regressor with no intercept:","category":"page"},{"location":"simple_user_defined_models/#","page":"Simple User Defined Models","title":"Simple User Defined Models","text":"import MLJBase\nusing LinearAlgebra\n\nmutable struct MyRegressor <: MLJBase.Deterministic\n    lambda::Float64\nend\nMyRegressor(; lambda=0.1) = MyRegressor(lambda)\n\n# fit returns coefficients minimizing a penalized rms loss function:\nfunction MLJBase.fit(model::MyRegressor, verbosity, X, y)\n    x = MLJBase.matrix(X)                     # convert table to matrix\n    fitresult = (x'x + model.lambda*I)\\(x'y)  # the coefficients\n    cache=nothing\n    report=nothing\n    return fitresult, cache, report\nend\n\n# predict uses coefficients to make new prediction:\nMLJBase.predict(::MyRegressor, fitresult, Xnew) = MLJBase.matrix(Xnew) * fitresult","category":"page"},{"location":"simple_user_defined_models/#","page":"Simple User Defined Models","title":"Simple User Defined Models","text":"using MLJ\nimport MLJBase\nusing LinearAlgebra\nMLJBase.color_off()\nmutable struct MyRegressor <: MLJBase.Deterministic\n    lambda::Float64\nend\nMyRegressor(; lambda=0.1) = MyRegressor(lambda)\nfunction MLJBase.fit(model::MyRegressor, verbosity, X, y)\n    x = MLJBase.matrix(X)\n    fitresult = (x'x + model.lambda*I)\\(x'y)\n    cache=nothing\n    report=nothing\n    return fitresult, cache, report\nend\nMLJBase.predict(::MyRegressor, fitresult, Xnew) = MLJBase.matrix(Xnew) * fitresult","category":"page"},{"location":"simple_user_defined_models/#","page":"Simple User Defined Models","title":"Simple User Defined Models","text":"After loading this code, all MLJ's basic meta-algorithms can be applied to MyRegressor:","category":"page"},{"location":"simple_user_defined_models/#","page":"Simple User Defined Models","title":"Simple User Defined Models","text":"X, y = @load_boston;\nmodel = MyRegressor(lambda=1.0)\nregressor = machine(model, X, y)\nevaluate!(regressor, resampling=CV(), measure=rms, verbosity=0)\n","category":"page"},{"location":"simple_user_defined_models/#A-simple-probabilistic-classifier-1","page":"Simple User Defined Models","title":"A simple probabilistic classifier","text":"","category":"section"},{"location":"simple_user_defined_models/#","page":"Simple User Defined Models","title":"Simple User Defined Models","text":"The following probabilistic model simply fits a probability distribution to the MultiClass training target (i.e., ignores X) and returns this pdf for any new pattern:","category":"page"},{"location":"simple_user_defined_models/#","page":"Simple User Defined Models","title":"Simple User Defined Models","text":"import MLJBase\nimport Distributions\n\nstruct MyClassifier <: MLJBase.Probabilistic\nend\n\n# `fit` ignores the inputs X and returns the training target y\n# probability distribution:\nfunction MLJBase.fit(model::MyClassifier, verbosity, X, y)\n    fitresult = Distributions.fit(MLJBase.UnivariateFinite, y)\n    cache = nothing\n    report = nothing\n    return fitresult, cache, report\nend\n\n# `predict` returns the passed fitresult (pdf) for all new patterns:\nMLJBase.predict(model::MyClassifier, fitresult, Xnew) =\n    [fitresult for r in 1:nrows(Xnew)]","category":"page"},{"location":"simple_user_defined_models/#","page":"Simple User Defined Models","title":"Simple User Defined Models","text":"julia> X, y = @load_iris\njulia> mach = fit!(machine(MyClassifier(), X, y))\njulia> predict(mach, selectrows(X, 1:2))\n2-element Array{UnivariateFinite{String,UInt32,Float64},1}:\n UnivariateFinite(setosa=>0.333, versicolor=>0.333, virginica=>0.333)\n UnivariateFinite(setosa=>0.333, versicolor=>0.333, virginica=>0.333)","category":"page"},{"location":"list_of_supported_models/#model_list-1","page":"List of Supported Models","title":"List of Supported Models","text":"","category":"section"},{"location":"list_of_supported_models/#","page":"List of Supported Models","title":"List of Supported Models","text":"MLJ provides access to to a wide variety of machine learning models. We are always looking for help adding new models or testing existing ones.  Currently available models are listed below; for the most up-to-date list, run using MLJ; models().","category":"page"},{"location":"list_of_supported_models/#","page":"List of Supported Models","title":"List of Supported Models","text":"experimental: indicates the package is fairly new and/or is under active development; you can help by testing these packages and making them more robust,\nmedium: indicates the package is fairly mature but may benefit from optimisations and/or extra features; you can help by suggesting either,\nhigh: indicates the package is very mature and functionalities are expected to have been fairly optimised and tested.","category":"page"},{"location":"list_of_supported_models/#","page":"List of Supported Models","title":"List of Supported Models","text":"Package Models Maturity Note\nClustering.jl KMeans, KMedoids high †\nDecisionTree.jl DecisionTreeClassifier, DecisionTreeRegressor, AdaBoostStumpClassifier, RandomForestClassifier, RandomForestRegressor high \nEvoTrees.jl EvoTreeRegressor, EvoTreeClassifier, EvoTreeCount, EvoTreeGaussian medium gradient boosting models\nGLM.jl LinearRegressor, LinearBinaryClassifier, LinearCountRegressor medium †\nLIBSVM.jl LinearSVC, SVC, NuSVC, NuSVR, EpsilonSVR, OneClassSVM high also via ScikitLearn.jl\nLightGBM.jl LightGBMClassifier, LightGBMRegressor high \nMLJFlux.jl NeuralNetworkRegressor, NeuralNetworkClassifier, MultitargetNeuralNetworkRegressor, ImageClassifier experimental \nMLJLinearModels.jl LinearRegressor, RidgeRegressor, LassoRegressor, ElasticNetRegressor, QuantileRegressor, HuberRegressor, RobustRegressor, LADRegressor, LogisticClassifier, MultinomialClassifier experimental \nMLJModels.jl (built-in) StaticTransformer, FeatureSelector, FillImputer, UnivariateStandardizer, Standardizer, UnivariateBoxCoxTransformer, OneHotEncoder, ContinuousEncoder, ConstantRegressor, ConstantClassifier, BinaryThreshholdPredictor medium \nMultivariateStats.jl LinearRegressor, RidgeRegressor, PCA, KernelPCA, ICA, LDA, BayesianLDA, SubspaceLDA, BayesianSubspaceLDA, FactorAnalysis, PPCA high \nNaiveBayes.jl GaussianNBClassifier, MultinomialNBClassifier, HybridNBClassifier experimental \nNearestNeighbors.jl KNNClassifier, KNNRegressor high \nParallelKMeans.jl KMeans experimental \nPartialLeastSquaresRegressor.jl PLSRegressor, KPLSRegressor experimental \nScikitLearn.jl ARDRegressor, AdaBoostClassifier, AdaBoostRegressor, AffinityPropagation, AgglomerativeClustering, BaggingClassifier, BaggingRegressor, BayesianLDA, BayesianQDA, BayesianRidgeRegressor, BernoulliNBClassifier, Birch, ComplementNBClassifier, DBSCAN, DummyClassifier, DummyRegressor, ElasticNetCVRegressor, ElasticNetRegressor, ExtraTreesClassifier, ExtraTreesRegressor, FeatureAgglomeration, GaussianNBClassifier, GaussianProcessClassifier, GaussianProcessRegressor, GradientBoostingClassifier, GradientBoostingRegressor, HuberRegressor, KMeans, KNeighborsClassifier, KNeighborsRegressor, LarsCVRegressor, LarsRegressor, LassoCVRegressor, LassoLarsCVRegressor, LassoLarsICRegressor, LassoLarsRegressor, LassoRegressor, LinearRegressor, LogisticCVClassifier, LogisticClassifier, MeanShift, MiniBatchKMeans, MultiTaskElasticNetCVRegressor, MultiTaskElasticNetRegressor, MultiTaskLassoCVRegressor, MultiTaskLassoRegressor, MultinomialNBClassifier, OPTICS, OrthogonalMatchingPursuitCVRegressor, OrthogonalMatchingPursuitRegressor, PassiveAggressiveClassifier, PassiveAggressiveRegressor, PerceptronClassifier, ProbabilisticSGDClassifier, RANSACRegressor, RandomForestClassifier, RandomForestRegressor, RidgeCVClassifier, RidgeCVRegressor, RidgeClassifier, RidgeRegressor, SGDClassifier, SGDRegressor, SVMClassifier, SVMLClassifier, SVMLRegressor, SVMNuClassifier, SVMNuRegressor, SVMRegressor, SpectralClustering, TheilSenRegressor high †\nXGBoost.jl XGBoostRegressor, XGBoostClassifier, XGBoostCount high ","category":"page"},{"location":"list_of_supported_models/#","page":"List of Supported Models","title":"List of Supported Models","text":"Note (†): Some models are missing and assistance is welcome to complete the interface. Post a message on the Julia #mlj Slack channel if you would like to help, thanks!","category":"page"},{"location":"benchmarking/#Benchmarking-1","page":"Benchmarking","title":"Benchmarking","text":"","category":"section"},{"location":"benchmarking/#","page":"Benchmarking","title":"Benchmarking","text":"This feature not yet available.","category":"page"},{"location":"benchmarking/#","page":"Benchmarking","title":"Benchmarking","text":"CONTRIBUTE.md","category":"page"},{"location":"composing_models/#Composing-Models-1","page":"Composing Models","title":"Composing Models","text":"","category":"section"},{"location":"composing_models/#","page":"Composing Models","title":"Composing Models","text":"MLJ has a flexible interface for composing multiple machine learning elements to form a learning network, whose complexity can extend beyond the \"pipelines\" of other machine learning toolboxes. While these learning networks can be applied directly to learning tasks, they are more commonly used to specify new re-usable, stand-alone, composite model types, that behave like any other model type. The main novelty of composite models is that they include other models as hyper-parameters.","category":"page"},{"location":"composing_models/#","page":"Composing Models","title":"Composing Models","text":"That said, MLJ also provides dedicated syntax for the most common composition use-cases, which are described first below. A description of the general framework begins at Learning Networks.","category":"page"},{"location":"composing_models/#Linear-pipelines-1","page":"Composing Models","title":"Linear pipelines","text":"","category":"section"},{"location":"composing_models/#","page":"Composing Models","title":"Composing Models","text":"In MLJ a pipeline is a composite model in which models are chained together in a linear (non-branching) chain. Pipelines can include learned or static target transformations, if one of the models is supervised.","category":"page"},{"location":"composing_models/#","page":"Composing Models","title":"Composing Models","text":"To illustrate basic construction of a pipeline, consider the following toy data:","category":"page"},{"location":"composing_models/#","page":"Composing Models","title":"Composing Models","text":"using MLJ\nMLJ.color_off()","category":"page"},{"location":"composing_models/#","page":"Composing Models","title":"Composing Models","text":"using MLJ\nX = (age    = [23, 45, 34, 25, 67],\n     gender = categorical(['m', 'm', 'f', 'm', 'f']));\nheight = [67.0, 81.5, 55.6, 90.0, 61.1]\nnothing # hide","category":"page"},{"location":"composing_models/#","page":"Composing Models","title":"Composing Models","text":"The code below defines a new model type, and an instance of that type called pipe, for performing the following operations:","category":"page"},{"location":"composing_models/#","page":"Composing Models","title":"Composing Models","text":"standardize the target variable :height to have mean zero and standard deviation one\ncoerce the :age field to have Continuous scitype\none-hot encode the categorical feature :gender\ntrain a K-nearest neighbor model on the transformed inputs and transformed target\nrestore the predictions of the KNN model to the original :height scale (i.e., invert the standardization)","category":"page"},{"location":"composing_models/#","page":"Composing Models","title":"Composing Models","text":"@load KNNRegressor","category":"page"},{"location":"composing_models/#","page":"Composing Models","title":"Composing Models","text":"pipe = @pipeline(X -> coerce(X, :age=>Continuous),\n                 OneHotEncoder,\n                 KNNRegressor(K=3),\n                 target = UnivariateStandardizer())\n\nPipeline406(\n    one_hot_encoder = OneHotEncoder(\n            features = Symbol[],\n            drop_last = false,\n            ordered_factor = true,\n            ignore = false),\n    knn_regressor = KNNRegressor(\n            K = 3,\n            algorithm = :kdtree,\n            metric = Distances.Euclidean(0.0),\n            leafsize = 10,\n            reorder = true,\n            weights = :uniform),\n    target = UnivariateStandardizer()) @719\n","category":"page"},{"location":"composing_models/#","page":"Composing Models","title":"Composing Models","text":"Notice that field names for the composite are automatically generated based on the component model type names. The automatically generated name of the new model composite model type, Pipeline406, can be replaced with a user-defined one by specifying, say, name=MyPipe. If you are planning on serializing (saving) a pipeline-machine, you will need to specify a name..","category":"page"},{"location":"composing_models/#","page":"Composing Models","title":"Composing Models","text":"The new model can be used just like any other non-composite model:","category":"page"},{"location":"composing_models/#","page":"Composing Models","title":"Composing Models","text":"pipe.knn_regressor.K = 2\npipe.one_hot_encoder.drop_last = true\nevaluate(pipe, X, height, resampling=Holdout(), measure=l2, verbosity=2)\n\n[ Info: Training Machine{Pipeline406} @959.\n[ Info: Training Machine{UnivariateStandardizer} @422.\n[ Info: Training Machine{OneHotEncoder} @745.\n[ Info: Spawning 1 sub-features to one-hot encode feature :gender.\n[ Info: Training Machine{KNNRegressor} @005.\n┌───────────┬───────────────┬────────────┐\n│ _.measure │ _.measurement │ _.per_fold │\n├───────────┼───────────────┼────────────┤\n│ l2        │ 55.5          │ [55.5]     │\n└───────────┴───────────────┴────────────┘\n_.per_observation = [[[55.502499999999934]]]\n","category":"page"},{"location":"composing_models/#","page":"Composing Models","title":"Composing Models","text":"For important details on including target transformations, see below.","category":"page"},{"location":"composing_models/#","page":"Composing Models","title":"Composing Models","text":"@pipeline","category":"page"},{"location":"composing_models/#MLJBase.@pipeline","page":"Composing Models","title":"MLJBase.@pipeline","text":"@pipeline model1 model2 ... modelk\n\nCreate an instance of an automatically generated composite model type, in which the specified models are composed in order. This means model1 receives inputs, whose output is passed to model2, and so forth. Model types or instances may be specified.\n\nImportant. By default a new model type name is automatically generated. To specify a different name add a keyword argument such as name=MyPipeType. This is necessary if serializing the pipeline; see MLJ.save.\n\nAt most one of the models may be a supervised model, but this model can appear in any position.\n\nThe @pipeline macro accepts several key-word arguments discussed further below.\n\nStatic (unlearned) transformations - that is, ordinary functions - may also be inserted in the pipeline as shown in the following example:\n\n@pipeline X->coerce(X, :age=>Continuous) OneHotEncoder ConstantClassifier\n\nTarget transformation and inverse transformation\n\nA learned target transformation (such as standardization) can also be specified, using the key-word target, provided the transformer provides an inverse_transform method:\n\n@pipeline OneHotEncoder KNNRegressor target=UnivariateTransformer\n\nA static transformation can be specified instead, but then an inverse must also be given:\n\n@pipeline(OneHotEncoder, KNNRegressor,\n          target = v -> log.(v),\n          inverse = v -> exp.(v))\n\nImportant. By default, the target inversion is applied immediately  following the (unique) supervised model in the pipeline. To apply  at the end of the pipeline, specify invert_last=true.\n\nOptional key-word arguments\n\ntarget=...  -  any Unsupervised model or Function\ninverse=...  -  any Function (unspecified if target is Unsupervised)\ninvert_last  -  set to true to delay target inversion to end of pipeline (default=true)\nprediction_type  -  prediction type of the pipeline; possible values: :deterministic, :probabilistic, :interval (default=:deterministic if not inferable)\noperation - operation applied to the supervised component model, when present; possible values: predict, predict_mean, predict_median, predict_mode (default=predict)\nname - new composite model type name; can be any name not already in current global namespace (autogenerated by default(\n\nSee also: @from_network\n\n\n\n\n\n","category":"macro"},{"location":"composing_models/#Homogeneous-Ensembles-1","page":"Composing Models","title":"Homogeneous Ensembles","text":"","category":"section"},{"location":"composing_models/#","page":"Composing Models","title":"Composing Models","text":"For performance reasons, creating a large ensemble of models sharing a common set of hyperparameters is achieved in MLJ through a model wrapper, rather than through the learning networks API. See the separate Homogeneous Ensembles section for details.","category":"page"},{"location":"composing_models/#Learning-Networks-1","page":"Composing Models","title":"Learning Networks","text":"","category":"section"},{"location":"composing_models/#","page":"Composing Models","title":"Composing Models","text":"Hand-crafting a learning network, as outlined below, is a relatively advanced MLJ feature, assuming familiarity with the basics outlined in Getting Started. The syntax for building a learning network is essentially an extension of the basic syntax but with data containers replaced with nodes (\"dynamic data\").","category":"page"},{"location":"composing_models/#","page":"Composing Models","title":"Composing Models","text":"In MLJ, a learning network is a directed acyclic graph whose nodes apply an operation, such as predict or transform, using a fixed machine (requiring training) - or which, alternatively, applies a regular (untrained) mathematical operation, such as +, log or vcat, to its input(s). In practice, a learning network works with fixed sources for its training/evaluation data, but can be built and tested in stages. By contrast, an exported learning network is a learning network exported as a stand-alone, re-usable Model object, to which all the MLJ Model meta-algorithms can be applied (ensembling, systematic tuning, etc).","category":"page"},{"location":"composing_models/#","page":"Composing Models","title":"Composing Models","text":"Different nodes can point to the same machine (i.e., can access a common set of learned parameters) and different machines can wrap a common model (allowing for hyperparameters in different machines to be coupled).","category":"page"},{"location":"composing_models/#","page":"Composing Models","title":"Composing Models","text":"By specifying data at the source nodes of a learning network, one can use and test the learning network as it is defined, which is also a good way to understand how learning networks work under the hood. This data, if specified, is ignored in the export process, for the exported composite model, like any other model, is not associated with any data until wrapped in a machine.","category":"page"},{"location":"composing_models/#","page":"Composing Models","title":"Composing Models","text":"In MLJ learning networks treat the flow of information during training and prediction/transforming separately. Also, different nodes may use the same parameters (fitresult) learned during the training of some model (that is, point to a common machine; see below). For these reasons, simple examples may appear more slightly more complicated than in other frameworks. However, in more sophisticated applications, the extra flexibility is essential.","category":"page"},{"location":"composing_models/#Building-a-simple-learning-network-1","page":"Composing Models","title":"Building a simple learning network","text":"","category":"section"},{"location":"composing_models/#","page":"Composing Models","title":"Composing Models","text":"(Image: )","category":"page"},{"location":"composing_models/#","page":"Composing Models","title":"Composing Models","text":"The diagram above depicts a learning network which standardizes the input data X, learns an optimal Box-Cox transformation for the target y, predicts new target values using ridge regression, and then inverse-transforms those predictions, for later comparison with the original test data. The machines, labeled in yellow, are where data to be used for training enters a node, and where training outcomes are stored, as in the basic fit/predict scenario.","category":"page"},{"location":"composing_models/#","page":"Composing Models","title":"Composing Models","text":"Looking ahead, we note that the new composite model type we will create later will be assigned a single hyperparameter regressor, and the learning network model RidgeRegressor(lambda=0.1) will become this parameter's default value. Since model hyperparameters are mutable, this regressor can be changed to a different one (e.g., HuberRegressor()).","category":"page"},{"location":"composing_models/#","page":"Composing Models","title":"Composing Models","text":"For testing purposes, we'll use a small synthetic data set:","category":"page"},{"location":"composing_models/#","page":"Composing Models","title":"Composing Models","text":"using Statistics\nimport DataFrames\n\nx1 = rand(300)\nx2 = rand(300)\nx3 = rand(300)\ny = exp.(x1 - x2 -2x3 + 0.1*rand(300))\nX = DataFrames.DataFrame(x1=x1, x2=x2, x3=x3)\n\ntrain, test  = partition(eachindex(y), 0.8)","category":"page"},{"location":"composing_models/#","page":"Composing Models","title":"Composing Models","text":"Step one is to wrap the data in source nodes:","category":"page"},{"location":"composing_models/#","page":"Composing Models","title":"Composing Models","text":"Xs = source(X)\nys = source(y)","category":"page"},{"location":"composing_models/#","page":"Composing Models","title":"Composing Models","text":"Note. One can omit the specification of data at the source nodes (by writing instead Xs = source() and ys = source()) and still export the resulting network as a stand-alone model using the @from_network macro described later; see the example under Static operations on nodes. However, one will be unable to fit or call network nodes, as illustrated below.","category":"page"},{"location":"composing_models/#","page":"Composing Models","title":"Composing Models","text":"The contents of a source node can be recovered by simply calling the node with no arguments:","category":"page"},{"location":"composing_models/#","page":"Composing Models","title":"Composing Models","text":"julia> ys()[1:2]\n2-element Array{Float64,1}:\n 0.12350299813414874\n 0.29425920370829295","category":"page"},{"location":"composing_models/#","page":"Composing Models","title":"Composing Models","text":"We label the nodes that we will define according to their outputs in the diagram. Notice that the nodes z and yhat use the same machine, namely box, for different operations.","category":"page"},{"location":"composing_models/#","page":"Composing Models","title":"Composing Models","text":"To construct the W node we first need to define the machine stand that it will use to transform inputs.","category":"page"},{"location":"composing_models/#","page":"Composing Models","title":"Composing Models","text":"stand_model = Standardizer()\nstand = machine(stand_model, Xs)","category":"page"},{"location":"composing_models/#","page":"Composing Models","title":"Composing Models","text":"Because Xs is a node, instead of concrete data, we can call transform on the machine without first training it, and the result is the new node W, instead of concrete transformed data:","category":"page"},{"location":"composing_models/#","page":"Composing Models","title":"Composing Models","text":"julia> W = transform(stand, Xs)\nNode{Machine{Standardizer}} @325\n  args:\n    1:  Source @085\n  formula:\n    transform(\n        Machine{Standardizer} @709,\n        Source @085)","category":"page"},{"location":"composing_models/#","page":"Composing Models","title":"Composing Models","text":"To get actual transformed data we call the node appropriately, which will require we first train the node. Training a node, rather than a machine, triggers training of all necessary machines in the network.","category":"page"},{"location":"composing_models/#","page":"Composing Models","title":"Composing Models","text":"fit!(W, rows=train)\nW()           # transform all data\nW(rows=test ) # transform only test data\nW(X[3:4,:])   # transform any data, new or old","category":"page"},{"location":"composing_models/#","page":"Composing Models","title":"Composing Models","text":"2×3 DataFrame\n│ Row │ x1       │ x2        │ x3        │\n│     │ Float64  │ Float64   │ Float64   │\n├─────┼──────────┼───────────┼───────────┤\n│ 1   │ 0.113486 │ 0.732189  │ 1.4783    │\n│ 2   │ 0.783227 │ -0.425371 │ -0.113503 │","category":"page"},{"location":"composing_models/#","page":"Composing Models","title":"Composing Models","text":"If you like, you can think of W (and the other nodes we will define) as \"dynamic data\": W is data, in the sense that it an be called (\"indexed\") on rows, but dynamic, in the sense the result depends on the outcome of training events.","category":"page"},{"location":"composing_models/#","page":"Composing Models","title":"Composing Models","text":"The other nodes of our network are defined similarly:","category":"page"},{"location":"composing_models/#","page":"Composing Models","title":"Composing Models","text":"@load RidgeRegressor pkg=MultivariateStats\n\nbox_model = UnivariateBoxCoxTransformer()  # for making data look normally-distributed\nbox = machine(box_model, ys)\nz = transform(box, ys)\n\nridge_model = RidgeRegressor(lambda=0.1)\nridge =machine(ridge_model, W, z)\nzhat = predict(ridge, W)\n\nyhat = inverse_transform(box, zhat)\n","category":"page"},{"location":"composing_models/#","page":"Composing Models","title":"Composing Models","text":"We are ready to train and evaluate the completed network. Notice that the standardizer, stand, is not retrained, as MLJ remembers that it was trained earlier:","category":"page"},{"location":"composing_models/#","page":"Composing Models","title":"Composing Models","text":"fit!(yhat, rows=train)","category":"page"},{"location":"composing_models/#","page":"Composing Models","title":"Composing Models","text":"[ Info: Not retraining Machine{Standardizer} @ 6…82. It is up-to-date.\n[ Info: Training Machine{UnivariateBoxCoxTransformer} @ 1…09.\n[ Info: Training Machine{RidgeRegressor} @ 2…66.","category":"page"},{"location":"composing_models/#","page":"Composing Models","title":"Composing Models","text":"rms(y[test], yhat(rows=test)) # evaluate","category":"page"},{"location":"composing_models/#","page":"Composing Models","title":"Composing Models","text":"0.022837595088079567","category":"page"},{"location":"composing_models/#","page":"Composing Models","title":"Composing Models","text":"We can change a hyperparameters and retrain:","category":"page"},{"location":"composing_models/#","page":"Composing Models","title":"Composing Models","text":"ridge_model.lambda = 0.01\nfit!(yhat, rows=train)","category":"page"},{"location":"composing_models/#","page":"Composing Models","title":"Composing Models","text":"[ Info: Not retraining Machine{UnivariateBoxCoxTransformer} @ 1…09. It is up-to-date.\n[ Info: Not retraining Machine{Standardizer} @ 6…82. It is up-to-date.\n[ Info: Updating Machine{RidgeRegressor} @ 2…66.\nNode @ 1…07 = inverse_transform(1…09, predict(2…66, transform(6…82, 3…40)))","category":"page"},{"location":"composing_models/#","page":"Composing Models","title":"Composing Models","text":"And re-evaluate:","category":"page"},{"location":"composing_models/#","page":"Composing Models","title":"Composing Models","text":"rms(y[test], yhat(rows=test))\n0.039410306910269116","category":"page"},{"location":"composing_models/#","page":"Composing Models","title":"Composing Models","text":"Notable feature. The machine, ridge::Machine{RidgeRegressor}, is retrained, because its underlying model has been mutated. However, since the outcome of this training has no effect on the training inputs of the machines stand and box, these transformers are left untouched. (During construction, each node and machine in a learning network determines and records all machines on which it depends.) This behavior, which extends to exported learning networks, means we can tune our wrapped regressor (using a holdout set) without re-computing transformations each time the hyperparameter is changed.","category":"page"},{"location":"composing_models/#Learning-network-machines-1","page":"Composing Models","title":"Learning network machines","text":"","category":"section"},{"location":"composing_models/#","page":"Composing Models","title":"Composing Models","text":"As we show next, a learning network needs to be exported to create a new stand-alone model type. Instances of that type can be bound with data in a machine, which can then be evaluated, for example. Somewhat paradoxically, one can wrap a learning network in a certain kind of machine, called a learning network machine, before exporting it, and in fact, the export process actually requires us to do so. Since a composite model type does not yet exist, one constructs the machine using a \"surrogate\" model, whose name indicates the ultimate model supertype (Deterministic, Probabilistic, Unsupervised or Static). This surrogate model has no fields.","category":"page"},{"location":"composing_models/#","page":"Composing Models","title":"Composing Models","text":"Continuing with the example above:","category":"page"},{"location":"composing_models/#","page":"Composing Models","title":"Composing Models","text":"julia> surrogate = Deterministic()\nDeterministicSurrogate() @047\n\nmach = machine(surrogate, Xs, ys; predict=yhat)","category":"page"},{"location":"composing_models/#","page":"Composing Models","title":"Composing Models","text":"Notice that a key-word argument declares which node is for making predictions, and the arguments Xs and ys declare which source nodes receive the input and target data. With mach constructed in this way, the code","category":"page"},{"location":"composing_models/#","page":"Composing Models","title":"Composing Models","text":"fit!(mach)\npredict(mach, X[test,:])","category":"page"},{"location":"composing_models/#","page":"Composing Models","title":"Composing Models","text":"is equivalent to","category":"page"},{"location":"composing_models/#","page":"Composing Models","title":"Composing Models","text":"fit!(yhat)\nyhat(X[test,:])","category":"page"},{"location":"composing_models/#","page":"Composing Models","title":"Composing Models","text":"While it's main purpose is for export (see below), this machine can actually be evaluated:","category":"page"},{"location":"composing_models/#","page":"Composing Models","title":"Composing Models","text":"evaluate!(mach, resampling=CV(nfolds=3), measure=l2)","category":"page"},{"location":"composing_models/#","page":"Composing Models","title":"Composing Models","text":"For more on constructing learning network machines, see machine.","category":"page"},{"location":"composing_models/#Exporting-a-learning-network-as-a-stand-alone-model-1","page":"Composing Models","title":"Exporting a learning network as a stand-alone model","text":"","category":"section"},{"location":"composing_models/#","page":"Composing Models","title":"Composing Models","text":"Having satisfied that our learning network works on the synthetic data, we are ready to export it as a stand-alone model.","category":"page"},{"location":"composing_models/#Method-I:-The-@from_network-macro-1","page":"Composing Models","title":"Method I: The @from_network macro","text":"","category":"section"},{"location":"composing_models/#","page":"Composing Models","title":"Composing Models","text":"Having defined a learning network machine, mach, as above, the following code defines a new model subtype WrappedRegressor <: Supervised with a single field regressor:","category":"page"},{"location":"composing_models/#","page":"Composing Models","title":"Composing Models","text":"@from_network mach begin\n    mutable struct WrappedRegressor\n        regressor=ridge_model\n    end\nend","category":"page"},{"location":"composing_models/#","page":"Composing Models","title":"Composing Models","text":"Note the declaration of the default value ridge_model, which must refer to an actual model appearing in the learning network. It can be typed, as in the alternative declaration below, which also declares some traits for the type (as shown by info(WrappedRegressor); see also Trait declarations).","category":"page"},{"location":"composing_models/#","page":"Composing Models","title":"Composing Models","text":"@from_network mach begin\n    mutable struct WrappedRegressor\n        regressor::Deterministic=ridge_model\n    end\n    input_scitype = Table(Continuous,Finite)\n    target_scitype = AbstractVector{<:Continuous}\nend\n","category":"page"},{"location":"composing_models/#","page":"Composing Models","title":"Composing Models","text":"We can now create an instance of this type and apply the meta-algorithms that apply to any MLJ model:","category":"page"},{"location":"composing_models/#","page":"Composing Models","title":"Composing Models","text":"julia> composite = WrappedRegressor()\nWrappedRegressor(\n    regressor = RidgeRegressor(\n            lambda = 0.01))\n\nX, y = @load_boston;\nevaluate(composite, X, y, resampling=CV(), measure=l2, verbosity=0)","category":"page"},{"location":"composing_models/#","page":"Composing Models","title":"Composing Models","text":"Since our new type is mutable, we can swap the RidgeRegressor out for any other regressor:","category":"page"},{"location":"composing_models/#","page":"Composing Models","title":"Composing Models","text":"@load KNNRegressor\ncomposite.regressor = KNNRegressor(K=7)\njulia> composite\nWrappedRegressor(regressor = KNNRegressor(K = 7,\n                                          algorithm = :kdtree,\n                                          metric = Distances.Euclidean(0.0),\n                                          leafsize = 10,\n                                          reorder = true,\n                                          weights = :uniform,),) @ 2…63","category":"page"},{"location":"composing_models/#Method-II:-Finer-control-(advanced)-1","page":"Composing Models","title":"Method II: Finer control (advanced)","text":"","category":"section"},{"location":"composing_models/#","page":"Composing Models","title":"Composing Models","text":"This section describes an advanced feature that can be skipped on a first reading.","category":"page"},{"location":"composing_models/#","page":"Composing Models","title":"Composing Models","text":"In Method I above, only models appearing in the network will appear as hyperparameters of the exported composite model. There is a second more flexible method for exporting the network, which allows finer control over the exported Model struct, and which also avoids macros. The two steps required are:","category":"page"},{"location":"composing_models/#","page":"Composing Models","title":"Composing Models","text":"Define a new mutable struct model type.\nWrap the learning network code in a model fit method.","category":"page"},{"location":"composing_models/#","page":"Composing Models","title":"Composing Models","text":"Let's start with an elementary illustration in the learning network we just exported using Method I.","category":"page"},{"location":"composing_models/#","page":"Composing Models","title":"Composing Models","text":"The mutable struct definition looks like this:","category":"page"},{"location":"composing_models/#","page":"Composing Models","title":"Composing Models","text":"mutable struct WrappedRegressor2 <: DeterministicComposite\n    regressor\nend\n\n# keyword constructor\nWrappedRegressor2(; regressor=RidgeRegressor()) = WrappedRegressor2(regressor)\nnothing #hide","category":"page"},{"location":"composing_models/#","page":"Composing Models","title":"Composing Models","text":"The other supertype options are ProbabilisticComposite, IntervalComposite, UnsupervisedComposite and StaticComposite.","category":"page"},{"location":"composing_models/#","page":"Composing Models","title":"Composing Models","text":"We now simply cut and paste the code defining the learning network into a model fit method (as opposed to a machine fit! method):","category":"page"},{"location":"composing_models/#","page":"Composing Models","title":"Composing Models","text":"function MLJ.fit(model::WrappedRegressor2, verbosity::Integer, X, y)\n    Xs = source(X)\n    ys = source(y)\n\n    stand_model = Standardizer()\n    stand = machine(stand_model, Xs)\n    W = transform(stand, Xs)\n\n    box_model = UnivariateBoxCoxTransformer()\n    box = machine(box_model, ys)\n    z = transform(box, ys)\n\n    ridge_model = model.regressor        ###\n    ridge =machine(ridge_model, W, z)\n    zhat = predict(ridge, W)\n\n    yhat = inverse_transform(box, zhat)\n\n    mach = machine(Deterministic(), Xs, ys; predict=yhat)\n    return!(mach, model, verbosity)\nend","category":"page"},{"location":"composing_models/#","page":"Composing Models","title":"Composing Models","text":"This completes the export process.","category":"page"},{"location":"composing_models/#","page":"Composing Models","title":"Composing Models","text":"Notes:","category":"page"},{"location":"composing_models/#","page":"Composing Models","title":"Composing Models","text":"The line marked ###, where the new exported model's hyperparameter regressor is spliced into the network, is the only modification to the previous code.\nAfter defining the network there is the additional step of constructing and fitting a learning network machine (see above).\nThe last call in the function return!(mach, model, verbosity) calls fit! on the learning network machine mach and splits it into various pieces, as required by the MLJ model interface. See also the return! doc-string.\nImportant note An MLJ fit method is not allowed to mutate its model argument. ","category":"page"},{"location":"composing_models/#","page":"Composing Models","title":"Composing Models","text":"What's going on here? MLJ's machine interface is built atop a more primitive model interface, implemented for each algorithm. Each supervised model type (eg, RidgeRegressor) requires model fit and predict methods, which are called by the corresponding machine fit! and predict methods. We don't need to define a  model predict method here because MLJ provides a fallback which simply calls the predict on the learning network machine created in the fit method. ","category":"page"},{"location":"composing_models/#A-composite-model-coupling-component-model-hyper-parameters-1","page":"Composing Models","title":"A composite model coupling component model hyper-parameters","text":"","category":"section"},{"location":"composing_models/#","page":"Composing Models","title":"Composing Models","text":"We now give a more complicated example of a composite model which exposes some parameters used in the network that are not simply component models. The model combines a clustering model (e.g., KMeans()) for dimension reduction with ridge regression, but has the following \"coupling\" of the hyper parameters: The ridge regularization depends on the number of clusters used (with less regularization for a greater number of clusters) and a user-specified \"coupling\" coefficient K.","category":"page"},{"location":"composing_models/#","page":"Composing Models","title":"Composing Models","text":"@load RidgeRegressor pkg=MLJLinearModels\n\nmutable struct MyComposite <: DeterministicComposite\n    clusterer     # the clustering model (e.g., KMeans())\n    ridge_solver  # a ridge regression parameter we want to expose\n    K::Float64    # a \"coupling\" coefficient\nend\n\nfunction MLJ.fit(composite::Composite, verbosity, X, y)\n\n    Xs = source(X)\n    ys = source(y)\n\n    clusterer = composite.clusterer\n    k = clusterer.k\n\n    clustererM = machine(clusterer, Xs)\n    Xsmall = transform(clustererM, Xs)\n\n    # the coupling: ridge regularization depends on number of\n    # clusters (and the specified coefficient `K`):\n    lambda = exp(-composite.K/clusterer.k)\n\n    ridge = RidgeRegressor(lambda=lambda, solver=composite.ridge_solver)\n    ridgeM = machine(ridge, Xsmall, ys)\n\n    yhat = predict(ridgeM, Xsmall)\n\n    mach = machine(Deterministic(), Xs, ys; predict=yhat)\n    return!(mach, composite, verbosity)\n\nend","category":"page"},{"location":"composing_models/#Static-operations-on-nodes-1","page":"Composing Models","title":"Static operations on nodes","text":"","category":"section"},{"location":"composing_models/#","page":"Composing Models","title":"Composing Models","text":"Continuing to view nodes as \"dynamic data\", we can, in addition to applying \"dynamic\" operations like predict and transform to nodes, overload ordinary \"static\" (unlearned) operations as well. These operations can be ordinary functions (with possibly multiple arguments) or they could be functions with parameters, such as \"take a weighted average of two nodes\", where the weights are parameters. Here we address the simpler case of ordinary functions. For the parametric case, see \"Static transformers\" in Transformers and other unsupervised models","category":"page"},{"location":"composing_models/#","page":"Composing Models","title":"Composing Models","text":"Let us first give a demonstration of operations that work out-of-the-box. These include:","category":"page"},{"location":"composing_models/#","page":"Composing Models","title":"Composing Models","text":"addition and scalar multiplication\nexp, log, vcat, hcat\ntabularization (MLJ.table) and matrixification (MLJ.matrix)","category":"page"},{"location":"composing_models/#","page":"Composing Models","title":"Composing Models","text":"As a demonstration of some of these, consider the learning network below that: (i) One-hot encodes the input table X; (ii) Log transforms the continuous target y; (iii) Fits specified K-nearest neighbour and ridge regressor models to the data; (iv) Computes an average of the individual model predictions; and (v) Inverse transforms (exponentiates) the blended predictions.","category":"page"},{"location":"composing_models/#","page":"Composing Models","title":"Composing Models","text":"Note, in particular, the lines defining zhat and yhat, which combine several static node operations.","category":"page"},{"location":"composing_models/#","page":"Composing Models","title":"Composing Models","text":"@load RidgeRegressor pkg=MultivariateStats\n@load KNNRegressor\n\nXs = source()\nys = source()\n\nhot = machine(OneHotEncoder(), Xs)\n\n# W, z, zhat and yhat are nodes in the network:\n\nW = transform(hot, Xs) # one-hot encode the input\nz = log(ys)            # transform the target\n\nmodel1 = RidgeRegressor(lambda=0.1)\nmodel2 = KNNRegressor(K=7)\n\nmach1 = machine(model1, W, z)\nmach2 = machine(model2, W, z)\n\n# average the predictions of the KNN and ridge models:\nzhat = 0.5*predict(mach1, W) + 0.5*predict(mach2, W)\n\n# inverse the target transformation\nyhat = exp(zhat)","category":"page"},{"location":"composing_models/#","page":"Composing Models","title":"Composing Models","text":"Exporting this learning network as a stand-alone model:","category":"page"},{"location":"composing_models/#","page":"Composing Models","title":"Composing Models","text":"@from_network machine(Deterministic(), Xs, ys; predict=yhat) begin\n    mutable struct DoubleRegressor\n        regressor1=model1\n        regressor2=model2\n    end\nend","category":"page"},{"location":"composing_models/#","page":"Composing Models","title":"Composing Models","text":"To deal with operations on nodes not supported out-of-the box, one can use the @node macro. Supposing, in the preceding example, we wanted the geometric mean rather than arithmetic mean. Then, the definition of zhat above can be replaced with","category":"page"},{"location":"composing_models/#","page":"Composing Models","title":"Composing Models","text":"yhat1 = predict(mach1, W)\nyhat2 = predict(mach2, W)\ngmean(y1, y2) = sqrt.(y1.*y2)\nzhat = @node gmean(yhat1, yhat2)","category":"page"},{"location":"composing_models/#","page":"Composing Models","title":"Composing Models","text":"There is also a node function, which would achieve the same in this way:","category":"page"},{"location":"composing_models/#","page":"Composing Models","title":"Composing Models","text":"zhat = node((y1, y2)->sqrt.(y1.*y2), predict(mach1, W), predict(mach2, W))","category":"page"},{"location":"composing_models/#More-node-examples-1","page":"Composing Models","title":"More node examples","text":"","category":"section"},{"location":"composing_models/#","page":"Composing Models","title":"Composing Models","text":"Here are some examples taken from MLJ source (at work in the example above) for overloading common operations for nodes:","category":"page"},{"location":"composing_models/#","page":"Composing Models","title":"Composing Models","text":"Base.log(v::Vector{<:Number}) = log.(v)\nBase.log(X::AbstractNode) = node(log, X)\n\nimport Base.+\n+(y1::AbstractNode, y2::AbstractNode) = node(+, y1, y2)\n+(y1, y2::AbstractNode) = node(+, y1, y2)\n+(y1::AbstractNode, y2) = node(+, y1, y2)","category":"page"},{"location":"composing_models/#","page":"Composing Models","title":"Composing Models","text":"Here AbstractNode is the common super-type of Node and Source.","category":"page"},{"location":"composing_models/#","page":"Composing Models","title":"Composing Models","text":"And a final example, using the @node macro to row-shuffle a table:","category":"page"},{"location":"composing_models/#","page":"Composing Models","title":"Composing Models","text":"using Random\nX = (x1 = [1, 2, 3, 4, 5],\n     x2 = [:one, :two, :three, :four, :five])\nrows(X) = 1:nrows(X)\n\nXs = source(X)\nrs  = @node rows(Xs)\nW = @node selectrows(Xs, @node shuffle(rs))\n\njulia> W()\n(x1 = [5, 1, 3, 2, 4],\n x2 = Symbol[:five, :one, :three, :two, :four],)\n","category":"page"},{"location":"composing_models/#The-learning-network-API-1","page":"Composing Models","title":"The learning network API","text":"","category":"section"},{"location":"composing_models/#","page":"Composing Models","title":"Composing Models","text":"Two new julia types are part of learning networks: Source and Node.","category":"page"},{"location":"composing_models/#","page":"Composing Models","title":"Composing Models","text":"Formally, a learning network defines two labeled directed acyclic graphs (DAG's) whose nodes are Node or Source objects, and whose labels are Machine objects. We obtain the first DAG from directed edges of the form N1 - N2 whenever N1 is an argument of N2 (see below). Only this DAG is relevant when calling a node, as discussed in examples above and below. To form the second DAG (relevant when calling or calling fit! on a node) one adds edges for which N1 is training argument of the the machine which labels N1. We call the second, larger DAG, the completed learning network (but note only edges of the smaller network are explicitly drawn in diagrams, for simplicity).","category":"page"},{"location":"composing_models/#Source-nodes-1","page":"Composing Models","title":"Source nodes","text":"","category":"section"},{"location":"composing_models/#","page":"Composing Models","title":"Composing Models","text":"Only source nodes reference concrete data. A Source object has a single field, data.","category":"page"},{"location":"composing_models/#","page":"Composing Models","title":"Composing Models","text":"source(X)\nrebind!\nsources\norigins","category":"page"},{"location":"composing_models/#MLJBase.source-Tuple{Any}","page":"Composing Models","title":"MLJBase.source","text":"Xs = source(X=nothing)\n\nDefine, a learning network Source object, wrapping some input data X, which can be nothing for purposes of exporting the network as stand-alone model. For training and testing the unexported network, appropriate vectors, tables, or other data containers are expected.\n\nThe calling behaviour of a Source object is this:\n\nXs() = X\nXs(rows=r) = selectrows(X, r)  # eg, X[r,:] for a DataFrame\nXs(Xnew) = Xnew\n\nSee also: [@from_network](@ref], sources, origins, node.\n\n\n\n\n\n","category":"method"},{"location":"composing_models/#MLJBase.rebind!","page":"Composing Models","title":"MLJBase.rebind!","text":"rebind!(s, X)\n\nAttach new data X to an existing source node s. Not a public method.\n\n\n\n\n\n","category":"function"},{"location":"composing_models/#MLJBase.sources","page":"Composing Models","title":"MLJBase.sources","text":"sources(N::AbstractNode)\n\nA vector of all sources referenced by calls N() and fit!(N). These are the sources of the ancestor graph of N when including training edges.\n\nNot to be confused with origins(N), in which training edges are excluded.\n\nSee also: origins, source.\n\n\n\n\n\n","category":"function"},{"location":"composing_models/#MLJBase.origins","page":"Composing Models","title":"MLJBase.origins","text":"origins(N)\n\nReturn a list of all origins of a node N accessed by a call N(). These are the source nodes of ancestor graph of N if edges corresponding to training arguments are excluded. A Node object cannot be called on new data unless it has a unique origin.\n\nNot to be confused with sources(N) which refers to the same graph but without the training edge deletions.\n\nSee also: node, source.\n\n\n\n\n\n","category":"function"},{"location":"composing_models/#Nodes-1","page":"Composing Models","title":"Nodes","text":"","category":"section"},{"location":"composing_models/#","page":"Composing Models","title":"Composing Models","text":"The key components of a Node are:","category":"page"},{"location":"composing_models/#","page":"Composing Models","title":"Composing Models","text":"An operation, which will either be static (a fixed function) or dynamic (such as predict or transform, dispatched on a machine).\nA machine on which to dispatch the operation (void if the operation is static). The training arguments of the machine are generally other nodes.\nUpstream connections to other nodes (including source nodes) specified by arguments (one for each argument of the operation).","category":"page"},{"location":"composing_models/#","page":"Composing Models","title":"Composing Models","text":"node","category":"page"},{"location":"composing_models/#MLJBase.node","page":"Composing Models","title":"MLJBase.node","text":"N = node(f::Function, args...)\n\nDefines a Node object N wrapping a static operation f and arguments args. Each of the n elements of args must be a Node or Source object. The node N has the following calling behaviour:\n\nN() = f(args[1](), args[2](), ..., args[n]())\nN(rows=r) = f(args[1](rows=r), args[2](rows=r), ..., args[n](rows=r))\nN(X) = f(args[1](X), args[2](X), ..., args[n](X))\n\nJ = node(f, mach::Machine, args...)\n\nDefines a dynamic Node object J wrapping a dynamic operation f (predict, predict_mean, transform, etc), a nodal machine mach and arguments args. Its calling behaviour, which depends on the outcome of training mach (and, implicitly, on training outcomes affecting its arguments) is this:\n\nJ() = f(mach, args[1](), args[2](), ..., args[n]())\nJ(rows=r) = f(mach, args[1](rows=r), args[2](rows=r), ..., args[n](rows=r))\nJ(X) = f(mach, args[1](X), args[2](X), ..., args[n](X))\n\nGenerally n=1 or n=2 in this latter case.\n\npredict(mach, X::AbsractNode, y::AbstractNode)\npredict_mean(mach, X::AbstractNode, y::AbstractNode)\npredict_median(mach, X::AbstractNode, y::AbstractNode)\npredict_mode(mach, X::AbstractNode, y::AbstractNode)\ntransform(mach, X::AbstractNode)\ninverse_transform(mach, X::AbstractNode)\n\nShortcuts for J = node(predict, mach, X, y), etc.\n\nCalling a node is a recursive operation which terminates in the call to a source node (or nodes). Calling nodes on new data X fails unless the number of such nodes is one.\n\nSee also: @node, source, origins.\n\n\n\n\n\n","category":"type"},{"location":"composing_models/#","page":"Composing Models","title":"Composing Models","text":"@node","category":"page"},{"location":"composing_models/#MLJBase.@node","page":"Composing Models","title":"MLJBase.@node","text":"@node f(...)\n\nConstruct a new node that applies the function f to some combination of nodes, sources and other arguments.\n\nImportant. An argument not in global scope is assumed to be a node  or source.\n\nExamples\n\nX = source(π)\nW = @node sin(X)\njulia> W()\n0\n\nX = source(1:10)\nY = @node selectrows(X, 3:4)\njulia> Y()\n3:4\n\njulia> Y([\"one\", \"two\", \"three\", \"four\"])\n2-element Array{Symbol,1}:\n \"three\"\n \"four\"\n\nX1 = source(4)\nX2 = source(5)\nadd(a, b, c) = a + b + c\nN = @node add(X1, 1, X2)\njulia> N()\n10\n\n\nSee also node\n\n\n\n\n\n","category":"macro"},{"location":"composing_models/#","page":"Composing Models","title":"Composing Models","text":"@from_network","category":"page"},{"location":"composing_models/#MLJBase.@from_network","page":"Composing Models","title":"MLJBase.@from_network","text":"@from_network mach [mutable] struct NewCompositeModel\n       ...\nend\n\nor\n\n@from_network mach begin\n    [mutable] struct NewCompositeModel\n       ...\n    end\n    <optional trait declarations>\nend\n\nCreate a new stand-alone model type called NewCompositeModel, using the specified learning network machine mach as a blueprint.\n\nFor more on learning network machines, see machine.\n\nExample\n\nConsider the following simple learning network for training a decision tree after one-hot encoding the inputs, and forcing the predictions to be point-predictions (rather than probabilistic):\n\nXs = source()\nys = source()\n\nhot = OneHotEncoder()\ntree = DecisionTreeClassifier()\n\nW = transform(machine(hot, Xs), Xs)\nyhat = predict_mode(machine(tree, W, ys), W)\n\nA learning network machine is defined by\n\nmach = machine(Deterministic(), Xs, ys; predict=yhat)\n\nTo specify a new Deterministic composite model type WrappedTree we specify the model instances appearing in the network as \"default\" values in the following decorated struct definition:\n\n@from_network mach struct WrappedTree\n    encoder=hot\n    decision_tree=tree\nend\n\nand create a new instance with WrappedTree().\n\nTo allow the second model component to be replaced by any other probabilistic model we instead make a mutable struct declaration and, if desired, annotate types appropriately.  In the following code illustration some model trait declarations have also been added:\n\n@from_network mach begin\n    mutable struct WrappedTree\n        encoder::OneHotEncoder=hot\n        classifier::Probabilistic=tree\n    end\n    input_scitype = Table(Continuous, Finite)\n    is_pure_julia = true\nend\n\n\n\n\n\n","category":"macro"},{"location":"composing_models/#","page":"Composing Models","title":"Composing Models","text":"return!","category":"page"},{"location":"composing_models/#MLJBase.return!","page":"Composing Models","title":"MLJBase.return!","text":"return!(mach::Machine{<:Surrogate}, model, verbosity)\n\nThe last call in custom code defining the MLJBase.fit method for a new composite model type. Here model is the instance of the new type appearing in the MLJBase.fit signature, while mach is a learning network machine constructed using model. Not relevant when defining composite models using @pipeline or @from_network.\n\nFor usage, see the example given below. Specificlly, the call does the following:\n\nDetermines which fields of model point to model instances in the learning network wrapped by mach, for recording in an object called cache, for passing onto the MLJ logic that handles smart updating (namely, an MLJBase.update fallback for composite models).\n\nCalls fit!(mach, verbosity=verbosity).\nMoves any data in sources nodes of the learning network into cache (for data-anonymization purposes).\nRecords a copy of model in cache.\nReturns cache and outcomes of training in an appropriate form (specifically, (mach.fitresult, cache, mach.report); see Adding Models for General Use for technical details.)\n\nExample\n\nThe following code defines, \"by hand\", a new model type MyComposite for composing standardization (whitening) with a deterministic regressor:\n\nmutable struct MyComposite <: DeterministicComposite\n    regressor\nend\n\nfunction MLJBase.fit(model::MyComposite, verbosity, X, y)\n    Xs = source(X)\n    ys = source(y)\n\n    mach1 = machine(Standardizer(), Xs)\n    Xwhite = transform(mach1, Xs)\n\n    mach2 = machine(model.regressor, Xwhite, ys)\n    yhat = predict(mach2, Xwhite)\n\n    mach = machine(Deterministic(), Xs, ys; predict=yhat)\n    return!(mach, model, verbosity)\nend\n\n\n\n\n\n","category":"function"},{"location":"composing_models/#","page":"Composing Models","title":"Composing Models","text":"See more on fitting nodes at fit! and fit_only!. ","category":"page"},{"location":"adding_models_for_general_use/#Adding-Models-for-General-Use-1","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"","category":"section"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"note: Note\nModels implementing the MLJ model interface according to the instructions given here should import MLJModelInterface version 0.3.5 or higher. This is enforced with a statement such as MLJModelInterface = \"^0.3.5\" under [compat] in the Project.toml file of the package containing the implementation.","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"This guide outlines the specification of the MLJ model interface and provides detailed guidelines for implementing the interface for models intended for general use. See also the more condensed Quick-Start Guide to Adding Models.","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"For sample implementations, see MLJModels/src.","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"The machine learning tools provided by MLJ can be applied to the models in any package that imports the package MLJModelInterface and implements the API defined there, as outlined below. For a quick-and-dirty implementation of user-defined models see Simple User Defined Models.  To make new models available to all MLJ users, see Where to place code implementing new models.","category":"page"},{"location":"adding_models_for_general_use/#Important-1","page":"Adding Models for General Use","title":"Important","text":"","category":"section"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"MLJModelInterface is a very light-weight interface allowing you to define your interface, but does not provide the functionality required to use or test your interface; this requires MLJBase.  So, while you only need to add MLJModelInterface to your project's [deps], for testing purposes you need to add MLJBase to your project's [extras] and [targets]. In testing, simply use MLJBase in place of MLJModelInterface.","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"It is assumed the reader has read Getting Started. To implement the API described here, some familiarity with the following packages is also helpful:","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"MLJScientificTypes.jl (for specifying model requirements of data)\nDistributions.jl (for probabilistic predictions)\nCategoricalArrays.jl (essential if you are implementing a model handling data of Multiclass or OrderedFactor scitype; familiarity with CategoricalPool objects required)\nTables.jl (if your algorithm needs input data in a novel format).","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"In MLJ, the basic interface exposed to the user, built atop the model interface described here, is the machine interface. After a first reading of this document, the reader may wish to refer to MLJ Internals for context.","category":"page"},{"location":"adding_models_for_general_use/#Overview-1","page":"Adding Models for General Use","title":"Overview","text":"","category":"section"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"A model is an object storing hyperparameters associated with some machine learning algorithm.  In MLJ, hyperparameters include configuration parameters, like the number of threads, and special instructions, such as \"compute feature rankings\", which may or may not affect the final learning outcome.  However, the logging level (verbosity below) is excluded.","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"The name of the Julia type associated with a model indicates the associated algorithm (e.g., DecisionTreeClassifier). The outcome of training a learning algorithm is called a fitresult. For ordinary multivariate regression, for example, this would be the coefficients and intercept. For a general supervised model, it is the (generally minimal) information needed to make new predictions.","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"The ultimate supertype of all models is MLJModelInterface.Model, which has two abstract subtypes:","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"abstract type Supervised <: Model end\nabstract type Unsupervised <: Model end","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"Supervised models are further divided according to whether they are able to furnish probabilistic predictions of the target (which they will then do by default) or directly predict \"point\" estimates, for each new input pattern:","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"abstract type Probabilistic <: Supervised end\nabstract type Deterministic <: Supervised end","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"Further division of model types is realized through Trait declarations.","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"Associated with every concrete subtype of Model there must be a fit method, which implements the associated algorithm to produce the fitresult. Additionally, every Supervised model has a predict method, while Unsupervised models must have a transform method. More generally, methods such as these, that are dispatched on a model instance and a fitresult (plus other data), are called operations. Probabilistic supervised models optionally implement a predict_mode operation (in the case of classifiers) or a predict_mean and/or predict_median operations (in the case of regressors) although MLJModelInterface also provides fallbacks that will suffice in most cases. Unsupervised models may implement an inverse_transform operation.","category":"page"},{"location":"adding_models_for_general_use/#New-model-type-declarations-and-optional-clean!-method-1","page":"Adding Models for General Use","title":"New model type declarations and optional clean! method","text":"","category":"section"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"Here is an example of a concrete supervised model type declaration:","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"import MLJModelInterface\nconst MMI = MLJModelInterface\n\nmutable struct RidgeRegressor <: MMI.Deterministic\n    lambda::Float64\nend","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"Models (which are mutable) should not be given internal constructors. It is recommended that they be given an external lazy keyword constructor of the same name. This constructor defines default values for every field, and optionally corrects invalid field values by calling a clean! method (whose fallback returns an empty message string):","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"function MMI.clean!(model::RidgeRegressor)\n    warning = \"\"\n    if model.lambda < 0\n        warning *= \"Need lambda ≥ 0. Resetting lambda=0. \"\n        model.lambda = 0\n    end\n    return warning\nend\n\n# keyword constructor\nfunction RidgeRegressor(; lambda=0.0)\n    model = RidgeRegressor(lambda)\n    message = MMI.clean!(model)\n    isempty(message) || @warn message\n    return model\nend","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"Important. The clean method must have the property that clean!(clean!(model)) == clean!(model) for any instance model.","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"Although not essential, try to avoid Union types for model fields. For example, a field declaration features::Vector{Symbol} with a default of Symbol[] (detected with isempty method) is preferred to features::Union{Vector{Symbol}, Nothing} with a default of nothing.","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"An alternative to declaring the model struct, clean! method and keyword constructor, is to use the @mlj_model macro, as in the following example:","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"@mlj_model mutable struct YourModel <: MMI.Deterministic\n    a::Float64 = 0.5::(_ > 0)\n    b::String  = \"svd\"::(_ in (\"svd\",\"qr\"))\nend","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"This declaration specifies:","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"A keyword constructor (here YourModel(; a=..., b=...)),\nDefault values for the hyperparameters,\nConstraints on the hyperparameters where _ refers to a value passed.","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"For example, a::Float64 = 0.5::(_ > 0) indicates that the field a is a Float64, takes 0.5 as default value, and expects its value to be positive.","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"You cannot use the @mlj_model macro if your model struct has type parameters.","category":"page"},{"location":"adding_models_for_general_use/#Known-issue-with-@mlj_macro-1","page":"Adding Models for General Use","title":"Known issue with @mlj_macro","text":"","category":"section"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"Defaults with negative values can trip up the @mlj_macro (see this issue). So, for example, this does not work:","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"@mlj_model mutable struct Bar\n    a::Int = -1::(_ > -2)\nend","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"But this does:","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"@mlj_model mutable struct Bar\n    a::Int = (-)(1)::(_ > -2)\nend","category":"page"},{"location":"adding_models_for_general_use/#Supervised-models-1","page":"Adding Models for General Use","title":"Supervised models","text":"","category":"section"},{"location":"adding_models_for_general_use/#Mathematical-assumptions-1","page":"Adding Models for General Use","title":"Mathematical assumptions","text":"","category":"section"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"At present, MLJ's performance estimate functionality (resampling using evaluate/evaluate!) tacitly assumes that feature-label pairs of observations (X1, y1), (X2, y2), (X2, y2), ... are being modelled as identically independent random variables (i.i.d.), and constructs some kind of representation of an estimate of the conditional probablility p(y | X) (y and X single observations). It may be that a model implementing the MLJ interface has the potential to make predictions under weaker assumptions (e.g., time series forecasting models). However the output of the compulsory predict method described below should be the output of the model under the i.i.d assumption.","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"In the future newer methods may be introduced to handle weaker assumptions (see, e.g., The predict_joint method below).","category":"page"},{"location":"adding_models_for_general_use/#Summary-of-methods-1","page":"Adding Models for General Use","title":"Summary of methods","text":"","category":"section"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"The compulsory and optional methods to be implemented for each concrete type SomeSupervisedModel <: MMI.Supervised are summarized below. ","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"An = indicates the return value for a fallback version of the method.","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"Compulsory:","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"MMI.fit(model::SomeSupervisedModel, verbosity, X, y) -> fitresult, cache, report\nMMI.predict(model::SomeSupervisedModel, fitresult, Xnew) -> yhat","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"Optional, to check and correct invalid hyperparameter values:","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"MMI.clean!(model::SomeSupervisedModel) = \"\"","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"Optional, to return user-friendly form of fitted parameters:","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"MMI.fitted_params(model::SomeSupervisedModel, fitresult) = fitresult","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"Optional, to avoid redundant calculations when re-fitting machines associated with a model:","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"MMI.update(model::SomeSupervisedModel, verbosity, old_fitresult, old_cache, X, y) =\n   MMI.fit(model, verbosity, X, y)","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"Optional, to specify default hyperparameter ranges (for use in tuning):","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"MMI.hyperparameter_ranges(T::Type) = Tuple(fill(nothing, length(fieldnames(T))))","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"Optional, if SomeSupervisedModel <: Probabilistic:","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"MMI.predict_mode(model::SomeSupervisedModel, fitresult, Xnew) =\n    mode.(predict(model, fitresult, Xnew))\nMMI.predict_mean(model::SomeSupervisedModel, fitresult, Xnew) =\n    mean.(predict(model, fitresult, Xnew))\nMMI.predict_median(model::SomeSupervisedModel, fitresult, Xnew) =\n    median.(predict(model, fitresult, Xnew))","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"Required, if the model is to be registered (findable by general users):","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"MMI.load_path(::Type{<:SomeSupervisedModel})    = \"\"\nMMI.package_name(::Type{<:SomeSupervisedModel}) = \"Unknown\"\nMMI.package_uuid(::Type{<:SomeSupervisedModel}) = \"Unknown\"","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"MMI.input_scitype(::Type{<:SomeSupervisedModel}) = Unknown","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"Strongly recommended, to constrain the form of target data passed to fit:","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"MMI.target_scitype(::Type{<:SomeSupervisedModel}) = Unknown","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"Optional but recommended:","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"MMI.package_url(::Type{<:SomeSupervisedModel})  = \"unknown\"\nMMI.is_pure_julia(::Type{<:SomeSupervisedModel}) = false\nMMI.package_license(::Type{<:SomeSupervisedModel}) = \"unknown\"","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"If SomeSupervisedModel supports sample weights, then instead of the fit above, one implements","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"MMI.fit(model::SomeSupervisedModel, verbosity, X, y, w=nothing) -> fitresult, cache, report","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"and, if appropriate","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"MMI.update(model::SomeSupervisedModel, verbosity, old_fitresult, old_cache, X, y, w=nothing) =\n   MMI.fit(model, verbosity, X, y, w)","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"Additionally, if SomeSupervisedModel supports sample weights, one must declare","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"MMI.supports_weights(model::Type{<:SomeSupervisedModel}) = true","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"Optionally, to customized support for serialization of machines (see Serialization), overload","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"MMI.save(filename, model::SomeModel, fitresult; kwargs...) = fitresult","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"and possibly","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"MMI.restore(filename, model::SomeModel, serializable_fitresult) -> serializable_fitresult","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"These last two are unlikely to be needed if wrapping pure Julia code.","category":"page"},{"location":"adding_models_for_general_use/#The-form-of-data-for-fitting-and-predicting-1","page":"Adding Models for General Use","title":"The form of data for fitting and predicting","text":"","category":"section"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"The model implementer does not have absolute control over the types of data X, y and Xnew appearing in the fit and predict methods they must implement. Rather, they can specify the scientific type of this data by making appropriate declarations of the traits input_scitype and target_scitype discussed later under Trait declarations.","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"Important Note. Unless it genuinely makes little sense to do so, the MLJ recommendation is to specify a Table scientific type for X (and hence Xnew) and an AbstractVector scientific type (e.g., AbstractVector{Continuous}) for targets y. Algorithms requiring matrix input can coerce their inputs appropriately; see below.","category":"page"},{"location":"adding_models_for_general_use/#Additional-type-coercions-1","page":"Adding Models for General Use","title":"Additional type coercions","text":"","category":"section"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"If the core algorithm being wrapped requires data in a different or more specific form, then fit will need to coerce the table into the form desired (and the same coercions applied to X will have to be repeated for Xnew in predict). To assist with common cases, MLJ provides the convenience method MMI.matrix. MMI.matrix(Xtable) has type Matrix{T} where T is the tightest common type of elements of Xtable, and Xtable is any table. (If Xtable is itself just a wrapped matrix, Xtable=Tables.table(A), then A=MMI.table(Xtable) will be returned without any copying.)","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"Other auxiliary methods provided by MLJModelInterface for handling tabular data are: selectrows, selectcols, select and schema (for extracting the size, names and eltypes of a table's columns). See Convenience methods below for details.","category":"page"},{"location":"adding_models_for_general_use/#Important-convention-1","page":"Adding Models for General Use","title":"Important convention","text":"","category":"section"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"It is to be understood that the columns of the table X correspond to features and the rows to observations. So, for example, the predict method for a linear regression model might look like predict(model, w, Xnew) = MMI.matrix(Xnew)*w, where w is the vector of learned coefficients.","category":"page"},{"location":"adding_models_for_general_use/#The-fit-method-1","page":"Adding Models for General Use","title":"The fit method","text":"","category":"section"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"A compulsory fit method returns three objects:","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"MMI.fit(model::SomeSupervisedModel, verbosity, X, y) -> fitresult, cache, report","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"Note. The Int typing of verbosity cannot be omitted.","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"fitresult is the fitresult in the sense above (which becomes an  argument for predict discussed below).\nreport is a (possibly empty) NamedTuple, for example,  report=(deviance=..., dof_residual=..., stderror=..., vcov=...).  Any training-related statistics, such as internal estimates of the  generalization error, and feature rankings, should be returned in  the report tuple. How, or if, these are generated should be  controlled by hyperparameters (the fields of model). Fitted  parameters, such as the coefficients of a linear model, do not go  in the report as they will be extractable from fitresult (and  accessible to MLJ through the fitted_params method described below).","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"3.\tThe value of cache can be nothing, unless one is also defining     an update method (see below). The Julia type of cache is not     presently restricted.","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"It is not necessary for fit to provide type or dimension checks on X or y or to call clean! on the model; MLJ will carry out such checks.","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"The method fit should never alter hyperparameter values, the sole exception being fields of type <:AbstractRNG. If the package is able to suggest better hyperparameters, as a byproduct of training, return these in the report field.","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"The verbosity level (0 for silent) is for passing to learning algorithm itself. A fit method wrapping such an algorithm should generally avoid doing any of its own logging.","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"Sample weight support. If supports_weights(::Type{<:SomeSupervisedModel}) has been declared true, then one instead implements the following variation on the above fit:","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"MMI.fit(model::SomeSupervisedModel, verbosity, X, y, w=nothing) -> fitresult, cache, report","category":"page"},{"location":"adding_models_for_general_use/#The-fitted_params-method-1","page":"Adding Models for General Use","title":"The fitted_params method","text":"","category":"section"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"A fitted_params method may be optionally overloaded. It's purpose is to provide MLJ access to a user-friendly representation of the learned parameters of the model (as opposed to the hyperparameters). They must be extractable from fitresult.","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"MMI.fitted_params(model::SomeSupervisedModel, fitresult) -> friendly_fitresult::NamedTuple","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"For a linear model, for example, one might declare something like friendly_fitresult=(coefs=[...], bias=...).","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"The fallback is to return (fitresult=fitresult,).","category":"page"},{"location":"adding_models_for_general_use/#The-predict-method-1","page":"Adding Models for General Use","title":"The predict method","text":"","category":"section"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"A compulsory predict method has the form","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"MMI.predict(model::SomeSupervisedModel, fitresult, Xnew) -> yhat","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"Here Xnew will have the same form as the X passed to fit. ","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"Note that while Xnew generally consists of multiple observations (e.g., has multiple rows in the case of a table) it is assumed, in view of the i.i.d assumption recalled above, that calling predict(..., Xnew) is equivalent to broadcasting some method predict_one(..., x) over the individual observations x in Xnew (a method implementing the probablility distribution p(X |y) above).","category":"page"},{"location":"adding_models_for_general_use/#Prediction-types-for-deterministic-responses.-1","page":"Adding Models for General Use","title":"Prediction types for deterministic responses.","text":"","category":"section"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"In the case of Deterministic models, yhat should have the same scitype as the y passed to fit (see above). Any CategoricalValue elements of yhat must have a pool == to the pool of the target y presented in training, even if not all levels appear in the training data or prediction itself. For example, in the case of a univariate target, such as scitype(y) <: AbstractVector{Multiclass{3}}, one requires MLJ.classes(yhat[i]) == MLJ.classes(y[j]) for all admissible i and j. (The method classes is described under Convenience methods below).","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"Unfortunately, code not written with the preservation of categorical levels in mind poses special problems. To help with this, MLJModelInterface provides three utility methods: int (for converting a CategoricalValue into an integer, the ordering of these integers being consistent with that of the pool), decoder (for constructing a callable object that decodes the integers back into CategoricalValue objects), and classes, for extracting all the CategoricalValue objects sharing the pool of a particular value. Refer to Convenience methods below for important details.","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"Note that a decoder created during fit may need to be bundled with fitresult to make it available to predict during re-encoding. So, for example, if the core algorithm being wrapped by fit expects a nominal target yint of type Vector{<:Integer} then a fit method may look something like this:","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"function MMI.fit(model::SomeSupervisedModel, verbosity, X, y)\n    yint = MMI.int(y)\n    a_target_element = y[1]                    # a CategoricalValue/String\n    decode = MMI.decoder(a_target_element) # can be called on integers\n\n    core_fitresult = SomePackage.fit(X, yint, verbosity=verbosity)\n\n    fitresult = (decode, core_fitresult)\n    cache = nothing\n    report = nothing\n    return fitresult, cache, report\nend","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"while a corresponding deterministic predict operation might look like this:","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"function MMI.predict(model::SomeSupervisedModel, fitresult, Xnew)\n    decode, core_fitresult = fitresult\n    yhat = SomePackage.predict(core_fitresult, Xnew)\n    return decode.(yhat)  # or decode(yhat) also works\nend","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"For a concrete example, refer to the code for SVMClassifier.","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"Of course, if you are coding a learning algorithm from scratch, rather than wrapping an existing one, these extra measures may be unnecessary.","category":"page"},{"location":"adding_models_for_general_use/#Prediction-types-for-probabilistic-responses-1","page":"Adding Models for General Use","title":"Prediction types for probabilistic responses","text":"","category":"section"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"In the case of Probabilistic models with univariate targets, yhat must be an AbstractVector whose elements are distributions (one distribution per row of Xnew).","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"Presently, a distribution is any object d for which MMI.isdistribution(::d) = true, which is the case for objects of type Distributions.Sampleable.","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"Use the distribution MMI.UnivariateFinite for Probabilistic models predicting a target with Finite scitype (classifiers). In this case the eltype of the training target y will be a CategoricalValue.","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"For efficiency, one should not construct UnivariateDistribution instances one at a time. Rather, once a probability vector or matrix is known, construct an instance of UnivariateFiniteVector <: AbstractArray{<:UnivariateFinite},1} to return. Both UnivariateFinite and UnivariateFiniteVector objects are constructed using the single UnivariateFinite function.","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"For example, suppose the target y arrives as a subsample of some ybig and is missing some classes:","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"ybig = categorical([:a, :b, :a, :a, :b, :a, :rare, :a, :b])\ny = ybig[1:6]","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"Your fit method has bundled the first element of y with the fitresult to make it available to predict for purposes of tracking the complete pool of classes. Let's call this an_element = y[1]. Then, supposing the corresponding probabilities of the observed classes [:a, :b] are in an n x 2 matrix probs (where n the number of rows of Xnew) then you return","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"yhat = UnivariateFinite([:a, :b], probs, pool=an_element)","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"This object automatically assigns zero-probability to the unseen class :rare (i.e., pdf.(yhat, :rare) works and returns a zero vector). If you would like to assign :rare non-zero probabilities, simply add it to the first vector (the support) and supply a larger probs matrix.","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"If instead of raw labels [:a, :b] you have the corresponding CategoricalElements (from, e.g., filter(cv->cv in unique(y), classes(y))) then you can use these instead and drop the pool specifier.","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"In a binary classification problem it suffices to specify a single vector of probabilities, provided you specify augment=true, as in the following example, and note carefully that these probablities are associated with the last (second) class you specify in the constructor:","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"y = categorical([:TRUE, :FALSE, :FALSE, :TRUE, :TRUE])\nan_element = y[1]\nprobs = rand(10)\nyhat = UnivariateFinite([:FALSE, :TRUE], probs, augment=true, pool=an_element)","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"The constructor has a lot of options, including passing a dictionary instead of vectors. See UnivariateFinite for details.","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"See LinearBinaryClassifier for an example of a Probabilistic classifier implementation.","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"Important note on binary classifiers. There is no \"Binary\" scitype distinct from Multiclass{2} or OrderedFactor{2}; Binary is just an alias for Union{Multiclass{2},OrderedFactor{2}}. The target_scitype of a binary classifier will generally be AbstractVector{<:Binary} and according to the mlj scitype convention, elements of y have type CategoricalValue, and not Bool. See BinaryClassifier for an example.","category":"page"},{"location":"adding_models_for_general_use/#The-predict_joint-method-1","page":"Adding Models for General Use","title":"The predict_joint method","text":"","category":"section"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"warning: Experimental\nThe following API is experimental. It is subject to breaking changes during minor or major releases without warning.","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"MMI.predict_joint(model::SomeSupervisedModel, fitresult, Xnew) -> yhat","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"Any Probabilistic model type SomeModelmay optionally implement a predict_joint method, which has the same signature as predict, but whose predictions are a single distribution (rather than a vector of per-observation distributions). ","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"Specifically, the output yhat of predict_joint should be an instance of Distributions.Sampleable{<:Multivariate,V}, where scitype(V) = target_scitype(SomeModel) and samples have length n, where n is the number of observations in Xnew.","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"If a new model type subtypes JointProbablistic <: Probabilistic then implementation of predict_joint is compulsory.","category":"page"},{"location":"adding_models_for_general_use/#Trait-declarations-1","page":"Adding Models for General Use","title":"Trait declarations","text":"","category":"section"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"Two trait functions allow the implementer to restrict the types of data X, y and Xnew discussed above. The MLJ task interface uses these traits for data type checks but also for model search. If they are omitted (and your model is registered) then a general user may attempt to use your model with inappropriately typed data.","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"The trait functions input_scitype and target_scitype take scientific data types as values. We assume here familiarity with MLJScientificTypes.jl (see Getting Started for the basics).","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"For example, to ensure that the X presented to the DecisionTreeClassifier fit method is a table whose columns all have Continuous element type (and hence AbstractFloat machine type), one declares","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"MMI.input_scitype(::Type{<:DecisionTreeClassifier}) = MMI.Table(MMI.Continuous)","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"or, equivalently,","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"MMI.input_scitype(::Type{<:DecisionTreeClassifier}) = Table(Continuous)","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"If, instead, columns were allowed to have either: (i) a mixture of Continuous and Missing values, or (ii) Count (i.e., integer) values, then the declaration would be","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"MMI.input_scitype(::Type{<:DecisionTreeClassifier}) = Table(Union{Continuous,Missing},Count)","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"Similarly, to ensure the target is an AbstractVector whose elements have Finite scitype (and hence CategoricalValue machine type) we declare","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"MMI.target_scitype(::Type{<:DecisionTreeClassifier}) = AbstractVector{<:Finite}","category":"page"},{"location":"adding_models_for_general_use/#Multivariate-targets-1","page":"Adding Models for General Use","title":"Multivariate targets","text":"","category":"section"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"The above remarks continue to hold unchanged for the case multivariate targets.  For example, if we declare","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"target_scitype(SomeSupervisedModel) = Table(Continuous)","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"then this constrains the target to be any table whose columns have Continous element scitype (i.e., AbstractFloat), while","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"target_scitype(SomeSupervisedModel) = Table(Continuous, Finite{2})","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"restricts to tables with continuous or binary (ordered or unordered) columns.","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"For predicting variable length sequences of, say, binary values (CategoricalValues) with some common size-two pool) we declare","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"target_scitype(SomeSupervisedModel) = AbstractVector{<:NTuple{<:Finite{2}}}","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"The trait functions controlling the form of data are summarized as follows:","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"method return type declarable return values fallback value\ninput_scitype Type some scientfic type Unknown\ntarget_scitype Type some scientific type Unknown","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"Additional trait functions tell MLJ's @load macro how to find your model if it is registered, and provide other self-explanatory metadata about the model:","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"method return type declarable return values fallback value\nload_path String unrestricted \"unknown\"\npackage_name String unrestricted \"unknown\"\npackage_uuid String unrestricted \"unknown\"\npackage_url String unrestricted \"unknown\"\npackage_license String unrestricted \"unknown\"\nis_pure_julia Bool true or false false\nsupports_weights Bool true or false false","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"New. A final trait you can optionally implement is the hyperparamter_ranges trait. It declares default ParamRange objects for one or more of your model's hyperparameters. This is for use (in the future) by tuning algorithms (e.g., grid generation). It does not represent the full space of allowed values. This information is encoded in your clean! method (or @mlj_model call).","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"The value returned by hyperparamter_ranges must be a tuple of ParamRange objects (query ?range for details) whose length is the number of hyperparameters (fields of your model). Note that varying a hyperparameter over a specified range should not alter any type parameters in your model struct (this never applies to numeric ranges). If it doesn't make sense to provide a range for a parameter, a nothing entry is allowed. The fallback returns a tuple of nothings.","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"For example, a three parameter model of the form","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"mutable struct MyModel{D} <: Deterministic\n    alpha::Float64\n        beta::Int\n    distribution::D\nend","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"you might declare (order matters):","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"MMI.hyperparameter_ranges(::Type{<:MyModel}) =\n    (range(Float64, :alpha, lower=0, upper=1, scale=:log),\n         range(Int, :beta, lower=1, upper=Inf, origin=100, unit=50, scale=:log),\n         nothing)","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"Here is the complete list of trait function declarations for DecisionTreeClassifier (source):","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"MMI.input_scitype(::Type{<:DecisionTreeClassifier}) = MMI.Table(MMI.Continuous)\nMMI.target_scitype(::Type{<:DecisionTreeClassifier}) = AbstractVector{<:MMI.Finite}\nMMI.load_path(::Type{<:DecisionTreeClassifier}) = \"MLJModels.DecisionTree_.DecisionTreeClassifier\"\nMMI.package_name(::Type{<:DecisionTreeClassifier}) = \"DecisionTree\"\nMMI.package_uuid(::Type{<:DecisionTreeClassifier}) = \"7806a523-6efd-50cb-b5f6-3fa6f1930dbb\"\nMMI.package_url(::Type{<:DecisionTreeClassifier}) = \"https://github.com/bensadeghi/DecisionTree.jl\"\nMMI.is_pure_julia(::Type{<:DecisionTreeClassifier}) = true","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"Alternatively these traits can also be declared using MMI.metadata_pkg and MMI.metadata_model helper functions as:","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"MMI.metadata_pkg(DecisionTreeClassifier,name=\"DecisionTree\",\n                     uuid=\"7806a523-6efd-50cb-b5f6-3fa6f1930dbb\",\n                     url=\"https://github.com/bensadeghi/DecisionTree.jl\",\n                     julia=true)\n\nMMI.metadata_model(DecisionTreeClassifier,\n                        input=MMI.Table(MMI.Continuous),\n                        target=AbstractVector{<:MMI.Finite},\n                        path=\"MLJModels.DecisionTree_.DecisionTreeClassifier\")","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"Important. Do not omit the path specifcation.","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"MMI.metadata_pkg","category":"page"},{"location":"adding_models_for_general_use/#MLJModelInterface.metadata_pkg","page":"Adding Models for General Use","title":"MLJModelInterface.metadata_pkg","text":"metadata_pkg(T; args...)\n\nHelper function to write the metadata for a package providing model T. Use it with broadcasting to define the metadata of the package providing a series of models.\n\nKeywords\n\nname=\"unknown\"   : package name\nuuid=\"unknown\"   : package uuid\nurl=\"unknown\"    : package url\njulia=missing    : whether the package is pure julia\nlicense=\"unknown\": package license\nis_wrapper=false : whether the package is a wrapper\n\nExample\n\nmetadata_pkg.((KNNRegressor, KNNClassifier),\n    name=\"NearestNeighbors\",\n    uuid=\"b8a86587-4115-5ab1-83bc-aa920d37bbce\",\n    url=\"https://github.com/KristofferC/NearestNeighbors.jl\",\n    julia=true,\n    license=\"MIT\",\n    is_wrapper=false)\n\n\n\n\n\n","category":"function"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"MMI.metadata_model","category":"page"},{"location":"adding_models_for_general_use/#MLJModelInterface.metadata_model","page":"Adding Models for General Use","title":"MLJModelInterface.metadata_model","text":"metadata_model(`T`; args...)\n\nHelper function to write the metadata for a model T.\n\nKeywords\n\ninput=Unknown : allowed scientific type of the input data\ntarget=Unknown: allowed sc. type of the target (supervised)\noutput=Unknown: allowed sc. type of the transformed data (unsupervised)\nweights=false : whether the model supports sample weights\ndescr=\"\"      : short description of the model\npath=\"\"       : where the model is (usually PackageName.ModelName)\n\nExample\n\nmetadata_model(KNNRegressor,\n    input=MLJModelInterface.Table(MLJModelInterface.Continuous),\n    target=AbstractVector{MLJModelInterface.Continuous},\n    weights=true,\n    descr=\"K-Nearest Neighbors classifier: ...\",\n    path=\"NearestNeighbors.KNNRegressor\")\n\n\n\n\n\n","category":"function"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"You can test all your declarations of traits by calling MLJBase.info_dict(SomeModel).","category":"page"},{"location":"adding_models_for_general_use/#Iterative-models-and-the-update!-method-1","page":"Adding Models for General Use","title":"Iterative models and the update! method","text":"","category":"section"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"An update method may be optionally overloaded to enable a call by MLJ to retrain a model (on the same training data) to avoid repeating computations unnecessarily.","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"MMI.update(model::SomeSupervisedModel, verbosity, old_fitresult, old_cache, X, y) -> fit\nresult, cache, report\nMMI.update(model::SomeSupervisedModel, verbosity, old_fitresult, old_cache, X, y, w=nothing) -> fit\nresult, cache, report","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"Here the second variation applies if SomeSupervisedModel supports sample weights.","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"If an MLJ Machine is being fit! and it is not the first time, then update is called instead of fit, unless the machine fit! has been called with a new rows keyword argument. However, MLJModelInterface defines a fallback for update which just calls fit. For context, see MLJ Internals.","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"Learning networks wrapped as models constitute one use-case (see Composing Models): one would like each component model to be retrained only when hyperparameter changes \"upstream\" make this necessary. In this case MLJ provides a fallback (specifically, the fallback is for any subtype of SupervisedNetwork = Union{DeterministicNetwork,ProbabilisticNetwork}). A second more generally relevant use-case is iterative models, where calls to increase the number of iterations only restarts the iterative procedure if other hyperparameters have also changed. (A useful method for inspecting model changes in such cases is MLJModelInterface.is_same_except. ) For an example, see the MLJ ensemble code.","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"A third use-case is to avoid repeating time-consuming preprocessing of X and y required by some models.","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"In the event that the argument fitresult (returned by a preceding call to fit) is not sufficient for performing an update, the author can arrange for fit to output in its cache return value any additional information required (for example, pre-processed versions of X and y), as this is also passed as an argument to the update method.","category":"page"},{"location":"adding_models_for_general_use/#Supervised-models-with-a-transform-method-1","page":"Adding Models for General Use","title":"Supervised models with a transform method","text":"","category":"section"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"A supervised model may optionally implement a transform method, whose signature is the same as predict. In that case the implementation should define a value for the output_scitype trait. A declaration","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"output_scitype(::Type{<:SomeSupervisedModel}) = T","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"is an assurance that scitype(transform(model, fitresult, Xnew)) <: T always holds, for any model of type SomeSupervisedModel.","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"A use-case for a transform method for a supervised model is a neural network that learns feature embeddings for categorical input features as part of overall training. Such a model becomes a transformer that other supervised models can use to transform the categorical features (instead of applying the higher-dimensional one-hot encoding representations).","category":"page"},{"location":"adding_models_for_general_use/#Models-that-learn-a-probability-distribution-1","page":"Adding Models for General Use","title":"Models that learn a probability distribution","text":"","category":"section"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"warning: Experimental\nThe following API is experimental. It is subject to breaking changes during minor or major releases without warning.","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"Models that fit a probability distribution to some data should be regarded as Probablisitic <: Supervised models with target y=data and X a vector of Nothing instances of the same length. So, for example, if one is fitting a UnivariateFinite distribution to y=categorical([:yes, :no, :yes]), then the input provided would be X = [nothing, nothing, nothing] = fill(nothing, length(y)).","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"If d is the distribution fit, then yhat = predict(fill(nothing, n)) returns fill(d, n). Then, if m is a probabilistic measure (e.g., m = cross_entropy) then m(yhat, ytest) is defined for any new observations ytest of the same length n.","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"Here is a working implementation of a model to fit a UnivariateFinite distribution to some categorical data using Laplace smoothing controlled by a hyper-parameter alpha:","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"import Distributions\n\nmutable struct UnivariateFiniteFitter <: MLJModelInterface.Probabilistic\n    alpha::Float64\nend\nUnivariateFiniteFitter(;alpha=1.0) = UnivariateFiniteFitter(alpha)\n\nfunction MLJModelInterface.fit(model::UnivariateFiniteFitter,\n                               verbosity, X, y)\n\n    α = model.alpha\n    N = length(y)\n    _classes = classes(y)\n    d = length(_classes)\n\n    frequency_given_class = Distributions.countmap(y)\n    prob_given_class =\n        Dict(c => (frequency_given_class[c] + α)/(N + α*d) for c in _classes)\n\n    fitresult = UnivariateFinite(prob_given_class)\n\n    report = (params=Distributions.params(fitresult),)\n    cache = nothing\n\n    verbosity > 0 && @info \"Fitted a $fitresult\"\n\n    return fitresult, cache, report\nend\n\nMLJModelInterface.predict(model::UnivariateFiniteFitter,\n                          fitresult,\n                          X) = fill(fitresult, length(X))\n\n\nMLJModelInterface.input_scitype(::Type{<:UnivariateFiniteFitter}) =\n    AbstractVector{Nothing}\nMLJModelInterface.target_scitype(::Type{<:UnivariateFiniteFitter}) =\n    AbstractVector{<:Finite}","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"And a demonstration (zero smoothing):","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"using MLJBase\ny = coerce(collect(\"aabbccaa\"), Multiclass)\nX = fill(nothing, length(y))\nmodel = UnivariateFiniteFitter(alpha=0)\nmach = machine(model, X, y) |> fit!\n\nytest = y[1:3]\nyhat = predict(mach, fill(nothing, 3))\njulia> @assert cross_entropy(yhat, ytest) ≈ [-log(1/2), -log(1/2), -log(1/4)]\ntrue","category":"page"},{"location":"adding_models_for_general_use/#Serialization-1","page":"Adding Models for General Use","title":"Serialization","text":"","category":"section"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"warning: Experimental\nThe following API is experimental. It is subject to breaking changes during minor or major releases without warning.","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"The MLJ user can serialize and deserialize a machine, which means serializing/deserializing:","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"the associated Model object (storing hyperparameters)\nthe fitresult (learned parameters)\nthe report generating during training","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"These are bundled into a single file or IO stream specified by the user using the package JLSO. There are two scenarios in which a new MLJ model API implementation will want to overload two additional methods save and restore to support serialization:","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"The algorithm-providing package already has it's own serialization format for learned parameters and/or hyper-parameters, which users may want to access. In that case the implementation overloads save.","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"The fitresult is not a sufficiently persistent object; for example, it is a pointer passed from wrapped C code. In that case the implementation overloads save and restore.","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"In case 2, 1 presumably applies also, for otherwise MLJ serialization is probably not going to be possible without changes to the algorithm-providing package. An example is given below.","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"Note that in case 1, MLJ will continue to create it's own self-contained serialization of the machine. Below filename refers to the corresponding serialization file name, as specified by the user, but with any final extension (e.g., \".jlso\", \".gz\") removed. If the user has alternatively specified an IO object for serialization, then filename is a randomly generated numeric string.","category":"page"},{"location":"adding_models_for_general_use/#The-save-method-1","page":"Adding Models for General Use","title":"The save method","text":"","category":"section"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"MMI.save(filename, model::SomeModel, fitresult; kwargs...) -> serializable_fitresult","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"Implement this method to serialize using a format specific to models of type SomeModel. The fitresult is the first return value of MMI.fit for such model types; kwargs is a list of keyword arguments specified by the user and understood to relate to a some model-specific serialization (cannot be format=... or compression=...). The value of serializable_fitresult should be a persistent representation of fitresult, from which a correct and valid fitresult can be reconstructed using restore (see below). ","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"The fallback of save performs no action and returns fitresult.","category":"page"},{"location":"adding_models_for_general_use/#The-restore-method-1","page":"Adding Models for General Use","title":"The restore method","text":"","category":"section"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"MMI.restore(filename, model::SomeModel, serializable_fitresult) -> fitresult","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"Implement this method to reconstruct a fitresult (as returned by MMI.fit) from a persistent representation constructed using MMI.save as described above. ","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"The fallback of restore returns serializable_fitresult.","category":"page"},{"location":"adding_models_for_general_use/#Example-1","page":"Adding Models for General Use","title":"Example","text":"","category":"section"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"Below is an example drawn from MLJ's XGBoost wrapper. In this example the fitresult returned by MMI.fit is a tuple (booster, a_target_element) where booster is the XGBoost.jl object storing the learned parameters (essentially a pointer to some object created by C code) and a_target_element is an ordinary CategoricalValue used to track the target classes (a persistent object, requiring no special treatment).","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"function MLJModelInterface.save(filename,\n                                ::XGBoostClassifier,\n                                fitresult;\n                                kwargs...)\n    booster, a_target_element = fitresult\n\n    xgb_filename = string(filename, \".xgboost.model\")\n    XGBoost.save(booster, xgb_filename)\n    persistent_booster = read(xgb_filename)\n    @info \"Additional XGBoost serialization file \\\"$xgb_filename\\\" generated. \"\n    return (persistent_booster, a_target_element)\nend\n\nfunction MLJModelInterface.restore(filename,\n                                   ::XGBoostClassifier,\n                                   serializable_fitresult)\n    persistent_booster, a_target_element = serializable_fitresult\n\n    xgb_filename = string(filename, \".tmp\")\n    open(xgb_filename, \"w\") do file\n        write(file, persistent_booster)\n    end\n    booster = XGBoost.Booster(model_file=xgb_filename)\n    rm(xgb_filename)\n    fitresult = (booster, a_target_element)\n    return fitresult\nend","category":"page"},{"location":"adding_models_for_general_use/#Unsupervised-models-1","page":"Adding Models for General Use","title":"Unsupervised models","text":"","category":"section"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"Unsupervised models implement the MLJ model interface in a very similar fashion. The main differences are:","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"The fit method has only one training argument X, as in MLJModelInterface.fit(model, verbosity, X). However, it has the same return value (fitresult, cache, report). An update method (e.g., for iterative models) can be optionally implemented in the same way.\nA transform method is compulsory and has the same signature as predict, as in MLJModelInterface.transform(model, fitresult, Xnew).\nInstead of defining the target_scitype trait, one declares an output_scitype trait (see above for the meaning).\nAn inverse_transform can be optionally implemented. The signature is the same as transform, as in MLJModelInterface.inverse_transform(model, fitresult, Xout), which:\nmust make sense for any Xout for which scitype(Xout) <: output_scitype(SomeSupervisedModel) (see below); and\nmust return an object Xin satisfying scitype(Xin) <: input_scitype(SomeSupervisedModel).\nA predict method may be optionally implemented, and has the same signature as for supervised models, as in MLJModelInterface.predict(model, fitresult, Xnew). A use-case is clustering algorithms that predict labels and transform new input features into a space of lower-dimension. See Transformers that also predict for an example.","category":"page"},{"location":"adding_models_for_general_use/#Convenience-methods-1","page":"Adding Models for General Use","title":"Convenience methods","text":"","category":"section"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"MLJBase.table\nMLJBase.matrix","category":"page"},{"location":"adding_models_for_general_use/#MLJModelInterface.table","page":"Adding Models for General Use","title":"MLJModelInterface.table","text":"table(columntable; prototype=nothing)\n\nConvert a named tuple of vectors or tuples columntable, into a table of the \"preferred sink type\" of prototype. This is often the type of prototype itself, when prototype is a sink; see the Tables.jl documentation. If prototype is not specified, then a named tuple of vectors is returned.\n\ntable(A::AbstractMatrix; names=nothing, prototype=nothing)\n\nWrap an abstract matrix A as a Tables.jl compatible table with the specified column names (a tuple of symbols). If names are not specified, names=(:x1, :x2, ..., :xn) is used, where n=size(A, 2).\n\nIf a prototype is specified, then the matrix is materialized as a table of the preferred sink type of prototype, rather than wrapped. Note that if prototype is not specified, then matrix(table(A)) is essentially a no-op.\n\n\n\n\n\n","category":"function"},{"location":"adding_models_for_general_use/#MLJModelInterface.matrix","page":"Adding Models for General Use","title":"MLJModelInterface.matrix","text":"matrix(X; transpose=false)\n\nIf X <: AbstractMatrix, return X or permutedims(X) if transpose=true. If X is a Tables.jl compatible table source, convert X into a Matrix.\n\n\n\n\n\n","category":"function"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"MLJModelInterface.int","category":"page"},{"location":"adding_models_for_general_use/#MLJModelInterface.int","page":"Adding Models for General Use","title":"MLJModelInterface.int","text":"int(x; type=nothing)\n\nThe positional integer of the CategoricalString or CategoricalValue x, in the ordering defined by the pool of x. The type of int(x) is the reference type of x.\n\nNot to be confused with x.ref, which is unchanged by reordering of the pool of x, but has the same type.\n\nint(X::CategoricalArray)\nint(W::Array{<:CategoricalString})\nint(W::Array{<:CategoricalValue})\n\nBroadcasted versions of int.\n\njulia> v = categorical([:c, :b, :c, :a])\njulia> levels(v)\n3-element Array{Symbol,1}:\n :a\n :b\n :c\njulia> int(v)\n4-element Array{UInt32,1}:\n 0x00000003\n 0x00000002\n 0x00000003\n 0x00000001\n\nSee also: decoder.\n\n\n\n\n\n","category":"function"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"MLJModelInterface.classes","category":"page"},{"location":"adding_models_for_general_use/#MLJModelInterface.classes","page":"Adding Models for General Use","title":"MLJModelInterface.classes","text":"classes(d::UnivariateFinite)\nclasses(d::UnivariateFiniteArray)\n\nA list of categorial elements in the common pool of classes used to construct d.\n\nv = categorical([\"yes\", \"maybe\", \"no\", \"yes\"])\nd = UnivariateFinite(v[1:2], [0.3, 0.7])\nclasses(d) # CategoricalArray{String,1,UInt32}[\"maybe\", \"no\", \"yes\"]\n\n\n\n\n\nclasses(x)\n\nAll the categorical elements with the same pool as x (including x), returned as a list, with an ordering consistent with the pool. Here x has CategoricalValue or CategoricalString type, and classes(x) is a vector of the same eltype. Note that x in classes(x) is always true.\n\nNot to be confused with levels(x.pool). See the example below.\n\njulia>  v = categorical([:c, :b, :c, :a])\n4-element CategoricalArrays.CategoricalArray{Symbol,1,UInt32}:\n :c\n :b\n :c\n :a\n\njulia> levels(v)\n3-element Array{Symbol,1}:\n :a\n :b\n :c\n\njulia> x = v[4]\nCategoricalArrays.CategoricalValue{Symbol,UInt32} :a\n\njulia> classes(x)\n3-element CategoricalArrays.CategoricalArray{Symbol,1,UInt32}:\n :a\n :b\n :c\n\njulia> levels(x.pool)\n3-element Array{Symbol,1}:\n :a\n :b\n :c\n\n\n\n\n\n","category":"function"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"MLJModelInterface.decoder","category":"page"},{"location":"adding_models_for_general_use/#MLJModelInterface.decoder","page":"Adding Models for General Use","title":"MLJModelInterface.decoder","text":"d = decoder(x)\n\nA callable object for decoding the integer representation of a CategoricalString or CategoricalValue sharing the same pool as x. (Here x is of one of these two types.) Specifically, one has d(int(y)) == y for all y in classes(x). One can also call d on integer arrays, in which case d is broadcast over all elements.\n\njulia> v = categorical([:c, :b, :c, :a])\njulia> int(v)\n4-element Array{UInt32,1}:\n 0x00000003\n 0x00000002\n 0x00000003\n 0x00000001\njulia> d = decoder(v[3])\njulia> d(int(v)) == v\ntrue\n\nWarning: It is not true that int(d(u)) == u always holds.\n\nSee also: int, classes.\n\n\n\n\n\n","category":"function"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"MLJModelInterface.select","category":"page"},{"location":"adding_models_for_general_use/#MLJModelInterface.select","page":"Adding Models for General Use","title":"MLJModelInterface.select","text":"select(X, r, c)\n\nSelect element(s) of a table or matrix at row(s) r and column(s) c. An object of the sink type of X (or a matrix) is returned unless c is a single integer or symbol. In that case a vector is returned, unless r is a single integer, in which case a single element is returned.\n\nSee also: selectrows, selectcols.\n\n\n\n\n\n","category":"function"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"MLJModelInterface.selectrows","category":"page"},{"location":"adding_models_for_general_use/#MLJModelInterface.selectrows","page":"Adding Models for General Use","title":"MLJModelInterface.selectrows","text":"selectrows(X::AbstractNode, r)\n\nReturns a Node object N such that N() = selectrows(X(), r) (and N(rows=s) = selectrows(X(rows=s), r)).\n\n\n\n\n\nselectrows(X, r)\n\nSelect single or multiple rows from a table, abstract vector or matrix X. If X is tabular, the object returned is a table of the preferred sink type of typeof(X), even if only a single row is selected.\n\n\n\n\n\n","category":"function"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"MLJModelInterface.selectcols","category":"page"},{"location":"adding_models_for_general_use/#MLJModelInterface.selectcols","page":"Adding Models for General Use","title":"MLJModelInterface.selectcols","text":"selectcols(X::AbstractNode, c)\n\nReturns Node object N such that N() = selectcols(X(), c).\n\n\n\n\n\nselectcols(X, c)\n\nSelect single or multiple columns from a matrix or table X. If c is an abstract vector of integers or symbols, then the object returned is a table of the preferred sink type of typeof(X). If c is a single integer or column, then an AbstractVector is returned.\n\n\n\n\n\n","category":"function"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"UnivariateFinite","category":"page"},{"location":"adding_models_for_general_use/#MLJBase.UnivariateFinite","page":"Adding Models for General Use","title":"MLJBase.UnivariateFinite","text":"UnivariateFinite(support,\n                 probs;\n                 pool=nothing,\n                 augmented=false,\n                 ordered=false)\n\nConstruct a discrete univariate distribution whose finite support is the elements of the vector support, and whose corresponding probabilities are elements of the vector probs. Alternatively, construct an abstract array of UnivariateFinite distributions by choosing probs to be an array of one higher dimension than the array generated.\n\nUnless pool is specified, support should have type  AbstractVector{<:CategoricalValue} and all elements are assumed to  share the same categorical pool, which may be larger than support.\n\nImportant. All levels of the common pool have associated probabilities, not just those in the specified support. However, these probabilities are always zero (see example below).\n\nIf probs is a matrix, it should have a column for each class in support (or one less, if augment=true). More generally, probs will be an array whose size is of the form (n1, n2, ..., nk, c), where c = length(support) (or one less, if augment=true) and the constructor then returns an array of size (n1, n2, ..., nk).\n\nusing CategoricalArrays\nv = categorical([:x, :x, :y, :x, :z])\n\njulia> UnivariateFinite(classes(v), [0.2, 0.3, 0.5])\nUnivariateFinite{Multiclass{3}}(x=>0.2, y=>0.3, z=>0.5)\n\njulia> d = UnivariateFinite([v[1], v[end]], [0.1, 0.9])\nUnivariateFinite{Multiclass{3}(x=>0.1, z=>0.9)\n\njulia> rand(d, 3)\n3-element Array{Any,1}:\n CategoricalArrays.CategoricalValue{Symbol,UInt32} :z\n CategoricalArrays.CategoricalValue{Symbol,UInt32} :z\n CategoricalArrays.CategoricalValue{Symbol,UInt32} :z\n\njulia> levels(d)\n3-element Array{Symbol,1}:\n :x\n :y\n :z\n\njulia> pdf(d, :y)\n0.0\n\nSpecifying a pool\n\nAlternatively, support may be a list of raw (non-categorical) elements if pool is:\n\nsome CategoricalArray, CategoricalValue or CategoricalPool, such that support is a subset of levels(pool)\nmissing, in which case a new categorical pool is created which has support as its only levels.\n\nIn the last case, specify ordered=true if the pool is to be considered ordered.\n\njulia> UnivariateFinite([:x, :z], [0.1, 0.9], pool=missing, ordered=true)\nUnivariateFinite{OrderedFactor{2}}(x=>0.1, z=>0.9)\n\njulia> d = UnivariateFinite([:x, :z], [0.1, 0.9], pool=v) # v defined above\nUnivariateFinite(x=>0.1, z=>0.9) (Multiclass{3} samples)\n\njulia> pdf(d, :y) # allowed as `:y in levels(v)`\n0.0\n\nv = categorical([:x, :x, :y, :x, :z, :w])\nprobs = rand(100, 3)\nprobs = probs ./ sum(probs, dims=2)\njulia> UnivariateFinite([:x, :y, :z], probs, pool=v)\n100-element UnivariateFiniteVector{Multiclass{4},Symbol,UInt32,Float64}:\n UnivariateFinite{Multiclass{4}}(x=>0.194, y=>0.3, z=>0.505)\n UnivariateFinite{Multiclass{4}}(x=>0.727, y=>0.234, z=>0.0391)\n UnivariateFinite{Multiclass{4}}(x=>0.674, y=>0.00535, z=>0.321)\n   ⋮\n UnivariateFinite{Multiclass{4}}(x=>0.292, y=>0.339, z=>0.369)\n\nProbability augmentation\n\nUnless augment=true, sums of elements along the last axis (row-sums in the case of a matrix) must be equal to one, and otherwise such an array is created by inserting appropriate elements ahead of those provided. This means the provided probabilities are associated with the the classes c2, c3, ..., cn.\n\n\n\nUnivariateFinite(prob_given_class; pool=nothing, ordered=false)\n\nConstruct a discrete univariate distribution whose finite support is the set of keys of the provided dictionary, prob_given_class, and whose values specify the corresponding probabilities.\n\nThe type requirements on the keys of the dictionary are the same as the elements of support given above with this exception: if non-categorical elements (raw labels) are used as keys, then pool=... must be specified and cannot be missing.\n\nIf the values (probabilities) are arrays instead of scalars, then an abstract array of UnivariateFinite elements is created, with the same size as the array.\n\n\n\n\n\n","category":"type"},{"location":"adding_models_for_general_use/#Where-to-place-code-implementing-new-models-1","page":"Adding Models for General Use","title":"Where to place code implementing new models","text":"","category":"section"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"Note that different packages can implement models having the same name without causing conflicts, although an MLJ user cannot simultaneously load two such models.","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"There are two options for making a new model implementation available to all MLJ users:","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"Native implementations (preferred option). The implementation code lives in the same package that contains the learning algorithms implementing the interface. In this case, it is sufficient to open an issue at MLJ requesting the package to be registered with MLJ. Registering a package allows the MLJ user to access its models' metadata and to selectively load them.\nExternal implementations (short-term alternative). The model implementation code is necessarily separate from the package SomePkg defining the learning algorithm being wrapped. In this case, the recommended procedure is to include the implementation code at MLJModels/src via a pull-request, and test code at MLJModels/test. Assuming SomePkg is the only package imported by the implementation code, one needs to: (i) register SomePkg with MLJ as explained above; and (ii) add a corresponding @require line in the PR to MLJModels/src/MLJModels.jl to enable lazy-loading of that package by MLJ (following the pattern of existing additions). If other packages must be imported, add them to the MLJModels project file after checking they are not already there. If it is really necessary, packages can be also added to Project.toml for testing purposes.","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"Additionally, one needs to ensure that the implementation code defines the package_name and load_path model traits appropriately, so that MLJ's @load macro can find the necessary code (see MLJModels/src for examples). The @load command can only be tested after registration. If changes are made, lodge an new issue at MLJ requesting your changes to be updated.","category":"page"},{"location":"adding_models_for_general_use/#How-to-add-models-to-the-MLJ-model-registry?-1","page":"Adding Models for General Use","title":"How to add models to the MLJ model registry?","text":"","category":"section"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"The MLJ model registry is located in the MLJModels.jl repository. To add a model, you need to follow these steps","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"Ensure your model conforms to the interface defined above\nRaise an issue at MLJModels.jl and point out where the MLJ-interface implementation is, e.g. by providing a link to the code.\nAn administrator will then review your implementation and work with you to add the model to the registry","category":"page"},{"location":"NEWS/#MLJ-News-1","page":"MLJ News","title":"MLJ News","text":"","category":"section"},{"location":"NEWS/#","page":"MLJ News","title":"MLJ News","text":"News for MLJ and its satellite packages: MLJBase, MLJModels, and ScientificTypes.","category":"page"},{"location":"NEWS/#Latest-release-notes-1","page":"MLJ News","title":"Latest release notes","text":"","category":"section"},{"location":"NEWS/#","page":"MLJ News","title":"MLJ News","text":"MLJ (general users)","category":"page"},{"location":"NEWS/#","page":"MLJ News","title":"MLJ News","text":"MLJBase |  MLJModels |  ScientificTypes (mainly for developers)","category":"page"},{"location":"NEWS/#News-1","page":"MLJ News","title":"News","text":"","category":"section"},{"location":"NEWS/#","page":"MLJ News","title":"MLJ News","text":"Note: New patch releases are no longer being announced below. Refer to the links above for complete release notes.","category":"page"},{"location":"NEWS/#Oct-2019-1","page":"MLJ News","title":"30 Oct 2019","text":"","category":"section"},{"location":"NEWS/#","page":"MLJ News","title":"MLJ News","text":"MLJModels 0.5.3 released.\nMLJBase 0.7.2 released.","category":"page"},{"location":"NEWS/#Oct-2019-2","page":"MLJ News","title":"22 Oct 2019","text":"","category":"section"},{"location":"NEWS/#","page":"MLJ News","title":"MLJ News","text":"MLJ 0.5.1 released.","category":"page"},{"location":"NEWS/#Oct-2019-3","page":"MLJ News","title":"21 Oct 2019","text":"","category":"section"},{"location":"NEWS/#","page":"MLJ News","title":"MLJ News","text":"MLJBase 0.7.1 released.\nScientificTypes 0.2.2 released.\nMLJModels  0.5.2 released.","category":"page"},{"location":"NEWS/#Oct-2019-4","page":"MLJ News","title":"17 Oct 2019","text":"","category":"section"},{"location":"NEWS/#","page":"MLJ News","title":"MLJ News","text":"MLJBase 0.7 released.","category":"page"},{"location":"NEWS/#Oct-2019-5","page":"MLJ News","title":"11 Oct 2019","text":"","category":"section"},{"location":"NEWS/#","page":"MLJ News","title":"MLJ News","text":"MLJModels 0.5.1 released.","category":"page"},{"location":"NEWS/#Sep-2019-1","page":"MLJ News","title":"30 Sep 2019","text":"","category":"section"},{"location":"NEWS/#","page":"MLJ News","title":"MLJ News","text":"MLJ 0.5 released.","category":"page"},{"location":"NEWS/#Sep-2019-2","page":"MLJ News","title":"29 Sep 2019","text":"","category":"section"},{"location":"NEWS/#","page":"MLJ News","title":"MLJ News","text":"MLJModels 0.5 released.","category":"page"},{"location":"NEWS/#Sep-2019-3","page":"MLJ News","title":"26 Sep 2019","text":"","category":"section"},{"location":"NEWS/#","page":"MLJ News","title":"MLJ News","text":"MLJBase 0.6 released.","category":"page"},{"location":"NEWS/#Older-release-notes-1","page":"MLJ News","title":"Older release notes","text":"","category":"section"},{"location":"NEWS/#MLJ-0.4.0-1","page":"MLJ News","title":"MLJ 0.4.0","text":"","category":"section"},{"location":"NEWS/#","page":"MLJ News","title":"MLJ News","text":"(Enhancment) Update to MLJBase 0.5.0 and MLJModels 0.4.0. In  particular, this updates considerably the list of wrapped  scikit-learn models available to the MLJ user:\nScikitLearn.jl\nSVM: SVMClassifier, SVMRegressor, SVMNuClassifier, SVMNuRegressor, SVMLClassifier, SVMLRegressor,\nLinear Models (regressors): ARDRegressor, BayesianRidgeRegressor, ElasticNetRegressor, ElasticNetCVRegressor, HuberRegressor, LarsRegressor, LarsCVRegressor, LassoRegressor, LassoCVRegressor, LassoLarsRegressor, LassoLarsCVRegressor, LassoLarsICRegressor, LinearRegressor, OrthogonalMatchingPursuitRegressor, OrthogonalMatchingPursuitCVRegressor, PassiveAggressiveRegressor, RidgeRegressor, RidgeCVRegressor, SGDRegressor, TheilSenRegressor\n(Enhancement) The macro @pipeline allows one to construct linear (non-branching) pipeline composite models with one line of code. One may include static transformations (ordinary functions) in the pipeline, as well as target transformations for the supervised case (when one component model is supervised).\n(Breaking) Source nodes (type Source) now have a kind field, which is either :input,:target or :other, with :input the default value in the source constructor.  If building a learning network, and the network is to be exported as a standalone model, then it is now necessary to tag the source nodes accordingly, as in Xs = source(X) and ys = source(y, kind=:target).\n(Breaking) By virtue of the preceding change, the syntax for exporting a learning network is simplified. Do?@from_network for details. Also, one now uses fitresults(N) instead of fit results(N, X, y) and fitresults(N, X) when exporting a learning network N \"by hand\"; see the updated manual for details.\n(Breaking) One must explicitly state if a supervised learning network being exported with @from_network is probabilistic by adding is_probablistic=true to the macro expression. Before, this information was unreliably inferred from the network.\n(Enhancement) Add macro-free method for loading model code into an arbitrary module. Do ?load for details.\n(Enhancement) @load now returns a mode instance with default hyperparameters (instead of nothing), as in tree_model = @load DecisionTreeRegressor\n(Breaking) info(\"PCA\") now returns a named-tuple, instead of a dictionary, of the properties of a the model named \"PCA\"\n(Breaking) The list returned by models(conditional) is now a list of complete metadata entries (named-tuples, as returned by info). An entry proxy appears in the list exactly when conditional(proxy) == true.  Model query is simplified; for example models() do model model.is_supervised && model.is_pure_julia end finds all pure julia supervised models.\n(Bug fix) Introduce new private methods to avoid relying on MLJBase type piracy MLJBase #30.\n(Enhancement) If composite is a a learning network exported as a model, and m = machine(composite, args...) then report(m) returns the reports for each machine in the learning network, and similarly for fitted_params(m).\n(Enhancement) MLJ.table, vcat and hcat now overloaded for AbstractNode, so that they can immediately be used in defining learning networks. For example, if X = source(rand(20,3)) and y=source(rand(20)) then MLJ.table(X) and vcat(y, y) both make sense and define new nodes.\n(Enhancement) pretty(X) prints a pretty version of any table X, complete with types and scitype annotations. Do ?pretty for options. A wrap of pretty_table from PrettyTables.jl.\n(Enhancement) std is re-exported from Statistics\n(Enhancement) The manual and MLJ cheatsheet have been updated.\nPerformance measures have been migrated to MLJBase, while the model registry and model load/search facilities have migrated to MLJModels. As relevant methods are re-exported to MLJ, this is unlikely to effect many users.","category":"page"},{"location":"NEWS/#MLJModels-0.4.0-1","page":"MLJ News","title":"MLJModels 0.4.0","text":"","category":"section"},{"location":"NEWS/#","page":"MLJ News","title":"MLJ News","text":"(Enhancement) Add a number of scikit-learn model wraps. See the above MLJ 0.4.0 release notes for a detailed list.\nThe following have all been migrated to MLJModels from MLJ:\nMLJ's built-in models (e.g., basic transformers such as OneHotEncoder)","category":"page"},{"location":"NEWS/#","page":"MLJ News","title":"MLJ News","text":"- The model registry metadata (src/registry/METADATA.toml)\n\n- The metadata `@update` facility for administrator registration\n  of new models\n  \n- The `@load` macro and `load` function for loading code for a registered model\n\n- The `models` and `localmodels` model-search functions\n\n- The `info` command for returning the metadata entry of a model","category":"page"},{"location":"NEWS/#","page":"MLJ News","title":"MLJ News","text":"(Breaking) MLJBase v0.5.0, which introduces some changes and additions to model traits, is a requirement, meaning the format of metadata as changed.\n(Breaking) The model method for retrieving model metadata has been renamed back to info, but continues to return a named-tuple. (The MLJBase.info method, returning the dictionary form of the metadata, is now called MLJBase.info_dic).","category":"page"},{"location":"NEWS/#MLJBase-0.5.0-1","page":"MLJ News","title":"MLJBase 0.5.0","text":"","category":"section"},{"location":"NEWS/#","page":"MLJ News","title":"MLJ News","text":"Bump ScientificTypes requirement to v0.2.0\n(Enhancement) The performance measures API (built-in measures + adaptor for external measures) from MLJ has been migrated to MLJBase. MLJ.\n(Breaking) info, which returns a dictionary (needed for TOML serialization) is renamed to info_dic. In this way \"info\" is reserved for a method in MLJModels/MLJ that returns a more-convenient named-tuple\n(Breaking) The is_probabilistic model trait is replaced with prediction_type, which can have the values :deterministic, :probabilistic or :interval, to allow for models predicting real intervals, and for consistency with measures API.\n(Bug fix, mildly breaking) The package_license model trait is now included in info_dict in the case of unsupervisd models.\n(Enhancement, mildly breaking) Add new model traits hyperparameters,  hyperparameter_types, docstring, and implemented_operations (fit, predict, inverse_transform, etc) (#36, #37, #38)\n(Enhancement) The MLJBase.table and MLJBase.matrix operations are now direct wraps of the corresponding Tables.jl operations for improved performance. In particular MLJBase.matrix(MLJBase.table(A)) is essentially a non-operation, and one can pass MLJBase.matrix the keyword argument transpose=... .\n(Breaking) The built-in dataset methods load_iris, load_boston, load_ames, load_reduced_ames, load_crabs return a raw DataFrame, instead of an MLJTask object, and continue to require import CSV to become available. However, macro versions @load_iris, etc, are always available, automatically triggering import CSV; these macros return a tuple (X, y) of input DataFrame and target vector y, with scitypes appropriately coerced. (MLJ #224)\n(Enhancement) selectrows now works for matrices. Needed to allow matrices as \"node type\" in MLJ learning networks; see MLJ #209.\n(Bug) Fix problem with == for MLJType objects (#35)\n(Breaking) Update requirement on ScientficTypes.jl to v0.2.0 to mitigate bug with coercion of column scitypes for tables that are also AbstractVectors, and to make coerce more convenient.\n(Enhancement) Add new method unpack for splitting tables, as in y, X = unpack(df,==(:target),!=(:dummy)). See  doc-string for details. \n(Bug fix) Remove type piracy in get/setproperty! (#30)","category":"page"},{"location":"NEWS/#ScientificTypes-0.2.0-1","page":"MLJ News","title":"ScientificTypes 0.2.0","text":"","category":"section"},{"location":"NEWS/#","page":"MLJ News","title":"MLJ News","text":"(Breaking) The argument order is switched in coerce methods. So now use coerce(v, T) for a vector v and scientific type T and coerce(X, d) for a table X and dictionary d.\n(Feature) You can now call coerce on tables without needing to wrap specs in a dictionary, as in scitype(X, :age => Continuous, :ncalls => Count).","category":"page"},{"location":"NEWS/#ScientficTypes-0.1.3-1","page":"MLJ News","title":"ScientficTypes 0.1.3","text":"","category":"section"},{"location":"NEWS/#","page":"MLJ News","title":"MLJ News","text":"Release notes","category":"page"},{"location":"NEWS/#MLJ-0.4.0-2","page":"MLJ News","title":"MLJ 0.4.0","text":"","category":"section"},{"location":"NEWS/#","page":"MLJ News","title":"MLJ News","text":"Introduction of traits for measures (loss functions, etc); see top of /src/measures.jl for definitions. This\nallows user to use loss functions from LossFunctions.jl,\nenables improved measure checks and error message reporting with measures\nallows evaluate! to report per-observation measures when available (for later use by Bayesian optimisers, for example)\nallows support for sample-weighted measures playing nicely with rest of API\nImprovements to resampling: \nevaluate! method now reports per-observation measures when available\nsample weights can be passed to evaluate! for use by measures that support weights\nuser can pass a list of train/evaluation pairs of row indices directly to evaluate!, in place of a ResamplingStrategy object\nimplementing a new ResamplingStrategy is now straightforward (see docs)\none can call evaluate (no exclamation mark) directly on model + data without first constructing a machine, if desired\nDoc strings and the manual have been revised and updated. The manual includes a new section \"Tuning models\", and extra material under \"Learning networks\" explaining how to export learning networks as stand-alone models using the @from_network macro.\nImproved checks and error-reporting for binding models to data in machines.\n(Breaking) CSV is now an optional dependency, which means you now need to import CSV before you can load tasks with load_boston(), load_iris(), load_crabs(), load_ames(), load_reduced_ames()\nAdded schema method for tables (re-exported from ScientificTypes.jl). Returns a named tuple with keys :names, :types, :scitypes and :nrows.\n(Breaking) Eliminate scitypes method. The scientific types of a table are returned as part of ScientificTypes schema method (see above)","category":"page"},{"location":"NEWS/#MLJModels-0.3.0-1","page":"MLJ News","title":"MLJModels 0.3.0","text":"","category":"section"},{"location":"NEWS/#","page":"MLJ News","title":"MLJ News","text":"Release notes","category":"page"},{"location":"NEWS/#MLJBase-v0.4.0-1","page":"MLJ News","title":"MLJBase v0.4.0","text":"","category":"section"},{"location":"NEWS/#","page":"MLJ News","title":"MLJ News","text":"Release notes","category":"page"},{"location":"NEWS/#ScientificTypes-0.1.2-1","page":"MLJ News","title":"ScientificTypes 0.1.2","text":"","category":"section"},{"location":"NEWS/#","page":"MLJ News","title":"MLJ News","text":"New package to which the scientific types API has been moved (from MLJBase).","category":"page"},{"location":"NEWS/#MLJBase-v0.3.0-1","page":"MLJ News","title":"MLJBase v0.3.0","text":"","category":"section"},{"location":"NEWS/#","page":"MLJ News","title":"MLJ News","text":"Make CSV an optional dependency (breaking). To use load_iris(), load_ames(), etc, need first to import CSV.","category":"page"},{"location":"NEWS/#MLJBase-v0.2.4-1","page":"MLJ News","title":"MLJBase v0.2.4","text":"","category":"section"},{"location":"NEWS/#","page":"MLJ News","title":"MLJ News","text":"Add ColorImage and GreyImage scitypes\nOverload in method for subtypes of Model (apparently causing Julia crashes in an untagged commit, because of a method signature ambiguity, now resolved).","category":"page"},{"location":"NEWS/#MLJ-v0.2.5-1","page":"MLJ News","title":"MLJ v0.2.5","text":"","category":"section"},{"location":"NEWS/#","page":"MLJ News","title":"MLJ News","text":"Add MLJ cheatsheet\nAllow models to query specific traits, in addition to tasks. Query ?models for details\nadd @from_networks macro for exporting learning networks as models (experimental). ","category":"page"},{"location":"NEWS/#MLJModels-v0.2.4-1","page":"MLJ News","title":"MLJModels v0.2.4","text":"","category":"section"},{"location":"NEWS/#","page":"MLJ News","title":"MLJ News","text":"Add compatibility requirement MLJBase=\"0.2.3\" ","category":"page"},{"location":"NEWS/#MLJBase-v0.2.3-1","page":"MLJ News","title":"MLJBase v0.2.3","text":"","category":"section"},{"location":"NEWS/#","page":"MLJ News","title":"MLJ News","text":"Small changes on definitions of == and isequal for MLJType objects. In particular, fields that are random number generators may change state without effecting an object's == equivalence class. \nAdd @set_defaults macro for generating keywork constructors for   Model subtypes. ","category":"page"},{"location":"NEWS/#","page":"MLJ News","title":"MLJ News","text":"Add abstract type UnsupervisedNetwork <: Unsupervised.","category":"page"},{"location":"NEWS/#MLJ-v0.2.3-1","page":"MLJ News","title":"MLJ v0.2.3","text":"","category":"section"},{"location":"NEWS/#","page":"MLJ News","title":"MLJ News","text":"Fixed bug in models(::MLJTask) method which excluded some relevant models. (#153)\nFixed some broken links to the tour.ipynb.","category":"page"},{"location":"NEWS/#MLJ-v0.2.2-1","page":"MLJ News","title":"MLJ v0.2.2","text":"","category":"section"},{"location":"NEWS/#","page":"MLJ News","title":"MLJ News","text":"Resolved these isssues: \nSpecifying new rows in calls to fit! on a Node not triggering retraining. (#147)","category":"page"},{"location":"NEWS/#","page":"MLJ News","title":"MLJ News","text":"- fit! of Node sometimes calls `update` on model when it should\n  call `fit` on model\n  [(#146)](https://github.com/alan-turing-institute/MLJ.jl/issues/146)\n\n- Error running the tour.ipynb notebook\n  [(#140)](https://github.com/alan-turing-institute/MLJ.jl/issues/140)\n\n- For reproducibility, include a Manifest.toml file with all\n  examples. [(#137)](https://github.com/alan-turing-institute/MLJ.jl/issues/137)","category":"page"},{"location":"NEWS/#","page":"MLJ News","title":"MLJ News","text":"Activated overalls code coverage (#131)","category":"page"},{"location":"NEWS/#","page":"MLJ News","title":"MLJ News","text":"Removed local version of MultivariateStats (now in MLJModels, see below).\nMinor changes to OneHotEncoder, in line with scitype philosophy.","category":"page"},{"location":"NEWS/#MLJBase-v0.2.2-1","page":"MLJ News","title":"MLJBase v0.2.2","text":"","category":"section"},{"location":"NEWS/#","page":"MLJ News","title":"MLJ News","text":"Fix some minor bugs. \nAdded compatibility requirement CSV v0.5 or higher to allow removal of allowmissing keyword in CSV.read, which is to be depreciated.","category":"page"},{"location":"NEWS/#Announcement:-MLJ-tutorial-and-development-sprint-1","page":"MLJ News","title":"Announcement: MLJ tutorial and development sprint","text":"","category":"section"},{"location":"NEWS/#","page":"MLJ News","title":"MLJ News","text":"Details here Applications close May 29th 5pm (GMTT + 1 = London)","category":"page"},{"location":"NEWS/#MLJModels-v0.2.3-1","page":"MLJ News","title":"MLJModels v0.2.3","text":"","category":"section"},{"location":"NEWS/#","page":"MLJ News","title":"MLJ News","text":"The following support vector machine models from LIBSVM.jl have been added: EpsilonSVR, LinearSVC, NuSVR, NuSVC, SVC, OneClassSVM.","category":"page"},{"location":"NEWS/#MLJModels-v0.2.2-1","page":"MLJ News","title":"MLJModels v0.2.2","text":"","category":"section"},{"location":"NEWS/#","page":"MLJ News","title":"MLJ News","text":"MulitivariateStats models RidgeRegressor and PCA migrated here from MLJ. Addresses: MLJ #125.","category":"page"},{"location":"NEWS/#MLJModels-v0.2.1-1","page":"MLJ News","title":"MLJModels v0.2.1","text":"","category":"section"},{"location":"NEWS/#","page":"MLJ News","title":"MLJ News","text":"ScikitLearn wraps ElasticNet and ElasticNetCV now available (and registered at MLJRegistry). Resolves: MLJ #112","category":"page"},{"location":"NEWS/#MLJ-v0.2.1-1","page":"MLJ News","title":"MLJ v0.2.1","text":"","category":"section"},{"location":"NEWS/#","page":"MLJ News","title":"MLJ News","text":"Fix a bug and related problem in \"Getting Started\" docs: [#126](https://github.com/alan-turing-institute/MLJ.jl/issues/126 .","category":"page"},{"location":"NEWS/#MLJBase-0.2.0,-MLJModels-0.2.0,-MLJ-0.2.0-1","page":"MLJ News","title":"MLJBase 0.2.0, MLJModels 0.2.0, MLJ 0.2.0","text":"","category":"section"},{"location":"NEWS/#","page":"MLJ News","title":"MLJ News","text":"Model API refactored to resolve #93 and #119 and hence simplify the model interface. This breaks all implementations of supervised models, and some scitype methods. However, for the regular user the effects are restricted to: (i) no more target_type hyperparameter for some models; (ii) Deterministic{Node} is now DeterministicNetwork and Probabillistic{Node} is now ProbabilisticNetwork when exporting learning networks as models.\nNew feature: Task constructors now allow the user to explicitly specify scitypes of features/target. There is a coerce method for vectors and tables for the user who wants to do this manually. Resolves: #119","category":"page"},{"location":"NEWS/#Official-registered-versions-of-MLJBase-0.1.1,-MLJModels-0.1.1,-MLJ-0.1.1-released-1","page":"MLJ News","title":"Official registered versions of MLJBase 0.1.1, MLJModels 0.1.1, MLJ 0.1.1 released","text":"","category":"section"},{"location":"NEWS/#","page":"MLJ News","title":"MLJ News","text":"Minor revisions to the repos, doc updates, and a small breaking change around scitype method names and associated traits. Resolves: #119","category":"page"},{"location":"NEWS/#unversioned-commits-12-April-2019-(around-00:10,-GMT)-1","page":"MLJ News","title":"unversioned commits 12 April 2019 (around 00:10, GMT)","text":"","category":"section"},{"location":"NEWS/#","page":"MLJ News","title":"MLJ News","text":"Added out-of-bag estimates for performance in homogeneous ensembles. Resolves: #77","category":"page"},{"location":"NEWS/#unversioned-commits-11-April-2019-(before-noon,-GMT)-1","page":"MLJ News","title":"unversioned commits 11 April 2019 (before noon, GMT)","text":"","category":"section"},{"location":"NEWS/#","page":"MLJ News","title":"MLJ News","text":"Removed dependency on unregistered package TOML.jl (using, Pkg.TOML instead). Resolves #113","category":"page"},{"location":"NEWS/#unversioned-commits-8-April-2019-(some-time-after-20:00-GMT)-1","page":"MLJ News","title":"unversioned commits 8 April 2019 (some time after 20:00 GMT)","text":"","category":"section"},{"location":"NEWS/#","page":"MLJ News","title":"MLJ News","text":"Addition of XGBoost models XGBoostRegressor, XGBoostClassifier and XGBoostCount. Resolves #65.\nDocumentation reorganized as GitHub pages. Includes some additions but still a work in progress.","category":"page"},{"location":"NEWS/#unversioned-commits-1-March-2019-(some-time-after-03:50-GMT)-1","page":"MLJ News","title":"unversioned commits 1 March 2019 (some time after 03:50 GMT)","text":"","category":"section"},{"location":"NEWS/#","page":"MLJ News","title":"MLJ News","text":"Addition of \"scientific type\" hierarchy, including Continuous, Discrete, Multiclass, and Other subtypes of Found (to complement Missing). See Getting Started for more one this.  Resolves: #86\nRevamp of model traits to take advantage of scientific types, with output_kind replaced with target_scitype_union, input_kind replaced with input_scitype. Also, output_quantity dropped, input_quantity replaced with Bool-valued input_is_multivariate, and is_pure_julia made Bool-valued. Trait definitions in all model implementations and effected meta-algorithms have been updated. Related: #81\nSubstantial update of the core guide Adding New Models to reflect above changes and in response to new model implementer queries. Some design \"decisions\" regarding multivariate targets now explict there.\nthe order the y and yhat arguments of measures (aka loss functions) have been reversed. Progress on: #91\nUpdate of Standardizer and OneHotEncoder to mesh with new scitypes.\nNew improved task constructors infer task metadata from data scitypes. This brings us close to a simple implementation of basic task-model matching. Query the doc-strings for SupervisedTask and UnsupervisedTask for details.  Machines can now dispatch on tasks instead of X and y. A task, task, is now callable: task() returns (X, y) for supervised models, and X for unsupervised models.  Progress on:  #86\nthe data in the load_ames() test task has been replaced by the full data set, and load_reduced_ames() now loads a reduced set.","category":"page"},{"location":"machines/#Machines-1","page":"Machines","title":"Machines","text":"","category":"section"},{"location":"machines/#","page":"Machines","title":"Machines","text":"Under the hood, calling fit! on a machine calls either MLJBase.fit or MLJBase.update, depending on the machine's internal state (as recorded in private fields old_model and old_rows). These lower-level fit and update methods, which are not ordinarily called directly by the user, dispatch on the model and a view of the data defined by the optional rows keyword argument of fit! (all rows by default). In this way, if a model update method has been implemented for the model, calls to fit! can avoid redundant calculations for certain kinds of model mutations (eg, increasing the number of epochs in a neural network).","category":"page"},{"location":"machines/#","page":"Machines","title":"Machines","text":"using MLJ; color_off() # hide\nforest = EnsembleModel(atom=(@load DecisionTreeClassifier), n=10);\nX, y = @load_iris;\nmach = machine(forest, X, y)\nfit!(mach, verbosity=2);","category":"page"},{"location":"machines/#","page":"Machines","title":"Machines","text":"Generally, changing a hyperparameter triggers retraining on calls to subsequent fit!:","category":"page"},{"location":"machines/#","page":"Machines","title":"Machines","text":"forest.bagging_fraction=0.5\nfit!(mach, verbosity=2);","category":"page"},{"location":"machines/#","page":"Machines","title":"Machines","text":"However, for this iterative model, increasing the iteration parameter only adds models to the existing ensemble:","category":"page"},{"location":"machines/#","page":"Machines","title":"Machines","text":"forest.n=15\nfit!(mach, verbosity=2);","category":"page"},{"location":"machines/#","page":"Machines","title":"Machines","text":"Call fit! again without making a change and no retraining occurs:","category":"page"},{"location":"machines/#","page":"Machines","title":"Machines","text":"fit!(mach);","category":"page"},{"location":"machines/#","page":"Machines","title":"Machines","text":"However, retraining can be forced:","category":"page"},{"location":"machines/#","page":"Machines","title":"Machines","text":"fit!(mach, force=true);","category":"page"},{"location":"machines/#","page":"Machines","title":"Machines","text":"And is re-triggered if the view of the data changes:","category":"page"},{"location":"machines/#","page":"Machines","title":"Machines","text":"fit!(mach, rows=1:100);","category":"page"},{"location":"machines/#","page":"Machines","title":"Machines","text":"fit!(mach, rows=1:100);","category":"page"},{"location":"machines/#Inspecting-machines-1","page":"Machines","title":"Inspecting machines","text":"","category":"section"},{"location":"machines/#","page":"Machines","title":"Machines","text":"There are two methods for inspecting the outcomes of training in MLJ. To obtain a named-tuple describing the learned parameters (in a user-friendly way where possible) use fitted_params(mach). All other training-related outcomes are inspected with report(mach).","category":"page"},{"location":"machines/#","page":"Machines","title":"Machines","text":"X, y = @load_iris\npca = @load PCA\nmach = machine(pca, X)\nfit!(mach)","category":"page"},{"location":"machines/#","page":"Machines","title":"Machines","text":"fitted_params(mach)\nreport(mach)","category":"page"},{"location":"machines/#","page":"Machines","title":"Machines","text":"fitted_params\nreport","category":"page"},{"location":"machines/#MLJModelInterface.fitted_params","page":"Machines","title":"MLJModelInterface.fitted_params","text":"fitted_params(mach)\n\nReturn the learned parameters for a machine mach that has been fit!, for example the coefficients in a linear model.\n\nThis is a named tuple and human-readable if possible.\n\nIf mach is a machine for a composite model, such as a model constructed using @pipeline, then the returned named tuple has the composite type's field names as keys. The corresponding value is the fitted parameters for the machine in the underlying learning network bound to that model. (If multiple machines share the same model, then the value is a vector.)\n\nusing MLJ\n@load LogisticClassifier pkg=MLJLinearModels\nX, y = @load_crabs;\npipe = @pipeline Standardizer LogisticClassifier\nmach = machine(pipe, X, y) |> fit!\n\njulia> fitted_params(mach).logistic_classifier\n(classes = CategoricalArrays.CategoricalValue{String,UInt32}[\"B\", \"O\"],\n coefs = Pair{Symbol,Float64}[:FL => 3.7095037897680405, :RW => 0.1135739140854546, :CL => -1.6036892745322038, :CW => -4.415667573486482, :BD => 3.238476051092471],\n intercept = 0.0883301599726305,)\n\nAdditional keys, machines and fitted_params_given_machine, give a list of all machines in the underlying network, and a dictionary of fitted parameters keyed on those machines.\n\n```\n\n\n\n\n\n","category":"function"},{"location":"machines/#MLJBase.report","page":"Machines","title":"MLJBase.report","text":"report(mach)\n\nReturn the report for a machine mach that has been fit!, for example the coefficients in a linear model.\n\nThis is a named tuple and human-readable if possible.\n\nIf mach is a machine for a composite model, such as a model constructed using @pipeline, then the returned named tuple has the composite type's field names as keys. The corresponding value is the report for the machine in the underlying learning network bound to that model. (If multiple machines share the same model, then the value is a vector.)\n\nusing MLJ\n@load LinearBinaryClassifier pkg=GLM\nX, y = @load_crabs;\npipe = @pipeline Standardizer LinearBinaryClassifier\nmach = machine(pipe, X, y) |> fit!\n\njulia> report(mach).linear_binary_classifier\n(deviance = 3.8893386087844543e-7,\n dof_residual = 195.0,\n stderror = [18954.83496713119, 6502.845740757159, 48484.240246060406, 34971.131004997274, 20654.82322484894, 2111.1294584763386],\n vcov = [3.592857686311793e8 9.122732393971942e6 … -8.454645589364915e7 5.38856837634321e6; 9.122732393971942e6 4.228700272808351e7 … -4.978433790526467e7 -8.442545425533723e6; … ; -8.454645589364915e7 -4.978433790526467e7 … 4.2662172244975924e8 2.1799125705781363e7; 5.38856837634321e6 -8.442545425533723e6 … 2.1799125705781363e7 4.456867590446599e6],)\n\n\nAdditional keys, machines and report_given_machine, give a list of all machines in the underlying network, and a dictionary of reports keyed on those machines.\n\n```\n\n\n\n\n\n","category":"function"},{"location":"machines/#Constructing-machines-1","page":"Machines","title":"Constructing machines","text":"","category":"section"},{"location":"machines/#","page":"Machines","title":"Machines","text":"A machine is constructed with the syntax machine(model, args...) where the possibilities for args (called training arguments) are summarized in table below. Here X and y represent inputs and target, respectively, and Xout the output of a transform call. Machines for supervised models may have additional training arguments, such as a vector of per-observation weights (in which case supports_weights(model) == true).","category":"page"},{"location":"machines/#","page":"Machines","title":"Machines","text":"model supertype machine constructor calls operation calls (first compulsory)\nDeterministic <: Supervised machine(model, X, y, extras...) predict(mach, Xnew), transform(mach, Xnew), inverse_transform(mach, Xout)\nProbabilistic <: Supervised machine(model, X, y, extras...) predict(mach, Xnew), predict_mean(mach, Xnew), predict_median(mach, Xnew), predict_mode(mach, Xnew), transform(mach, Xnew), inverse_transform(mach, Xout)\nUnsupervised (except Static) machine(model, X) transform(mach, Xnew), inverse_transform(mach, Xout), predict(mach, Xnew)\nStatic machine(model) transform(mach, Xnews...), inverse_transform(mach, Xout)","category":"page"},{"location":"machines/#","page":"Machines","title":"Machines","text":"All operations on machines (predict, transform, etc) have exactly one argument (Xnew or Xout above) after mach, the machine instance. An exception is a machine bound to a Static model, which can have any number of arguments after mach. For more on Static transformers (which have no training arguments) see Static transformers.","category":"page"},{"location":"machines/#","page":"Machines","title":"Machines","text":"A machine is reconstructed from a file using the syntax machine(\"my_machine.jlso\"), or machine(\"my_machine.jlso\", args...) if retraining using new data. See Saving machines below.","category":"page"},{"location":"machines/#Constructing-machines-in-learning-networks-1","page":"Machines","title":"Constructing machines in learning networks","text":"","category":"section"},{"location":"machines/#","page":"Machines","title":"Machines","text":"Instead of data X, y, etc,  the machine constructor is provided Node or Source objects (\"dynamic data\") when building a learning network. See Composing Models for more on this advanced feature. One also uses machine to wrap a machine around a whole learning network; see Learning network machines.","category":"page"},{"location":"machines/#Saving-machines-1","page":"Machines","title":"Saving machines","text":"","category":"section"},{"location":"machines/#","page":"Machines","title":"Machines","text":"To save a machine to file, use the MLJ.save command:","category":"page"},{"location":"machines/#","page":"Machines","title":"Machines","text":"tree = @load DecisionTreeClassifier\nmach = fit!(machine(tree, X, y))\nMLJ.save(\"my_machine.jlso\", mach)","category":"page"},{"location":"machines/#","page":"Machines","title":"Machines","text":"To de-serialize, one uses the machine constructor:","category":"page"},{"location":"machines/#","page":"Machines","title":"Machines","text":"mach2 = machine(\"my_machine.jlso\")\npredict(mach2, Xnew);","category":"page"},{"location":"machines/#","page":"Machines","title":"Machines","text":"The machine mach2 cannot be retrained; however, by providing data to the constructor one can enable retraining using the saved model hyperparameters (which overwrites the saved learned parameters):","category":"page"},{"location":"machines/#","page":"Machines","title":"Machines","text":"mach3 = machine(\"my_machine.jlso\", Xnew, ynew)\nfit!(mach3)","category":"page"},{"location":"machines/#Internals-1","page":"Machines","title":"Internals","text":"","category":"section"},{"location":"machines/#","page":"Machines","title":"Machines","text":"For a supervised machine the predict method calls a lower-level MLJBase.predict method, dispatched on the underlying model and the fitresult (see below). To see predict in action, as well as its unsupervised cousins transform and inverse_transform, see Getting Started.","category":"page"},{"location":"machines/#","page":"Machines","title":"Machines","text":"The fields of a Machine instance (which should not generally be accessed by the user) are:","category":"page"},{"location":"machines/#","page":"Machines","title":"Machines","text":"model - the struct containing the hyperparameters to be used in calls to fit!\nfitresult - the learned parameters in a raw form, initially undefined\nargs - a tuple of the data, each element wrapped in a source node; see Learning Networks (in the supervised learning example above, args = (source(X), source(y)))\nreport - outputs of training not encoded in fitresult (eg, feature rankings)\nold_model - a deep copy of the model used in the last call to fit!\nold_rows -  a copy of the row indices used in last call to fit!\ncache","category":"page"},{"location":"machines/#","page":"Machines","title":"Machines","text":"The interested reader can learn more on machine internals by examining the simplified code excerpt in Internals.","category":"page"},{"location":"machines/#API-Reference-1","page":"Machines","title":"API Reference","text":"","category":"section"},{"location":"machines/#","page":"Machines","title":"Machines","text":"MLJBase.machine\nfit!\nfit_only!\nMLJBase.save","category":"page"},{"location":"machines/#MLJBase.machine","page":"Machines","title":"MLJBase.machine","text":"machine(model, args...)\n\nConstruct a Machine object binding a model, storing hyper-parameters of some machine learning algorithm, to some data, args. When building a learning network, Node objects can be substituted for concrete data.\n\nmachine(Xs; oper1=node1, oper2=node2)\nmachine(Xs, ys; oper1=node1, oper2=node2)\nmachine(Xs, ys, extras...; oper1=node1, oper2=node2, ...)\n\nConstruct a special machine called a learning network machine, that \"wraps\" a learning network, usually in preparation to export the network as a stand-alone composite model type. The keyword arguments declare what nodes are called when operations, such as predict and transform, are called on the machine.\n\nIn addition to the operations named in the constructor, the methods fit!, report, and fitted_params can be applied as usual to the machine constructed.\n\nmachine(Probablistic(), args...; kwargs...)\nmachine(Deterministic(), args...; kwargs...)\nmachine(Unsupervised(), args...; kwargs...)\nmachine(Static(), args...; kwargs...)\n\nSame as above, but specifying explicitly the kind of model the learning network is to meant to represent.\n\nLearning network machines are not to be confused with an ordinary machine that happens to be bound to a stand-alone composite model (i.e., an exported learning network).\n\nExamples\n\nSupposing a supervised learning network's final predictions are obtained by calling a node yhat, then the code\n\nmach = machine(Deterministic(), Xs, ys; predict=yhat)\nfit!(mach; rows=train)\npredictions = predict(mach, Xnew) # `Xnew` concrete data\n\nis  equivalent to\n\nfit!(yhat, rows=train)\npredictions = yhat(Xnew)\n\nHere Xs and ys are the source nodes receiving, respectively, the input and target data.\n\nIn a unsupervised learning network for clustering, with single source node Xs for inputs, and in which the node Xout delivers the output of dimension reduction, and yhat the class labels, one can write\n\nmach = machine(Unsupervised(), Xs; transform=Xout, predict=yhat)\nfit!(mach)\ntransformed = transform(mach, Xnew) # `Xnew` concrete data\npredictions = predict(mach, Xnew)\n\nwhich is equivalent to\n\nfit!(Xout)\nfit!(yhat)\ntransformed = Xout(Xnew)\npredictions = yhat(Xnew)\n\n\n\n\n\n","category":"function"},{"location":"machines/#StatsBase.fit!","page":"Machines","title":"StatsBase.fit!","text":"fit!(mach::Machine, rows=nothing, verbosity=1, force=false)\n\nFit the machine mach. In the case that mach has Node arguments, first train all other machines on which mach depends.\n\nTo attempt to fit a machine without touching any other machine, use fit_only!. For more on the internal logic of fitting see fit_only!\n\n\n\n\n\nfit!(N::Node;\n     rows=nothing,\n     verbosity=1,\n     force=false,\n     acceleration=CPU1())\n\nTrain all machines required to call the node N, in an appropriate order.  These machines are those returned by machines(N).\n\n\n\n\n\nfit!(mach::Machine{<:Surrogate};\n     rows=nothing,\n     acceleration=CPU1(),\n     verbosity=1,\n     force=false))\n\nTrain the complete learning network wrapped by the machine mach.\n\nMore precisely, if s is the learning network signature used to construct mach, then call fit!(N), where N = glb(values(s)...) is a greatest lower bound on the nodes appearing in the signature. For example, if s = (predict=yhat, transform=W), then call fit!(glb(yhat, W)). Here glb is tuple overloaded for nodes.\n\nSee also machine\n\n\n\n\n\n","category":"function"},{"location":"machines/#MLJBase.fit_only!","page":"Machines","title":"MLJBase.fit_only!","text":"MLJBase.fit_only!(mach::Machine; rows=nothing, verbosity=1, force=false)\n\nWithout mutating any other machine on which it may depend, perform one of the following actions to the machine mach, using the data and model bound to it, and restricting the data to rows if specified:\n\nAb initio training. Ignoring any previous learned parameters and cache, compute and store new learned parameters. Increment mach.state.\nTraining update. Making use of previous learned parameters and/or  cache, replace or mutate existing learned parameters. The effect is  the same (or nearly the same) as in ab initio training, but may be  faster or use less memory, assuming the model supports an update  option (implements MLJBase.update). Increment mach.state.\nNo-operation. Leave existing learned parameters untouched. Do not  increment mach.state.\n\nTraining action logic\n\nFor the action to be a no-operation, either mach.frozen == true or none of the following apply:\n\n(i) mach has never been trained (mach.state == 0).\n(ii) force == true\n(iii) The state of some other machine on which mach depends has changed since the last time mach was trained (ie, the last time mach.state was last incremented)\n(iv) The specified rows have changed since the last retraining.\n(v) mach.model has changed since the last retraining.\n\nIn cases (i) - (iv), mach is trained ab initio. In case (v) a training update is applied.\n\nTo freeze or unfreeze mach, use freeze!(mach) or thaw!(mach).\n\nImplementation detail\n\nThe data to which a machine is bound is stored in mach.args. Each element of args is either a Node object, or, in the case that concrete data was bound to the machine, it is concrete data wrapped in a Source node. In all cases, to obtain concrete data for actual training, each argument N is called, as in N() or N(rows=rows), and either MLJBase.fit (ab initio training) or MLJBase.update (training update) is dispatched on mach.model and this data. See the \"Adding models for general use\" section of the MLJ documentation for more on these lower-level training methods.\n\n\n\n\n\n","category":"function"},{"location":"machines/#MLJModelInterface.save","page":"Machines","title":"MLJModelInterface.save","text":"MLJ.save(filename, mach::Machine; kwargs...)\nMLJ.save(io, mach::Machine; kwargs...)\n\nMLJBase.save(filename, mach::Machine; kwargs...)\nMLJBase.save(io, mach::Machine; kwargs...)\n\nSerialize the machine mach to a file with path filename, or to an input/output stream io (at least IOBuffer instances are supported).\n\nThe format is JLSO (a wrapper for julia native or BSON serialization). For some model types, a custom serialization will be additionally performed.\n\nKeyword arguments\n\nThese keyword arguments are passed to the JLSO serializer:\n\nkeyword values default\nformat :julia_serialize, :BSON :julia_serialize\ncompression :gzip, :none :none\n\nSee https://github.com/invenia/JLSO.jl for details.\n\nAny additional keyword arguments are passed to model-specific serializers.\n\nMachines are de-serialized using the machine constructor as shown in the example below. Data (or nodes) may be optionally passed to the constructor for retraining on new data using the saved model.\n\nExample\n\nusing MLJ\ntree = @load DecisionTreeClassifier\nX, y = @load_iris\nmach = fit!(machine(tree, X, y))\n\nMLJ.save(\"tree.jlso\", mach, compression=:none)\nmach_predict_only = machine(\"tree.jlso\")\npredict(mach_predict_only, X)\n\nmach2 = machine(\"tree.jlso\", selectrows(X, 1:100), y[1:100])\npredict(mach2, X) # same as above\n\nfit!(mach2) # saved learned parameters are over-written\npredict(mach2, X) # not same as above\n\n# using a buffer:\nio = IOBuffer()\nMLJ.save(io, mach)\nseekstart(io)\npredict_only_mach = machine(io)\npredict(predict_only_mach, X)\n\nwarning: Only load files from trusted sources\nMaliciously constructed JLSO files, like pickles, and most other general purpose serialization formats, can allow for arbitrary code execution during loading. This means it is possible for someone to use a JLSO file that looks like a serialized MLJ machine as a Trojan horse.\n\n\n\n\n\n","category":"function"},{"location":"glossary/#Glossary-1","page":"Glossary","title":"Glossary","text":"","category":"section"},{"location":"glossary/#","page":"Glossary","title":"Glossary","text":"Note: This glossary includes some detail intended mainly for MLJ developers.","category":"page"},{"location":"glossary/#Basics-1","page":"Glossary","title":"Basics","text":"","category":"section"},{"location":"glossary/#hyper-parameters-1","page":"Glossary","title":"hyper-parameters","text":"","category":"section"},{"location":"glossary/#","page":"Glossary","title":"Glossary","text":"Parameters on which some learning algorithm depends, specified before the algorithm is applied, and where learning is interpreted in the broadest sense. For example, PCA feature reduction is a \"preprocessing\" transformation \"learning\" a projection from training data, governed by a dimension hyperparameter. Hyper-Parameters in our sense may specify configuration (eg, number of parallel processes) even when this does not effect the end-product of learning. (But we exclude verbosity level.)","category":"page"},{"location":"glossary/#model-(object-of-abstract-type-Model)-1","page":"Glossary","title":"model (object of abstract type Model)","text":"","category":"section"},{"location":"glossary/#","page":"Glossary","title":"Glossary","text":"Object collecting together hyperameters of a single algorithm.  Models are classified either as supervised or unsupervised models (eg, \"transformers\"), with corresponding subtypes Supervised <: Model and Unsupervised <: Model.","category":"page"},{"location":"glossary/#fit-result-(type-generally-defined-outside-of-MLJ)-1","page":"Glossary","title":"fit-result (type generally defined outside of MLJ)","text":"","category":"section"},{"location":"glossary/#","page":"Glossary","title":"Glossary","text":"Also known as \"learned\" or \"fitted\" parameters, these are \"weights\", \"coefficients\", or similar paramaters learned by an algorithm, after adopting the prescribed hyper-parameters. For example, decision trees of a random forest, the coefficients and intercept of a linear model, or the rotation and projection matrices of PCA reduction scheme.","category":"page"},{"location":"glossary/#operation-1","page":"Glossary","title":"operation","text":"","category":"section"},{"location":"glossary/#","page":"Glossary","title":"Glossary","text":"Data-manipulating operations (methods) parameterized by some fit-result. For supervised learners, the predict, predict_mean, predict_median, or predict_mode methods; for transformers, the transform or inverse_transform method. An operation may also refer to an ordinary data-manipulating method that does not depend on a fit-result (e.g., a broadcasted logarithm) which is then called static operation for clarity. An operation that is not static is dynamic.","category":"page"},{"location":"glossary/#machine-(object-of-type-Machine)-1","page":"Glossary","title":"machine (object of type Machine)","text":"","category":"section"},{"location":"glossary/#","page":"Glossary","title":"Glossary","text":"An object consisting of:","category":"page"},{"location":"glossary/#","page":"Glossary","title":"Glossary","text":"(1) A model","category":"page"},{"location":"glossary/#","page":"Glossary","title":"Glossary","text":"(2) A fit-result (undefined until training)","category":"page"},{"location":"glossary/#","page":"Glossary","title":"Glossary","text":"(3) Training arguments (one for each data argument of the model's associated fit method). A training argument is data used for training. Generally, there are two training arguments for supervised models, and just one for unsuperivsed models. In a learning network (see below) the training arguments are nodes, instead of concrete data, but which can be called to (lazily) return concrete data.","category":"page"},{"location":"glossary/#","page":"Glossary","title":"Glossary","text":"In addition, machines store \"report\" metadata, for recording algorithm-specific statistics of training (eg, internal estimate of generalization error, feature importances); and they cache information allowing the fit-result to be updated without repeating unnecessary information.","category":"page"},{"location":"glossary/#","page":"Glossary","title":"Glossary","text":"Machines are trained by calls to a fit! method which may be passed an optional argument specifying the rows of data to be used in training.","category":"page"},{"location":"glossary/#Learning-Networks-and-Composite-Models-1","page":"Glossary","title":"Learning Networks and Composite Models","text":"","category":"section"},{"location":"glossary/#","page":"Glossary","title":"Glossary","text":"Note: Multiple machines in a learning network may share the same model, and multiple learning nodes may share the same machine.","category":"page"},{"location":"glossary/#source-node-(object-of-type-Source)-1","page":"Glossary","title":"source node (object of type Source)","text":"","category":"section"},{"location":"glossary/#","page":"Glossary","title":"Glossary","text":"A container for training data and point of entry for new data in a learning network (see below).","category":"page"},{"location":"glossary/#node-(object-of-type-Node)-1","page":"Glossary","title":"node (object of type Node)","text":"","category":"section"},{"location":"glossary/#","page":"Glossary","title":"Glossary","text":"Essentially a machine (whose arguments are possibly other nodes) wrapped in an associated operation (e.g., predict or inverse_transform). It consists primarily of:","category":"page"},{"location":"glossary/#","page":"Glossary","title":"Glossary","text":"An operation, static or dynamic.\nA machine, or nothing if the operation is static.\nUpstream connections to other nodes, specified by a list of arguments (one for each argument of the operation). These are the arguments on which the operation \"acts\" when the node N is called, as in N().","category":"page"},{"location":"glossary/#learning-network-1","page":"Glossary","title":"learning network","text":"","category":"section"},{"location":"glossary/#","page":"Glossary","title":"Glossary","text":"An acyclic directed graph implicit in the connections of a collection of source(s) and nodes. ","category":"page"},{"location":"glossary/#wrapper-1","page":"Glossary","title":"wrapper","text":"","category":"section"},{"location":"glossary/#","page":"Glossary","title":"Glossary","text":"Any model with one or more other models as hyper-parameters.","category":"page"},{"location":"glossary/#composite-model-1","page":"Glossary","title":"composite model","text":"","category":"section"},{"location":"glossary/#","page":"Glossary","title":"Glossary","text":"Any wrapper, or any learning network, \"exported\" as a model (see Composing Models).","category":"page"},{"location":"internals/#Internals-1","page":"Internals","title":"Internals","text":"","category":"section"},{"location":"internals/#The-machine-interface,-simplified-1","page":"Internals","title":"The machine interface, simplified","text":"","category":"section"},{"location":"internals/#","page":"Internals","title":"Internals","text":"The following is simplified description of the Machine interface. See also the Glossary","category":"page"},{"location":"internals/#The-Machine-type-1","page":"Internals","title":"The Machine type","text":"","category":"section"},{"location":"internals/#","page":"Internals","title":"Internals","text":"mutable struct Machine{M<Model}\n\n    model::M\n    fitresult\n    cache\n    args::Tuple    # e.g., (X, y) for supervised models\n    report\n    previous_rows # remember last rows used\n\n    function Machine{M}(model::M, args...) where M<:Model\n        machine = new{M}(model)\n        machine.args = args\n        machine.report = Dict{Symbol,Any}()\n        return machine\n    end\n\nend","category":"page"},{"location":"internals/#Constructor-1","page":"Internals","title":"Constructor","text":"","category":"section"},{"location":"internals/#","page":"Internals","title":"Internals","text":"machine(model::M, Xtable, y) = Machine{M}(model, Xtable, y)","category":"page"},{"location":"internals/#fit!-and-predict/transform-1","page":"Internals","title":"fit! and predict/transform","text":"","category":"section"},{"location":"internals/#","page":"Internals","title":"Internals","text":"function fit!(mach::Machine; rows=nothing, force=false, verbosity=1)\n\n    warning = clean!(mach.model)\n    isempty(warning) || verbosity < 0 || @warn warning\n\n    if rows === nothing\n        rows = (:)\n    end\n\n    rows_have_changed  = (!isdefined(mach, :previous_rows) ||\n\t    rows != mach.previous_rows)\n\n    args = [MLJ.selectrows(arg, rows) for arg in mach.args]\n\n    if !isdefined(mach, :fitresult) || rows_have_changed || force\n        mach.fitresult, mach.cache, report =\n            fit(mach.model, verbosity, args...)\n    else # call `update`:\n        mach.fitresult, mach.cache, report =\n            update(mach.model, verbosity, mach.fitresult, mach.cache, args...)\n    end\n\n    if rows_have_changed\n        mach.previous_rows = deepcopy(rows)\n    end\n\n    if report !== nothing\n        merge!(mach.report, report)\n    end\n\n    return mach\n\nend\n\nfunction predict(machine::Machine{<:Supervised}, Xnew)\n    if isdefined(machine, :fitresult)\n        return predict(machine.model, machine.fitresult, Xnew))\n    else\n        throw(error(\"$machine is not trained and so cannot predict.\"))\n    end\nend\n\nfunction transform(machine::Machine{<:Unsupervised}, Xnew)\n    if isdefined(machine, :fitresult)\n        return transform(machine.model, machine.fitresult, Xnew))\n    else\n        throw(error(\"$machine is not trained and so cannot transform.\"))\n    end\nend","category":"page"},{"location":"julia_blogpost/#","page":"Julia BlogPost","title":"Julia BlogPost","text":"warning: Old post\nThis post is quite old. For a newer overview of the design of MLJ, see here","category":"page"},{"location":"julia_blogpost/#Beyond-machine-learning-pipelines-with-MLJ-1","page":"Julia BlogPost","title":"Beyond machine learning pipelines with MLJ","text":"","category":"section"},{"location":"julia_blogpost/#","page":"Julia BlogPost","title":"Julia BlogPost","text":"Anthony Blaom, Diego Arenas, Franz Kiraly, Yiannis Simillides, Sebastian Vollmer","category":"page"},{"location":"julia_blogpost/#","page":"Julia BlogPost","title":"Julia BlogPost","text":"May 1st, 2019. Blog post also posted on the Julia Language Blog","category":"page"},{"location":"julia_blogpost/#","page":"Julia BlogPost","title":"Julia BlogPost","text":"(Image: ) (Image: )\n(Image: ) (Image: )","category":"page"},{"location":"julia_blogpost/#Introducing-MLJ-1","page":"Julia BlogPost","title":"Introducing MLJ","text":"","category":"section"},{"location":"julia_blogpost/#","page":"Julia BlogPost","title":"Julia BlogPost","text":"MLJ is an open-source machine learning toolbox written in pure Julia. It provides a uniform interface for interacting with supervised and unsupervised learning models currently scattered in different Julia packages.","category":"page"},{"location":"julia_blogpost/#","page":"Julia BlogPost","title":"Julia BlogPost","text":"Building on a earlier proof-of-concept, development began in earnest at The Alan Turing Institute in December 2018. In a short time interest grew and the project is now the Institute's most starred software repository.","category":"page"},{"location":"julia_blogpost/#","page":"Julia BlogPost","title":"Julia BlogPost","text":"After outlining MLJ's current functionality, this post introduces MLJ learning networks, a super-charged pipelining feature for model composition.","category":"page"},{"location":"julia_blogpost/#","page":"Julia BlogPost","title":"Julia BlogPost","text":"Quick links:","category":"page"},{"location":"julia_blogpost/#","page":"Julia BlogPost","title":"Julia BlogPost","text":"MLJ vs ScikitLearn.jl  \nVideo from London Julia User Group meetup in March 2019 (skip to demo at 21'39) &nbsp;\nMLJ Tutorials\nImplementing the MLJ interface for a new model\nHow to contribute\nJulia Slack channel: #mlj.\nStar'ing us to show support for MLJ would be greatly appreciated!","category":"page"},{"location":"julia_blogpost/#MLJ-features-1","page":"Julia BlogPost","title":"MLJ features","text":"","category":"section"},{"location":"julia_blogpost/#","page":"Julia BlogPost","title":"Julia BlogPost","text":"MLJ already has substantial functionality:","category":"page"},{"location":"julia_blogpost/#","page":"Julia BlogPost","title":"Julia BlogPost","text":"Learning networks. Flexible model composition beyond traditional pipelines (more on this below).\nAutomatic tuning. Automated tuning of hyperparameters, including composite models. Tuning implemented as a model wrapper for composition with other meta-algorithms.\nHomogeneous model ensembling.\nRegistry for model metadata. Metadata available without loading model code. Basis of a \"task\" interface and facilitates model composition.\nTask interface. Automatically match models to specified learning tasks, to streamline benchmarking and model selection.\nClean probabilistic API. Improves support for Bayesian statistics and probabilistic graphical models.\nData container agnostic. Present and manipulate data in your favorite Tables.jl format.\nUniversal adoption of categorical data types. Enables model implementations to properly account for classes seen in training but not in evaluation.","category":"page"},{"location":"julia_blogpost/#","page":"Julia BlogPost","title":"Julia BlogPost","text":"Enhancements planned for the near future include integration of Flux.jl deep learning models, and gradient descent tuning of continuous hyperparameters using automatic differentiation.","category":"page"},{"location":"julia_blogpost/#","page":"Julia BlogPost","title":"Julia BlogPost","text":"While a relatively small number of machine learning models currently implement the MLJ interface, work in progress aims to wrap models supported by the popular python framework, scikit-learn, as a temporary expedient. For a comparison of the MLJ's design with the Julia wrap ScitLearn.jl, see this FAQ.","category":"page"},{"location":"julia_blogpost/#Learning-networks-1","page":"Julia BlogPost","title":"Learning networks","text":"","category":"section"},{"location":"julia_blogpost/#","page":"Julia BlogPost","title":"Julia BlogPost","text":"MLJ's model composition interface is flexible enough to implement, for example, the model stacks popular in data science competitions. To treat examples of this kind, the interface design must account for the fact that information flow in prediction and training modes is different. This can be seen from the following schematic of a simple two-model stack, viewed as a network:","category":"page"},{"location":"julia_blogpost/#","page":"Julia BlogPost","title":"Julia BlogPost","text":"(Image: )","category":"page"},{"location":"julia_blogpost/#Building-a-simple-network-1","page":"Julia BlogPost","title":"Building a simple network","text":"","category":"section"},{"location":"julia_blogpost/#","page":"Julia BlogPost","title":"Julia BlogPost","text":"In MLJ, networks of models are built using a declarative syntax already familiar from basic use of the package. For example, the ordinary syntax for training a decision tree in MLJ, after one-hot encoding the categorical features, looks like this:","category":"page"},{"location":"julia_blogpost/#","page":"Julia BlogPost","title":"Julia BlogPost","text":"using MLJ\n@load DecisionTreeRegressor\n\n# load some data:\ntask = load_reduced_ames();\nX, y = task();\n\n# one-hot encode the inputs, X:\nhot_model = OneHotEncoder()\nhot = machine(hot_model, X)\nfit!(hot)\nXt = transform(hot, X)\n\n# fit a decision tree to the transformed data:\ntree_model = DecisionTreeRegressor()\ntree = machine(tree_model, Xt, y)\nfit!(tree, rows = 1:1300)","category":"page"},{"location":"julia_blogpost/#","page":"Julia BlogPost","title":"Julia BlogPost","text":"Note that a model in MLJ is just a struct containing hyperparameters. Wrapping a model in data delivers a machine struct, which will additionally record the results of training.","category":"page"},{"location":"julia_blogpost/#","page":"Julia BlogPost","title":"Julia BlogPost","text":"Without a pipeline, each time we want to present new data for prediction we must first apply one-hot encoding:","category":"page"},{"location":"julia_blogpost/#","page":"Julia BlogPost","title":"Julia BlogPost","text":"Xnew = X[1301:1400,:];\nXnewt = transform(hot, Xnew);\nyhat = predict(tree, Xnewt);\nyhat[1:3]\n 3-element Array{Float64,1}:\n  223956.9999999999\n  320142.85714285733\n  161227.49999999994","category":"page"},{"location":"julia_blogpost/#","page":"Julia BlogPost","title":"Julia BlogPost","text":"To build a pipeline one simply wraps the supplied data in source nodes and repeats similar declarations, omitting calls to fit!. The difference now is that each \"variable\" (e.g., Xt, yhat) is a node of our pipeline, instead of concrete data:","category":"page"},{"location":"julia_blogpost/#","page":"Julia BlogPost","title":"Julia BlogPost","text":"Xs = source(X)\nys = source(y)\n\nhot = machine(hot_model, Xs)\nXt = transform(hot, Xs);\n\ntree = machine(tree_model, Xt, ys)\nyhat = predict(tree, Xt)","category":"page"},{"location":"julia_blogpost/#","page":"Julia BlogPost","title":"Julia BlogPost","text":"If we like, we can think of a node as dynamic data - \"data\" because it can be called (indexed) on rows, but \"dynamic\" because the result depends on the outcome of training events, which in turn depend on hyperparameter values. For example, after fitting the completed pipeline, we can make new predictions like this:","category":"page"},{"location":"julia_blogpost/#","page":"Julia BlogPost","title":"Julia BlogPost","text":"fit!(yhat, rows=1:1300)\n [ Info: Training NodalMachine @ 1…51.\n [ Info: Spawned 1300 sub-features to one-hot encode feature :Neighborhood.\n [ Info: Spawned 1300 sub-features to one-hot encode feature :MSSubClass.\n [ Info: Training NodalMachine @ 1…17.\n Node @ 1…79 = predict(1…17, transform(1…51, 1…07))\n\nyhat(rows=1301:1302) # to predict on rows of source node\nyhat(Xnew)           # to predict on new data\n156-element Array{Float64,1}:\n 223956.9999999999\n 320142.85714285733\n ...","category":"page"},{"location":"julia_blogpost/#Exporting-and-retraining-1","page":"Julia BlogPost","title":"Exporting and retraining","text":"","category":"section"},{"location":"julia_blogpost/#","page":"Julia BlogPost","title":"Julia BlogPost","text":"Once a pipeline like this has been built and tested on sample data, it can be exported as a stand-alone model, ready to be trained on any dataset. For details, see the MLJ documentation. In the future, Julia macros will allow common architectures (e.g., linear pipelines) to be built in a couple of lines.","category":"page"},{"location":"julia_blogpost/#","page":"Julia BlogPost","title":"Julia BlogPost","text":"Finally, we mention that MLJ learning networks, and their exported counterparts, are \"smart\" in the sense that changing a hyperparameter does not trigger retraining of component models upstream of the change:","category":"page"},{"location":"julia_blogpost/#","page":"Julia BlogPost","title":"Julia BlogPost","text":"tree_model.max_depth = 4\nfit!(yhat, rows=1:1300)\n [ Info: Not retraining NodalMachine @ 1…51. It is up-to-date.\n [ Info: Updating NodalMachine @ 1…17.\n Node @ 1…79 = predict(1…17, transform(1…51, 1…07))","category":"page"},{"location":"julia_blogpost/#Just-\"Write-the-math!\"-1","page":"Julia BlogPost","title":"Just \"Write the math!\"","text":"","category":"section"},{"location":"julia_blogpost/#","page":"Julia BlogPost","title":"Julia BlogPost","text":"Because of Julia's generic programming features, any kind of operation you would normally apply to data (arithmetic, row selection, column concatenation, etc) can be overloaded to work with nodes. In this way, MLJ's network-building syntax is economical, intuitive and easy to read. In this respect we have been inspired by On Machine Learning and Programming Languages.","category":"page"},{"location":"julia_blogpost/#Invitation-to-the-community-1","page":"Julia BlogPost","title":"Invitation to the community","text":"","category":"section"},{"location":"julia_blogpost/#","page":"Julia BlogPost","title":"Julia BlogPost","text":"We now invite the community to try out our newly registered packages, MLJalongside MLJModels, and provide any feedback or suggestions you may have going forward. We are also particularly interested in hearing how you would use our package, and what features it may be lacking.","category":"page"},{"location":"api/#Index-of-Methods-1","page":"Index of Methods","title":"Index of Methods","text":"","category":"section"},{"location":"api/#","page":"Index of Methods","title":"Index of Methods","text":"","category":"page"},{"location":"mlj_cheatsheet/#MLJ-Cheatsheet-1","page":"MLJ Cheatsheet","title":"MLJ Cheatsheet","text":"","category":"section"},{"location":"mlj_cheatsheet/#Starting-an-interactive-MLJ-session-1","page":"MLJ Cheatsheet","title":"Starting an interactive MLJ session","text":"","category":"section"},{"location":"mlj_cheatsheet/#","page":"MLJ Cheatsheet","title":"MLJ Cheatsheet","text":"using MLJ\nMLJ_VERSION # version of MLJ for this cheatsheet","category":"page"},{"location":"mlj_cheatsheet/#Model-search-and-code-loading-1","page":"MLJ Cheatsheet","title":"Model search and code loading","text":"","category":"section"},{"location":"mlj_cheatsheet/#","page":"MLJ Cheatsheet","title":"MLJ Cheatsheet","text":"info(\"PCA\") retrieves registry metadata for the model called \"PCA\"","category":"page"},{"location":"mlj_cheatsheet/#","page":"MLJ Cheatsheet","title":"MLJ Cheatsheet","text":"info(\"RidgeRegressor\", pkg=\"MultivariateStats\") retrieves metadata for \"RidgeRegresssor\", which is provided by multiple packages","category":"page"},{"location":"mlj_cheatsheet/#","page":"MLJ Cheatsheet","title":"MLJ Cheatsheet","text":"models() lists metadata of every registered model.","category":"page"},{"location":"mlj_cheatsheet/#","page":"MLJ Cheatsheet","title":"MLJ Cheatsheet","text":"models(x -> x.is_supervised && x.is_pure_julia) lists all supervised models written in pure julia.","category":"page"},{"location":"mlj_cheatsheet/#","page":"MLJ Cheatsheet","title":"MLJ Cheatsheet","text":"models(matching(X)) lists all unsupervised models compatible with input X.","category":"page"},{"location":"mlj_cheatsheet/#","page":"MLJ Cheatsheet","title":"MLJ Cheatsheet","text":"models(matching(X, y)) lists all supervised models compatible with input/target X/y.","category":"page"},{"location":"mlj_cheatsheet/#","page":"MLJ Cheatsheet","title":"MLJ Cheatsheet","text":"With additional conditions:","category":"page"},{"location":"mlj_cheatsheet/#","page":"MLJ Cheatsheet","title":"MLJ Cheatsheet","text":"models() do model\n    matching(model, X, y) &&\n    model.prediction_type == :probabilistic &&\n        model.is_pure_julia\nend","category":"page"},{"location":"mlj_cheatsheet/#","page":"MLJ Cheatsheet","title":"MLJ Cheatsheet","text":"tree = @load DecisionTreeClassifier to load code and instantiate \"DecisionTreeClassifier\" model","category":"page"},{"location":"mlj_cheatsheet/#","page":"MLJ Cheatsheet","title":"MLJ Cheatsheet","text":"tree2  = DecisionTreeClassifier(max_depth=2) instantiates a model type already in scope","category":"page"},{"location":"mlj_cheatsheet/#","page":"MLJ Cheatsheet","title":"MLJ Cheatsheet","text":"ridge = @load RidgeRegressor pkg=MultivariateStats loads and instantiates a model provided by multiple packages","category":"page"},{"location":"mlj_cheatsheet/#Scitypes-and-coercion-1","page":"MLJ Cheatsheet","title":"Scitypes and coercion","text":"","category":"section"},{"location":"mlj_cheatsheet/#","page":"MLJ Cheatsheet","title":"MLJ Cheatsheet","text":"scitype(x) is the scientific type of x. For example scitype(2.4) == Continuous","category":"page"},{"location":"mlj_cheatsheet/#","page":"MLJ Cheatsheet","title":"MLJ Cheatsheet","text":"(Image: scitypes.png)","category":"page"},{"location":"mlj_cheatsheet/#","page":"MLJ Cheatsheet","title":"MLJ Cheatsheet","text":"type scitype\nAbstractFloat Continuous\nInteger Count\nCategoricalValue and CategoricalString Multiclass or OrderedFactor","category":"page"},{"location":"mlj_cheatsheet/#","page":"MLJ Cheatsheet","title":"MLJ Cheatsheet","text":"Figure and Table for common scalar scitypes","category":"page"},{"location":"mlj_cheatsheet/#","page":"MLJ Cheatsheet","title":"MLJ Cheatsheet","text":"Use schema(X) to get the column scitypes of a table X","category":"page"},{"location":"mlj_cheatsheet/#","page":"MLJ Cheatsheet","title":"MLJ Cheatsheet","text":"coerce(y, Multiclass) attempts coercion of all elements of y into scitype Multiclass","category":"page"},{"location":"mlj_cheatsheet/#","page":"MLJ Cheatsheet","title":"MLJ Cheatsheet","text":"coerce(X, :x1 => Continuous, :x2 => OrderedFactor) to coerce columns :x1 and :x2 of table X.","category":"page"},{"location":"mlj_cheatsheet/#","page":"MLJ Cheatsheet","title":"MLJ Cheatsheet","text":"coerce(X, Count => Continuous) to coerce all columns with Count scitype to Continuous.","category":"page"},{"location":"mlj_cheatsheet/#Ingesting-data-1","page":"MLJ Cheatsheet","title":"Ingesting data","text":"","category":"section"},{"location":"mlj_cheatsheet/#","page":"MLJ Cheatsheet","title":"MLJ Cheatsheet","text":"Splitting any table into target and input (note semicolon):","category":"page"},{"location":"mlj_cheatsheet/#","page":"MLJ Cheatsheet","title":"MLJ Cheatsheet","text":"using RDatasets\nchanning = dataset(\"boot\", \"channing\")\ny, X =  unpack(channing,\n               ==(:Exit),            # y is the :Exit column\n               !=(:Time);            # X is the rest, except :Time\n               :Exit=>Continuous,    # correcting wrong scitypes (optional)\n               :Entry=>Continuous,\n               :Cens=>Multiclass)","category":"page"},{"location":"mlj_cheatsheet/#","page":"MLJ Cheatsheet","title":"MLJ Cheatsheet","text":"Warning. Before julia 1.2 use col -> col != :Time insead of !=(:Time).","category":"page"},{"location":"mlj_cheatsheet/#","page":"MLJ Cheatsheet","title":"MLJ Cheatsheet","text":"Splitting row indices into train/validation/test:","category":"page"},{"location":"mlj_cheatsheet/#","page":"MLJ Cheatsheet","title":"MLJ Cheatsheet","text":"train, valid, test = partition(eachindex(y), 0.7, 0.2, shuffle=true, rng=1234) for 70:20:10 ratio","category":"page"},{"location":"mlj_cheatsheet/#","page":"MLJ Cheatsheet","title":"MLJ Cheatsheet","text":"For a stratified split:","category":"page"},{"location":"mlj_cheatsheet/#","page":"MLJ Cheatsheet","title":"MLJ Cheatsheet","text":"train, test = partition(eachindex(y), 0.8, stratify=y)","category":"page"},{"location":"mlj_cheatsheet/#","page":"MLJ Cheatsheet","title":"MLJ Cheatsheet","text":"Getting data from OpenML:","category":"page"},{"location":"mlj_cheatsheet/#","page":"MLJ Cheatsheet","title":"MLJ Cheatsheet","text":"table = openML.load(91)","category":"page"},{"location":"mlj_cheatsheet/#","page":"MLJ Cheatsheet","title":"MLJ Cheatsheet","text":"Creating synthetic classification data:","category":"page"},{"location":"mlj_cheatsheet/#","page":"MLJ Cheatsheet","title":"MLJ Cheatsheet","text":"X, y = make_blobs(100, 2) (also: make_moons, make_circles)","category":"page"},{"location":"mlj_cheatsheet/#","page":"MLJ Cheatsheet","title":"MLJ Cheatsheet","text":"Creating synthetic regression data:","category":"page"},{"location":"mlj_cheatsheet/#","page":"MLJ Cheatsheet","title":"MLJ Cheatsheet","text":"X, y = make_regression(100, 2)","category":"page"},{"location":"mlj_cheatsheet/#Machine-construction-1","page":"MLJ Cheatsheet","title":"Machine construction","text":"","category":"section"},{"location":"mlj_cheatsheet/#","page":"MLJ Cheatsheet","title":"MLJ Cheatsheet","text":"Supervised case:","category":"page"},{"location":"mlj_cheatsheet/#","page":"MLJ Cheatsheet","title":"MLJ Cheatsheet","text":"model = KNNRegressor(K=1) and mach = machine(model, X, y)","category":"page"},{"location":"mlj_cheatsheet/#","page":"MLJ Cheatsheet","title":"MLJ Cheatsheet","text":"Unsupervised case:","category":"page"},{"location":"mlj_cheatsheet/#","page":"MLJ Cheatsheet","title":"MLJ Cheatsheet","text":"model = OneHotEncoder() and mach = machine(model, X)","category":"page"},{"location":"mlj_cheatsheet/#Fitting-1","page":"MLJ Cheatsheet","title":"Fitting","text":"","category":"section"},{"location":"mlj_cheatsheet/#","page":"MLJ Cheatsheet","title":"MLJ Cheatsheet","text":"fit!(mach, rows=1:100, verbosity=1, force=false)","category":"page"},{"location":"mlj_cheatsheet/#Prediction-1","page":"MLJ Cheatsheet","title":"Prediction","text":"","category":"section"},{"location":"mlj_cheatsheet/#","page":"MLJ Cheatsheet","title":"MLJ Cheatsheet","text":"Supervised case: predict(mach, Xnew) or predict(mach, rows=1:100)","category":"page"},{"location":"mlj_cheatsheet/#","page":"MLJ Cheatsheet","title":"MLJ Cheatsheet","text":"Similarly, for probabilistic models: predict_mode, predict_mean and predict_median.","category":"page"},{"location":"mlj_cheatsheet/#","page":"MLJ Cheatsheet","title":"MLJ Cheatsheet","text":"Unsupervised case: transform(mach, rows=1:100) or inverse_transform(mach, rows), etc.","category":"page"},{"location":"mlj_cheatsheet/#Inspecting-objects-1","page":"MLJ Cheatsheet","title":"Inspecting objects","text":"","category":"section"},{"location":"mlj_cheatsheet/#","page":"MLJ Cheatsheet","title":"MLJ Cheatsheet","text":"@more gets detail on last object in REPL","category":"page"},{"location":"mlj_cheatsheet/#","page":"MLJ Cheatsheet","title":"MLJ Cheatsheet","text":"params(model) gets nested-tuple of all hyperparameters, even nested ones","category":"page"},{"location":"mlj_cheatsheet/#","page":"MLJ Cheatsheet","title":"MLJ Cheatsheet","text":"info(ConstantRegressor()), info(\"PCA\"), info(\"RidgeRegressor\", pkg=\"MultivariateStats\") gets all properties (aka traits) of registered models","category":"page"},{"location":"mlj_cheatsheet/#","page":"MLJ Cheatsheet","title":"MLJ Cheatsheet","text":"info(rms) gets all properties of a performance measure","category":"page"},{"location":"mlj_cheatsheet/#","page":"MLJ Cheatsheet","title":"MLJ Cheatsheet","text":"schema(X) get column names, types and scitypes, and nrows, of a table X","category":"page"},{"location":"mlj_cheatsheet/#","page":"MLJ Cheatsheet","title":"MLJ Cheatsheet","text":"scitype(X) gets scientific type of a table","category":"page"},{"location":"mlj_cheatsheet/#","page":"MLJ Cheatsheet","title":"MLJ Cheatsheet","text":"fitted_params(mach) gets learned parameters of fitted machine","category":"page"},{"location":"mlj_cheatsheet/#","page":"MLJ Cheatsheet","title":"MLJ Cheatsheet","text":"report(mach) gets other training results (e.g. feature rankings)","category":"page"},{"location":"mlj_cheatsheet/#Saving-and-retrieving-machines-1","page":"MLJ Cheatsheet","title":"Saving and retrieving machines","text":"","category":"section"},{"location":"mlj_cheatsheet/#","page":"MLJ Cheatsheet","title":"MLJ Cheatsheet","text":"MLJ.save(\"trained_for_five_days.jlso\", mach) to save machine mach","category":"page"},{"location":"mlj_cheatsheet/#","page":"MLJ Cheatsheet","title":"MLJ Cheatsheet","text":"predict_only_mach = machine(\"trained_for_five_days.jlso\") to deserialize.","category":"page"},{"location":"mlj_cheatsheet/#Performance-estimation-1","page":"MLJ Cheatsheet","title":"Performance estimation","text":"","category":"section"},{"location":"mlj_cheatsheet/#","page":"MLJ Cheatsheet","title":"MLJ Cheatsheet","text":"evaluate(model, X, y, resampling=CV(), measure=rms, operation=predict, weights=..., verbosity=1)","category":"page"},{"location":"mlj_cheatsheet/#","page":"MLJ Cheatsheet","title":"MLJ Cheatsheet","text":"evaluate!(mach, resampling=Holdout(), measure=[rms, mav], operation=predict, weights=..., verbosity=1)","category":"page"},{"location":"mlj_cheatsheet/#","page":"MLJ Cheatsheet","title":"MLJ Cheatsheet","text":"evaluate!(mach, resampling=[(fold1, fold2), (fold2, fold1)], measure=rms)","category":"page"},{"location":"mlj_cheatsheet/#Resampling-strategies-(resampling...)-1","page":"MLJ Cheatsheet","title":"Resampling strategies (resampling=...)","text":"","category":"section"},{"location":"mlj_cheatsheet/#","page":"MLJ Cheatsheet","title":"MLJ Cheatsheet","text":"Holdout(fraction_train=0.7, rng=1234) for simple holdout","category":"page"},{"location":"mlj_cheatsheet/#","page":"MLJ Cheatsheet","title":"MLJ Cheatsheet","text":"CV(nfolds=6, rng=1234) for cross-validation","category":"page"},{"location":"mlj_cheatsheet/#","page":"MLJ Cheatsheet","title":"MLJ Cheatsheet","text":"StratifiedCV(nfolds=6, rng=1234) for stratified cross-validation","category":"page"},{"location":"mlj_cheatsheet/#","page":"MLJ Cheatsheet","title":"MLJ Cheatsheet","text":"or a list of pairs of row indices:","category":"page"},{"location":"mlj_cheatsheet/#","page":"MLJ Cheatsheet","title":"MLJ Cheatsheet","text":"[(train1, eval1), (train2, eval2), ... (traink, evalk)]","category":"page"},{"location":"mlj_cheatsheet/#Tuning-1","page":"MLJ Cheatsheet","title":"Tuning","text":"","category":"section"},{"location":"mlj_cheatsheet/#Tuning-model-wrapper-1","page":"MLJ Cheatsheet","title":"Tuning model wrapper","text":"","category":"section"},{"location":"mlj_cheatsheet/#","page":"MLJ Cheatsheet","title":"MLJ Cheatsheet","text":"tuned_model = TunedModel(model=…, tuning=RandomSearch(), resampling=Holdout(), measure=…, operation=predict, range=…)","category":"page"},{"location":"mlj_cheatsheet/#Ranges-for-tuning-(range...)-1","page":"MLJ Cheatsheet","title":"Ranges for tuning (range=...)","text":"","category":"section"},{"location":"mlj_cheatsheet/#","page":"MLJ Cheatsheet","title":"MLJ Cheatsheet","text":"If r = range(KNNRegressor(), :K, lower=1, upper = 20, scale=:log)","category":"page"},{"location":"mlj_cheatsheet/#","page":"MLJ Cheatsheet","title":"MLJ Cheatsheet","text":"then Grid() search uses iterator(r, 6) == [1, 2, 3, 6, 11, 20].","category":"page"},{"location":"mlj_cheatsheet/#","page":"MLJ Cheatsheet","title":"MLJ Cheatsheet","text":"lower=-Inf and upper=Inf are allowed.","category":"page"},{"location":"mlj_cheatsheet/#","page":"MLJ Cheatsheet","title":"MLJ Cheatsheet","text":"Non-numeric ranges: r = range(model, :parameter, values=…)","category":"page"},{"location":"mlj_cheatsheet/#","page":"MLJ Cheatsheet","title":"MLJ Cheatsheet","text":"Nested ranges: Use dot syntax, as in r = range(EnsembleModel(atom=tree), :(atom.max_depth), ...)","category":"page"},{"location":"mlj_cheatsheet/#","page":"MLJ Cheatsheet","title":"MLJ Cheatsheet","text":"Can specify multiple ranges, as in range=[r1, r2, r3]. For more range options do ?Grid or ?RandomSearch","category":"page"},{"location":"mlj_cheatsheet/#Tuning-strategies-1","page":"MLJ Cheatsheet","title":"Tuning strategies","text":"","category":"section"},{"location":"mlj_cheatsheet/#","page":"MLJ Cheatsheet","title":"MLJ Cheatsheet","text":"Grid(resolution=10) or Grid(goal=50) for basic grid search","category":"page"},{"location":"mlj_cheatsheet/#","page":"MLJ Cheatsheet","title":"MLJ Cheatsheet","text":"RandomSearch(rng=1234) for basic random search","category":"page"},{"location":"mlj_cheatsheet/#Learning-curves-1","page":"MLJ Cheatsheet","title":"Learning curves","text":"","category":"section"},{"location":"mlj_cheatsheet/#","page":"MLJ Cheatsheet","title":"MLJ Cheatsheet","text":"For generating plot of performance against parameter specified by range:","category":"page"},{"location":"mlj_cheatsheet/#","page":"MLJ Cheatsheet","title":"MLJ Cheatsheet","text":"curve = learning_curve(mach, resolution=30, resampling=Holdout(), measure=…, operation=predict, range=…, n=1)","category":"page"},{"location":"mlj_cheatsheet/#","page":"MLJ Cheatsheet","title":"MLJ Cheatsheet","text":"curve = learning_curve(model, X, y, resolution=30, resampling=Holdout(), measure=…, operation=predict, range=…, n=1)","category":"page"},{"location":"mlj_cheatsheet/#","page":"MLJ Cheatsheet","title":"MLJ Cheatsheet","text":"If using Plots.jl:","category":"page"},{"location":"mlj_cheatsheet/#","page":"MLJ Cheatsheet","title":"MLJ Cheatsheet","text":"plot(curve.parameter_values, curve.measurements, xlab=curve.parameter_name, xscale=curve.parameter_scale)","category":"page"},{"location":"mlj_cheatsheet/#Performance-measures-(metrics)-1","page":"MLJ Cheatsheet","title":"Performance measures (metrics)","text":"","category":"section"},{"location":"mlj_cheatsheet/#","page":"MLJ Cheatsheet","title":"MLJ Cheatsheet","text":"area_under_curve, accuracy, balanced_accuracy, cross_entropy, FScore, false_discovery_rate, false_negative, false_negative_rate, false_positive, false_positive_rate, l1, l2, mae, matthews_correlation, misclassification_rate, negative_predictive_value, positive_predictive_value, rms, rmsl, rmslp1, rmsp, true_negative, true_negative_rate, true_positive, true_positive_rate, BrierScore(), confusion_matrix","category":"page"},{"location":"mlj_cheatsheet/#","page":"MLJ Cheatsheet","title":"MLJ Cheatsheet","text":"Available after doing using LossFunctions:","category":"page"},{"location":"mlj_cheatsheet/#","page":"MLJ Cheatsheet","title":"MLJ Cheatsheet","text":"DWDMarginLoss(), ExpLoss(), L1HingeLoss(), L2HingeLoss(), L2MarginLoss(), LogitMarginLoss(), ModifiedHuberLoss(), PerceptronLoss(), SigmoidLoss(), SmoothedL1HingeLoss(), ZeroOneLoss(), HuberLoss(), L1EpsilonInsLoss(), L2EpsilonInsLoss(), LPDistLoss(), LogitDistLoss(), PeriodicLoss(), QuantileLoss()","category":"page"},{"location":"mlj_cheatsheet/#","page":"MLJ Cheatsheet","title":"MLJ Cheatsheet","text":"measures() to get full list","category":"page"},{"location":"mlj_cheatsheet/#","page":"MLJ Cheatsheet","title":"MLJ Cheatsheet","text":"info(rms) to list properties (aka traits) of the rms measure","category":"page"},{"location":"mlj_cheatsheet/#Transformers-1","page":"MLJ Cheatsheet","title":"Transformers","text":"","category":"section"},{"location":"mlj_cheatsheet/#","page":"MLJ Cheatsheet","title":"MLJ Cheatsheet","text":"Built-ins include: Standardizer, OneHotEncoder, UnivariateBoxCoxTransformer, FeatureSelector, FillImputer, UnivariateDiscretizer, UnivariateStandardizer, ContinuousEncoder","category":"page"},{"location":"mlj_cheatsheet/#","page":"MLJ Cheatsheet","title":"MLJ Cheatsheet","text":"Externals include: PCA (in MultivariateStats), KMeans, KMedoids (in Clustering).","category":"page"},{"location":"mlj_cheatsheet/#","page":"MLJ Cheatsheet","title":"MLJ Cheatsheet","text":"models(m -> !m.is_supervised) to get full list","category":"page"},{"location":"mlj_cheatsheet/#Ensemble-model-wrapper-1","page":"MLJ Cheatsheet","title":"Ensemble model wrapper","text":"","category":"section"},{"location":"mlj_cheatsheet/#","page":"MLJ Cheatsheet","title":"MLJ Cheatsheet","text":"EnsembleModel(atom=…, weights=Float64[], bagging_fraction=0.8, rng=GLOBAL_RNG, n=100, parallel=true, out_of_bag_measure=[])","category":"page"},{"location":"mlj_cheatsheet/#Pipelines-1","page":"MLJ Cheatsheet","title":"Pipelines","text":"","category":"section"},{"location":"mlj_cheatsheet/#","page":"MLJ Cheatsheet","title":"MLJ Cheatsheet","text":"With deterministic (point) predictions:","category":"page"},{"location":"mlj_cheatsheet/#","page":"MLJ Cheatsheet","title":"MLJ Cheatsheet","text":"pipe = @pipeline OneHotEncoder KNNRegressor(K=3) target=UnivariateStandardizer","category":"page"},{"location":"mlj_cheatsheet/#","page":"MLJ Cheatsheet","title":"MLJ Cheatsheet","text":"pipe = @pipeline OneHotEncoder KNNRegressor(K=3) target=v->log.(V) inverse=v->exp.(v))","category":"page"},{"location":"mlj_cheatsheet/#","page":"MLJ Cheatsheet","title":"MLJ Cheatsheet","text":"Unsupervised:","category":"page"},{"location":"mlj_cheatsheet/#","page":"MLJ Cheatsheet","title":"MLJ Cheatsheet","text":"pipe = @pipeline Standardizer OneHotEncoder","category":"page"},{"location":"mlj_cheatsheet/#Define-a-supervised-learning-network:-1","page":"MLJ Cheatsheet","title":"Define a supervised learning network:","text":"","category":"section"},{"location":"mlj_cheatsheet/#","page":"MLJ Cheatsheet","title":"MLJ Cheatsheet","text":"Xs = source(X) ys = source(y)","category":"page"},{"location":"mlj_cheatsheet/#","page":"MLJ Cheatsheet","title":"MLJ Cheatsheet","text":"... define further nodal machines and nodes ...","category":"page"},{"location":"mlj_cheatsheet/#","page":"MLJ Cheatsheet","title":"MLJ Cheatsheet","text":"yhat = predict(knn_machine, W, ys) (final node)","category":"page"},{"location":"mlj_cheatsheet/#Exporting-a-learning-network-as-stand-alone-model:-1","page":"MLJ Cheatsheet","title":"Exporting a learning network as stand-alone model:","text":"","category":"section"},{"location":"mlj_cheatsheet/#","page":"MLJ Cheatsheet","title":"MLJ Cheatsheet","text":"Supervised, with final node yhat returning point-predictions:","category":"page"},{"location":"mlj_cheatsheet/#","page":"MLJ Cheatsheet","title":"MLJ Cheatsheet","text":"@from_network machine(Deterministic(), Xs, ys; predict=yhat) begin\n    mutable struct Composite\n\t    reducer=network_pca\n\t\tregressor=network_knn\n    end","category":"page"},{"location":"mlj_cheatsheet/#","page":"MLJ Cheatsheet","title":"MLJ Cheatsheet","text":"Here network_pca and network_knn are models appearing in the learning network.","category":"page"},{"location":"mlj_cheatsheet/#","page":"MLJ Cheatsheet","title":"MLJ Cheatsheet","text":"Supervised, with yhat final node returning probabilistic predictions:","category":"page"},{"location":"mlj_cheatsheet/#","page":"MLJ Cheatsheet","title":"MLJ Cheatsheet","text":"@from_network machine(Probabilistic(), Xs, ys; predict=yhat) begin\n    mutable struct Composite\n        reducer=network_pca\n        classifier=network_tree\n    end","category":"page"},{"location":"mlj_cheatsheet/#","page":"MLJ Cheatsheet","title":"MLJ Cheatsheet","text":"Unsupervised, with final node Xout:","category":"page"},{"location":"mlj_cheatsheet/#","page":"MLJ Cheatsheet","title":"MLJ Cheatsheet","text":"@from_network machine(Unsupervised(), Xs; transform=Xout) begin\n    mutable struct Composite\n\t    reducer1=network_pca\n\t\treducer2=clusterer\n    end\nend","category":"page"},{"location":"tuning_models/#Tuning-models-1","page":"Tuning Models","title":"Tuning models","text":"","category":"section"},{"location":"tuning_models/#","page":"Tuning Models","title":"Tuning Models","text":"Below we illustrate tuning model hyperparameters by grid and random searches. For a complete list of available and planned tuning strategies, see the MLJTuning page","category":"page"},{"location":"tuning_models/#","page":"Tuning Models","title":"Tuning Models","text":"In MLJ tuning is implemented as a model wrapper. After wrapping a model in a tuning strategy and binding the wrapped model to data in a machine, mach, calling fit!(mach) instigates a search for optimal model hyperparameters, within a specified range, and then uses all supplied data to train the best model. To predict using the optimal model, one just calls predict(mach, Xnew). In this way the wrapped model may be viewed as a \"self-tuning\" version of the unwrapped model.","category":"page"},{"location":"tuning_models/#Tuning-a-single-hyperparameter-using-a-grid-search-1","page":"Tuning Models","title":"Tuning a single hyperparameter using a grid search","text":"","category":"section"},{"location":"tuning_models/#","page":"Tuning Models","title":"Tuning Models","text":"using MLJ\nX = MLJ.table(rand(100, 10));\ny = 2X.x1 - X.x2 + 0.05*rand(100);\ntree_model = @load DecisionTreeRegressor;","category":"page"},{"location":"tuning_models/#","page":"Tuning Models","title":"Tuning Models","text":"Let's tune min_purity_increase in the model above, using a grid-search. To do so we will use the simplest range object, a one-dimensional range object constructed using the range method:","category":"page"},{"location":"tuning_models/#","page":"Tuning Models","title":"Tuning Models","text":"r = range(tree_model, :min_purity_increase, lower=0.001, upper=1.0, scale=:log);\nself_tuning_tree_model = TunedModel(model=tree_model,\n                                    resampling = CV(nfolds=3),\n                                    tuning = Grid(resolution=10),\n                                    range = r,\n                                    measure = rms);","category":"page"},{"location":"tuning_models/#","page":"Tuning Models","title":"Tuning Models","text":"Incidentally, a grid is generated internally \"over the range\" by calling the iterator method with an appropriate resolution:","category":"page"},{"location":"tuning_models/#","page":"Tuning Models","title":"Tuning Models","text":"iterator(r, 5)","category":"page"},{"location":"tuning_models/#","page":"Tuning Models","title":"Tuning Models","text":"Non-numeric hyperparameters are handled a little differently:","category":"page"},{"location":"tuning_models/#","page":"Tuning Models","title":"Tuning Models","text":"selector_model = FeatureSelector();\nr2 = range(selector_model, :features, values = [[:x1,], [:x1, :x2]]);\niterator(r2)","category":"page"},{"location":"tuning_models/#","page":"Tuning Models","title":"Tuning Models","text":"Unbounded ranges are also permitted. See the range and iterator docstrings below for details, and the sampler docstring for generating random samples from one-dimensional ranges (used internally by the RandomSearch strategy).","category":"page"},{"location":"tuning_models/#","page":"Tuning Models","title":"Tuning Models","text":"Returning to the wrapped tree model:","category":"page"},{"location":"tuning_models/#","page":"Tuning Models","title":"Tuning Models","text":"self_tuning_tree = machine(self_tuning_tree_model, X, y);\nfit!(self_tuning_tree, verbosity=0);","category":"page"},{"location":"tuning_models/#","page":"Tuning Models","title":"Tuning Models","text":"We can inspect the detailed results of the grid search with report(self_tuning_tree) or just retrieve the optimal model, as here:","category":"page"},{"location":"tuning_models/#","page":"Tuning Models","title":"Tuning Models","text":"fitted_params(self_tuning_tree).best_model","category":"page"},{"location":"tuning_models/#","page":"Tuning Models","title":"Tuning Models","text":"Predicting on new input observations using the optimal model:","category":"page"},{"location":"tuning_models/#","page":"Tuning Models","title":"Tuning Models","text":"Xnew  = MLJ.table(rand(3, 10));\npredict(self_tuning_tree, Xnew)","category":"page"},{"location":"tuning_models/#Tuning-multiple-nested-hyperparameters-1","page":"Tuning Models","title":"Tuning multiple nested hyperparameters","text":"","category":"section"},{"location":"tuning_models/#","page":"Tuning Models","title":"Tuning Models","text":"The following model has another model, namely a DecisionTreeRegressor, as a hyperparameter:","category":"page"},{"location":"tuning_models/#","page":"Tuning Models","title":"Tuning Models","text":"tree_model = @load DecisionTreeRegressor\nforest_model = EnsembleModel(atom=tree_model);","category":"page"},{"location":"tuning_models/#","page":"Tuning Models","title":"Tuning Models","text":"julia> tree_model = DecisionTreeRegressor()\njulia> forest_model = EnsembleModel(atom=tree_model);","category":"page"},{"location":"tuning_models/#","page":"Tuning Models","title":"Tuning Models","text":"Ranges for nested hyperparameters are specified using dot syntax. In this case we will specify a goal for the total number of grid points:","category":"page"},{"location":"tuning_models/#","page":"Tuning Models","title":"Tuning Models","text":"r1 = range(forest_model, :(atom.n_subfeatures), lower=1, upper=9);\nr2 = range(forest_model, :bagging_fraction, lower=0.4, upper=1.0);\nself_tuning_forest_model = TunedModel(model=forest_model,\n                                      tuning=Grid(goal=30),\n                                      resampling=CV(nfolds=6),\n                                      range=[r1, r2],\n                                      measure=rms);\nself_tuning_forest = machine(self_tuning_forest_model, X, y);\nfit!(self_tuning_forest, verbosity=0)","category":"page"},{"location":"tuning_models/#","page":"Tuning Models","title":"Tuning Models","text":"In this two-parameter case, a plot of the grid search results is also available:","category":"page"},{"location":"tuning_models/#","page":"Tuning Models","title":"Tuning Models","text":"using Plots\nplot(self_tuning_forest)","category":"page"},{"location":"tuning_models/#","page":"Tuning Models","title":"Tuning Models","text":"(Image: )","category":"page"},{"location":"tuning_models/#","page":"Tuning Models","title":"Tuning Models","text":"Instead of specifying a goal, we can declare a global resolution, which is overriden for a particular parameter by pairing it's range with the resolution desired. In the next example, the default resolution=100 is applied to the r2 field, but a resolution of 3 is applied to the r1 field. Additionally, we ask that the grid points be randomly traversed, and the the total number of evaluations be limited to 25.","category":"page"},{"location":"tuning_models/#","page":"Tuning Models","title":"Tuning Models","text":"tuning = Grid(resolution=100, shuffle=true, rng=1234)\nself_tuning_forest_model = TunedModel(model=forest_model,\n                                      tuning=tuning,\n                                      resampling=CV(nfolds=6),\n                                      range=[(r1, 3), r2],\n                                      measure=rms,\n                                      n=25);\nfit!(machine(self_tuning_forest_model, X, y), verbosity=0)","category":"page"},{"location":"tuning_models/#","page":"Tuning Models","title":"Tuning Models","text":"For more options for a grid search, see Grid below.","category":"page"},{"location":"tuning_models/#Tuning-using-a-random-search-1","page":"Tuning Models","title":"Tuning using a random search","text":"","category":"section"},{"location":"tuning_models/#","page":"Tuning Models","title":"Tuning Models","text":"Let's attempt to tune the same hyperparameters using a RandomSearch tuning strategy. By default, bounded numeric ranges like r1 and r2 are sampled uniformly (before rounding, in the case of the integer range r1). Positive unbounded ranges are sampled using a Gamma distribution by default, and all others using a (truncated) normal distribution.","category":"page"},{"location":"tuning_models/#","page":"Tuning Models","title":"Tuning Models","text":"self_tuning_forest_model = TunedModel(model=forest_model,\n                                      tuning=RandomSearch(),\n                                      resampling=CV(nfolds=6),\n                                      range=[r1, r2],\n                                      measure=rms,\n                                      n=25);\nself_tuning_forest = machine(self_tuning_forest_model, X, y);\nfit!(self_tuning_forest, verbosity=0)","category":"page"},{"location":"tuning_models/#","page":"Tuning Models","title":"Tuning Models","text":"using Plots\nplot(self_tuning_forest)","category":"page"},{"location":"tuning_models/#","page":"Tuning Models","title":"Tuning Models","text":"(Image: )","category":"page"},{"location":"tuning_models/#","page":"Tuning Models","title":"Tuning Models","text":"The prior distributions used for sampling each hyperparameter can be customized, as can the global fallbacks. See the RandomSearch doc-string below for details.","category":"page"},{"location":"tuning_models/#API-1","page":"Tuning Models","title":"API","text":"","category":"section"},{"location":"tuning_models/#","page":"Tuning Models","title":"Tuning Models","text":"MLJBase.range\nMLJBase.iterator\nMLJBase.sampler\nDistributions.fit(::Type{D}, ::MLJBase.NumericRange) where D<:Distributions.Distribution\nMLJTuning.TunedModel\nMLJTuning.Grid\nMLJTuning.RandomSearch","category":"page"},{"location":"tuning_models/#Base.range","page":"Tuning Models","title":"Base.range","text":"r = range(model, :hyper; values=nothing)\n\nDefine a one-dimensional NominalRange object for a field hyper of model. Note that r is not directly iterable but iterator(r) is.\n\nA nested hyperparameter is specified using dot notation. For example, :(atom.max_depth) specifies the max_depth hyperparameter of the submodel model.atom.\n\nr = range(model, :hyper; upper=nothing, lower=nothing,\n          scale=nothing, values=nothing)\n\nAssuming values is not specified, define a one-dimensional NumericRange object for a Real field hyper of model.  Note that r is not directly iteratable but iterator(r, n)is an iterator of length n. To generate random elements from r, instead apply rand methods to sampler(r). The supported scales are :linear,:log, :logminus, :log10, :log2, or a callable object.\n\nNote that r is not directly iterable, but iterator(r, n) is, for given resolution (length) n.\n\nBy default, the behaviour of the constructed object depends on the type of the value of the hyperparameter :hyper at model at the time of construction. To override this behaviour (for instance if model is not available) specify a type in place of model so the behaviour is determined by the value of the specified type.\n\nA nested hyperparameter is specified using dot notation (see above).\n\nIf scale is unspecified, it is set to :linear, :log, :logminus, or :linear, according to whether the interval (lower, upper) is bounded, right-unbounded, left-unbounded, or doubly unbounded, respectively.  Note upper=Inf and lower=-Inf are allowed.\n\nIf values is specified, the other keyword arguments are ignored and a NominalRange object is returned (see above).\n\nSee also: iterator, sampler\n\n\n\n\n\n","category":"function"},{"location":"tuning_models/#MLJBase.iterator","page":"Tuning Models","title":"MLJBase.iterator","text":"iterator([rng, ], r::NominalRange, [,n])\niterator([rng, ], r::NumericRange, n)\n\nReturn an iterator (currently a vector) for a ParamRange object r. In the first case iteration is over all values stored in the range (or just the first n, if n is specified). In the second case, the iteration is over approximately n ordered values, generated as follows:\n\n(i) First, exactly n values are generated between U and L, with a spacing determined by r.scale (uniform if scale=:linear) where U and L are given by the following table:\n\nr.lower r.upper L U\nfinite finite r.lower r.upper\n-Inf finite r.upper - 2r.unit r.upper\nfinite Inf r.lower r.lower + 2r.unit\n-Inf Inf r.origin - r.unit r.origin + r.unit\n\n(ii) If a callable f is provided as scale, then a uniform spacing is always applied in (i) but f is broadcast over the results. (Unlike ordinary scales, this alters the effective range of values generated, instead of just altering the spacing.)\n\n(iii) If r is a discrete numeric range (r isa NumericRange{<:Integer}) then the values are additionally rounded, with any duplicate values removed. Otherwise all the values are used (and there are exacltly n of them).\n\n(iv) Finally, if a random number generator rng is specified, then the values are returned in random order (sampling without replacement), and otherwise they are returned in numeric order, or in the order provided to the range constructor, in the case of a NominalRange.\n\n\n\n\n\n","category":"function"},{"location":"tuning_models/#Distributions.sampler","page":"Tuning Models","title":"Distributions.sampler","text":"sampler(r::NominalRange, probs::AbstractVector{<:Real})\nsampler(r::NominalRange)\nsampler(r::NumericRange{T}, d)\n\nConstruct an object s which can be used to generate random samples from a ParamRange object r (a one-dimensional range) using one of the following calls:\n\nrand(s)             # for one sample\nrand(s, n)          # for n samples\nrand(rng, s [, n])  # to specify an RNG\n\nThe argument probs can be any probability vector with the same length as r.values. The second sampler method above calls the first with a uniform probs vector.\n\nThe argument d can be either an arbitrary instance of UnivariateDistribution from the Distributions.jl package, or one of a Distributions.jl types for which fit(d, ::NumericRange) is defined. These include: Arcsine, Uniform, Biweight, Cosine, Epanechnikov, SymTriangularDist, Triweight, Normal, Gamma, InverseGaussian, Logistic, LogNormal, Cauchy, Gumbel, Laplace, and Poisson; but see the doc-string for Distributions.fit for an up-to-date list.\n\nIf d is an instance, then sampling is from a truncated form of the supplied distribution d, the truncation bounds being r.lower and r.upper (the attributes r.origin and r.unit attributes are ignored). For discrete numeric ranges (T <: Integer) the samples are rounded.\n\nIf d is a type then a suitably truncated distribution is automatically generated using Distributions.fit(d, r).\n\nImportant. Values are generated with no regard to r.scale, except in the special case r.scale is a callable object f. In that case, f is applied to all values generated by rand as described above (prior to rounding, in the case of discrete numeric ranges).\n\nExamples\n\nr = range(Char, :letter, values=collect(\"abc\"))\ns = sampler(r, [0.1, 0.2, 0.7])\nsamples =  rand(s, 1000);\nStatsBase.countmap(samples)\nDict{Char,Int64} with 3 entries:\n  'a' => 107\n  'b' => 205\n  'c' => 688\n\nr = range(Int, :k, lower=2, upper=6) # numeric but discrete\ns = sampler(r, Normal)\nsamples = rand(s, 1000);\nUnicodePlots.histogram(samples)\n           ┌                                        ┐\n[2.0, 2.5) ┤▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 119\n[2.5, 3.0) ┤ 0\n[3.0, 3.5) ┤▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 296\n[3.5, 4.0) ┤ 0\n[4.0, 4.5) ┤▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 275\n[4.5, 5.0) ┤ 0\n[5.0, 5.5) ┤▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 221\n[5.5, 6.0) ┤ 0\n[6.0, 6.5) ┤▇▇▇▇▇▇▇▇▇▇▇ 89\n           └                                        ┘\n\n\n\n\n\n","category":"function"},{"location":"tuning_models/#StatsBase.fit-Union{Tuple{D}, Tuple{Type{D},MLJBase.NumericRange}} where D<:Distributions.Distribution","page":"Tuning Models","title":"StatsBase.fit","text":"Distributions.fit(D, r::MLJBase.NumericRange)\n\nFit and return a distribution d of type D to the one-dimensional range r.\n\nOnly types D in the table below are supported.\n\nThe distribution d is constructed in two stages. First, a distributon d0, characterized by the conditions in the second column of the table, is fit to r. Then d0 is truncated between r.lower and r.upper to obtain d.\n\nDistribution type D Characterization of d0\nArcsine, Uniform, Biweight, Cosine, Epanechnikov, SymTriangularDist, Triweight minimum(d) = r.lower, maximum(d) = r.upper\nNormal, Gamma, InverseGaussian, Logistic, LogNormal mean(d) = r.origin, std(d) = r.unit\nCauchy, Gumbel, Laplace, (Normal) Dist.location(d) = r.origin, Dist.scale(d)  = r.unit\nPoisson Dist.mean(d) = r.unit\n\nHere Dist = Distributions.\n\n\n\n\n\n","category":"method"},{"location":"tuning_models/#MLJTuning.TunedModel","page":"Tuning Models","title":"MLJTuning.TunedModel","text":"tuned_model = TunedModel(; model=nothing,\n                         tuning=Grid(),\n                         resampling=Holdout(),\n                         measure=nothing,\n                         weights=nothing,\n                         repeats=1,\n                         operation=predict,\n                         range=nothing,\n                         selection_heuristic=NaiveSelection(),\n                         n=default_n(tuning, range),\n                         train_best=true,\n                         acceleration=default_resource(),\n                         acceleration_resampling=CPU1(),\n                         check_measure=true)\n\nConstruct a model wrapper for hyperparameter optimization of a supervised learner.\n\nCalling fit!(mach) on a machine mach=machine(tuned_model, X, y) or mach=machine(tuned_model, X, y, w) will:\n\nInstigate a search, over clones of model, with the hyperparameter mutations specified by range, for a model optimizing the specified measure, using performance evaluations carried out using the specified tuning strategy and resampling strategy.\nFit an internal machine, based on the optimal model fitted_params(mach).best_model, wrapping the optimal model object in all the provided data X, y(, w). Calling predict(mach, Xnew) then returns predictions on Xnew of this internal machine. The final train can be supressed by setting train_best=false.\n\nThe range objects supported depend on the tuning strategy specified. Query the strategy docstring for details. To optimize over an explicit list v of models of the same type, use strategy=Explicit() and specify model=v[1] and range=v.\n\nThe number of models searched is specified by n. If unspecified, then MLJTuning.default_n(tuning, range) is used. When n is increased and fit!(mach) called again, the old search history is re-instated and the search continues where it left off.\n\nIf measure supports weights (supports_weights(measure) == true) then any weights specified will be passed to the measure. If more than one measure is specified, then only the first is optimized (unless strategy is multi-objective) but the performance against every measure specified will be computed and reported in report(mach).best_performance and other relevant attributes of the generated report.\n\nSpecify repeats > 1 for repeated resampling per model evaluation. See evaluate! options for details.\n\nImportant. If a custom measure is used, and the measure is a score, rather than a loss, be sure to check that MLJ.orientation(measure) == :score to ensure maximization of the measure, rather than minimization. Override an incorrect value with MLJ.orientation(::typeof(measure)) = :score.\n\nIn the case of two-parameter tuning, a Plots.jl plot of performance estimates is returned by plot(mach) or heatmap(mach).\n\nOnce a tuning machine mach has bee trained as above, then fitted_params(mach) has these keys/values:\n\nkey value\nbest_model optimal model instance\nbest_fitted_params learned parameters of the optimal model\n\nThe named tuple report(mach) includes these keys/values:\n\nkey value\nbest_model optimal model instance\nbest_history_entry corresponding entry in the history, including performance estimate\nbest_report report generated by fitting the optimal model to all data\nhistory tuning strategy-specific history of all evaluations\n\nplus other key/value pairs specific to the tuning strategy.\n\nSummary of key-word arguments\n\nmodel: Supervised model prototype that is cloned and mutated to generate models for evaluation\ntuning=Grid(): tuning strategy to be applied (eg, RandomSearch())\nresampling=Holdout(): resampling strategy (eg, Holdout(), CV()), StratifiedCV()) to be applied in performance evaluations\nmeasure: measure or measures to be applied in performance evaluations; only the first used in optimization (unless the strategy is multi-objective) but all reported to the history\nweights: sample weights to be passed the measure(s) in performance evaluations, if supported.\nrepeats=1: for generating train/test sets multiple times in resampling; see evaluate! for details\noperation=predict: operation to be applied to each fitted model; usually predict but predict_mean, predict_median or predict_mode can be used for Probabilistic models, if the specified measures are Deterministic\nrange: range object; tuning strategy documentation describes supported types\nselection_heuristic: the rule determining how the best model is decided. According to the default heuristic, NaiveSelection(), measure (or the first element of measure) is evaluated for each resample and these per-fold measurements are aggregrated. The model with the lowest (resp. highest) aggregate is chosen if the measure is a :loss (resp. a :score).\nn: number of iterations (ie, models to be evaluated); set by tuning strategy if left unspecified\ntrain_best=true: whether to train the optimal model\nacceleration=default_resource(): mode of parallelization for tuning strategies that support this\nacceleration_resampling=CPU1(): mode of parallelization for resampling\ncheck_measure: whether to check measure is compatible with the specified model and operation)\n\n\n\n\n\n","category":"function"},{"location":"tuning_models/#MLJTuning.Grid","page":"Tuning Models","title":"MLJTuning.Grid","text":"Grid(goal=nothing, resolution=10, rng=Random.GLOBAL_RNG, shuffle=true)\n\nInstantiate a Cartesian grid-based hyperparameter tuning strategy with a specified number of grid points as goal, or using a specified default resolution in each numeric dimension.\n\nSupported ranges:\n\nA single one-dimensional range or vector of one-dimensioinal ranges can be specified. Specifically, in Grid search, the range field of a TunedModel instance can be:\n\nA single one-dimensional range - ie, ParamRange object - r, or pair of the form (r, res) where res specifies a resolution to override the default resolution.\nAny vector of objects of the above form\n\nTwo elements of a range vector may share the same field attribute, with the effect that their grids are combined, as in Example 3 below.\n\nParamRange objects are constructed using the range method.\n\nExample 1:\n\nrange(model, :hyper1, lower=1, origin=2, unit=1)\n\nExample 2:\n\n[(range(model, :hyper1, lower=1, upper=10), 15),\n  range(model, :hyper2, lower=2, upper=4),\n  range(model, :hyper3, values=[:ball, :tree])]\n\nExample 3:\n\n# a range generating the grid `[1, 2, 10, 20, 30]` for `:hyper1`:\n[range(model, :hyper1, values=[1, 2]),\n (range(model, :hyper1, lower= 10, upper=30), 3)]\n\nNote: All the field values of the ParamRange objects (:hyper1, :hyper2, :hyper3 in the preceding example) must refer to field names a of single model (the model specified during TunedModel construction).\n\nAlgorithm\n\nThis is a standard grid search with the following specifics: In all cases all values of each specified NominalRange are exhausted. If goal is specified, then all resolutions are ignored, and a global resolution is applied to the NumericRange objects that maximizes the number of grid points, subject to the restriction that this not exceed goal. (This assumes no field appears twice in the range vector.) Otherwise the default resolution and any parameter-specific resolutions apply.\n\nIn all cases the models generated are shuffled using rng, unless shuffle=false.\n\nSee also TunedModel, range.\n\n\n\n\n\n","category":"type"},{"location":"tuning_models/#MLJTuning.RandomSearch","page":"Tuning Models","title":"MLJTuning.RandomSearch","text":"RandomSearch(bounded=Distributions.Uniform,\n             positive_unbounded=Distributions.Gamma,\n             other=Distributions.Normal,\n             rng=Random.GLOBAL_RNG)\n\nInstantiate a random search tuning strategy, for searching over Cartesian hyperparameter domains, with customizable priors in each dimension.\n\nSupported ranges\n\nA single one-dimensional range or vector of one-dimensioinal ranges can be specified. If not paired with a prior, then one is fitted, according to fallback distribution types specified by the tuning strategy hyperparameters. Specifically, in RandomSearch, the range field of a TunedModel instance can be:\n\na single one-dimensional range (ParamRange object) r\na pair of the form (r, d), with r as above and where d is:\na probability vector of the same length as r.values (r a NominalRange)\nany Distributions.UnivariateDistribution instance (r a NumericRange)\none of the subtypes of Distributions.UnivariateDistribution listed in the table below, for automatic fitting using Distributions.fit(d, r), a distribution whose support always lies between r.lower and r.upper (r a NumericRange)\nany pair of the form (field, s), where field is the (possibly nested) name of a field of the model to be tuned, and s an arbitrary sampler object for that field. This means only that rand(rng, s) is defined and returns valid values for the field.\nany vector of objects of the above form\n\nA range vector may contain multiple entries for the same model field, as in range = [(:lambda, s1), (:alpha, s), (:lambda, s2)]. In that case the entry used in each iteration is random.\n\ndistribution types for fitting to ranges of this type\nArcsine, Uniform, Biweight, Cosine, Epanechnikov, SymTriangularDist, Triweight bounded\nGamma, InverseGaussian, Poisson positive (bounded or unbounded)\nNormal, Logistic, LogNormal, Cauchy, Gumbel, Laplace any\n\nParamRange objects are constructed using the range method.\n\nExamples\n\nusing Distributions\n\nrange1 = range(model, :hyper1, lower=0, upper=1)\n\nrange2 = [(range(model, :hyper1, lower=1, upper=10), Arcsine),\n          range(model, :hyper2, lower=2, upper=Inf, unit=1, origin=3),\n          (range(model, :hyper2, lower=2, upper=4), Normal(0, 3)),\n          (range(model, :hyper3, values=[:ball, :tree]), [0.3, 0.7])]\n\n# uniform sampling of :(atom.λ) from [0, 1] without defining a NumericRange:\nstruct MySampler end\nBase.rand(rng::Random.AbstractRNG, ::MySampler) = rand(rng)\nrange3 = (:(atom.λ), MySampler())\n\nAlgorithm\n\nIn each iteration, a model is generated for evaluation by mutating the fields of a deep copy of model. The range vector is shuffled and the fields sampled according to the new order (repeated fields being mutated more than once). For a range entry of the form (field, s) the algorithm calls rand(rng, s) and mutates the field field of the model clone to have this value. For an entry of the form (r, d), s is substituted with sampler(r, d). If no d is specified, then sampling is uniform (with replacement) if r is a NominalRange, and is otherwise given by the defaults specified by the tuning strategy parameters bounded, positive_unbounded, and other, depending on the field values of the NumericRange object r.\n\nSee also TunedModel, range, sampler.\n\n\n\n\n\n","category":"type"},{"location":"acceleration_and_parallelism/#Acceleration-and-Parallelism-1","page":"Acceleration and Parallelism","title":"Acceleration and Parallelism","text":"","category":"section"},{"location":"acceleration_and_parallelism/#","page":"Acceleration and Parallelism","title":"Acceleration and Parallelism","text":"warning: Experimental API\nThe acceleration API is experimental and may not work correctly in all cases, especially if trying to use an acceleration method that your version of Julia or installed packages cannot support. The API is also subject to breaking changes during minor or major releases without warning.","category":"page"},{"location":"acceleration_and_parallelism/#User-facing-interface-1","page":"Acceleration and Parallelism","title":"User-facing interface","text":"","category":"section"},{"location":"acceleration_and_parallelism/#","page":"Acceleration and Parallelism","title":"Acceleration and Parallelism","text":"To enable composable, extensible acceleration of core MLJ methods, ComputationalResources.jl is utilized to provide some basic types and functions to make implementing acceleration easy. However, ambitious users or package authors have the option to define their own types to be passed as resources to acceleration, which must be <:ComputationalResources.AbstractResource.","category":"page"},{"location":"acceleration_and_parallelism/#","page":"Acceleration and Parallelism","title":"Acceleration and Parallelism","text":"Methods which support some form of acceleration support the acceleration keyword argument, which can be passed a \"resource\" from ComputationalResources. For example, passing acceleration=CPUProcesses() will utilize Distributed's multiprocessing functionality to accelerate the computation, while acceleration=CPUThreads() will use Julia's PARTR threading model to perform acceleration.","category":"page"},{"location":"acceleration_and_parallelism/#","page":"Acceleration and Parallelism","title":"Acceleration and Parallelism","text":"The default computational resource is CPU1(), which is simply serial processing via CPU. The default resource can be changed as in this example: MLJ.default_resource(CPUProcesses()). The argument must always have type <:ComputationalResource.AbstractResource. To inspect the current default, use MLJ.default_resource().","category":"page"},{"location":"acceleration_and_parallelism/#","page":"Acceleration and Parallelism","title":"Acceleration and Parallelism","text":"note: Note\nThe CPUThreads() resource is only available when running a version of Julia with Threads.@spawn available.","category":"page"},{"location":"acceleration_and_parallelism/#","page":"Acceleration and Parallelism","title":"Acceleration and Parallelism","text":"note: Note\nYou cannot use CPUThreads() with models wrapping python code.","category":"page"},{"location":"getting_started/#Getting-Started-1","page":"Getting Started","title":"Getting Started","text":"","category":"section"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"For an outline of MLJ's goals and features, see the Introduction.","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"This section introduces the most basic MLJ operations and concepts. It assumes MJL has been successfully installed. See Installation if this is not the case.","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"import Random.seed!\nusing MLJ\nusing InteractiveUtils\nMLJ.color_off()\nseed!(1234)","category":"page"},{"location":"getting_started/#Choosing-and-evaluating-a-model-1","page":"Getting Started","title":"Choosing and evaluating a model","text":"","category":"section"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"To load some demonstration data, add RDatasets.jl to your load path and enter","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"import RDatasets\niris = RDatasets.dataset(\"datasets\", \"iris\"); # a DataFrame","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"and then split the data into input and target parts:","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"using MLJ\ny, X = unpack(iris, ==(:Species), colname -> true);\nfirst(X, 3) |> pretty","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"To list all models available in MLJ's model registry do models(). Listing the models compatible with the present data:","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"models(matching(X,y))","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"In MLJ a model is a struct storing the hyperparameters of the learning algorithm indicated by the struct name (and nothing else).","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"Assuming the DecisionTree.jl package is in your load path, we can use @load to load the code defining the DecisionTreeClassifier model type. This macro also returns an instance, with default hyperparameters.","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"tree_model = @load DecisionTreeClassifier","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"Important: DecisionTree.jl and most other packages implementing machine learning algorithms for use in MLJ are not MLJ dependencies. If such a package is not in your load path you will receive an error explaining how to add the package to your current environment.","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"Once loaded, a model's performance can be evaluated with the evaluate method:","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"evaluate(tree_model, X, y,\n         resampling=CV(shuffle=true), measure=cross_entropy, verbosity=0)","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"Evaluating against multiple performance measures is also possible. See Evaluating Model Performance for details.","category":"page"},{"location":"getting_started/#A-preview-of-data-type-specification-in-MLJ-1","page":"Getting Started","title":"A preview of data type specification in MLJ","text":"","category":"section"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"The target y above is a categorical vector, which is appropriate because our model is a decision tree classifier:","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"typeof(y)","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"However, MLJ models do not actually prescribe the machine types for the data they operate on. Rather, they specify a scientific type, which refers to the way data is to be interpreted, as opposed to how it is encoded:","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"julia> info(\"DecisionTreeClassifier\").target_scitype\nAbstractArray{<:Finite, 1}","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"Here Finite is an example of a \"scalar\" scientific type with two subtypes:","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"subtypes(Finite)","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"We use the scitype function to check how MLJ is going to interpret given data. Our choice of encoding for y works for DecisionTreeClassfier, because we have:","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"scitype(y)","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"and Multiclass{3} <: Finite. If we would encode with integers instead, we obtain:","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"yint = Int.(y.refs);\nscitype(yint)","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"and using yint in place of y in classification problems will fail. See also Working with Categorical Data.","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"For more on scientific types, see Data containers and scientific types below. ","category":"page"},{"location":"getting_started/#Fit-and-predict-1","page":"Getting Started","title":"Fit and predict","text":"","category":"section"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"To illustrate MLJ's fit and predict interface, let's perform our performance evaluations by hand, but using a simple holdout set, instead of cross-validation.","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"Wrapping the model in data creates a machine which will store training outcomes:","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"tree = machine(tree_model, X, y)","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"Training and testing on a hold-out set:","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"train, test = partition(eachindex(y), 0.7, shuffle=true); # 70:30 split\nfit!(tree, rows=train);\nyhat = predict(tree, X[test,:]);\nyhat[3:5]\ncross_entropy(yhat, y[test]) |> mean","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"Notice that yhat is a vector of Distribution objects (because DecisionTreeClassifier makes probabilistic predictions). The methods of the Distributions package can be applied to such distributions:","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"broadcast(pdf, yhat[3:5], \"virginica\") # predicted probabilities of virginica\nbroadcast(pdf, yhat, y[test])[3:5] # predicted probability of observed class\nmode.(yhat[3:5])","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"Or, one can explicitly get modes by using predict_mode instead of predict:","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"predict_mode(tree, X[test[3:5],:])","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"Finally, we note that pdf() is overloaded to allow the retrieval of probabilities for all levels at once:","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"L = levels(y)\npdf(yhat[3:5], L)","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"Unsupervised models have a transform method instead of predict, and may optionally implement an inverse_transform method:","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"v = [1, 2, 3, 4]\nstand_model = UnivariateStandardizer()\nstand = machine(stand_model, v)\nfit!(stand)\nw = transform(stand, v)\ninverse_transform(stand, w)","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"Machines have an internal state which allows them to avoid redundant calculations when retrained, in certain conditions - for example when increasing the number of trees in a random forest, or the number of epochs in a neural network. The machine building syntax also anticipates a more general syntax for composing multiple models, as explained in Composing Models.","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"There is a version of evaluate for machines as well as models. This time we'll add a second performance measure. (An exclamation point is added to the method name because machines are generally mutated when trained):","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"evaluate!(tree, resampling=Holdout(fraction_train=0.7, shuffle=true),\n                measures=[cross_entropy, BrierScore()],\n                verbosity=0)","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"Changing a hyperparameter and re-evaluating:","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"tree_model.max_depth = 3\nevaluate!(tree, resampling=Holdout(fraction_train=0.7, shuffle=true),\n          measures=[cross_entropy, BrierScore()],\n          verbosity=0)","category":"page"},{"location":"getting_started/#Next-steps-1","page":"Getting Started","title":"Next steps","text":"","category":"section"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"To learn a little more about what MLJ can do, browse Common MLJ Workflows or Data Science Tutorials in Julia or try the JuliaCon2020 Workshop on MLJ (recorded here)  returning to the manual as needed. ","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"Read at least the remainder of this page before considering serious use of MLJ.","category":"page"},{"location":"getting_started/#Data-containers-and-scientific-types-1","page":"Getting Started","title":"Data containers and scientific types","text":"","category":"section"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"The MLJ user should acquaint themselves with some basic assumptions about the form of data expected by MLJ, as outlined below. The basic machine constructions look like this (see also Constructing machines):","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"machine(model::Supervised, X, y) \nmachine(model::Unsupervised, X)\n","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"Each supervised model in MLJ declares the permitted scientific type of the inputs X and targets y that can be bound to it in the first constructor above, rather than specifying specific machine types (such as Array{Float32, 2}). Similar remarks apply to the input X of an unsupervised model.","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"Scientific types are julia types defined in the package ScientificTypes.jl; the package MLJScientificTypes.jl implements the particular convention used in the MLJ universe for assigning a specific scientific type (interpretation) to each julia object (see the scitype examples below).","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"The basic \"scalar\" scientific types are Continuous, Multiclass{N}, OrderedFactor{N} and Count. Be sure you read Scalar scientific types below to be guarantee your scalar data is interpreted correctly. Tools exist to coerce the data to have the appropriate scientfic type; see MLJScientificTypes.jl or run ?coerce for details.","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"Additionally, most data containers - such as tuples, vectors, matrices and tables - have a scientific type.","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"(Image: )","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"Figure 1. Part of the scientific type hierarchy in ScientificTypes.jl.","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"scitype(4.6)\nscitype(42)\nx1 = coerce([\"yes\", \"no\", \"yes\", \"maybe\"], Multiclass);\nscitype(x1)\nX = (x1=x1, x2=rand(4), x3=rand(4))  # a \"column table\"\nscitype(X)","category":"page"},{"location":"getting_started/#Two-dimensional-data-1","page":"Getting Started","title":"Two-dimensional data","text":"","category":"section"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"Generally, two-dimensional data in MLJ is expected to be tabular. All data containers compatible with the Tables.jl interface (which includes all source formats listed here) have the scientific type Table{K}, where K depends on the scientific types of the columns, which can be individually inspected using schema:","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"schema(X)","category":"page"},{"location":"getting_started/#Matrix-data-1","page":"Getting Started","title":"Matrix data","text":"","category":"section"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"MLJ models expecting a table do not generally accept a matrix instead. However, a matrix can be wrapped as a table, using MLJ.table:","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"matrix_table = MLJ.table(rand(2,3))\nschema(matrix_table)","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"┌─────────┬─────────┬────────────┐\n│ _.names │ _.types │ _.scitypes │\n├─────────┼─────────┼────────────┤\n│ x1      │ Float64 │ Continuous │\n│ x2      │ Float64 │ Continuous │\n│ x3      │ Float64 │ Continuous │\n└─────────┴─────────┴────────────┘\n_.nrows = 2\n","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"The matrix is not copied, only wrapped.  To manifest a table as a matrix, use MLJ.matrix`. ","category":"page"},{"location":"getting_started/#Inputs-1","page":"Getting Started","title":"Inputs","text":"","category":"section"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"Since an MLJ model only specifies the scientific type of data, if that type is Table - which is the case for the majority of MLJ models - then any Tables.jl format is permitted. ","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"Specifically, the requirement for an arbitrary model's input is scitype(X) <: input_scitype(model).","category":"page"},{"location":"getting_started/#Targets-1","page":"Getting Started","title":"Targets","text":"","category":"section"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"The target y expected by MLJ models is generally an AbstractVector. A multivariate target y will generally be a table.","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"Specifically, the type requirement for a model target is scitype(y) <: target_scitype(model).","category":"page"},{"location":"getting_started/#Querying-a-model-for-acceptable-data-types-1","page":"Getting Started","title":"Querying a model for acceptable data types","text":"","category":"section"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"Given a model instance, one can inspect the admissible scientific types of its input and target, and without loading the code defining the model;","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"tree = @load DecisionTreeClassifier","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"i = info(\"DecisionTreeClassifier\")\ni.input_scitype\ni.target_scitype","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"But see also Model Search. ","category":"page"},{"location":"getting_started/#Scalar-scientific-types-1","page":"Getting Started","title":"Scalar scientific types","text":"","category":"section"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"Models in MLJ will always apply the MLJ convention described in MLJScientificTypes.jl to decide how to interpret the elements of your container types. Here are the key features of that convention:","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"Any AbstractFloat is interpreted as Continuous.\nAny Integer is interpreted as Count.\nAny CategoricalValue x, is interpreted as Multiclass or OrderedFactor, depending on the value of x.pool.ordered.\nStrings and Chars are not interpreted as Multiclass or OrderedFactor (they have scitypes Textual and Unknown respectively).\nIn particular, integers (including Bools) cannot be used to represent categorical data. Use the preceding coerce operations to coerce to a Finite scitype.\nThe scientific types of nothing and missing are Nothing and Missing, native types we also regard as scientific.","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"Use coerce(v, OrderedFactor) or coerce(v, Multiclass) to coerce a vector v of integers, strings or characters to a vector with an appropriate Finite (categorical) scitype.  See Working with Categorical Data).","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"For more on scitype coercion of arrays and tables, see coerce, autotype and unpack below and the examples at MLJScientificTypes.jl.","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"scitype\ncoerce\nautotype\nunpack","category":"page"},{"location":"getting_started/#ScientificTypes.scitype","page":"Getting Started","title":"ScientificTypes.scitype","text":"scitype(X)\n\nThe scientific type (interpretation) of X, as distinct from its machine type, as specified by the active convention.\n\nExamples from the MLJ convention\n\njulia> using MLJScientificTypes # or `using MLJ`\njulia> scitype(3.14)\nContinuous\n\njulia> scitype([1, 2, 3, missing])\nAbstractArray{Union{Missing, Count},1}\n\njulia> scitype((5, \"beige\"))\nTuple{Count, Textual}\n\njulia> using CategoricalArrays\njulia> X = (gender = categorical(['M', 'M', 'F', 'M', 'F']),\n            ndevices = [1, 3, 2, 3, 2])\njulia> scitype(X)\nTable{Union{AbstractArray{Count,1}, AbstractArray{Multiclass{2},1}}}\n\nThe specific behavior of scitype is governed by the active convention, as returned by ScientificTypes.convention(). The MLJScientificTypes.jl documentation details the convention demonstrated above.\n\n\n\n\n\n","category":"function"},{"location":"getting_started/#MLJScientificTypes.coerce","page":"Getting Started","title":"MLJScientificTypes.coerce","text":"coerce(A, ...; tight=false, verbosity=1)\n\nGiven a table A, return a copy of A ensuring that the scitype of the columns match new specifications. The specifications can be given as a a bunch of colname=>Scitype pairs or as a dictionary whose keys are names and values are scientific types:\n\ncoerce(X, col1=>scitype1, col2=>scitype2, ... ; verbosity=1)\ncoerce(X, d::AbstractDict; verbosity=1)\n\nOne can also specify pairs of type T1=>T2 in which case all columns with scientific element type subtyping Union{T1,Missing} will be coerced to the new specified scitype T2.\n\nExamples\n\nSpecifiying (name, scitype) pairs:\n\nusing CategoricalArrays, DataFrames, Tables\nX = DataFrame(name=[\"Siri\", \"Robo\", \"Alexa\", \"Cortana\"],\n              height=[152, missing, 148, 163],\n              rating=[1, 5, 2, 1])\nXc = coerce(X, :name=>Multiclass, :height=>Continuous, :rating=>OrderedFactor)\nschema(Xc).scitypes # (Multiclass, Continuous, OrderedFactor)\n\nSpecifying (T1, T2) pairs:\n\nX  = (x = [1, 2, 3],\n      y = rand(3),\n      z = [10, 20, 30])\nXc = coerce(X, Count=>Continuous)\nschema(Xfixed).scitypes # (Continuous, Continuous, Continuous)\n\n\n\n\n\n","category":"function"},{"location":"getting_started/#MLJScientificTypes.autotype","page":"Getting Started","title":"MLJScientificTypes.autotype","text":"autotype(X; kw...)\n\nReturn a dictionary of suggested scitypes for each column of X, a table or an array based on rules\n\nKwargs\n\nonly_changes=true:       if true, return only a dictionary of the names for                             which applying autotype differs from just using                             the ambient convention. When coercing with                             autotype, only_changes should be true.\nrules=(:few_to_finite,): the set of rules to apply.\n\n\n\n\n\n","category":"function"},{"location":"getting_started/#MLJBase.unpack","page":"Getting Started","title":"MLJBase.unpack","text":"t1, t2, ...., tk = unnpack(table, f1, f2, ... fk;\n                           wrap_singles=false,\n                           shuffle=false,\n                           rng::Union{AbstractRNG,Int}=nothing)\n\n)\n\nSplit any Tables.jl compatible table into smaller tables (or vectors) t1, t2, ..., tk by making selections without replacement from the column names defined by the filters f1, f2, ..., fk. A filter is any object f such that f(name) is true or false for each column name::Symbol of table.\n\nWhenever a returned table contains a single column, it is converted to a vector unless wrap_singles=true.\n\nScientific type conversions can be optionally specified (note semicolon):\n\nunpack(table, t...; wrap_singles=false, col1=>scitype1, col2=>scitype2, ... )\n\nIf shuffle=true then the rows of table are first shuffled, using the global RNG, unless rng is specified; if rng is an integer, it specifies the seed of an automatically generated Mersenne twister. If rng is specified then shuffle=true is implicit.\n\nExample\n\njulia> table = DataFrame(x=[1,2], y=['a', 'b'], z=[10.0, 20.0], w=[\"A\", \"B\"])\njulia> Z, XY = unpack(table, ==(:z), !=(:w);\n               :x=>Continuous, :y=>Multiclass)\njulia> XY\n2×2 DataFrame\n│ Row │ x       │ y            │\n│     │ Float64 │ Categorical… │\n├─────┼─────────┼──────────────┤\n│ 1   │ 1.0     │ 'a'          │\n│ 2   │ 2.0     │ 'b'          │\n\njulia> Z\n2-element Array{Float64,1}:\n 10.0\n 20.0\n\n\n\n\n\n","category":"function"},{"location":"homogeneous_ensembles/#Homegeneous-Ensembles-1","page":"Homogeneous Ensembles","title":"Homegeneous Ensembles","text":"","category":"section"},{"location":"homogeneous_ensembles/#","page":"Homogeneous Ensembles","title":"Homogeneous Ensembles","text":"EnsembleModel","category":"page"},{"location":"homogeneous_ensembles/#MLJ.EnsembleModel","page":"Homogeneous Ensembles","title":"MLJ.EnsembleModel","text":"EnsembleModel(atom=nothing,\n              atomic_weights=Float64[],\n              bagging_fraction=0.8,\n              n=100,\n              rng=GLOBAL_RNG,\n              acceleration=default_resource(),\n              out_of_bag_measure=[])\n\nCreate a model for training an ensemble of n learners, with optional bagging, each with associated model atom. Ensembling is useful if fit!(machine(atom, data...)) does not create identical models on repeated calls (ie, is a stochastic model, such as a decision tree with randomized node selection criteria), or if bagging_fraction is set to a value less than 1.0, or both. The constructor fails if no atom is specified.\n\nOnly atomic models supporting targets with scitype AbstractVector{<:Finite} (univariate classifiers) or AbstractVector{<:Continuous} (univariate regressors) are supported.\n\nIf rng is an integer, then MersenneTwister(rng) is the random number generator used for bagging. Otherwise some AbstractRNG object is expected.\n\nThe atomic predictions are weighted according to the vector atomic_weights (to allow for external optimization) except in the case that atom is a Deterministic classifier. Uniform atomic weights are used if weight has zero length.\n\nThe ensemble model is Deterministic or Probabilistic, according to the corresponding supertype of atom. In the case of deterministic classifiers (target_scitype(atom) <: Abstract{<:Finite}), the predictions are majority votes, and for regressors (target_scitype(atom)<: AbstractVector{<:Continuous}) they are ordinary averages.  Probabilistic predictions are obtained by averaging the atomic probability distribution/mass functions; in particular, for regressors, the ensemble prediction on each input pattern has the type MixtureModel{VF,VS,D} from the Distributions.jl package, where D is the type of predicted distribution for atom.\n\nThe acceleration keyword argument is used to specify the compute resource (a subtype of ComputationalResources.AbstractResource) that will be used to accelerate/parallelize ensemble fitting.\n\nIf a single measure or non-empty vector of measures is specified by out_of_bag_measure, then out-of-bag estimates of performance are written to the trainig report (call report on the trained machine wrapping the ensemble model).\n\nImportant: If sample weights w (as opposed to atomic weights) are specified when constructing a machine for the ensemble model, as in mach = machine(ensemble_model, X, y, w), then w is used by any measures specified in out_of_bag_measure that support sample weights.\n\n\n\n\n\n","category":"function"},{"location":"transformers/#Transformers-and-other-unsupervised-models-1","page":"Transformers and other unsupervised models","title":"Transformers and other unsupervised models","text":"","category":"section"},{"location":"transformers/#","page":"Transformers and other unsupervised models","title":"Transformers and other unsupervised models","text":"Several unsupervised models used to perform common transformations, such as one-hot encoding, are available in MLJ out-of-the-box. These are detailed in Built-in transformers below.","category":"page"},{"location":"transformers/#","page":"Transformers and other unsupervised models","title":"Transformers and other unsupervised models","text":"A transformer is static if it has no learned parameters. While such a transformer is tantamount to an ordinary function, realizing it as an MLJ static transformer (subtype of Static <: Unsupervised) can be useful, especially if the function depends on parameters the user would like to manipulate (which become hyper-parameters of the model). The necessary syntax for defining your own static transformers is described in Static transformers below.","category":"page"},{"location":"transformers/#","page":"Transformers and other unsupervised models","title":"Transformers and other unsupervised models","text":"Some unsupervised models, such as clustering algorithms, have a predict method in addition to a transform method. We give an example of this in Transformers that also predict","category":"page"},{"location":"transformers/#","page":"Transformers and other unsupervised models","title":"Transformers and other unsupervised models","text":"Finally we note that models that fit a distribution, or more generally a sampler object, to some data, which are sometimes viewed as unsupervised, are treated in MLJ as supervised models. See Models that learn a probability distribution for an example.","category":"page"},{"location":"transformers/#Built-in-transformers-1","page":"Transformers and other unsupervised models","title":"Built-in transformers","text":"","category":"section"},{"location":"transformers/#","page":"Transformers and other unsupervised models","title":"Transformers and other unsupervised models","text":"MLJModels.UnivariateStandardizer\nMLJModels.Standardizer\nMLJModels.OneHotEncoder\nMLJModels.ContinuousEncoder\nMLJModels.FeatureSelector\nMLJModels.UnivariateBoxCoxTransformer\nMLJModels.UnivariateDiscretizer\nMLJModels.FillImputer","category":"page"},{"location":"transformers/#MLJModels.UnivariateStandardizer","page":"Transformers and other unsupervised models","title":"MLJModels.UnivariateStandardizer","text":"UnivariateStandardizer()\n\nUnsupervised model for standardizing (whitening) univariate data.\n\n\n\n\n\n","category":"type"},{"location":"transformers/#MLJModels.Standardizer","page":"Transformers and other unsupervised models","title":"MLJModels.Standardizer","text":"Standardizer(; features=Symbol[],\n               ignore=false,\n               ordered_factor=false,\n               count=false)\n\nUnsupervised model for standardizing (whitening) the columns of tabular data.  If features is unspecified then all columns having Continuous element scitype are standardized. Otherwise, the features standardized are the Continuous features named in features (ignore=false) or Continuous features not named in features (ignore=true). To allow standarization of Count or OrderedFactor features as well, set the appropriate flag to true.\n\nInstead of supplying a features vector, a Bool-valued callable with one  argument can be also be specified. For example, specifying  Standardizer(features = name -> name in [:x1, :x3], ignore = true, count=true)   has the same effect as Standardizer(features = [:x1, :x3], ignore = true, count=true), namely to standardise all Continuous and Count features,  with the exception of :x1 and :x3.\n\nThe inverse_tranform method is supported provided count=false and ordered_factor=false at time of fit.\n\nExample\n\nX = (ordinal1 = [1, 2, 3],\n     ordinal2 = coerce([:x, :y, :x], OrderedFactor),\n     ordinal3 = [10.0, 20.0, 30.0],\n     ordinal4 = [-20.0, -30.0, -40.0],\n     nominal = coerce([\"Your father\", \"he\", \"is\"], Multiclass));\nstand1 = Standardizer();\njulia> transform(fit!(machine(stand1, X)), X)\n[ Info: Training Machine{Standardizer} @ 7…97.\n(ordinal1 = [1, 2, 3],\n ordinal2 = CategoricalValue{Symbol,UInt32}[:x, :y, :x],\n ordinal3 = [-1.0, 0.0, 1.0],\n ordinal4 = [1.0, 0.0, -1.0],\n nominal = CategoricalVale{String,UInt32}[\"Your father\", \"he\", \"is\"],)\n\nstand2 = Standardizer(features=[:ordinal3, ], ignore=true, count=true);\njulia> transform(fit!(machine(stand2, X)), X)\n[ Info: Training Machine{Standardizer} @ 1…87.\n(ordinal1 = [-1.0, 0.0, 1.0],\n ordinal2 = CategoricalValue{Symbol,UInt32}[:x, :y, :x],\n ordinal3 = [10.0, 20.0, 30.0],\n ordinal4 = [1.0, 0.0, -1.0],\n nominal = CategoricalValue{String,UInt32}[\"Your father\", \"he\", \"is\"],)\n\n\n\n\n\n","category":"type"},{"location":"transformers/#MLJModels.OneHotEncoder","page":"Transformers and other unsupervised models","title":"MLJModels.OneHotEncoder","text":"OneHotEncoder(; features=Symbol[],\n                ignore=false,\n                ordered_factor=true,\n                drop_last=false)\n\nUnsupervised model for one-hot encoding the Finite features (columns) of some table. If features is unspecified all features with Finite element scitype are encoded. Otherwise, encoding is applied to all Finite features named in features (ignore=false) or all Finite features not named in features (ignore=true).\n\nIf ordered_factor=false then the above holds with Finite replaced with Multiclass, ie OrderedFactor features are not transformed.\n\nSpecify drop_last=true if the column for the last level of each categorical feature is to be dropped.\n\nNew data to be transformed may lack features present in the fit data, but no new features can be present.\n\nWarning: This transformer assumes that levels(col) for any Multiclass or OrderedFactor column is the same in new data being transformed as it is in the data used to fit the transformer.\n\nExample\n\nX = (name=categorical([\"Danesh\", \"Lee\", \"Mary\", \"John\"]),\n     grade=categorical([:A, :B, :A, :C], ordered=true),\n     height=[1.85, 1.67, 1.5, 1.67],\n     n_devices=[3, 2, 4, 3])\nschema(X)\n\n┌───────────┬─────────────────────────────────┬──────────────────┐\n│ _.names   │ _.types                         │ _.scitypes       │\n├───────────┼─────────────────────────────────┼──────────────────┤\n│ name      │ CategoricalValue{String,UInt32} │ Multiclass{4}    │\n│ grade     │ CategoricalValue{Symbol,UInt32} │ OrderedFactor{3} │\n│ height    │ Float64                         │ Continuous       │\n│ n_devices │ Int64                           │ Count            │\n└───────────┴─────────────────────────────────┴──────────────────┘\n_.nrows = 4\n\nhot = OneHotEncoder(ordered_factor=true);\nmach = fit!(machine(hot, X))\ntransform(mach, X) |> schema\n\n┌──────────────┬─────────┬────────────┐\n│ _.names      │ _.types │ _.scitypes │\n├──────────────┼─────────┼────────────┤\n│ name__Danesh │ Float64 │ Continuous │\n│ name__John   │ Float64 │ Continuous │\n│ name__Lee    │ Float64 │ Continuous │\n│ name__Mary   │ Float64 │ Continuous │\n│ grade__A     │ Float64 │ Continuous │\n│ grade__B     │ Float64 │ Continuous │\n│ grade__C     │ Float64 │ Continuous │\n│ height       │ Float64 │ Continuous │\n│ n_devices    │ Int64   │ Count      │\n└──────────────┴─────────┴────────────┘\n_.nrows = 4\n\n\n\n\n\n","category":"type"},{"location":"transformers/#MLJModels.ContinuousEncoder","page":"Transformers and other unsupervised models","title":"MLJModels.ContinuousEncoder","text":"ContinuousEncoder(one_hot_ordered_factors=false, drop_last=false)\n\nUnsupervised model for arranging all features (columns) of a table to have Continuous element scitype, by applying the following protocol to each feature ftr:\n\nIf ftr is already Continuous retain it.\nIf ftr is Multiclass, one-hot encode it.\nIf ftr is OrderedFactor, replace it with coerce(ftr, Continuous) (vector of floating point integers), unless ordered_factors=false is specified, in which case one-hot encode it.\nIf ftr is Count, replace it with coerce(ftr, Continuous).\nIf ftr is of some other element scitype, or was not observed in fitting the encoder, drop it from the table.\n\nIf drop_last=true is specified, then one-hot encoding always drops the last class indicator column.\n\nWarning: This transformer assumes that levels(col) for any Multiclass or OrderedFactor column is the same in new data being transformed as it is in the data used to fit the transformer.\n\nExample\n\nX = (name=categorical([\"Danesh\", \"Lee\", \"Mary\", \"John\"]),\n     grade=categorical([:A, :B, :A, :C], ordered=true),\n     height=[1.85, 1.67, 1.5, 1.67],\n     n_devices=[3, 2, 4, 3],\n     comments=[\"the force\", \"be\", \"with you\", \"too\"])\nschema(X)\n\n┌───────────┬─────────────────────────────────┬──────────────────┐\n│ _.names   │ _.types                         │ _.scitypes       │\n├───────────┼─────────────────────────────────┼──────────────────┤\n│ name      │ CategoricalValue{String,UInt32} │ Multiclass{4}    │\n│ grade     │ CategoricalValue{Symbol,UInt32} │ OrderedFactor{3} │\n│ height    │ Float64                         │ Continuous       │\n│ n_devices │ Int64                           │ Count            │\n│ comments  │ String                          │ Textual          │\n└───────────┴─────────────────────────────────┴──────────────────┘\n_.nrows = 4\n\ncont = ContinuousEncoder(drop_last=true);\nmach = fit!(machine(cont, X))\ntransform(mach, X) |> schema\n\n┌──────────────┬─────────┬────────────┐\n│ _.names      │ _.types │ _.scitypes │\n├──────────────┼─────────┼────────────┤\n│ name__Danesh │ Float64 │ Continuous │\n│ name__John   │ Float64 │ Continuous │\n│ name__Lee    │ Float64 │ Continuous │\n│ grade        │ Float64 │ Continuous │\n│ height       │ Float64 │ Continuous │\n│ n_devices    │ Float64 │ Continuous │\n└──────────────┴─────────┴────────────┘\n_.nrows = 4\n\n\n\n\n\n","category":"type"},{"location":"transformers/#MLJModels.FeatureSelector","page":"Transformers and other unsupervised models","title":"MLJModels.FeatureSelector","text":"FeatureSelector(features=Symbol[], ignore=false)\n\nAn unsupervised model for filtering features (columns) of a table. Only those features encountered during fitting will appear in transformed tables if features is empty (the default). Alternatively, if a non-empty features is specified, then only the specified features encountered during fitting are used (ignore=false) or all features encountered during fitting which are not named in features are used (ignore=true).\n\nThrows an error if a recorded or specified feature is not present in the transformation  input.\n\nInstead of supplying a features vector, a Bool-valued callable with one argument  can be also be specified. For example, specifying FeatureSelector(features = name -> name in [:x1, :x3], ignore = true) has the same effect as  FeatureSelector(features = [:x1, :x3], ignore = true), namely to select  all features, with the exception of :x1 and :x3.\n\nExample\n\njulia> X = (ordinal1 = [1, 2, 3],\n            ordinal2 = coerce([:x, :y, :x], OrderedFactor),\n            ordinal3 = [10.0, 20.0, 30.0],\n            ordinal4 = [-20.0, -30.0, -40.0],\n            nominal = coerce([\"Your father\", \"he\", \"is\"], Multiclass));\n\njulia> select1 = FeatureSelector();\n\njulia> transform(fit!(machine(select1, X)), X)\n[ Info: Training Machine{FeatureSelector} @811.\n(ordinal1 = [1, 2, 3],\n ordinal2 = CategoricalValue{Symbol,UInt32}[:x, :y, :x],\n ordinal3 = [-1.0, 0.0, 1.0],\n ordinal4 = [1.0, 0.0, -1.0],\n nominal = CategoricalVale{String,UInt32}[\"Your father\", \"he\", \"is\"],)\n\njulia> select2 = FeatureSelector(features=[:ordinal3, ], ignore=true);\n\njulia> transform(fit!(machine(select2, X)), X)\n[ Info: Training Machine{FeatureSelector} @721.\n(ordinal1 = [1, 2, 3],\n ordinal2 = CategoricalValue{Symbol,UInt32}[:x, :y, :x],\n ordinal4 = [-20.0, -30.0, -40.0],\n nominal = CategoricalValue{String,UInt32}[\"Your father\", \"he\", \"is\"],)\n\n\n\n\n\n\n","category":"type"},{"location":"transformers/#MLJModels.UnivariateBoxCoxTransformer","page":"Transformers and other unsupervised models","title":"MLJModels.UnivariateBoxCoxTransformer","text":"UnivariateBoxCoxTransformer(; n=171, shift=false)\n\nUnsupervised model specifying a univariate Box-Cox transformation of a single variable taking non-negative values, with a possible preliminary shift. Such a transformation is of the form\n\nx -> ((x + c)^λ - 1)/λ for λ not 0\nx -> log(x + c) for λ = 0\n\nOn fitting to data n different values of the Box-Cox exponent λ (between -0.4 and 3) are searched to fix the value maximizing normality. If shift=true and zero values are encountered in the data then the transformation sought includes a preliminary positive shift c of 0.2 times the data mean. If there are no zero values, then no shift is applied.\n\n\n\n\n\n","category":"type"},{"location":"transformers/#MLJModels.UnivariateDiscretizer","page":"Transformers and other unsupervised models","title":"MLJModels.UnivariateDiscretizer","text":"UnivariateDiscretizer(n_classes=512)\n\nReturns an MLJModel for for discretizing any continuous vector v  (scitype(v) <: AbstractVector{Continuous}), where n_classes  describes the resolution of the discretization.\n\nTransformed output w is a vector of ordered factors (scitype(w) <:  AbstractVector{<:OrderedFactor}). Specifically, w is a  CategoricalVector, with element type  CategoricalValue{R,R}, where R<Unsigned is optimized.\n\nThe transformation is chosen so that the vector on which the  transformer is fit has, in transformed form, an approximately uniform  distribution of values.\n\nExample\n\nusing MLJ\nt = UnivariateDiscretizer(n_classes=10)\ndiscretizer = machine(t, randn(1000))\nfit!(discretizer)\nv = rand(10)\nw = transform(discretizer, v)\nv_approx = inverse_transform(discretizer, w) # reconstruction of v from w\n\n\n\n\n\n","category":"type"},{"location":"transformers/#MLJModels.FillImputer","page":"Transformers and other unsupervised models","title":"MLJModels.FillImputer","text":"FillImputer(\n features        = [],\n continuous_fill = e -> skipmissing(e) |> median\n count_fill      = e -> skipmissing(e) |> (f -> round(eltype(f), median(f)))\n finite_fill     = e -> skipmissing(e) |> mode\n\nImputes missing data with a fixed value computed on the non-missing values. A different imputing function can be specified for Continuous, Count and Finite data. \n\nFields\n\ncontinuous_fill: function to use on Continuous data, by default the median\ncount_fill: function to use on Count data, by default the rounded median\nfinite_fill: function to use on Multiclass and OrderedFactor data (including binary data), by default the mode\n\n\n\n\n\n","category":"type"},{"location":"transformers/#Static-transformers-1","page":"Transformers and other unsupervised models","title":"Static transformers","text":"","category":"section"},{"location":"transformers/#","page":"Transformers and other unsupervised models","title":"Transformers and other unsupervised models","text":"The main use-case for static transformers is for insertion into a @pipeline or other exported learning network (see Composing Models). If a static transformer has no hyper-parameters, it is tantamount to an ordinary function. An ordinary function can be inserted directly into a @pipeline; the situation for learning networks is only slightly more complicated; see Static operations on nodes.","category":"page"},{"location":"transformers/#","page":"Transformers and other unsupervised models","title":"Transformers and other unsupervised models","text":"The following example defines a new model type Averager to perform the weighted average of two vectors (target predictions, for example). We suppose the weighting is normalized, and therefore controlled by a single hyper-parameter, mix.","category":"page"},{"location":"transformers/#","page":"Transformers and other unsupervised models","title":"Transformers and other unsupervised models","text":"mutable struct Averager <: Static\n    mix::Float64\nend\n\nimport MLJBase\nMLJBase.transform(a::Averager, _, y1, y2) = (1 - a.mix)*y1 + a.mix*y2","category":"page"},{"location":"transformers/#","page":"Transformers and other unsupervised models","title":"Transformers and other unsupervised models","text":"Important. Note the sub-typing <: Static.","category":"page"},{"location":"transformers/#","page":"Transformers and other unsupervised models","title":"Transformers and other unsupervised models","text":"Such static transformers with (unlearned) parameters can have arbitrarily many inputs, but only one output. In the single input case an inverse_transform can also be defined. Since they have no real learned parameters, you bind a static transformer to a machine without specifying training arguments.","category":"page"},{"location":"transformers/#","page":"Transformers and other unsupervised models","title":"Transformers and other unsupervised models","text":"mach = machine(Averager(0.5)) |> fit!\ntransform(mach, [1, 2, 3], [3, 2, 1])\n3-element Array{Float64,1}:\n 2.0\n 2.0\n 2.0","category":"page"},{"location":"transformers/#","page":"Transformers and other unsupervised models","title":"Transformers and other unsupervised models","text":"Let's see how we can include our Averager in a learning network (see Composing Models) to mix the predictions of two regressors, with one-hot encoding of the inputs:","category":"page"},{"location":"transformers/#","page":"Transformers and other unsupervised models","title":"Transformers and other unsupervised models","text":"X = source()\ny = source() #MLJ will automatically infer this a target node \n\nridge = @load RidgeRegressor pkg=MultivariateStats\nknn = @load KNNRegressor\naverager = Averager(0.5)\n\nhotM = machine(OneHotEncoder(), X)\nW = transform(hotM, X) # one-hot encode the input\n\nridgeM = machine(ridge, W, y)\ny1 = predict(ridgeM, W)\n\nknnM = machine(knn, W, y)\ny2 = predict(knnM, W)\n\naveragerM= machine(averager)\nyhat = transform(averagerM, y1, y2)","category":"page"},{"location":"transformers/#","page":"Transformers and other unsupervised models","title":"Transformers and other unsupervised models","text":"Now we export to obtain a Deterministic composite model and then  instantiate composite model","category":"page"},{"location":"transformers/#","page":"Transformers and other unsupervised models","title":"Transformers and other unsupervised models","text":"learning_mach = machine(Deterministic(), X, y; predict=yhat)\nMachine{DeterministicSurrogate} @772 trained 0 times.\n  args: \n    1:\tSource @415 ⏎ `Unknown`\n    2:\tSource @389 ⏎ `Unknown`\n\n\n@from_network learning_mach struct DoubleRegressor\n       regressor1=ridge\n       regressor2=knn\n       averager=averager\n       end\n       \ncomposite = DoubleRegressor()\njulia> composite = DoubleRegressor()\nDoubleRegressor(\n    regressor1 = RidgeRegressor(\n            lambda = 1.0),\n    regressor2 = KNNRegressor(\n            K = 5,\n            algorithm = :kdtree,\n            metric = Distances.Euclidean(0.0),\n            leafsize = 10,\n            reorder = true,\n            weights = :uniform),\n    averager = Averager(\n            mix = 0.5)) @301\n","category":"page"},{"location":"transformers/#","page":"Transformers and other unsupervised models","title":"Transformers and other unsupervised models","text":"which can be can be evaluated like any other model:","category":"page"},{"location":"transformers/#","page":"Transformers and other unsupervised models","title":"Transformers and other unsupervised models","text":"composite.averager.mix = 0.25 # adjust mix from default of 0.5\nevaluate(composite, (@load_reduced_ames)..., measure=rms)\njulia> evaluate(composite, (@load_reduced_ames)..., measure=rms)\nEvaluating over 6 folds: 100%[=========================] Time: 0:00:00\n┌───────────┬───────────────┬────────────────────────────────────────────────────────┐\n│ _.measure │ _.measurement │ _.per_fold                                             │\n├───────────┼───────────────┼────────────────────────────────────────────────────────┤\n│ rms       │ 26800.0       │ [21400.0, 23700.0, 26800.0, 25900.0, 30800.0, 30700.0] │\n└───────────┴───────────────┴────────────────────────────────────────────────────────┘\n_.per_observation = [missing]","category":"page"},{"location":"transformers/#Transformers-that-also-predict-1","page":"Transformers and other unsupervised models","title":"Transformers that also predict","text":"","category":"section"},{"location":"transformers/#","page":"Transformers and other unsupervised models","title":"Transformers and other unsupervised models","text":"Commonly, clustering algorithms learn to label data by identifying a collection of \"centroids\" in the training data. Any new input observation is labeled with the cluster to which it is closest (this is the output of predict) while the vector of all distances from the centroids defines a lower-dimensional representation of the observation (the output of transform). In the following example a K-means clustering algorithm assigns one of three labels 1, 2, 3 to the input features of the iris data set and compares them with the actual species recorded in the target (not seen by the algorithm).","category":"page"},{"location":"transformers/#","page":"Transformers and other unsupervised models","title":"Transformers and other unsupervised models","text":"import Random.seed!\nseed!(123)\n\nX, y = @load_iris;\nmodel = @load KMeans pkg=ParallelKMeans\nmach = machine(model, X) |> fit!\n\n# transforming:\nXsmall = transform(mach);\nselectrows(Xsmall, 1:4) |> pretty\njulia> selectrows(Xsmall, 1:4) |> pretty\n┌─────────────────────┬────────────────────┬────────────────────┐\n│ x1                  │ x2                 │ x3                 │\n│ Float64             │ Float64            │ Float64            │\n│ Continuous          │ Continuous         │ Continuous         │\n├─────────────────────┼────────────────────┼────────────────────┤\n│ 0.0215920000000267  │ 25.314260355029603 │ 11.645232464391299 │\n│ 0.19199200000001326 │ 25.882721893491123 │ 11.489658693899486 │\n│ 0.1699920000000077  │ 27.58656804733728  │ 12.674412792260142 │\n│ 0.26919199999998966 │ 26.28656804733727  │ 11.64392098898145  │\n└─────────────────────┴────────────────────┴────────────────────┘\n\n# predicting:\nyhat = predict(mach);\ncompare = zip(yhat, y) |> collect;\ncompare[1:8]\n8-element Array{Tuple{CategoricalValue{Int64,UInt32},CategoricalString{UInt32}},1}:\n (1, \"setosa\")\n (1, \"setosa\")\n (1, \"setosa\")\n (1, \"setosa\")\n (1, \"setosa\")\n (1, \"setosa\")\n (1, \"setosa\")\n (1, \"setosa\")\n\ncompare[51:58]\n8-element Array{Tuple{CategoricalValue{Int64,UInt32},CategoricalString{UInt32}},1}:\n (2, \"versicolor\")\n (3, \"versicolor\")\n (2, \"versicolor\")\n (3, \"versicolor\")\n (3, \"versicolor\")\n (3, \"versicolor\")\n (3, \"versicolor\")\n (3, \"versicolor\")\n\ncompare[101:108]\n8-element Array{Tuple{CategoricalValue{Int64,UInt32},CategoricalString{UInt32}},1}:\n (2, \"virginica\")\n (3, \"virginica\")\n (2, \"virginica\")\n (2, \"virginica\")\n (2, \"virginica\")\n (2, \"virginica\")\n (3, \"virginica\")\n (2, \"virginica\")","category":"page"},{"location":"openml_integration/#OpenML-Integration-1","page":"OpenML Integration","title":"OpenML Integration","text":"","category":"section"},{"location":"openml_integration/#","page":"OpenML Integration","title":"OpenML Integration","text":"OpenML provides an integration platform for carrying out and comparing machine learning solutions across a broad collection of public datasets and software platforms. Integration of MLJ with OpenML is a work in progress.","category":"page"},{"location":"openml_integration/#Loading-IRIS-Dataset-1","page":"OpenML Integration","title":"Loading IRIS Dataset","text":"","category":"section"},{"location":"openml_integration/#","page":"OpenML Integration","title":"OpenML Integration","text":"As an example, we will try to load the iris dataset using OpenML.load(taskID).","category":"page"},{"location":"openml_integration/#","page":"OpenML Integration","title":"OpenML Integration","text":"using MLJ.MLJBase","category":"page"},{"location":"openml_integration/#Task-ID-1","page":"OpenML Integration","title":"Task ID","text":"","category":"section"},{"location":"openml_integration/#","page":"OpenML Integration","title":"OpenML Integration","text":"OpenML.load requires task ID of the the dataset to be loaded. This ID can  be found on the OpenML website.  The task ID for the iris dataset is 61, as mentioned in this OpenML Page","category":"page"},{"location":"openml_integration/#","page":"OpenML Integration","title":"OpenML Integration","text":"rowtable = OpenML.load(61)","category":"page"},{"location":"openml_integration/#Converting-to-DataFrame-1","page":"OpenML Integration","title":"Converting to DataFrame","text":"","category":"section"},{"location":"openml_integration/#","page":"OpenML Integration","title":"OpenML Integration","text":"using DataFrames\ndf = DataFrame(rowtable)\ndf2 = coerce(df, :class=>Multiclass)","category":"page"},{"location":"openml_integration/#","page":"OpenML Integration","title":"OpenML Integration","text":"OpenML.load","category":"page"},{"location":"openml_integration/#MLJBase.OpenML.load","page":"OpenML Integration","title":"MLJBase.OpenML.load","text":"OpenML.load(id)\n\nLoad the OpenML dataset with specified id, from those listed on the OpenML site.\n\nReturns a \"row table\", i.e., a Vector of identically typed NamedTuples. A row table is compatible with the Tables.jl interface and can therefore be readily converted to other compatible formats. For example:\n\nusing DataFrames\nrowtable = OpenML.load(61);\ndf = DataFrame(rowtable);\ndf2 = coerce(df, :class=>Multiclass)\n\n\n\n\n\n","category":"function"},{"location":"frequently_asked_questions/#Frequently-Asked-Questions-1","page":"FAQ","title":"Frequently Asked Questions","text":"","category":"section"},{"location":"frequently_asked_questions/#Julia-already-has-a-great-machine-learning-toolbox,-ScitkitLearn.jl.-Why-MLJ?-1","page":"FAQ","title":"Julia already has a great machine learning toolbox, ScitkitLearn.jl. Why MLJ?","text":"","category":"section"},{"location":"frequently_asked_questions/#","page":"FAQ","title":"FAQ","text":"An alternative machine learning toolbox for Julia users is ScikitLearn.jl. Initially intended as a Julia wrapper for the popular python library scikit-learn, ML algorithms written in Julia can also implement the ScikitLearn.jl API. Meta-algorithms (systematic tuning, pipelining, etc) remain python wrapped code, however.","category":"page"},{"location":"frequently_asked_questions/#","page":"FAQ","title":"FAQ","text":"While ScitkiLearn.jl provides the Julia user with access to a mature and large library of machine learning models, the scikit-learn API on which it is modeled, dating back to 2007, is not likely to evolve significantly in the future. MLJ enjoys (or will enjoy) several features that should make it an attractive alternative in the longer term:","category":"page"},{"location":"frequently_asked_questions/#","page":"FAQ","title":"FAQ","text":"One language. ScikitLearn.jl wraps python code, which in turn wraps C code for performance-critical routines. A Julia machine learning algorithm that implements the MLJ model interface is 100% Julia. Writing code in Julia is almost as fast as python and well-written Julia code runs almost as fast as C. Additionally, a single language design provides superior interoperability. For example, one can implement: (i) gradient-descent tuning of hyperparameters, using automatic differentiation libraries such as Flux.jl; and (ii) GPU performance boosts without major code refactoring, using CuArrays.jl.\nRegistry for model metadata. In ScikitLearn.jl the list of available models, as well as model metadata (whether a model handles categorical inputs, whether is can make probabilistic predictions, etc) must be gleaned from documentation. In MLJ, this information is more structured and is accessible to MLJ via a searchable model registry (without the models needing to be loaded).\nFlexible API for model composition. Pipelines in scikit-learn are more of an afterthought than an integral part of the original design. By contrast, MLJ's user-interaction API was predicated on the requirements of a flexible \"learning network\" API, one that allows models to be connected in essentially arbitrary ways (including target transforming and inverse-transforming). Networks can be built and tested in stages before being exported as first-class stand-alone models. Networks feature \"smart\" training (only necessary components are retrained after parameter changes) and will eventually be trainable using a DAG scheduler. With the help of Julia's meta-programming features, constructing common architectures, such as linear pipelines and stacks, will be one-line operations.\nClean probabilistic API. The scikit-learn API does not specify a universal standard for the form of probabilistic predictions. By fixing a probabilistic API along the lines of the skpro project, MLJ aims to improve support for Bayesian statistics and probabilistic graphical models.\nUniversal adoption of categorical data types. Python's scientific array library NumPy has no dedicated data type for representing categorical data (i.e., no type that tracks the pool of all possible classes). Generally scikit-learn models deal with this by requiring data to be relabeled as integers. However, the naive user trains a model on relabeled categorical data only to discover that evaluation on a test set crashes their code because a categorical feature takes on a value not observed in training. MLJ mitigates such issues by insisting on the use of categorical data types, and by insisting that MLJ model implementations preserve the class pools. If, for example, a training target contains classes in the pool that do not actually appear in the training set, a probabilistic prediction will nevertheless predict a distribution whose support includes the missing class, but which is appropriately weighted with probability zero.","category":"page"},{"location":"frequently_asked_questions/#","page":"FAQ","title":"FAQ","text":"Finally, we note that a large number of ScikitLearn.jl models are now wrapped for use in MLJ.","category":"page"},{"location":"generating_synthetic_data/#Generating-Synthetic-Data-1","page":"Generating Synthetic Data","title":"Generating Synthetic Data","text":"","category":"section"},{"location":"generating_synthetic_data/#","page":"Generating Synthetic Data","title":"Generating Synthetic Data","text":"MLJ has a set of functions - make_blobs, make_circles, make_moons and make_regression (closely resembling functions in scikit-learn of the same name) - for generating synthetic data sets. These are useful for testing machine learning models (e.g., testing user-defined composite models; see Composing Models)","category":"page"},{"location":"generating_synthetic_data/#Generating-Gaussian-blobs-1","page":"Generating Synthetic Data","title":"Generating Gaussian blobs","text":"","category":"section"},{"location":"generating_synthetic_data/#","page":"Generating Synthetic Data","title":"Generating Synthetic Data","text":"make_blobs","category":"page"},{"location":"generating_synthetic_data/#MLJBase.make_blobs","page":"Generating Synthetic Data","title":"MLJBase.make_blobs","text":"X, y = make_blobs(n=100, p=2; kwargs...)\n\nGenerate Gaussian blobs for clustering and classification problems.\n\nReturn value\n\nBy default, a table X with p columns (features) and n rows (observations), together with a corresponding vector of n Multiclass target observations y, indicating blob membership.\n\nKeyword arguments\n\nshuffle=true: whether to shuffle the resulting points,\ncenters=3: either a number of centers or a c x p matrix with c pre-determined centers,\ncluster_std=1.0: the standard deviation(s) of each blob,\ncenter_box=(-10. => 10.): the limits of the p-dimensional cube within which the cluster centers are drawn if they are not provided,\neltype=Float64: machine type of points (any subtype of  AbstractFloat).\nrng=nothing: any AbstractRNG object, or integer to seed a MersenneTwister (for reproducibility).\nas_table=true: whether to return the points as a table (true) or a matrix (false). If false the target y has integer element type. \n\nExample\n\nX, y = make_blobs(100, 3; centers=2, cluster_std=[1.0, 3.0])\n\n\n\n\n\n","category":"function"},{"location":"generating_synthetic_data/#","page":"Generating Synthetic Data","title":"Generating Synthetic Data","text":"using MLJ, DataFrames\nX, y = make_blobs(100, 3; centers=2, cluster_std=[1.0, 3.0])\ndfBlobs = DataFrame(X)\ndfBlobs.y = y\nfirst(dfBlobs, 3)","category":"page"},{"location":"generating_synthetic_data/#","page":"Generating Synthetic Data","title":"Generating Synthetic Data","text":"using VegaLite\ndfBlobs |> @vlplot(:point, x=:x1, y=:x2, color = :\"y:n\") ","category":"page"},{"location":"generating_synthetic_data/#","page":"Generating Synthetic Data","title":"Generating Synthetic Data","text":"(Image: svg)","category":"page"},{"location":"generating_synthetic_data/#","page":"Generating Synthetic Data","title":"Generating Synthetic Data","text":"dfBlobs |> @vlplot(:point, x=:x1, y=:x3, color = :\"y:n\") ","category":"page"},{"location":"generating_synthetic_data/#","page":"Generating Synthetic Data","title":"Generating Synthetic Data","text":"(Image: svg)","category":"page"},{"location":"generating_synthetic_data/#Generating-concentric-circles-1","page":"Generating Synthetic Data","title":"Generating concentric circles","text":"","category":"section"},{"location":"generating_synthetic_data/#","page":"Generating Synthetic Data","title":"Generating Synthetic Data","text":"make_circles","category":"page"},{"location":"generating_synthetic_data/#MLJBase.make_circles","page":"Generating Synthetic Data","title":"MLJBase.make_circles","text":"X, y = make_circles(n=100; kwargs...)\n\nGenerate n labeled points close to two concentric circles for classification and clustering models.\n\nReturn value\n\nBy default, a table X with 2 columns and n rows (observations), together with a corresponding vector of n Multiclass target observations y. The target is either 0 or 1, corresponding to membership to the smaller or larger circle, respectively.\n\nKeyword arguments\n\nshuffle=true: whether to shuffle the resulting points,\nnoise=0: standard deviation of the Gaussian noise added to the data,\nfactor=0.8: ratio of the smaller radius over the larger one,\n\neltype=Float64: machine type of points (any subtype of  AbstractFloat).\nrng=nothing: any AbstractRNG object, or integer to seed a MersenneTwister (for reproducibility).\nas_table=true: whether to return the points as a table (true) or a matrix (false). If false the target y has integer element type. \n\nExample\n\nX, y = make_circles(100; noise=0.5, factor=0.3)\n\n\n\n\n\n","category":"function"},{"location":"generating_synthetic_data/#","page":"Generating Synthetic Data","title":"Generating Synthetic Data","text":"using MLJ, DataFrames\nX, y = make_circles(100; noise=0.05, factor=0.3)\ndfCircles = DataFrame(X)\ndfCircles.y = y\nfirst(dfCircles, 3)","category":"page"},{"location":"generating_synthetic_data/#","page":"Generating Synthetic Data","title":"Generating Synthetic Data","text":"using VegaLite\ndfCircles |> @vlplot(:circle, x=:x1, y=:x2, color = :\"y:n\") ","category":"page"},{"location":"generating_synthetic_data/#","page":"Generating Synthetic Data","title":"Generating Synthetic Data","text":"(Image: svg)","category":"page"},{"location":"generating_synthetic_data/#Sampling-from-two-interleaved-half-circles-1","page":"Generating Synthetic Data","title":"Sampling from two interleaved half-circles","text":"","category":"section"},{"location":"generating_synthetic_data/#","page":"Generating Synthetic Data","title":"Generating Synthetic Data","text":"make_moons","category":"page"},{"location":"generating_synthetic_data/#MLJBase.make_moons","page":"Generating Synthetic Data","title":"MLJBase.make_moons","text":"    make_moons(n::Int=100; kwargs...)\n\nGenerates labeled two-dimensional points lying close to two interleaved semi-circles, for use with classification and clustering models.\n\nReturn value\n\nBy default, a table X with 2 columns and n rows (observations), together with a corresponding vector of n Multiclass target observations y. The target is either 0 or 1, corresponding to membership to the left or right semi-circle.\n\nKeyword arguments\n\nshuffle=true: whether to shuffle the resulting points,\nnoise=0.1: standard deviation of the Gaussian noise added to the data,\nxshift=1.0: horizontal translation of the second center with respect to the first one.\nyshift=0.3: vertical translation of the second center with respect to the first one.  \neltype=Float64: machine type of points (any subtype of  AbstractFloat).\nrng=nothing: any AbstractRNG object, or integer to seed a MersenneTwister (for reproducibility).\nas_table=true: whether to return the points as a table (true) or a matrix (false). If false the target y has integer element type. \n\nExample\n\nX, y = make_moons(100; noise=0.5)\n\n\n\n\n\n","category":"function"},{"location":"generating_synthetic_data/#","page":"Generating Synthetic Data","title":"Generating Synthetic Data","text":"using MLJ, DataFrames\nX, y = make_moons(100; noise=0.05)\ndfHalfCircles = DataFrame(X)\ndfHalfCircles.y = y\nfirst(dfHalfCircles, 3)","category":"page"},{"location":"generating_synthetic_data/#","page":"Generating Synthetic Data","title":"Generating Synthetic Data","text":"using VegaLite\ndfHalfCircles |> @vlplot(:circle, x=:x1, y=:x2, color = :\"y:n\") ","category":"page"},{"location":"generating_synthetic_data/#","page":"Generating Synthetic Data","title":"Generating Synthetic Data","text":"(Image: svg)","category":"page"},{"location":"generating_synthetic_data/#Regression-data-generated-from-noisy-linear-models-1","page":"Generating Synthetic Data","title":"Regression data generated from noisy linear models","text":"","category":"section"},{"location":"generating_synthetic_data/#","page":"Generating Synthetic Data","title":"Generating Synthetic Data","text":"make_regression","category":"page"},{"location":"generating_synthetic_data/#MLJBase.make_regression","page":"Generating Synthetic Data","title":"MLJBase.make_regression","text":"make_regression(n, p; kwargs...)\n\nGenerate Gaussian input features and a linear response with Gaussian noise, for use with regression models.\n\nReturn value\n\nBy default, a table X with p columns and n rows (observations), together with a corresponding vector of n Continuous target observations y.\n\nKeywords\n\n`intercept=true: whether to generate data from a model with intercept,\nsparse=0: portion of the generating weight vector that is sparse,\nnoise=0.1: standard deviation of the Gaussian noise added to the response,\noutliers=0: portion of the response vector to make as outliers by adding a random quantity with high variance. (Only applied if binary is false)\nbinary=false: whether the target should be binarized (via a sigmoid).\neltype=Float64: machine type of points (any subtype of  AbstractFloat).\nrng=nothing: any AbstractRNG object, or integer to seed a MersenneTwister (for reproducibility).\nas_table=true: whether to return the points as a table (true) or a matrix (false). \n\nExample\n\nX, y = make_regression(100, 5; noise=0.5, sparse=0.2, outliers=0.1)\n\n\n\n\n\n","category":"function"},{"location":"generating_synthetic_data/#","page":"Generating Synthetic Data","title":"Generating Synthetic Data","text":"using MLJ, DataFrames\nX, y = make_regression(100, 5; noise=0.5, sparse=0.2, outliers=0.1)\ndfRegression = DataFrame(X)\ndfRegression.y = y\nfirst(dfRegression, 3)","category":"page"},{"location":"#","page":"Introduction","title":"Introduction","text":"<span style=\"color:darkslateblue;font-size:2.25em;font-style:italic;\">\nA Machine Learning Framework for Julia\n</span>\n<br>\n<br>\n<div style=\"font-size:1.25em;font-weight:bold;\">\n  <a href=\"#Installation-1\">Installation</a>    &nbsp;|&nbsp;\n  <a href=\"mlj_cheatsheet\">Cheatsheet</a>       &nbsp;|&nbsp;\n  <a href=\"common_mlj_workflows\">Workflows</a>  &nbsp;|&nbsp;\n  <a href=\"https://alan-turing-institute.github.io/DataScienceTutorials.jl/\">Tutorials</a>       &nbsp;|&nbsp;\n  <a href=\"https://github.com/alan-turing-institute/MLJ.jl/\">For Developers</a> &nbsp;|&nbsp;\n  <a href=\"https://mybinder.org/v2/gh/alan-turing-institute/MLJ.jl/master?filepath=binder%2FMLJ_demo.ipynb\">Live Demo</a>\n</div>","category":"page"},{"location":"#Introduction-1","page":"Introduction","title":"Introduction","text":"","category":"section"},{"location":"#","page":"Introduction","title":"Introduction","text":"MLJ (Machine Learning in Julia) is a toolbox written in Julia providing a common interface and meta-algorithms for selecting, tuning, evaluating, composing and comparing over 150 machine learning models written in Julia and other languages. In particular MLJ wraps a large number of scikit-learn models.","category":"page"},{"location":"#","page":"Introduction","title":"Introduction","text":"MLJ is released under the MIT licensed and sponsored by the Alan Turing Institute.","category":"page"},{"location":"#Lightning-tour-1","page":"Introduction","title":"Lightning tour","text":"","category":"section"},{"location":"#","page":"Introduction","title":"Introduction","text":"For a more elementary introduction to MLJ usage see Getting Started.","category":"page"},{"location":"#","page":"Introduction","title":"Introduction","text":"The first code snippet below creates a new Julia environment MLJ_tour and installs just those packages needed for the tour. See Installation for more on creating a Julia environment for use with MLJ.","category":"page"},{"location":"#","page":"Introduction","title":"Introduction","text":"Julia installation instructions are here.","category":"page"},{"location":"#","page":"Introduction","title":"Introduction","text":"using Pkg\nPkg.activate(\"MLJ_tour\", shared=true)\nPkg.add(\"MLJ\")\nPkg.add(\"MLJModels\")\nPkg.add(\"EvoTrees\")","category":"page"},{"location":"#","page":"Introduction","title":"Introduction","text":"Load a selection of features and labels from the Ames House Price dataset:","category":"page"},{"location":"#","page":"Introduction","title":"Introduction","text":"using MLJ\nX, y = @load_reduced_ames;","category":"page"},{"location":"#","page":"Introduction","title":"Introduction","text":"Load and instantiate a gradient tree-boosting model:","category":"page"},{"location":"#","page":"Introduction","title":"Introduction","text":"booster = @load EvoTreeRegressor\nbooster.max_depth = 2\nbooster.nrounds=50","category":"page"},{"location":"#","page":"Introduction","title":"Introduction","text":"Combine the model with categorical feature encoding:","category":"page"},{"location":"#","page":"Introduction","title":"Introduction","text":"pipe = @pipeline ContinuousEncoder booster","category":"page"},{"location":"#","page":"Introduction","title":"Introduction","text":"Define a hyper-parameter range for optimization:","category":"page"},{"location":"#","page":"Introduction","title":"Introduction","text":"max_depth_range = range(pipe,\n                        :(evo_tree_regressor.max_depth),\n                        lower = 1,\n                        upper = 10)","category":"page"},{"location":"#","page":"Introduction","title":"Introduction","text":"Wrap the pipeline model in an optimization strategy:","category":"page"},{"location":"#","page":"Introduction","title":"Introduction","text":"self_tuning_pipe = TunedModel(model=pipe,\n                              tuning=RandomSearch(),\n                              ranges = max_depth_range,\n                              resampling=CV(nfolds=3, rng=456),\n                              measure=l1,\n                              acceleration=CPUThreads(),\n                              n=50)","category":"page"},{"location":"#","page":"Introduction","title":"Introduction","text":"Bind the \"self-tuning\" pipeline model (just a container for hyper-parameters) to data in a machine (which will additionally store learned parameters):","category":"page"},{"location":"#","page":"Introduction","title":"Introduction","text":"mach = machine(self_tuning_pipe, X, y)","category":"page"},{"location":"#","page":"Introduction","title":"Introduction","text":"Evaluate the \"self-tuning\" pipeline model's performance (implies nested resampling):","category":"page"},{"location":"#","page":"Introduction","title":"Introduction","text":"julia> evaluate!(mach,\n                measures=[l1, l2],\n                resampling=CV(nfolds=6, rng=123),\n                acceleration=CPUProcesses(), verbosity=2)\n┌───────────┬───────────────┬────────────────────────────────────────────────────────┐\n│ _.measure │ _.measurement │ _.per_fold                                             │\n├───────────┼───────────────┼────────────────────────────────────────────────────────┤\n│ l1        │ 16700.0       │ [16100.0, 16400.0, 14500.0, 17000.0, 16400.0, 19500.0] │\n│ l2        │ 6.43e8        │ [5.88e8, 6.81e8, 4.35e8, 6.35e8, 5.98e8, 9.18e8]       │\n└───────────┴───────────────┴────────────────────────────────────────────────────────┘\n_.per_observation = [[[29100.0, 9990.0, ..., 103.0], [12100.0, 1330.0, ..., 13200.0], [6490.0, 22000.0, ..., 13800.0], [9090.0, 9530.0, ..., 13900.0], [50800.0, 22700.0, ..., 1550.0], [32800.0, 4940.0, ..., 1110.0]], [[8.45e8, 9.98e7, ..., 10500.0], [1.46e8, 1.77e6, ..., 1.73e8], [4.22e7, 4.86e8, ..., 1.9e8], [8.26e7, 9.09e7, ..., 1.93e8], [2.58e9, 5.13e8, ..., 2.42e6], [1.07e9, 2.44e7, ..., 1.24e6]]]\n_.fitted_params_per_fold = [ … ]\n_.report_per_fold = [ … ]\n","category":"page"},{"location":"#","page":"Introduction","title":"Introduction","text":"Try out MLJ yourself in the following batteries-included Binder notebook. No installation required.","category":"page"},{"location":"#Key-goals-1","page":"Introduction","title":"Key goals","text":"","category":"section"},{"location":"#","page":"Introduction","title":"Introduction","text":"Offer a consistent way to use, compose and tune machine learning models in Julia,\nPromote the improvement of the Julia ML/Stats ecosystem by making it easier to use models from a wide range of packages,\nUnlock performance gains by exploiting Julia's support for parallelism, automatic differentiation, GPU, optimisation etc.","category":"page"},{"location":"#Key-features-1","page":"Introduction","title":"Key features","text":"","category":"section"},{"location":"#","page":"Introduction","title":"Introduction","text":"Data agnostic, train models on any data supported by the Tables.jl interface,\nExtensive, state-of-the art, support for model composition (pipelines and learning networks) (see more below),\nConvenient syntax to tune and evaluate (composite) models.\nConsistent interface to handle probabilistic predictions.\nExtensible tuning interface, to support growing number of optimization strategies, and designed to play well with model composition.","category":"page"},{"location":"#Model-composability-1","page":"Introduction","title":"Model composability","text":"","category":"section"},{"location":"#","page":"Introduction","title":"Introduction","text":"The generic model composition API's provided by other toolboxes we have surveyed share one or more of the following shortcomings, which do not exist in MLJ:","category":"page"},{"location":"#","page":"Introduction","title":"Introduction","text":"Composite models do not inherit all the behavior of ordinary models.\nComposition is limited to linear (non-branching) pipelines.\nSupervised components in a linear pipeline can only occur at the end of the pipeline.\nOnly static (unlearned) target transformations/inverse transformations are supported.\nHyper-parameters in homogeneous model ensembles cannot be coupled.\nModel stacking, with out-of-sample predictions for base learners, cannot be implemented (using the generic API alone).\nHyper-parameters and/or learned parameters of component models are not easily inspected or manipulated (by tuning algorithms, for example)\nComposite models cannot implement multiple opertations, for example, both a predict and transform method (as in clustering models) or both a transform and inverse_transform method.","category":"page"},{"location":"#","page":"Introduction","title":"Introduction","text":"Some of these features are demonstrated in this notebook","category":"page"},{"location":"#","page":"Introduction","title":"Introduction","text":"For more information see the MLJ design paper","category":"page"},{"location":"#Reporting-problems-1","page":"Introduction","title":"Reporting problems","text":"","category":"section"},{"location":"#","page":"Introduction","title":"Introduction","text":"Users are encouraged to provide feedback on their experience using MLJ and to report issues. You can do so here or on the #mlj Julia slack channel.","category":"page"},{"location":"#","page":"Introduction","title":"Introduction","text":"For known issues that are not strictly MLJ bugs, see here","category":"page"},{"location":"#Installation-1","page":"Introduction","title":"Installation","text":"","category":"section"},{"location":"#","page":"Introduction","title":"Introduction","text":"Initially it is recommended that MLJ and associated packages be installed in a new environment to avoid package conflicts. You can do this with","category":"page"},{"location":"#","page":"Introduction","title":"Introduction","text":"julia> using Pkg; Pkg.activate(\"my_MLJ_env\", shared=true)","category":"page"},{"location":"#","page":"Introduction","title":"Introduction","text":"Installing MLJ is also done with the package manager:","category":"page"},{"location":"#","page":"Introduction","title":"Introduction","text":"julia> Pkg.add(\"MLJ\")","category":"page"},{"location":"#","page":"Introduction","title":"Introduction","text":"Optional: To test your installation, run","category":"page"},{"location":"#","page":"Introduction","title":"Introduction","text":"julia> Pkg.test(\"MLJ\")","category":"page"},{"location":"#","page":"Introduction","title":"Introduction","text":"It is important to note that MLJ is essentially a big wrapper providing a unified access to model providing packages and so you will also need to make sure these packages are available in your environment.  For instance, if you want to use a Decision Tree Classifier, you need to have DecisionTree.jl installed:","category":"page"},{"location":"#","page":"Introduction","title":"Introduction","text":"julia> Pkg.add(\"DecisionTree\");\njulia> using MLJ;\njulia> @load DecisionTreeClassifier","category":"page"},{"location":"#","page":"Introduction","title":"Introduction","text":"For a list of models and their packages run","category":"page"},{"location":"#","page":"Introduction","title":"Introduction","text":"using MLJ\nmodels()","category":"page"},{"location":"#","page":"Introduction","title":"Introduction","text":"or refer to List of Supported Models","category":"page"},{"location":"#","page":"Introduction","title":"Introduction","text":"It is recommended that you start with models marked as coming from mature packages such as DecisionTree.jl, ScikitLearn.jl or XGBoost.jl.","category":"page"},{"location":"#","page":"Introduction","title":"Introduction","text":"MLJ is supported by a number of satelite packages (MLJTuning, MLJModelInterface, etc) which the general user is not required to install directly. Developers can learn more about these here","category":"page"},{"location":"#Learning-Julia-1","page":"Introduction","title":"Learning Julia","text":"","category":"section"},{"location":"#","page":"Introduction","title":"Introduction","text":"If you have experience in programming in another language but are new to Julia, then we highly recommend Aaron Christinson's tutorial Dispatching Design Patterns which is nicely compressed in his half-hour video presentation.","category":"page"},{"location":"#","page":"Introduction","title":"Introduction","text":"However, one doesn't need to be able to program in Julia to start using MLJ.","category":"page"},{"location":"#Learning-to-use-MLJ-1","page":"Introduction","title":"Learning to use MLJ","text":"","category":"section"},{"location":"#","page":"Introduction","title":"Introduction","text":"The present document, although littered with examples, is primarily intended as a complete reference. For a short introduction to basic MLJ functionality, read the Getting Started section of this manual. For extensive tutorials, we recommend the MLJ Tutorials website.  Each tutorial can be downloaded as a notebook or Julia script to facilitate experimentation. Finally, you may like to checkout the JuliaCon2020 Workshop on MLJ (recorded here).","category":"page"},{"location":"#","page":"Introduction","title":"Introduction","text":"You can try also MLJ out in the following notebook on Binder, without installing Julia or MLJ.","category":"page"},{"location":"#","page":"Introduction","title":"Introduction","text":"Users are also welcome to join the #mlj Julia slack channel to ask questions and make suggestions.","category":"page"},{"location":"#Citing-MLJ-1","page":"Introduction","title":"Citing MLJ","text":"","category":"section"},{"location":"#","page":"Introduction","title":"Introduction","text":"When presenting work that uses MLJ, please cite the MLJ design paper:","category":"page"},{"location":"#","page":"Introduction","title":"Introduction","text":"(Image: DOI)","category":"page"},{"location":"#","page":"Introduction","title":"Introduction","text":"@article{Blaom2020,\n  doi = {10.21105/joss.02704},\n  url = {https://doi.org/10.21105/joss.02704},\n  year = {2020},\n  publisher = {The Open Journal},\n  volume = {5},\n  number = {55},\n  pages = {2704},\n  author = {Anthony D. Blaom and Franz Kiraly and Thibaut Lienart and Yiannis Simillides and Diego Arenas and Sebastian J. Vollmer},\n  title = {MLJ: A Julia package for composable machine learning},\n  journal = {Journal of Open Source Software}\n}","category":"page"},{"location":"evaluating_model_performance/#Evaluating-Model-Performance-1","page":"Evaluating Model Performance","title":"Evaluating Model Performance","text":"","category":"section"},{"location":"evaluating_model_performance/#","page":"Evaluating Model Performance","title":"Evaluating Model Performance","text":"MLJ allows quick evaluation of a supervised model's performance against a battery of selected losses or scores. For more on available performance measures, see Performance Measures.","category":"page"},{"location":"evaluating_model_performance/#","page":"Evaluating Model Performance","title":"Evaluating Model Performance","text":"In addition to hold-out and cross-validation, the user can specify their own list of train/test pairs of row indices for resampling, or define their own re-usable resampling strategies.","category":"page"},{"location":"evaluating_model_performance/#","page":"Evaluating Model Performance","title":"Evaluating Model Performance","text":"For simultaneously evaluating multiple models and/or data sets, see Benchmarking.","category":"page"},{"location":"evaluating_model_performance/#Evaluating-against-a-single-measure-1","page":"Evaluating Model Performance","title":"Evaluating against a single measure","text":"","category":"section"},{"location":"evaluating_model_performance/#","page":"Evaluating Model Performance","title":"Evaluating Model Performance","text":"using MLJ\nMLJ.color_off()","category":"page"},{"location":"evaluating_model_performance/#","page":"Evaluating Model Performance","title":"Evaluating Model Performance","text":"using MLJ\nX = (a=rand(12), b=rand(12), c=rand(12));\ny = X.a + 2X.b + 0.05*rand(12);\nmodel = @load RidgeRegressor pkg=MultivariateStats\ncv=CV(nfolds=3)\nevaluate(model, X, y, resampling=cv, measure=l2, verbosity=0)","category":"page"},{"location":"evaluating_model_performance/#","page":"Evaluating Model Performance","title":"Evaluating Model Performance","text":"Alternatively, instead of applying evaluate to a model + data, one may call evaluate! on an existing machine wrapping the model in data:","category":"page"},{"location":"evaluating_model_performance/#","page":"Evaluating Model Performance","title":"Evaluating Model Performance","text":"mach = machine(model, X, y)\nevaluate!(mach, resampling=cv, measure=l2, verbosity=0)","category":"page"},{"location":"evaluating_model_performance/#","page":"Evaluating Model Performance","title":"Evaluating Model Performance","text":"(The latter call is a mutating call as the learned parameters stored in the machine potentially change. )","category":"page"},{"location":"evaluating_model_performance/#Multiple-measures-1","page":"Evaluating Model Performance","title":"Multiple measures","text":"","category":"section"},{"location":"evaluating_model_performance/#","page":"Evaluating Model Performance","title":"Evaluating Model Performance","text":"evaluate!(mach,\n          resampling=cv,\n          measure=[l1, rms, rmslp1], verbosity=0)","category":"page"},{"location":"evaluating_model_performance/#Custom-measures-and-weighted-measures-1","page":"Evaluating Model Performance","title":"Custom measures and weighted measures","text":"","category":"section"},{"location":"evaluating_model_performance/#","page":"Evaluating Model Performance","title":"Evaluating Model Performance","text":"my_loss(yhat, y) = maximum((yhat - y).^2);\n\nmy_per_observation_loss(yhat, y) = abs.(yhat - y);\nMLJ.reports_each_observation(::typeof(my_per_observation_loss)) = true;\n\nmy_weighted_score(yhat, y) = 1/mean(abs.(yhat - y));\nmy_weighted_score(yhat, y, w) = 1/mean(abs.((yhat - y).^w));\nMLJ.supports_weights(::typeof(my_weighted_score)) = true;\nMLJ.orientation(::typeof(my_weighted_score)) = :score;\n\nholdout = Holdout(fraction_train=0.8)\nweights = [1, 1, 2, 1, 1, 2, 3, 1, 1, 2, 3, 1];\nevaluate!(mach,\n          resampling=CV(nfolds=3),\n          measure=[my_loss, my_per_observation_loss, my_weighted_score, l1],\n          weights=weights, verbosity=0)","category":"page"},{"location":"evaluating_model_performance/#User-specified-train/test-sets-1","page":"Evaluating Model Performance","title":"User-specified train/test sets","text":"","category":"section"},{"location":"evaluating_model_performance/#","page":"Evaluating Model Performance","title":"Evaluating Model Performance","text":"Users can either provide their own list of train/test pairs of row indices for resampling, as in this example:","category":"page"},{"location":"evaluating_model_performance/#","page":"Evaluating Model Performance","title":"Evaluating Model Performance","text":"fold1 = 1:6; fold2 = 7:12;\nevaluate!(mach,\n          resampling = [(fold1, fold2), (fold2, fold1)],\n          measure=[l1, l2], verbosity=0)","category":"page"},{"location":"evaluating_model_performance/#","page":"Evaluating Model Performance","title":"Evaluating Model Performance","text":"Or define their own re-usable ResamplingStrategy objects, - see Custom resampling strategies below.","category":"page"},{"location":"evaluating_model_performance/#Built-in-resampling-strategies-1","page":"Evaluating Model Performance","title":"Built-in resampling strategies","text":"","category":"section"},{"location":"evaluating_model_performance/#","page":"Evaluating Model Performance","title":"Evaluating Model Performance","text":"MLJBase.Holdout","category":"page"},{"location":"evaluating_model_performance/#MLJBase.Holdout","page":"Evaluating Model Performance","title":"MLJBase.Holdout","text":"holdout = Holdout(; fraction_train=0.7,\n                     shuffle=nothing,\n                     rng=nothing)\n\nHoldout resampling strategy, for use in evaluate!, evaluate and in tuning.\n\ntrain_test_pairs(holdout, rows)\n\nReturns the pair [(train, test)], where train and test are vectors such that rows=vcat(train, test) and length(train)/length(rows) is approximatey equal to fraction_train`.\n\nPre-shuffling of rows is controlled by rng and shuffle. If rng is an integer, then the Holdout keyword constructor resets it to MersenneTwister(rng). Otherwise some AbstractRNG object is expected.\n\nIf rng is left unspecified, rng is reset to Random.GLOBAL_RNG, in which case rows are only pre-shuffled if shuffle=true is specified.\n\n\n\n\n\n","category":"type"},{"location":"evaluating_model_performance/#","page":"Evaluating Model Performance","title":"Evaluating Model Performance","text":"MLJBase.CV","category":"page"},{"location":"evaluating_model_performance/#MLJBase.CV","page":"Evaluating Model Performance","title":"MLJBase.CV","text":"cv = CV(; nfolds=6,  shuffle=nothing, rng=nothing)\n\nCross-validation resampling strategy, for use in evaluate!, evaluate and tuning.\n\ntrain_test_pairs(cv, rows)\n\nReturns an nfolds-length iterator of (train, test) pairs of vectors (row indices), where each train and test is a sub-vector of rows. The test vectors are mutually exclusive and exhaust rows. Each train vector is the complement of the corresponding test vector. With no row pre-shuffling, the order of rows is preserved, in the sense that rows coincides precisely with the concatenation of the test vectors, in the order they are generated. The first r test vectors have length n + 1, where n, r = divrem(length(rows), nfolds), and the remaining test vectors have length n.\n\nPre-shuffling of rows is controlled by rng and shuffle. If rng is an integer, then the CV keyword constructor resets it to MersenneTwister(rng). Otherwise some AbstractRNG object is expected.\n\nIf rng is left unspecified, rng is reset to Random.GLOBAL_RNG, in which case rows are only pre-shuffled if shuffle=true is explicitly specified.\n\n\n\n\n\n","category":"type"},{"location":"evaluating_model_performance/#","page":"Evaluating Model Performance","title":"Evaluating Model Performance","text":"MLJBase.StratifiedCV","category":"page"},{"location":"evaluating_model_performance/#MLJBase.StratifiedCV","page":"Evaluating Model Performance","title":"MLJBase.StratifiedCV","text":"stratified_cv = StratifiedCV(; nfolds=6,\n                               shuffle=false,\n                               rng=Random.GLOBAL_RNG)\n\nStratified cross-validation resampling strategy, for use in evaluate!, evaluate and in tuning. Applies only to classification problems (OrderedFactor or Multiclass targets).\n\ntrain_test_pairs(stratified_cv, rows, y)\n\nReturns an nfolds-length iterator of (train, test) pairs of vectors (row indices) where each train and test is a sub-vector of rows. The test vectors are mutually exclusive and exhaust rows. Each train vector is the complement of the corresponding test vector.\n\nUnlike regular cross-validation, the distribution of the levels of the target y corresponding to each train and test is constrained, as far as possible, to replicate that of y[rows] as a whole.\n\nThe stratified train_test_pairs algorithm is invariant to label renaming. For example, if you run replace!(y, 'a' => 'b', 'b' => 'a') and then re-run train_test_pairs, the returned (train, test) pairs will be the same.\n\nPre-shuffling of rows is controlled by rng and shuffle. If rng is an integer, then the StratifedCV keyword constructor resets it to MersenneTwister(rng). Otherwise some AbstractRNG object is expected.\n\nIf rng is left unspecified, rng is reset to Random.GLOBAL_RNG, in which case rows are only pre-shuffled if shuffle=true is explicitly specified.\n\n\n\n\n\n","category":"type"},{"location":"evaluating_model_performance/#Custom-resampling-strategies-1","page":"Evaluating Model Performance","title":"Custom resampling strategies","text":"","category":"section"},{"location":"evaluating_model_performance/#","page":"Evaluating Model Performance","title":"Evaluating Model Performance","text":"To define your own resampling strategy, make relevant parameters of your strategy the fields of a new type MyResamplingStrategy <: MLJ.ResamplingStrategy, and implement one of the following methods:","category":"page"},{"location":"evaluating_model_performance/#","page":"Evaluating Model Performance","title":"Evaluating Model Performance","text":"MLJ.train_test_pairs(my_strategy::MyResamplingStrategy, rows)\nMLJ.train_test_pairs(my_strategy::MyResamplingStrategy, rows, y)\nMLJ.train_test_pairs(my_strategy::MyResamplingStrategy, rows, X, y)","category":"page"},{"location":"evaluating_model_performance/#","page":"Evaluating Model Performance","title":"Evaluating Model Performance","text":"Each method takes a vector of indices rows and return a vector [(t1, e1), (t2, e2), ... (tk, ek)] of train/test pairs of row indices selected from rows. Here X, y are the input and target data (ignored in simple strategies, such as Holdout and CV).","category":"page"},{"location":"evaluating_model_performance/#","page":"Evaluating Model Performance","title":"Evaluating Model Performance","text":"Here is the code for the Holdout strategy as an example:","category":"page"},{"location":"evaluating_model_performance/#","page":"Evaluating Model Performance","title":"Evaluating Model Performance","text":"struct Holdout <: ResamplingStrategy\n    fraction_train::Float64\n    shuffle::Bool\n    rng::Union{Int,AbstractRNG}\n\n    function Holdout(fraction_train, shuffle, rng)\n        0 < fraction_train < 1 ||\n            error(\"`fraction_train` must be between 0 and 1.\")\n        return new(fraction_train, shuffle, rng)\n    end\nend\n\n# Keyword Constructor\nfunction Holdout(; fraction_train::Float64=0.7, shuffle=nothing, rng=nothing)\n    if rng isa Integer\n        rng = MersenneTwister(rng)\n    end\n    if shuffle === nothing\n        shuffle = ifelse(rng===nothing, false, true)\n    end\n    if rng === nothing\n        rng = Random.GLOBAL_RNG\n    end\n    return Holdout(fraction_train, shuffle, rng)\nend\n\nfunction train_test_pairs(holdout::Holdout, rows)\n    train, test = partition(rows, holdout.fraction_train,\n                          shuffle=holdout.shuffle, rng=holdout.rng)\n    return [(train, test),]\nend","category":"page"},{"location":"evaluating_model_performance/#API-1","page":"Evaluating Model Performance","title":"API","text":"","category":"section"},{"location":"evaluating_model_performance/#","page":"Evaluating Model Performance","title":"Evaluating Model Performance","text":"MLJBase.evaluate!","category":"page"},{"location":"evaluating_model_performance/#MLJBase.evaluate!","page":"Evaluating Model Performance","title":"MLJBase.evaluate!","text":"evaluate!(mach,\n          resampling=CV(),\n          measure=nothing,\n          rows=nothing,\n          weights=nothing,\n          operation=predict,\n          repeats=1,\n          acceleration=default_resource(),\n          force=false,\n          verbosity=1,\n          check_measure=true)\n\nEstimate the performance of a machine mach wrapping a supervised model in data, using the specified resampling strategy (defaulting to 6-fold cross-validation) and measure, which can be a single measure or vector.\n\nDo subtypes(MLJ.ResamplingStrategy) to obtain a list of available resampling strategies. If resampling is not an object of type MLJ.ResamplingStrategy, then a vector of pairs (of the form (train_rows, test_rows) is expected. For example, setting\n\nresampling = [(1:100), (101:200)),\n               (101:200), (1:100)]\n\ngives two-fold cross-validation using the first 200 rows of data.\n\nThe resampling strategy is applied repeatedly (Monte Carlo resampling) if repeats > 1. For example, if repeats = 10, then resampling = CV(nfolds=5, shuffle=true), generates a total of 50 (train, test) pairs for evaluation and subsequent aggregation.\n\nIf resampling isa MLJ.ResamplingStrategy then one may optionally restrict the data used in evaluation by specifying rows.\n\nAn optional weights vector may be passed for measures that support sample weights (MLJ.supports_weights(measure) == true), which is ignored by those that don't. These weights are not to be confused with any any weights w bound to mach (as in mach = machine(model, X, y, w)). To pass these to the performance evaluation measures you must explictly specify weights=w in the evaluate! call.\n\nUser-defined measures are supported; see the manual for details.\n\nIf no measure is specified, then default_measure(mach.model) is used, unless this default is nothing and an error is thrown.\n\nThe acceleration keyword argument is used to specify the compute resource (a subtype of ComputationalResources.AbstractResource) that will be used to accelerate/parallelize the resampling operation.\n\nAlthough evaluate! is mutating, mach.model and mach.args are untouched.\n\nSummary of key-word arguments\n\nresampling - resampling strategy (default is CV(nfolds=6))\nmeasure/measures - measure or vector of measures (losses, scores, etc)\nrows - vector of observation indices from which both train and test folds are constructed (default is all observations)\nweights - per-sample weights for measures (not to be confused with weights used in training)\noperation - predict, predict_mean, predict_mode or predict_median; predict is the default but cannot be used with a deterministic measure if model isa Probabilistic\nrepeats - default is 1; set to a higher value for repeated (Monte Carlo) resampling\nacceleration - parallelization option; currently supported  options are instances of CPU1 (single-threaded computation)  CPUThreads (multi-threaded computation) and CPUProcesses  (multi-process computation); default is default_resource().\nforce - default is false; set to true for force cold-restart of each training event\nverbosity level, an integer defaulting to 1.\ncheck_measure - default is true\n\nReturn value\n\nA property-accessible object of type PerformanceEvaluation with these properties:\n\nmeasure: the vector of specified measures\nmeasurements: the corresponding measurements, aggregated across the test folds using the aggregation method defined for each measure (do aggregation(measure) to inspect)\nper_fold: a vector of vectors of individual test fold evaluations (one vector per measure)\nper_observation: a vector of vectors of individual observation evaluations of those measures for which reports_each_observation(measure) is true, which is otherwise reported missing\n\n-fitted_params_per_fold: a vector containing fitted pamarms(mach) for each   machine mach trained during resampling.\n\nreport_per_fold: a vector containing report(mach) for each  machine mach training in resampling\n\n\n\n\n\n","category":"function"},{"location":"evaluating_model_performance/#","page":"Evaluating Model Performance","title":"Evaluating Model Performance","text":"MLJBase.evaluate","category":"page"},{"location":"evaluating_model_performance/#MLJModelInterface.evaluate","page":"Evaluating Model Performance","title":"MLJModelInterface.evaluate","text":"some meta-models may choose to implement the evaluate operations\n\n\n\n\n\n","category":"function"}]
}
