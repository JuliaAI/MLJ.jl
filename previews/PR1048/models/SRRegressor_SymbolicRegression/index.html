<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>SRRegressor · MLJ</title><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.svg" alt="MLJ logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">MLJ</a></span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><a class="tocitem" href="../../model_browser/">Model Browser</a></li><li><a class="tocitem" href="../../about_mlj/">About MLJ</a></li><li><a class="tocitem" href="../../learning_mlj/">Learning MLJ</a></li><li><a class="tocitem" href="../../getting_started/">Getting Started</a></li><li><a class="tocitem" href="../../common_mlj_workflows/">Common MLJ Workflows</a></li><li><a class="tocitem" href="../../working_with_categorical_data/">Working with Categorical Data</a></li><li><a class="tocitem" href="../../model_search/">Model Search</a></li><li><a class="tocitem" href="../../loading_model_code/">Loading Model Code</a></li><li><a class="tocitem" href="../../machines/">Machines</a></li><li><a class="tocitem" href="../../evaluating_model_performance/">Evaluating Model Performance</a></li><li><a class="tocitem" href="../../performance_measures/">Performance Measures</a></li><li><a class="tocitem" href="../../weights/">Weights</a></li><li><a class="tocitem" href="../../tuning_models/">Tuning Models</a></li><li><a class="tocitem" href="../../learning_curves/">Learning Curves</a></li><li><a class="tocitem" href="../../preparing_data/">Preparing Data</a></li><li><a class="tocitem" href="../../transformers/">Transformers and Other Unsupervised models</a></li><li><a class="tocitem" href="../../more_on_probabilistic_predictors/">More on Probabilistic Predictors</a></li><li><a class="tocitem" href="../../composing_models/">Composing Models</a></li><li><a class="tocitem" href="../../linear_pipelines/">Linear Pipelines</a></li><li><a class="tocitem" href="../../target_transformations/">Target Transformations</a></li><li><a class="tocitem" href="../../homogeneous_ensembles/">Homogeneous Ensembles</a></li><li><a class="tocitem" href="../../model_stacking/">Model Stacking</a></li><li><a class="tocitem" href="../../learning_networks/">Learning Networks</a></li><li><a class="tocitem" href="../../controlling_iterative_models/">Controlling Iterative Models</a></li><li><a class="tocitem" href="../../generating_synthetic_data/">Generating Synthetic Data</a></li><li><a class="tocitem" href="../../logging_workflows/">Logging Workflows</a></li><li><a class="tocitem" href="../../openml_integration/">OpenML Integration</a></li><li><a class="tocitem" href="../../acceleration_and_parallelism/">Acceleration and Parallelism</a></li><li><a class="tocitem" href="../../simple_user_defined_models/">Simple User Defined Models</a></li><li><a class="tocitem" href="../../quick_start_guide_to_adding_models/">Quick-Start Guide to Adding Models</a></li><li><a class="tocitem" href="../../adding_models_for_general_use/">Adding Models for General Use</a></li><li><a class="tocitem" href="../../modifying_behavior/">Modifying Behavior</a></li><li><a class="tocitem" href="../../internals/">Internals</a></li><li><a class="tocitem" href="../../list_of_supported_models/">List of Supported Models</a></li><li><a class="tocitem" href="../../third_party_packages/">Third Party Packages</a></li><li><a class="tocitem" href="../../glossary/">Glossary</a></li><li><a class="tocitem" href="../../mlj_cheatsheet/">MLJ Cheatsheet</a></li><li><a class="tocitem" href="../../known_issues/">Known Issues</a></li><li><a class="tocitem" href="../../frequently_asked_questions/">FAQ</a></li><li><a class="tocitem" href="../../julia_blogpost/">Julia BlogPost</a></li><li><a class="tocitem" href="../../api/">Index of Methods</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>SRRegressor</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>SRRegressor</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/alan-turing-institute/MLJ.jl/blob/dev/docs/src/models/SRRegressor_SymbolicRegression.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="SRRegressor_SymbolicRegression"><a class="docs-heading-anchor" href="#SRRegressor_SymbolicRegression">SRRegressor</a><a id="SRRegressor_SymbolicRegression-1"></a><a class="docs-heading-anchor-permalink" href="#SRRegressor_SymbolicRegression" title="Permalink"></a></h1><pre><code class="nohighlight hljs">SRRegressor</code></pre><p>A model type for constructing a Symbolic Regression via Evolutionary Search, based on <a href="https://github.com/MilesCranmer/SymbolicRegression.jl">SymbolicRegression.jl</a>, and implementing the MLJ model interface.</p><p>From MLJ, the type can be imported using</p><pre><code class="nohighlight hljs">SRRegressor = @load SRRegressor pkg=SymbolicRegression</code></pre><p>Do <code>model = SRRegressor()</code> to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in <code>SRRegressor(binary_operators=...)</code>.</p><p>Single-target Symbolic Regression regressor (<code>SRRegressor</code>) searches for symbolic expressions that predict a single target variable from a set of input variables. All data is assumed to be <code>Continuous</code>. The search is performed using an evolutionary algorithm. This algorithm is described in the paper https://arxiv.org/abs/2305.01582.</p><h2 id="Training-data"><a class="docs-heading-anchor" href="#Training-data">Training data</a><a id="Training-data-1"></a><a class="docs-heading-anchor-permalink" href="#Training-data" title="Permalink"></a></h2><p>In MLJ or MLJBase, bind an instance <code>model</code> to data with</p><pre><code class="nohighlight hljs">mach = machine(model, X, y)</code></pre><p>OR</p><pre><code class="nohighlight hljs">mach = machine(model, X, y, w)</code></pre><p>Here:</p><ul><li><code>X</code> is any table of input features (eg, a <code>DataFrame</code>) whose columns are of scitype <code>Continuous</code>; check column scitypes with <code>schema(X)</code>. Variable names in discovered expressions will be taken from the column names of <code>X</code>, if available. Units in columns of <code>X</code> (use <code>DynamicQuantities</code> for units) will trigger dimensional analysis to be used.</li><li><code>y</code> is the target, which can be any <code>AbstractVector</code> whose element scitype is   <code>Continuous</code>; check the scitype with <code>scitype(y)</code>. Units in <code>y</code> (use <code>DynamicQuantities</code>   for units) will trigger dimensional analysis to be used.</li><li><code>w</code> is the observation weights which can either be <code>nothing</code> (default) or an  <code>AbstractVector</code> whoose element scitype is <code>Count</code> or <code>Continuous</code>.</li></ul><p>Train the machine using <code>fit!(mach)</code>, inspect the discovered expressions with <code>report(mach)</code>, and predict on new data with <code>predict(mach, Xnew)</code>. Note that unlike other regressors, symbolic regression stores a list of trained models. The model chosen from this list is defined by the function <code>selection_method</code> keyword argument, which by default balances accuracy and complexity.</p><h2 id="Hyper-parameters"><a class="docs-heading-anchor" href="#Hyper-parameters">Hyper-parameters</a><a id="Hyper-parameters-1"></a><a class="docs-heading-anchor-permalink" href="#Hyper-parameters" title="Permalink"></a></h2><ul><li><p><code>binary_operators</code>: Vector of binary operators (functions) to use.   Each operator should be defined for two input scalars,   and one output scalar. All operators   need to be defined over the entire real line (excluding infinity - these   are stopped before they are input), or return <code>NaN</code> where not defined.   For speed, define it so it takes two reals   of the same type as input, and outputs the same type. For the SymbolicUtils   simplification backend, you will need to define a generic method of the   operator so it takes arbitrary types.</p></li><li><p><code>unary_operators</code>: Same, but for   unary operators (one input scalar, gives an output scalar).</p></li><li><p><code>constraints</code>: Array of pairs specifying size constraints   for each operator. The constraints for a binary operator should be a 2-tuple   (e.g., <code>(-1, -1)</code>) and the constraints for a unary operator should be an <code>Int</code>.   A size constraint is a limit to the size of the subtree   in each argument of an operator. e.g., <code>[(^)=&gt;(-1, 3)]</code> means that the   <code>^</code> operator can have arbitrary size (<code>-1</code>) in its left argument,   but a maximum size of <code>3</code> in its right argument. Default is   no constraints.</p></li><li><p><code>batching</code>: Whether to evolve based on small mini-batches of data,   rather than the entire dataset.</p></li><li><p><code>batch_size</code>: What batch size to use if using batching.</p></li><li><p><code>elementwise_loss</code>: What elementwise loss function to use. Can be one of   the following losses, or any other loss of type   <code>SupervisedLoss</code>. You can also pass a function that takes   a scalar target (left argument), and scalar predicted (right   argument), and returns a scalar. This will be averaged   over the predicted data. If weights are supplied, your   function should take a third argument for the weight scalar.   Included losses:       Regression:           - <code>LPDistLoss{P}()</code>,           - <code>L1DistLoss()</code>,           - <code>L2DistLoss()</code> (mean square),           - <code>LogitDistLoss()</code>,           - <code>HuberLoss(d)</code>,           - <code>L1EpsilonInsLoss(ϵ)</code>,           - <code>L2EpsilonInsLoss(ϵ)</code>,           - <code>PeriodicLoss(c)</code>,           - <code>QuantileLoss(τ)</code>,       Classification:           - <code>ZeroOneLoss()</code>,           - <code>PerceptronLoss()</code>,           - <code>L1HingeLoss()</code>,           - <code>SmoothedL1HingeLoss(γ)</code>,           - <code>ModifiedHuberLoss()</code>,           - <code>L2MarginLoss()</code>,           - <code>ExpLoss()</code>,           - <code>SigmoidLoss()</code>,           - <code>DWDMarginLoss(q)</code>.</p></li><li><p><code>loss_function</code>: Alternatively, you may redefine the loss used   as any function of <code>tree::Node{T}</code>, <code>dataset::Dataset{T}</code>,   and <code>options::Options</code>, so long as you output a non-negative   scalar of type <code>T</code>. This is useful if you want to use a loss   that takes into account derivatives, or correlations across   the dataset. This also means you could use a custom evaluation   for a particular expression. If you are using   <code>batching=true</code>, then your function should   accept a fourth argument <code>idx</code>, which is either <code>nothing</code>   (indicating that the full dataset should be used), or a vector   of indices to use for the batch.   For example,</p><pre><code class="nohighlight hljs">  function my_loss(tree, dataset::Dataset{T,L}, options)::L where {T,L}
      prediction, flag = eval_tree_array(tree, dataset.X, options)
      if !flag
          return L(Inf)
      end
      return sum((prediction .- dataset.y) .^ 2) / dataset.n
  end</code></pre></li><li><p><code>npopulations</code>: How many populations of equations to use. By default   this is set equal to the number of cores</p></li><li><p><code>npop</code>: How many equations in each population.</p></li><li><p><code>ncycles_per_iteration</code>: How many generations to consider per iteration.</p></li><li><p><code>tournament_selection_n</code>: Number of expressions considered in each tournament.</p></li><li><p><code>tournament_selection_p</code>: The fittest expression in a tournament is to be   selected with probability <code>p</code>, the next fittest with probability <code>p*(1-p)</code>,   and so forth.</p></li><li><p><code>topn</code>: Number of equations to return to the host process, and to   consider for the hall of fame.</p></li><li><p><code>complexity_of_operators</code>: What complexity should be assigned to each operator,   and the occurrence of a constant or variable. By default, this is 1   for all operators. Can be a real number as well, in which case   the complexity of an expression will be rounded to the nearest integer.   Input this in the form of, e.g., [(^) =&gt; 3, sin =&gt; 2].</p></li><li><p><code>complexity_of_constants</code>: What complexity should be assigned to use of a constant.   By default, this is 1.</p></li><li><p><code>complexity_of_variables</code>: What complexity should be assigned to each variable.   By default, this is 1.</p></li><li><p><code>alpha</code>: The probability of accepting an equation mutation   during regularized evolution is given by exp(-delta_loss/(alpha * T)),   where T goes from 1 to 0. Thus, alpha=infinite is the same as no annealing.</p></li><li><p><code>maxsize</code>: Maximum size of equations during the search.</p></li><li><p><code>maxdepth</code>: Maximum depth of equations during the search, by default   this is set equal to the maxsize.</p></li><li><p><code>parsimony</code>: A multiplicative factor for how much complexity is   punished.</p></li><li><p><code>dimensional_constraint_penalty</code>: An additive factor if the dimensional   constraint is violated.</p></li><li><p><code>use_frequency</code>: Whether to use a parsimony that adapts to the   relative proportion of equations at each complexity; this will   ensure that there are a balanced number of equations considered   for every complexity.</p></li><li><p><code>use_frequency_in_tournament</code>: Whether to use the adaptive parsimony described   above inside the score, rather than just at the mutation accept/reject stage.</p></li><li><p><code>adaptive_parsimony_scaling</code>: How much to scale the adaptive parsimony term   in the loss. Increase this if the search is spending too much time   optimizing the most complex equations.</p></li><li><p><code>fast_cycle</code>: Whether to thread over subsamples of equations during   regularized evolution. Slightly improves performance, but is a different   algorithm.</p></li><li><p><code>turbo</code>: Whether to use <code>LoopVectorization.@turbo</code> to evaluate expressions.   This can be significantly faster, but is only compatible with certain   operators. <em>Experimental!</em></p></li><li><p><code>migration</code>: Whether to migrate equations between processes.</p></li><li><p><code>hof_migration</code>: Whether to migrate equations from the hall of fame   to processes.</p></li><li><p><code>fraction_replaced</code>: What fraction of each population to replace with   migrated equations at the end of each cycle.</p></li><li><p><code>fraction_replaced_hof</code>: What fraction to replace with hall of fame   equations at the end of each cycle.</p></li><li><p><code>should_simplify</code>: Whether to simplify equations. If you   pass a custom objective, this will be set to <code>false</code>.</p></li><li><p><code>should_optimize_constants</code>: Whether to use an optimization algorithm   to periodically optimize constants in equations.</p></li><li><p><code>optimizer_nrestarts</code>: How many different random starting positions to consider   for optimization of constants.</p></li><li><p><code>optimizer_algorithm</code>: Select algorithm to use for optimizing constants. Default   is &quot;BFGS&quot;, but &quot;NelderMead&quot; is also supported.</p></li><li><p><code>optimizer_options</code>: General options for the constant optimization. For details   we refer to the documentation on <code>Optim.Options</code> from the <code>Optim.jl</code> package.   Options can be provided here as <code>NamedTuple</code>, e.g. <code>(iterations=16,)</code>, as a   <code>Dict</code>, e.g. Dict(:x_tol =&gt; 1.0e-32,), or as an <code>Optim.Options</code> instance.</p></li><li><p><code>output_file</code>: What file to store equations to, as a backup.</p></li><li><p><code>perturbation_factor</code>: When mutating a constant, either   multiply or divide by (1+perturbation_factor)^(rand()+1).</p></li><li><p><code>probability_negate_constant</code>: Probability of negating a constant in the equation   when mutating it.</p></li><li><p><code>mutation_weights</code>: Relative probabilities of the mutations. The struct   <code>MutationWeights</code> should be passed to these options.   See its documentation on <code>MutationWeights</code> for the different weights.</p></li><li><p><code>crossover_probability</code>: Probability of performing crossover.</p></li><li><p><code>annealing</code>: Whether to use simulated annealing.</p></li><li><p><code>warmup_maxsize_by</code>: Whether to slowly increase the max size from 5 up to   <code>maxsize</code>. If nonzero, specifies the fraction through the search   at which the maxsize should be reached.</p></li><li><p><code>verbosity</code>: Whether to print debugging statements or   not.</p></li><li><p><code>print_precision</code>: How many digits to print when printing   equations. By default, this is 5.</p></li><li><p><code>save_to_file</code>: Whether to save equations to a file during the search.</p></li><li><p><code>bin_constraints</code>: See <code>constraints</code>. This is the same, but specified for binary   operators only (for example, if you have an operator that is both a binary   and unary operator).</p></li><li><p><code>una_constraints</code>: Likewise, for unary operators.</p></li><li><p><code>seed</code>: What random seed to use. <code>nothing</code> uses no seed.</p></li><li><p><code>progress</code>: Whether to use a progress bar output (<code>verbosity</code> will   have no effect).</p></li><li><p><code>early_stop_condition</code>: Float - whether to stop early if the mean loss gets below this value.   Function - a function taking (loss, complexity) as arguments and returning true or false.</p></li><li><p><code>timeout_in_seconds</code>: Float64 - the time in seconds after which to exit (as an alternative to the number of iterations).</p></li><li><p><code>max_evals</code>: Int (or Nothing) - the maximum number of evaluations of expressions to perform.</p></li><li><p><code>skip_mutation_failures</code>: Whether to simply skip over mutations that fail or are rejected, rather than to replace the mutated   expression with the original expression and proceed normally.</p></li><li><p><code>enable_autodiff</code>: Whether to enable automatic differentiation functionality. This is turned off by default.   If turned on, this will be turned off if one of the operators does not have well-defined gradients.</p></li><li><p><code>nested_constraints</code>: Specifies how many times a combination of operators can be nested. For example,   <code>[sin =&gt; [cos =&gt; 0], cos =&gt; [cos =&gt; 2]]</code> specifies that <code>cos</code> may never appear within a <code>sin</code>,   but <code>sin</code> can be nested with itself an unlimited number of times. The second term specifies that <code>cos</code>   can be nested up to 2 times within a <code>cos</code>, so that <code>cos(cos(cos(x)))</code> is allowed (as well as any combination   of <code>+</code> or <code>-</code> within it), but <code>cos(cos(cos(cos(x))))</code> is not allowed. When an operator is not specified,   it is assumed that it can be nested an unlimited number of times. This requires that there is no operator   which is used both in the unary operators and the binary operators (e.g., <code>-</code> could be both subtract, and negation).   For binary operators, both arguments are treated the same way, and the max of each argument is constrained.</p></li><li><p><code>deterministic</code>: Use a global counter for the birth time, rather than calls to <code>time()</code>. This gives   perfect resolution, and is therefore deterministic. However, it is not thread safe, and must be used   in serial mode.</p></li><li><p><code>define_helper_functions</code>: Whether to define helper functions   for constructing and evaluating trees.</p></li><li><p><code>niterations::Int=10</code>: The number of iterations to perform the search.   More iterations will improve the results.</p></li><li><p><code>parallelism=:multithreading</code>: What parallelism mode to use.   The options are <code>:multithreading</code>, <code>:multiprocessing</code>, and <code>:serial</code>.   By default, multithreading will be used. Multithreading uses less memory,   but multiprocessing can handle multi-node compute. If using <code>:multithreading</code>   mode, the number of threads available to julia are used. If using   <code>:multiprocessing</code>, <code>numprocs</code> processes will be created dynamically if   <code>procs</code> is unset. If you have already allocated processes, pass them   to the <code>procs</code> argument and they will be used.   You may also pass a string instead of a symbol, like <code>&quot;multithreading&quot;</code>.</p></li><li><p><code>numprocs::Union{Int, Nothing}=nothing</code>:  The number of processes to use,   if you want <code>equation_search</code> to set this up automatically. By default   this will be <code>4</code>, but can be any number (you should pick a number &lt;=   the number of cores available).</p></li><li><p><code>procs::Union{Vector{Int}, Nothing}=nothing</code>: If you have set up   a distributed run manually with <code>procs = addprocs()</code> and <code>@everywhere</code>,   pass the <code>procs</code> to this keyword argument.</p></li><li><p><code>addprocs_function::Union{Function, Nothing}=nothing</code>: If using multiprocessing   (<code>parallelism=:multithreading</code>), and are not passing <code>procs</code> manually,   then they will be allocated dynamically using <code>addprocs</code>. However,   you may also pass a custom function to use instead of <code>addprocs</code>.   This function should take a single positional argument,   which is the number of processes to use, as well as the <code>lazy</code> keyword argument.   For example, if set up on a slurm cluster, you could pass   <code>addprocs_function = addprocs_slurm</code>, which will set up slurm processes.</p></li><li><p><code>runtests::Bool=true</code>: Whether to run (quick) tests before starting the   search, to see if there will be any problems during the equation search   related to the host environment.</p></li><li><p><code>loss_type::Type=Nothing</code>: If you would like to use a different type   for the loss than for the data you passed, specify the type here.   Note that if you pass complex data <code>::Complex{L}</code>, then the loss   type will automatically be set to <code>L</code>.</p></li><li><p><code>selection_method::Function</code>: Function to selection expression from   the Pareto frontier for use in <code>predict</code>. See <code>SymbolicRegression.MLJInterfaceModule.choose_best</code>   for an example. This function should return a single integer specifying   the index of the expression to use. By default, <code>choose_best</code> maximizes   the score (a pound-for-pound rating) of expressions reaching the threshold   of 1.5x the minimum loss. To fix the index at <code>5</code>, you could just write <code>Returns(5)</code>.</p></li><li><p><code>dimensions_type::AbstractDimensions</code>: The type of dimensions to use when storing   the units of the data. By default this is <code>DynamicQuantities.DEFAULT_DIM_TYPE</code>.</p></li></ul><h2 id="Operations"><a class="docs-heading-anchor" href="#Operations">Operations</a><a id="Operations-1"></a><a class="docs-heading-anchor-permalink" href="#Operations" title="Permalink"></a></h2><ul><li><code>predict(mach, Xnew)</code>: Return predictions of the target given features <code>Xnew</code>, which should have same scitype as <code>X</code> above. The expression used for prediction is defined by the <code>selection_method</code> function, which can be seen by viewing <code>report(mach).best_idx</code>.</li></ul><h2 id="Fitted-parameters"><a class="docs-heading-anchor" href="#Fitted-parameters">Fitted parameters</a><a id="Fitted-parameters-1"></a><a class="docs-heading-anchor-permalink" href="#Fitted-parameters" title="Permalink"></a></h2><p>The fields of <code>fitted_params(mach)</code> are:</p><ul><li><code>best_idx::Int</code>: The index of the best expression in the Pareto frontier,  as determined by the <code>selection_method</code> function.</li><li><code>equations::Vector{Node{T}}</code>: The expressions discovered by the search, represented in a dominating Pareto frontier (i.e., the best expressions found for each complexity). <code>T</code> is equal to the element type of the passed data.</li><li><code>equation_strings::Vector{String}</code>: The expressions discovered by the search, represented as strings for easy inspection.</li></ul><h2 id="Report"><a class="docs-heading-anchor" href="#Report">Report</a><a id="Report-1"></a><a class="docs-heading-anchor-permalink" href="#Report" title="Permalink"></a></h2><p>The fields of <code>report(mach)</code> are:</p><ul><li><code>best_idx::Int</code>: The index of the best expression in the Pareto frontier,  as determined by the <code>selection_method</code> function.</li><li><code>equations::Vector{Node{T}}</code>: The expressions discovered by the search, represented in a dominating Pareto frontier (i.e., the best expressions found for each complexity).</li><li><code>equation_strings::Vector{String}</code>: The expressions discovered by the search, represented as strings for easy inspection.</li><li><code>complexities::Vector{Int}</code>: The complexity of each expression in the Pareto frontier.</li><li><code>losses::Vector{L}</code>: The loss of each expression in the Pareto frontier, according to the loss function specified in the model. The type <code>L</code> is the loss type, which is usually the same as the element type of data passed (i.e., <code>T</code>), but can differ if complex data types are passed.</li><li><code>scores::Vector{L}</code>: A metric which considers both the complexity and loss of an expression, equal to the change in the log-loss divided by the change in complexity, relative to the previous expression along the Pareto frontier. A larger score aims to indicate an expression is more likely to be the true expression generating the data, but this is very problem-dependent and generally several other factors should be considered.</li></ul><h2 id="Examples"><a class="docs-heading-anchor" href="#Examples">Examples</a><a id="Examples-1"></a><a class="docs-heading-anchor-permalink" href="#Examples" title="Permalink"></a></h2><pre><code class="language-julia hljs">using MLJ
SRRegressor = @load SRRegressor pkg=SymbolicRegression
X, y = @load_boston
model = SRRegressor(binary_operators=[+, -, *], unary_operators=[exp], niterations=100)
mach = machine(model, X, y)
fit!(mach)
y_hat = predict(mach, X)
## View the equation used:
r = report(mach)
println(&quot;Equation used:&quot;, r.equation_strings[r.best_idx])</code></pre><p>With units and variable names:</p><pre><code class="language-julia hljs">using MLJ
using DynamicQuantities
SRegressor = @load SRRegressor pkg=SymbolicRegression

X = (; x1=rand(32) .* us&quot;km/h&quot;, x2=rand(32) .* us&quot;km&quot;)
y = @. X.x2 / X.x1 + 0.5us&quot;h&quot;
model = SRRegressor(binary_operators=[+, -, *, /])
mach = machine(model, X, y)
fit!(mach)
y_hat = predict(mach, X)
## View the equation used:
r = report(mach)
println(&quot;Equation used:&quot;, r.equation_strings[r.best_idx])</code></pre><p>See also <a href="models/@ref"><code>MultitargetSRRegressor</code></a>.</p></article><nav class="docs-footer"><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.25 on <span class="colophon-date" title="Monday 4 September 2023 02:57">Monday 4 September 2023</span>. Using Julia version 1.9.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
