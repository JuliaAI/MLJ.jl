<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>About MLJ · MLJ</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.svg" alt="MLJ logo"/></a><div class="docs-package-name"><span class="docs-autofit">MLJ</span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li class="is-active"><a class="tocitem" href>About MLJ</a><ul class="internal"><li><a class="tocitem" href="#Lightning-tour"><span>Lightning tour</span></a></li><li><a class="tocitem" href="#Key-goals"><span>Key goals</span></a></li><li><a class="tocitem" href="#Key-features"><span>Key features</span></a></li><li><a class="tocitem" href="#Model-composability"><span>Model composability</span></a></li><li><a class="tocitem" href="#Getting-help-and-reporting-problems"><span>Getting help and reporting problems</span></a></li><li><a class="tocitem" href="#Installation"><span>Installation</span></a></li><li><a class="tocitem" href="#Funding"><span>Funding</span></a></li><li><a class="tocitem" href="#Citing-MLJ"><span>Citing MLJ</span></a></li></ul></li><li><a class="tocitem" href="../learning_mlj/">Learning MLJ</a></li><li><a class="tocitem" href="../getting_started/">Getting Started</a></li><li><a class="tocitem" href="../common_mlj_workflows/">Common MLJ Workflows</a></li><li><a class="tocitem" href="../working_with_categorical_data/">Working with Categorical Data</a></li><li><a class="tocitem" href="../model_search/">Model Search</a></li><li><a class="tocitem" href="../loading_model_code/">Loading Model Code</a></li><li><a class="tocitem" href="../machines/">Machines</a></li><li><a class="tocitem" href="../evaluating_model_performance/">Evaluating Model Performance</a></li><li><a class="tocitem" href="../performance_measures/">Performance Measures</a></li><li><a class="tocitem" href="../weights/">Weights</a></li><li><a class="tocitem" href="../tuning_models/">Tuning Models</a></li><li><a class="tocitem" href="../learning_curves/">Learning Curves</a></li><li><a class="tocitem" href="../preparing_data/">Preparing Data</a></li><li><a class="tocitem" href="../transformers/">Transformers and Other Unsupervised models</a></li><li><a class="tocitem" href="../more_on_probabilistic_predictors/">More on Probablistic Predictors</a></li><li><a class="tocitem" href="../composing_models/">Composing Models</a></li><li><a class="tocitem" href="../linear_pipelines/">Linear Pipelines</a></li><li><a class="tocitem" href="../target_transformations/">Target Transformations</a></li><li><a class="tocitem" href="../homogeneous_ensembles/">Homogeneous Ensembles</a></li><li><a class="tocitem" href="../model_stacking/">Model Stacking</a></li><li><a class="tocitem" href="../controlling_iterative_models/">Controlling Iterative Models</a></li><li><a class="tocitem" href="../generating_synthetic_data/">Generating Synthetic Data</a></li><li><a class="tocitem" href="../openml_integration/">OpenML Integration</a></li><li><a class="tocitem" href="../acceleration_and_parallelism/">Acceleration and Parallelism</a></li><li><a class="tocitem" href="../simple_user_defined_models/">Simple User Defined Models</a></li><li><a class="tocitem" href="../quick_start_guide_to_adding_models/">Quick-Start Guide to Adding Models</a></li><li><a class="tocitem" href="../adding_models_for_general_use/">Adding Models for General Use</a></li><li><a class="tocitem" href="../modifying_behavior/">Modifying Behavior</a></li><li><a class="tocitem" href="../internals/">Internals</a></li><li><a class="tocitem" href="../list_of_supported_models/">List of Supported Models</a></li><li><a class="tocitem" href="../third_party_packages/">Third Party Packages</a></li><li><a class="tocitem" href="../glossary/">Glossary</a></li><li><a class="tocitem" href="../mlj_cheatsheet/">MLJ Cheatsheet</a></li><li><a class="tocitem" href="../known_issues/">Known Issues</a></li><li><a class="tocitem" href="../frequently_asked_questions/">FAQ</a></li><li><a class="tocitem" href="../julia_blogpost/">Julia BlogPost</a></li><li><a class="tocitem" href="../api/">Index of Methods</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>About MLJ</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>About MLJ</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/alan-turing-institute/MLJ.jl/blob/master/docs/src/about_mlj.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="About-MLJ"><a class="docs-heading-anchor" href="#About-MLJ">About MLJ</a><a id="About-MLJ-1"></a><a class="docs-heading-anchor-permalink" href="#About-MLJ" title="Permalink"></a></h1><p>MLJ (Machine Learning in Julia) is a toolbox written in Julia providing a common interface and meta-algorithms for selecting, tuning, evaluating, composing and comparing <a href="../list_of_supported_models/#model_list">over 180 machine learning models</a> written in Julia and other languages. In particular MLJ wraps a large number of <a href="https://scikit-learn.org/stable/">scikit-learn</a> models.</p><p>MLJ is released under the MIT licensed.</p><h2 id="Lightning-tour"><a class="docs-heading-anchor" href="#Lightning-tour">Lightning tour</a><a id="Lightning-tour-1"></a><a class="docs-heading-anchor-permalink" href="#Lightning-tour" title="Permalink"></a></h2><p><em>For help learning to use MLJ, see <a href="../learning_mlj/#Learning-MLJ">Learning MLJ</a></em>.</p><p>A self-contained notebook and julia script of this demonstration is also available <a href="https://github.com/alan-turing-institute/MLJ.jl/tree/dev/examples/lightning_tour">here</a>.</p><p>The first code snippet below creates a new Julia environment <code>MLJ_tour</code> and installs just those packages needed for the tour. See <a href="#Installation">Installation</a> for more on creating a Julia environment for use with MLJ.</p><p>Julia installation instructions are <a href="https://julialang.org/downloads/">here</a>.</p><pre><code class="language-julia">using Pkg
Pkg.activate(&quot;MLJ_tour&quot;, shared=true)
Pkg.add(&quot;MLJ&quot;)
Pkg.add(&quot;MLJIteration&quot;)
Pkg.add(&quot;EvoTrees&quot;)</code></pre><p>In MLJ a <em>model</em> is just a container for hyper-parameters, and that&#39;s all. Here we will apply several kinds of model composition before binding the resulting &quot;meta-model&quot; to data in a <em>machine</em> for evaluation using cross-validation.</p><p>Loading and instantiating a gradient tree-boosting model:</p><pre><code class="language-julia">using MLJ
Booster = @load EvoTreeRegressor # loads code defining a model type
booster = Booster(max_depth=2)   # specify hyper-parameter at construction
booster.nrounds=50               # or mutate afterwards</code></pre><p>This model is an example of an iterative model. As is stands, the number of iterations <code>nrounds</code> is fixed.</p><h4 id="Composition-1:-Wrapping-the-model-to-make-it-&quot;self-iterating&quot;"><a class="docs-heading-anchor" href="#Composition-1:-Wrapping-the-model-to-make-it-&quot;self-iterating&quot;">Composition 1: Wrapping the model to make it &quot;self-iterating&quot;</a><a id="Composition-1:-Wrapping-the-model-to-make-it-&quot;self-iterating&quot;-1"></a><a class="docs-heading-anchor-permalink" href="#Composition-1:-Wrapping-the-model-to-make-it-&quot;self-iterating&quot;" title="Permalink"></a></h4><p>Let&#39;s create a new model that automatically learns the number of iterations, using the <code>NumberSinceBest(3)</code> criterion, as applied to an out-of-sample <code>l1</code> loss:</p><pre><code class="language-julia">using MLJIteration
iterated_booster = IteratedModel(model=booster,
                                 resampling=Holdout(fraction_train=0.8),
                                 controls=[Step(2), NumberSinceBest(3), NumberLimit(300)],
                                 measure=l1,
                                 retrain=true)</code></pre><h4 id="Composition-2:-Preprocess-the-input-features"><a class="docs-heading-anchor" href="#Composition-2:-Preprocess-the-input-features">Composition 2: Preprocess the input features</a><a id="Composition-2:-Preprocess-the-input-features-1"></a><a class="docs-heading-anchor-permalink" href="#Composition-2:-Preprocess-the-input-features" title="Permalink"></a></h4><p>Combining the model with categorical feature encoding:</p><pre><code class="language-julia">pipe = ContinuousEncoder() |&gt; iterated_booster</code></pre><h4 id="Composition-3:-Wrapping-the-model-to-make-it-&quot;self-tuning&quot;"><a class="docs-heading-anchor" href="#Composition-3:-Wrapping-the-model-to-make-it-&quot;self-tuning&quot;">Composition 3: Wrapping the model to make it &quot;self-tuning&quot;</a><a id="Composition-3:-Wrapping-the-model-to-make-it-&quot;self-tuning&quot;-1"></a><a class="docs-heading-anchor-permalink" href="#Composition-3:-Wrapping-the-model-to-make-it-&quot;self-tuning&quot;" title="Permalink"></a></h4><p>First, we define a hyper-parameter range for optimization of a (nested) hyper-parameter:</p><pre><code class="language-julia">max_depth_range = range(pipe,
                        :(deterministic_iterated_model.model.max_depth),
                        lower = 1,
                        upper = 10)</code></pre><p>Now we can wrap the pipeline model in an optimization strategy to make it &quot;self-tuning&quot;:</p><pre><code class="language-julia">self_tuning_pipe = TunedModel(model=pipe,
                              tuning=RandomSearch(),
                              ranges = max_depth_range,
                              resampling=CV(nfolds=3, rng=456),
                              measure=l1,
                              acceleration=CPUThreads(),
                              n=50)</code></pre><h4 id="Binding-to-data-and-evaluating-performance"><a class="docs-heading-anchor" href="#Binding-to-data-and-evaluating-performance">Binding to data and evaluating performance</a><a id="Binding-to-data-and-evaluating-performance-1"></a><a class="docs-heading-anchor-permalink" href="#Binding-to-data-and-evaluating-performance" title="Permalink"></a></h4><p>Loading a selection of features and labels from the Ames House Price dataset:</p><pre><code class="language-julia">X, y = @load_reduced_ames;</code></pre><p>Evaluating the &quot;self-tuning&quot; pipeline model&#39;s performance using 5-fold cross-validation (implies multiple layers of nested resampling):</p><pre><code class="language-julia">julia&gt; evaluate(self_tuning_pipe, X, y,
                measures=[l1, l2],
                resampling=CV(nfolds=5, rng=123),
                acceleration=CPUThreads(),
                verbosity=2)
PerformanceEvaluation object with these fields:
  measure, measurement, operation, per_fold,
  per_observation, fitted_params_per_fold,
  report_per_fold, train_test_pairs
Extract:
┌───────────────┬─────────────┬───────────┬───────────────────────────────────────────────┐
│ measure       │ measurement │ operation │ per_fold                                      │
├───────────────┼─────────────┼───────────┼───────────────────────────────────────────────┤
│ LPLoss(p = 1) │ 17200.0     │ predict   │ [16500.0, 17100.0, 16300.0, 17500.0, 18900.0] │
│ LPLoss(p = 2) │ 6.83e8      │ predict   │ [6.14e8, 6.64e8, 5.98e8, 6.37e8, 9.03e8]      │
└───────────────┴─────────────┴───────────┴───────────────────────────────────────────────┘</code></pre><h2 id="Key-goals"><a class="docs-heading-anchor" href="#Key-goals">Key goals</a><a id="Key-goals-1"></a><a class="docs-heading-anchor-permalink" href="#Key-goals" title="Permalink"></a></h2><ul><li><p>Offer a consistent way to use, compose and tune machine learning models in Julia,</p></li><li><p>Promote the improvement of the Julia ML/Stats ecosystem by making it easier to use models from a wide range of packages,</p></li><li><p>Unlock performance gains by exploiting Julia&#39;s support for parallelism, automatic differentiation, GPU, optimization etc.</p></li></ul><h2 id="Key-features"><a class="docs-heading-anchor" href="#Key-features">Key features</a><a id="Key-features-1"></a><a class="docs-heading-anchor-permalink" href="#Key-features" title="Permalink"></a></h2><ul><li><p>Data agnostic, train models on any data supported by the <a href="https://github.com/JuliaData/Tables.jl">Tables.jl</a> interface.</p></li><li><p>Extensive, state-of-the art, support for model composition (<em>pipelines</em>, <em>stacks</em> and, more generally, <em>learning networks</em>). See more <a href="#model-composability">below</a>.</p></li><li><p>Convenient syntax to tune and evaluate (composite) models.</p></li><li><p>Consistent interface to handle probabilistic predictions.</p></li><li><p>Extensible <a href="https://github.com/JuliaAI/MLJTuning.jl">tuning interface</a>, to support growing number of optimization strategies, and designed to play well with model composition.</p></li><li><p>Options to accelerate model evaluation and tuning with multithreading and/or distributed processing.</p></li></ul><h2 id="Model-composability"><a class="docs-heading-anchor" href="#Model-composability">Model composability</a><a id="Model-composability-1"></a><a class="docs-heading-anchor-permalink" href="#Model-composability" title="Permalink"></a></h2><p>The generic model composition API&#39;s provided by other toolboxes we have surveyed share one or more of the following shortcomings, which do not exist in MLJ:</p><ul><li><p>Composite models do not inherit all the behavior of ordinary models.</p></li><li><p>Composition is limited to linear (non-branching) pipelines.</p></li><li><p>Supervised components in a linear pipeline can only occur at the end of the pipeline.</p></li><li><p>Only static (unlearned) target transformations/inverse transformations are supported.</p></li><li><p>Hyper-parameters in homogeneous model ensembles cannot be coupled.</p></li><li><p>Model stacking, with out-of-sample predictions for base learners, cannot be implemented (using the generic API alone).</p></li><li><p>Hyper-parameters and/or learned parameters of component models are not easily inspected or manipulated (by tuning algorithms, for example)</p></li><li><p>Composite models cannot implement multiple operations, for example, both a <code>predict</code> and <code>transform</code> method (as in clustering models) or both a <code>transform</code> and <code>inverse_transform</code> method.</p></li></ul><p>Some of these features are demonstrated in <a href="https://github.com/ablaom/MachineLearningInJulia2020/blob/master/wow.ipynb">this notebook</a></p><p>For more information see the <a href="https://doi.org/10.21105/joss.02704">MLJ design paper</a> or our detailed <a href="https://arxiv.org/abs/2012.15505">paper</a> on the composition interface.</p><h2 id="Getting-help-and-reporting-problems"><a class="docs-heading-anchor" href="#Getting-help-and-reporting-problems">Getting help and reporting problems</a><a id="Getting-help-and-reporting-problems-1"></a><a class="docs-heading-anchor-permalink" href="#Getting-help-and-reporting-problems" title="Permalink"></a></h2><p>Users are encouraged to provide feedback on their experience using MLJ and to report issues.</p><p>For a query to have maximum exposure to maintainers and users, start a discussion thread at <a href="https://github.com/alan-turing-institute/MLJ.jl">Julia Discourse Machine Learning</a> and tag your issue &quot;mlj&quot;. Queries can also be posted as <a href="https://github.com/alan-turing-institute/MLJ.jl/issues">issues</a>, or on the <code>#mlj</code> slack workspace in the Julia Slack channel.</p><p>Bugs, suggestions, and feature requests can be posted <a href="https://github.com/alan-turing-institute/MLJ.jl/issues">here</a>.</p><p>Users are also welcome to join the <code>#mlj</code> Julia slack channel to ask questions and make suggestions.</p><p>See also, <a href="../known_issues/#Known-Issues">Known Issues</a></p><h2 id="Installation"><a class="docs-heading-anchor" href="#Installation">Installation</a><a id="Installation-1"></a><a class="docs-heading-anchor-permalink" href="#Installation" title="Permalink"></a></h2><p>Initially it is recommended that MLJ and associated packages be installed in a new <a href="https://julialang.github.io/Pkg.jl/v1/environments/">environment</a> to avoid package conflicts. You can do this with</p><pre><code class="language-julia">julia&gt; using Pkg; Pkg.activate(&quot;my_MLJ_env&quot;, shared=true)</code></pre><p>Installing MLJ is also done with the package manager:</p><pre><code class="language-julia">julia&gt; Pkg.add(&quot;MLJ&quot;)</code></pre><p><strong>Optional:</strong> To test your installation, run</p><pre><code class="language-julia">julia&gt; Pkg.test(&quot;MLJ&quot;)</code></pre><p>It is important to note that MLJ is essentially a big wrapper providing unified access to <em>model providing packages</em>. For this reason, one generally needs to add further packages to your environment to make model-specific code available. This happens automatically when you use MLJ&#39;s interactive load command <code>@iload</code>, as in</p><pre><code class="language-julia">julia&gt; Tree = @iload DecisionTreeClassifier # load type
julia&gt; tree = Tree() # instance</code></pre><p>where you will also be asked to choose a providing package, for more than one provide a <code>DecisionTreeClassifier</code> model. For more on identifying the name of an applicable model, see <a href="../model_search/#model_search">Model Search</a>. For non-interactive loading of code (e.g., from a module or function) see <a href="../loading_model_code/#Loading-Model-Code">Loading Model Code</a>.</p><p>It is recommended that you start with models from more mature packages such as DecisionTree.jl, ScikitLearn.jl or XGBoost.jl.</p><p>MLJ is supported by a number of satellite packages (MLJTuning, MLJModelInterface, etc) which the general user is <em>not</em> required to install directly. Developers can learn more about these <a href="https://github.com/alan-turing-institute/MLJ.jl/blob/master/ORGANIZATION.md">here</a>.</p><p>See also the alternative instalation instructions for <a href="../modifying_behavior/#Modifying-Behavior">Modifying Behavior</a>.</p><h2 id="Funding"><a class="docs-heading-anchor" href="#Funding">Funding</a><a id="Funding-1"></a><a class="docs-heading-anchor-permalink" href="#Funding" title="Permalink"></a></h2><p>MLJ was initially created as a Tools, Practices and Systems project at the <a href="https://www.turing.ac.uk/">Alan Turing Institute</a> in 2019. Current funding is provided by a <a href="https://www.mbie.govt.nz/science-and-technology/science-and-innovation/funding-information-and-opportunities/investment-funds/strategic-science-investment-fund/ssif-funded-programmes/university-of-auckland/">New Zealand Strategic Science Investment Fund</a> awarded to the University of Auckland.</p><h2 id="Citing-MLJ"><a class="docs-heading-anchor" href="#Citing-MLJ">Citing MLJ</a><a id="Citing-MLJ-1"></a><a class="docs-heading-anchor-permalink" href="#Citing-MLJ" title="Permalink"></a></h2><p>An overview of MLJ design:</p><p><a href="https://doi.org/10.21105/joss.02704"><img src="https://joss.theoj.org/papers/10.21105/joss.02704/status.svg" alt="DOI"/></a></p><pre><code class="language-bibtex">@article{Blaom2020,
  doi = {10.21105/joss.02704},
  url = {https://doi.org/10.21105/joss.02704},
  year = {2020},
  publisher = {The Open Journal},
  volume = {5},
  number = {55},
  pages = {2704},
  author = {Anthony D. Blaom and Franz Kiraly and Thibaut Lienart and Yiannis Simillides and Diego Arenas and Sebastian J. Vollmer},
  title = {{MLJ}: A Julia package for composable machine learning},
  journal = {Journal of Open Source Software}
}</code></pre><p>An in-depth view of MLJ&#39;s model composition design:</p><p><a href="https://arxiv.org/abs/2012.15505"><img src="https://img.shields.io/badge/arXiv-2012.15505-&lt;COLOR&gt;.svg" alt="arXiv"/></a></p><pre><code class="language-bibtex">@misc{blaom2020flexible,
      title={Flexible model composition in machine learning and its implementation in {MLJ}},
      author={Anthony D. Blaom and Sebastian J. Vollmer},
      year={2020},
      eprint={2012.15505},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}</code></pre></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../">« Home</a><a class="docs-footer-nextpage" href="../learning_mlj/">Learning MLJ »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Monday 23 May 2022 22:07">Monday 23 May 2022</span>. Using Julia version 1.7.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
