<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Introduction · MLJ</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="assets/documenter.js"></script><script src="siteinfo.js"></script><script src="../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href><img src="assets/logo.svg" alt="MLJ logo"/></a><div class="docs-package-name"><span class="docs-autofit">MLJ</span></div><form class="docs-search" action="search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li class="is-active"><a class="tocitem" href>Introduction</a><ul class="internal"><li><a class="tocitem" href="#Lightning-tour"><span>Lightning tour</span></a></li><li><a class="tocitem" href="#Key-goals"><span>Key goals</span></a></li><li><a class="tocitem" href="#Key-features"><span>Key features</span></a></li><li><a class="tocitem" href="#Model-composability"><span>Model composability</span></a></li><li><a class="tocitem" href="#Reporting-problems"><span>Reporting problems</span></a></li><li><a class="tocitem" href="#Installation"><span>Installation</span></a></li><li><a class="tocitem" href="#Learning-Julia"><span>Learning Julia</span></a></li><li><a class="tocitem" href="#Learning-to-use-MLJ"><span>Learning to use MLJ</span></a></li><li><a class="tocitem" href="#Citing-MLJ"><span>Citing MLJ</span></a></li></ul></li><li><a class="tocitem" href="getting_started/">Getting Started</a></li><li><a class="tocitem" href="common_mlj_workflows/">Common MLJ Workflows</a></li><li><a class="tocitem" href="working_with_categorical_data/">Working with Categorical Data</a></li><li><a class="tocitem" href="model_search/">Model Search</a></li><li><a class="tocitem" href="loading_model_code/">Loading Model Code</a></li><li><a class="tocitem" href="machines/">Machines</a></li><li><a class="tocitem" href="evaluating_model_performance/">Evaluating Model Performance</a></li><li><a class="tocitem" href="performance_measures/">Performance Measures</a></li><li><a class="tocitem" href="tuning_models/">Tuning Models</a></li><li><a class="tocitem" href="learning_curves/">Learning Curves</a></li><li><a class="tocitem" href="transformers/">Transformers and other unsupervised models</a></li><li><a class="tocitem" href="composing_models/">Composing Models</a></li><li><a class="tocitem" href="controlling_iterative_models/">Controlling Iterative Models</a></li><li><a class="tocitem" href="homogeneous_ensembles/">Homogeneous Ensembles</a></li><li><a class="tocitem" href="generating_synthetic_data/">Generating Synthetic Data</a></li><li><a class="tocitem" href="openml_integration/">OpenML Integration</a></li><li><a class="tocitem" href="acceleration_and_parallelism/">Acceleration and Parallelism</a></li><li><a class="tocitem" href="simple_user_defined_models/">Simple User Defined Models</a></li><li><a class="tocitem" href="quick_start_guide_to_adding_models/">Quick-Start Guide to Adding Models</a></li><li><a class="tocitem" href="adding_models_for_general_use/">Adding Models for General Use</a></li><li><a class="tocitem" href="benchmarking/">Benchmarking</a></li><li><a class="tocitem" href="internals/">Internals</a></li><li><a class="tocitem" href="list_of_supported_models/">List of Supported Models</a></li><li><a class="tocitem" href="third_party_packages/">Third Party Packages</a></li><li><a class="tocitem" href="glossary/">Glossary</a></li><li><a class="tocitem" href="mlj_cheatsheet/">MLJ Cheatsheet</a></li><li><a class="tocitem" href="frequently_asked_questions/">FAQ</a></li><li><a class="tocitem" href="julia_blogpost/">Julia BlogPost</a></li><li><a class="tocitem" href="api/">Index of Methods</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Introduction</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Introduction</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/alan-turing-institute/MLJ.jl/blob/master/docs/src/index.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><script async defer src="https://buttons.github.io/buttons.js"></script>

<span style="color:darkslateblue;font-size:2.25em;font-style:italic;">
A Machine Learning Framework for Julia
</span>  &nbsp; &nbsp; &nbsp; &nbsp;
<a class="github-button" href="https://github.com/alan-turing-institute/MLJ.jl" data-icon="octicon-star" data-size="large" data-show-count="true" aria-label="Star alan-turing-institute/MLJ.jl on GitHub">Star</a>

<br>
<br>
<div style="font-size:1.25em;font-weight:bold;">
  <a href="#Installation-1" style="color: orange;">Install</a>         &nbsp;|&nbsp;
  <a href="#Learning-to-use-MLJ-1" style="color: orange;">Learn</a>    &nbsp;|&nbsp;
  <a href="mlj_cheatsheet">Cheatsheet</a>       &nbsp;|&nbsp;
  <a href="common_mlj_workflows">Workflows</a>  &nbsp;|&nbsp;
  <a href="https://github.com/alan-turing-institute/MLJ.jl/">For Developers</a> &nbsp;|&nbsp;
  <a href="https://mybinder.org/v2/gh/alan-turing-institute/MLJ.jl/master?filepath=binder%2FMLJ_demo.ipynb">Live Demo</a> &nbsp;|&nbsp;
  <a href="third_party_packages">3rd Party Packages</a>
</div><h1 id="Introduction"><a class="docs-heading-anchor" href="#Introduction">Introduction</a><a id="Introduction-1"></a><a class="docs-heading-anchor-permalink" href="#Introduction" title="Permalink"></a></h1><p>MLJ (Machine Learning in Julia) is a toolbox written in Julia providing a common interface and meta-algorithms for selecting, tuning, evaluating, composing and comparing <a href="list_of_supported_models/#model_list">over 150 machine learning models</a> written in Julia and other languages. In particular MLJ wraps a large number of <a href="https://scikit-learn.org/stable/">scikit-learn</a> models.</p><p>MLJ is released under the MIT licensed and sponsored by the <a href="https://www.turing.ac.uk/">Alan Turing Institute</a>.</p><h2 id="Lightning-tour"><a class="docs-heading-anchor" href="#Lightning-tour">Lightning tour</a><a id="Lightning-tour-1"></a><a class="docs-heading-anchor-permalink" href="#Lightning-tour" title="Permalink"></a></h2><p><em>For more elementary introductions to MLJ usage see <a href="#Basic-introductions">Basic introductions</a> below.</em></p><p>The first code snippet below creates a new Julia environment <code>MLJ_tour</code> and installs just those packages needed for the tour. See <a href="#Installation">Installation</a> for more on creating a Julia environment for use with MLJ.</p><p>Julia installation instructions are <a href="https://julialang.org/downloads/">here</a>.</p><pre><code class="language-julia">using Pkg
Pkg.activate(&quot;MLJ_tour&quot;, shared=true)
Pkg.add(&quot;MLJ&quot;)
Pkg.add(&quot;MLJIteration&quot;)
Pkg.add(&quot;EvoTrees&quot;)</code></pre><p>In MLJ a <em>model</em> is just a container for hyper-parameters, and that&#39;s all. Here we will apply several kinds of model composition before binding the resulting &quot;meta-model&quot; to data in a <em>machine</em> for evaluation using cross-validation.</p><p>Loading and instantiating a gradient tree-boosting model:</p><pre><code class="language-julia">using MLJ
Booster = @load EvoTreeRegressor # loads code defining a model type
booster = Booster(max_depth=2)   # specify hyper-parameter at construction
booster.nrounds=50               # or mutate post facto</code></pre><p>This model is an example of an iterative model. As is stands, the number of iterations <code>nrounds</code> is fixed.</p><h4 id="Composition-1:-Wrapping-the-model-to-make-it-&quot;self-iterating&quot;"><a class="docs-heading-anchor" href="#Composition-1:-Wrapping-the-model-to-make-it-&quot;self-iterating&quot;">Composition 1: Wrapping the model to make it &quot;self-iterating&quot;</a><a id="Composition-1:-Wrapping-the-model-to-make-it-&quot;self-iterating&quot;-1"></a><a class="docs-heading-anchor-permalink" href="#Composition-1:-Wrapping-the-model-to-make-it-&quot;self-iterating&quot;" title="Permalink"></a></h4><p>Let&#39;s create a new model that automatically learns the number of iterations, using the <code>NumberSinceBest(3)</code> criterion, as applied to an out-of-sample <code>l1</code> loss:</p><pre><code class="language-julia">using MLJIteration
iterated_booster = IteratedModel(model=booster,
                                 resampling=Holdout(fraction_train=0.8),
                                 controls=[Step(2), NumberSinceBest(3), NumberLimit(300)],
                                 measure=l1,
                                 retrain=true)</code></pre><h4 id="Composition-2:-Preprocess-the-input-features"><a class="docs-heading-anchor" href="#Composition-2:-Preprocess-the-input-features">Composition 2: Preprocess the input features</a><a id="Composition-2:-Preprocess-the-input-features-1"></a><a class="docs-heading-anchor-permalink" href="#Composition-2:-Preprocess-the-input-features" title="Permalink"></a></h4><p>Combining the model with categorical feature encoding:</p><pre><code class="language-julia">pipe = @pipeline ContinuousEncoder iterated_booster</code></pre><h4 id="Composition-3:-Wrapping-the-model-to-make-it-&quot;self-tuning&quot;"><a class="docs-heading-anchor" href="#Composition-3:-Wrapping-the-model-to-make-it-&quot;self-tuning&quot;">Composition 3: Wrapping the model to make it &quot;self-tuning&quot;</a><a id="Composition-3:-Wrapping-the-model-to-make-it-&quot;self-tuning&quot;-1"></a><a class="docs-heading-anchor-permalink" href="#Composition-3:-Wrapping-the-model-to-make-it-&quot;self-tuning&quot;" title="Permalink"></a></h4><p>First, we define a hyper-parameter range for optimization of a (nested) hyper-parameter:</p><pre><code class="language-julia">max_depth_range = range(pipe,
                        :(deterministic_iterated_model.model.max_depth),
                        lower = 1,
                        upper = 10)</code></pre><p>Now we can wrap the pipeline model in an optimization strategy to make it &quot;self-tuning&quot;:</p><pre><code class="language-julia">self_tuning_pipe = TunedModel(model=pipe,
                              tuning=RandomSearch(),
                              ranges = max_depth_range,
                              resampling=CV(nfolds=3, rng=456),
                              measure=l1,
                              acceleration=CPUThreads(),
                              n=50)</code></pre><h4 id="Binding-to-data-and-evaluating-performance"><a class="docs-heading-anchor" href="#Binding-to-data-and-evaluating-performance">Binding to data and evaluating performance</a><a id="Binding-to-data-and-evaluating-performance-1"></a><a class="docs-heading-anchor-permalink" href="#Binding-to-data-and-evaluating-performance" title="Permalink"></a></h4><p>Loading a selection of features and labels from the Ames House Price dataset:</p><pre><code class="language-julia">X, y = @load_reduced_ames;</code></pre><p>Binding the &quot;self-tuning&quot; pipeline model to data in a <em>machine</em> (which will additionally store <em>learned</em> parameters):</p><pre><code class="language-julia">mach = machine(self_tuning_pipe, X, y)</code></pre><p>Evaluating the &quot;self-tuning&quot; pipeline model&#39;s performance using 5-fold cross-validation (implies multiple layers of nested resampling):</p><pre><code class="language-julia">julia&gt; evaluate!(mach,
                 measures=[l1, l2],
                 resampling=CV(nfolds=5, rng=123),
                 acceleration=CPUThreads(),
                 verbosity=2)
┌────────────────────┬───────────────┬───────────────────────────────────────────────┐
│ _.measure          │ _.measurement │ _.per_fold                                    │
├────────────────────┼───────────────┼───────────────────────────────────────────────┤
│ LPLoss{Int64} @410 │ 16900.0       │ [17000.0, 16200.0, 16200.0, 16400.0, 18600.0] │
│ LPLoss{Int64} @632 │ 6.57e8        │ [6.38e8, 6.19e8, 5.92e8, 5.67e8, 8.7e8]       │
└────────────────────┴───────────────┴───────────────────────────────────────────────┘
_.per_observation = [[[20300.0, 21800.0, ..., 7910.0], [4300.0, 31900.0, ..., 12600.0], [22000.0, 91600.0, ..., 35500.0], [2980.0, 35700.0, ..., 6240.0], [9140.0, 30000.0, ..., 3050.0]], [[4.13e8, 4.74e8, ..., 6.26e7], [1.85e7, 1.02e9, ..., 1.59e8], [4.83e8, 8.38e9, ..., 1.26e9], [8.86e6, 1.28e9, ..., 3.89e7], [8.35e7, 9.01e8, ..., 9.31e6]]]
_.fitted_params_per_fold = [ … ]
_.report_per_fold = [ … ]</code></pre><p>Try out MLJ yourself in the following batteries-included Binder <a href="https://mybinder.org/v2/gh/alan-turing-institute/MLJ.jl/master?filepath=binder%2FMLJ_demo.ipynb">notebook</a>. No installation required.</p><h2 id="Key-goals"><a class="docs-heading-anchor" href="#Key-goals">Key goals</a><a id="Key-goals-1"></a><a class="docs-heading-anchor-permalink" href="#Key-goals" title="Permalink"></a></h2><ul><li><p>Offer a consistent way to use, compose and tune machine learning models in Julia,</p></li><li><p>Promote the improvement of the Julia ML/Stats ecosystem by making it easier to use models from a wide range of packages,</p></li><li><p>Unlock performance gains by exploiting Julia&#39;s support for parallelism, automatic differentiation, GPU, optimization etc.</p></li></ul><h2 id="Key-features"><a class="docs-heading-anchor" href="#Key-features">Key features</a><a id="Key-features-1"></a><a class="docs-heading-anchor-permalink" href="#Key-features" title="Permalink"></a></h2><ul><li><p>Data agnostic, train models on any data supported by the <a href="https://github.com/JuliaData/Tables.jl">Tables.jl</a> interface,</p></li><li><p>Extensive, state-of-the art, support for model composition (<em>pipelines</em> and <em>learning networks</em>) (see more <a href="#model-composability">below</a>),</p></li><li><p>Convenient syntax to tune and evaluate (composite) models.</p></li><li><p>Consistent interface to handle probabilistic predictions.</p></li><li><p>Extensible <a href="https://github.com/alan-turing-institute/MLJTuning.jl">tuning interface</a>, to support growing number of optimization strategies, and designed to play well with model composition.</p></li></ul><h2 id="Model-composability"><a class="docs-heading-anchor" href="#Model-composability">Model composability</a><a id="Model-composability-1"></a><a class="docs-heading-anchor-permalink" href="#Model-composability" title="Permalink"></a></h2><p>The generic model composition API&#39;s provided by other toolboxes we have surveyed share one or more of the following shortcomings, which do not exist in MLJ:</p><ul><li><p>Composite models do not inherit all the behavior of ordinary models.</p></li><li><p>Composition is limited to linear (non-branching) pipelines.</p></li><li><p>Supervised components in a linear pipeline can only occur at the end of the pipeline.</p></li><li><p>Only static (unlearned) target transformations/inverse transformations are supported.</p></li><li><p>Hyper-parameters in homogeneous model ensembles cannot be coupled.</p></li><li><p>Model stacking, with out-of-sample predictions for base learners, cannot be implemented (using the generic API alone).</p></li><li><p>Hyper-parameters and/or learned parameters of component models are not easily inspected or manipulated (by tuning algorithms, for example)</p></li><li><p>Composite models cannot implement multiple operations, for example, both a <code>predict</code> and <code>transform</code> method (as in clustering models) or both a <code>transform</code> and <code>inverse_transform</code> method.</p></li></ul><p>Some of these features are demonstrated in <a href="https://github.com/ablaom/MachineLearningInJulia2020/blob/master/wow.ipynb">this notebook</a></p><p>For more information see the <a href="https://doi.org/10.21105/joss.02704">MLJ design paper</a> or our detailed <a href="https://arxiv.org/abs/2012.15505">paper</a> on the composition interface.</p><h2 id="Reporting-problems"><a class="docs-heading-anchor" href="#Reporting-problems">Reporting problems</a><a id="Reporting-problems-1"></a><a class="docs-heading-anchor-permalink" href="#Reporting-problems" title="Permalink"></a></h2><p>Users are encouraged to provide feedback on their experience using MLJ and to report issues. You can do so <a href="https://github.com/alan-turing-institute/MLJ.jl/issues">here</a> or on the <code>#mlj</code> Julia slack channel.</p><p>For known issues that are not strictly MLJ bugs, see <a href="https://github.com/alan-turing-institute/MLJ.jl#known-issues">here</a></p><h2 id="Installation"><a class="docs-heading-anchor" href="#Installation">Installation</a><a id="Installation-1"></a><a class="docs-heading-anchor-permalink" href="#Installation" title="Permalink"></a></h2><p>Initially it is recommended that MLJ and associated packages be installed in a new <a href="https://julialang.github.io/Pkg.jl/v1/environments/">environment</a> to avoid package conflicts. You can do this with</p><pre><code class="language-julia">julia&gt; using Pkg; Pkg.activate(&quot;my_MLJ_env&quot;, shared=true)</code></pre><p>Installing MLJ is also done with the package manager:</p><pre><code class="language-julia">julia&gt; Pkg.add(&quot;MLJ&quot;)</code></pre><p><strong>Optional:</strong> To test your installation, run</p><pre><code class="language-julia">julia&gt; Pkg.test(&quot;MLJ&quot;)</code></pre><p>It is important to note that MLJ is essentially a big wrapper providing unified access to <em>model providing packages</em>. For this reason, one generally needs to add further packages to your environment to make model-specific code available. This happens automatically when you use MLJ&#39;s interactive load command <code>@iload</code>, as in</p><pre><code class="language-julia">julia&gt; Tree = @iload DecisionTreeClassifier # load type
julia&gt; tree = Tree() # instance</code></pre><p>where you will also be asked to choose a providing package, for more than one provide a <code>DecisionTreeClassifier</code> model. For more on identifying the name of an applicable model, see <a href="model_search/#Model-Search">Model Search</a>. For non-interactive loading of code (e.g., from a module or function) see <a href="loading_model_code/#Loading-Model-Code">Loading Model Code</a>.</p><p>It is recommended that you start with models from more mature packages such as DecisionTree.jl, ScikitLearn.jl or XGBoost.jl.</p><p>MLJ is supported by a number of satellite packages (MLJTuning, MLJModelInterface, etc) which the general user is <em>not</em> required to install directly. Developers can learn more about these <a href="https://github.com/alan-turing-institute/MLJ.jl/blob/master/ORGANIZATION.md">here</a></p><h2 id="Learning-Julia"><a class="docs-heading-anchor" href="#Learning-Julia">Learning Julia</a><a id="Learning-Julia-1"></a><a class="docs-heading-anchor-permalink" href="#Learning-Julia" title="Permalink"></a></h2><p>If you have experience in programming in another language but are new to Julia, then we highly recommend Aaron Christinson&#39;s tutorial <a href="https://github.com/ninjaaron/dispatching-design-patterns">Dispatching Design Patterns</a> which is nicely compressed in his <a href="https://live.juliacon.org/talk/JYNERU">half-hour video presentation</a>.</p><p>However, one doesn&#39;t need to be able to program in Julia to start using MLJ.</p><h2 id="Learning-to-use-MLJ"><a class="docs-heading-anchor" href="#Learning-to-use-MLJ">Learning to use MLJ</a><a id="Learning-to-use-MLJ-1"></a><a class="docs-heading-anchor-permalink" href="#Learning-to-use-MLJ" title="Permalink"></a></h2><p>The present document, although littered with examples, is primarily intended as a complete reference. Resources for learning MLJ are:</p><h3 id="Basic-introductions"><a class="docs-heading-anchor" href="#Basic-introductions">Basic introductions</a><a id="Basic-introductions-1"></a><a class="docs-heading-anchor-permalink" href="#Basic-introductions" title="Permalink"></a></h3><ul><li><p>the <a href="getting_started/#Getting-Started">Getting Started</a> section of this manual</p></li><li><p>an introductory <a href="https://mybinder.org/v2/gh/alan-turing-institute/MLJ.jl/master?filepath=binder%2FMLJ_demo.ipynb">binder notebook</a> (no Julia/MLJ installation required)</p></li></ul><h3 id="In-depth"><a class="docs-heading-anchor" href="#In-depth">In depth</a><a id="In-depth-1"></a><a class="docs-heading-anchor-permalink" href="#In-depth" title="Permalink"></a></h3><ul><li><p>the MLJ JuliaCon2020 Workshop <a href="https://github.com/ablaom/MachineLearningInJulia2020">materials</a> and <a href="https://www.youtube.com/watch?time_continue=27&amp;v=qSWbCn170HU&amp;feature=emb_title">video recording</a></p></li><li><p><a href="https://alan-turing-institute.github.io/DataScienceTutorials.jl/">Data Science Tutorials in Julia</a></p></li></ul><p>Users are also welcome to join the <code>#mlj</code> Julia slack channel to ask questions and make suggestions.</p><h2 id="Citing-MLJ"><a class="docs-heading-anchor" href="#Citing-MLJ">Citing MLJ</a><a id="Citing-MLJ-1"></a><a class="docs-heading-anchor-permalink" href="#Citing-MLJ" title="Permalink"></a></h2><p>When presenting work that uses MLJ, please cite the MLJ design paper:</p><p><a href="https://doi.org/10.21105/joss.02704"><img src="https://joss.theoj.org/papers/10.21105/joss.02704/status.svg" alt="DOI"/></a></p><pre><code class="language-bibtex">@article{Blaom2020,
  doi = {10.21105/joss.02704},
  url = {https://doi.org/10.21105/joss.02704},
  year = {2020},
  publisher = {The Open Journal},
  volume = {5},
  number = {55},
  pages = {2704},
  author = {Anthony D. Blaom and Franz Kiraly and Thibaut Lienart and Yiannis Simillides and Diego Arenas and Sebastian J. Vollmer},
  title = {{MLJ}: A Julia package for composable machine learning},
  journal = {Journal of Open Source Software}
}</code></pre><p>If using the model composition features of MLJ (learning networks) please additionally cite</p><pre><code class="language-bitex">@misc{blaom2020flexible,
  title={{Flexible model composition in machine learning and its implementation in MLJ}},
  author={Anthony D. Blaom and Sebastian J. Vollmer},
  year={2020},
  eprint={2012.15505},
  archivePrefix={arXiv},
  primaryClass={cs.LG}
}</code></pre></article><nav class="docs-footer"><a class="docs-footer-nextpage" href="getting_started/">Getting Started »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Monday 3 May 2021 01:25">Monday 3 May 2021</span>. Using Julia version 1.6.1.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
