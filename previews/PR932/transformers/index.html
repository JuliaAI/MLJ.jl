<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Transformers and Other Unsupervised models · MLJ</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.svg" alt="MLJ logo"/></a><div class="docs-package-name"><span class="docs-autofit">MLJ</span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../about_mlj/">About MLJ</a></li><li><a class="tocitem" href="../learning_mlj/">Learning MLJ</a></li><li><a class="tocitem" href="../getting_started/">Getting Started</a></li><li><a class="tocitem" href="../common_mlj_workflows/">Common MLJ Workflows</a></li><li><a class="tocitem" href="../working_with_categorical_data/">Working with Categorical Data</a></li><li><a class="tocitem" href="../model_search/">Model Search</a></li><li><a class="tocitem" href="../loading_model_code/">Loading Model Code</a></li><li><a class="tocitem" href="../machines/">Machines</a></li><li><a class="tocitem" href="../evaluating_model_performance/">Evaluating Model Performance</a></li><li><a class="tocitem" href="../performance_measures/">Performance Measures</a></li><li><a class="tocitem" href="../weights/">Weights</a></li><li><a class="tocitem" href="../tuning_models/">Tuning Models</a></li><li><a class="tocitem" href="../learning_curves/">Learning Curves</a></li><li><a class="tocitem" href="../preparing_data/">Preparing Data</a></li><li class="is-active"><a class="tocitem" href>Transformers and Other Unsupervised models</a><ul class="internal"><li><a class="tocitem" href="#Built-in-transformers"><span>Built-in transformers</span></a></li><li><a class="tocitem" href="#Static-transformers"><span>Static transformers</span></a></li><li><a class="tocitem" href="#Transformers-that-also-predict"><span>Transformers that also predict</span></a></li></ul></li><li><a class="tocitem" href="../more_on_probabilistic_predictors/">More on Probablistic Predictors</a></li><li><a class="tocitem" href="../composing_models/">Composing Models</a></li><li><a class="tocitem" href="../linear_pipelines/">Linear Pipelines</a></li><li><a class="tocitem" href="../target_transformations/">Target Transformations</a></li><li><a class="tocitem" href="../homogeneous_ensembles/">Homogeneous Ensembles</a></li><li><a class="tocitem" href="../model_stacking/">Model Stacking</a></li><li><a class="tocitem" href="../controlling_iterative_models/">Controlling Iterative Models</a></li><li><a class="tocitem" href="../generating_synthetic_data/">Generating Synthetic Data</a></li><li><a class="tocitem" href="../openml_integration/">OpenML Integration</a></li><li><a class="tocitem" href="../acceleration_and_parallelism/">Acceleration and Parallelism</a></li><li><a class="tocitem" href="../simple_user_defined_models/">Simple User Defined Models</a></li><li><a class="tocitem" href="../quick_start_guide_to_adding_models/">Quick-Start Guide to Adding Models</a></li><li><a class="tocitem" href="../adding_models_for_general_use/">Adding Models for General Use</a></li><li><a class="tocitem" href="../modifying_behavior/">Modifying Behavior</a></li><li><a class="tocitem" href="../internals/">Internals</a></li><li><a class="tocitem" href="../list_of_supported_models/">List of Supported Models</a></li><li><a class="tocitem" href="../third_party_packages/">Third Party Packages</a></li><li><a class="tocitem" href="../glossary/">Glossary</a></li><li><a class="tocitem" href="../mlj_cheatsheet/">MLJ Cheatsheet</a></li><li><a class="tocitem" href="../known_issues/">Known Issues</a></li><li><a class="tocitem" href="../frequently_asked_questions/">FAQ</a></li><li><a class="tocitem" href="../julia_blogpost/">Julia BlogPost</a></li><li><a class="tocitem" href="../api/">Index of Methods</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Transformers and Other Unsupervised models</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Transformers and Other Unsupervised models</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/alan-turing-institute/MLJ.jl/blob/master/docs/src/transformers.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Transformers-and-Other-Unsupervised-Models"><a class="docs-heading-anchor" href="#Transformers-and-Other-Unsupervised-Models">Transformers and Other Unsupervised Models</a><a id="Transformers-and-Other-Unsupervised-Models-1"></a><a class="docs-heading-anchor-permalink" href="#Transformers-and-Other-Unsupervised-Models" title="Permalink"></a></h1><p>Several unsupervised models used to perform common transformations, such as one-hot encoding, are available in MLJ out-of-the-box. These are detailed in <a href="#Built-in-transformers">Built-in transformers</a> below.</p><p>A transformer is <em>static</em> if it has no learned parameters. While such a transformer is tantamount to an ordinary function, realizing it as an MLJ static transformer (subtype of <code>Static &lt;: Unsupervised</code>) can be useful, especially if the function depends on parameters the user would like to manipulate (which become <em>hyper-parameters</em> of the model). The necessary syntax for defining your own static transformers is described in <a href="#Static-transformers">Static transformers</a> below.</p><p>Some unsupervised models, such as clustering algorithms, have a <code>predict</code> method in addition to a <code>transform</code> method. We give an example of this in <a href="#Transformers-that-also-predict">Transformers that also predict</a></p><p>Finally we note that models that fit a distribution, or more generally a sampler object, to some data, which are sometimes viewed as unsupervised, are treated in MLJ as <em>supervised</em> models. See <a href="../adding_models_for_general_use/#Models-that-learn-a-probability-distribution">Models that learn a probability distribution</a> for an example.</p><h2 id="Built-in-transformers"><a class="docs-heading-anchor" href="#Built-in-transformers">Built-in transformers</a><a id="Built-in-transformers-1"></a><a class="docs-heading-anchor-permalink" href="#Built-in-transformers" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="MLJModels.Standardizer" href="#MLJModels.Standardizer"><code>MLJModels.Standardizer</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">Standardizer</code></pre><p>A model type for constructing a standardizer, based on <a href="https://github.com/alan-turing-institute/MLJModels.jl">MLJModels.jl</a>, and implementing the MLJ model interface.</p><p>From MLJ, the type can be imported using</p><pre><code class="language-none">Standardizer = @load Standardizer pkg=MLJModels</code></pre><p>Do <code>model = Standardizer()</code> to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in <code>Standardizer(features=...)</code>.</p><p>Use this model to standardize (whiten) a <code>Continuous</code> vector, or relevant columns of a table. The rescalings applied by this transformer to new data are always those learned during the training phase, which are generally different from what would actually standardize the new data.</p><p><strong>Training data</strong></p><p>In MLJ or MLJBase, bind an instance <code>model</code> to data with</p><pre><code class="language-none">mach = machine(model, X)</code></pre><p>where</p><ul><li><code>X</code>: any Tables.jl compatible table or any abstract vector with <code>Continuous</code> element scitype (any abstract float vector). Only features in a table with <code>Continuous</code> scitype can be standardized; check column scitypes with <code>schema(X)</code>.</li></ul><p>Train the machine using <code>fit!(mach, rows=...)</code>.</p><p><strong>Hyper-parameters</strong></p><ul><li><p><code>features</code>: one of the following, with the behavior indicated below:</p><ul><li><p><code>[]</code> (empty, the default): standardize all features (columns) having <code>Continuous</code> element scitype</p></li><li><p>non-empty vector of feature names (symbols): standardize only the <code>Continuous</code> features in the vector (if <code>ignore=false</code>) or <code>Continuous</code> features <em>not</em> named in the vector (<code>ignore=true</code>).</p></li><li><p>function or other callable: standardize a feature if the callable returns <code>true</code> on its name. For example, <code>Standardizer(features = name -&gt; name in [:x1, :x3], ignore = true, count=true)</code> has the same effect as <code>Standardizer(features = [:x1, :x3], ignore = true, count=true)</code>, namely to standardize all <code>Continuous</code> and <code>Count</code> features, with the exception of <code>:x1</code> and <code>:x3</code>.</p></li></ul><p>Note this behavior is further modified if the <code>ordered_factor</code> or <code>count</code> flags are set to <code>true</code>; see below</p></li><li><p><code>ignore=false</code>: whether to ignore or standardize specified <code>features</code>, as explained above</p></li><li><p><code>ordered_factor=false</code>: if <code>true</code>, standardize any <code>OrderedFactor</code> feature wherever a <code>Continuous</code> feature would be standardized, as described above</p></li><li><p><code>count=false</code>: if <code>true</code>, standardize any <code>Count</code> feature wherever a <code>Continuous</code> feature would be standardized, as described above</p></li></ul><p><strong>Operations</strong></p><ul><li><p><code>transform(mach, Xnew)</code>: return <code>Xnew</code> with relevant features standardized according to the rescalings learned during fitting of <code>mach</code>.</p></li><li><p><code>inverse_transform(mach, Z)</code>: apply the inverse transformation to <code>Z</code>, so that <code>inverse_transform(mach, transform(mach, Xnew))</code> is approximately the same as <code>Xnew</code>; unavailable if <code>ordered_factor</code> or <code>count</code> flags were set to <code>true</code>.</p></li></ul><p><strong>Fitted parameters</strong></p><p><code>fitted_params(mach)</code> is a dictionary of the rescaling parameters, keyed on feature name. In each value the first component is the training data mean, the second the standard deviation.</p><p><strong>Warning:</strong> This format for <code>fitted_params(mach)</code> is not standard and may change in the future.</p><p><strong>Report</strong></p><p>The fields of <code>report(mach)</code> are:</p><ul><li><code>features_fit</code>: the names of features that will be standardized</li></ul><p><strong>Examples</strong></p><pre><code class="language-none">using MLJ

X = (ordinal1 = [1, 2, 3],
     ordinal2 = coerce([:x, :y, :x], OrderedFactor),
     ordinal3 = [10.0, 20.0, 30.0],
     ordinal4 = [-20.0, -30.0, -40.0],
     nominal = coerce([&quot;Your father&quot;, &quot;he&quot;, &quot;is&quot;], Multiclass));

julia&gt; schema(X)
┌──────────┬──────────────────┐
│ names    │ scitypes         │
├──────────┼──────────────────┤
│ ordinal1 │ Count            │
│ ordinal2 │ OrderedFactor{2} │
│ ordinal3 │ Continuous       │
│ ordinal4 │ Continuous       │
│ nominal  │ Multiclass{3}    │
└──────────┴──────────────────┘

stand1 = Standardizer();

julia&gt; transform(fit!(machine(stand1, X)), X)
(ordinal1 = [1, 2, 3],
 ordinal2 = CategoricalValue{Symbol,UInt32}[:x, :y, :x],
 ordinal3 = [-1.0, 0.0, 1.0],
 ordinal4 = [1.0, 0.0, -1.0],
 nominal = CategoricalValue{String,UInt32}[&quot;Your father&quot;, &quot;he&quot;, &quot;is&quot;],)

stand2 = Standardizer(features=[:ordinal3, ], ignore=true, count=true);

julia&gt; transform(fit!(machine(stand2, X)), X)
(ordinal1 = [-1.0, 0.0, 1.0],
 ordinal2 = CategoricalValue{Symbol,UInt32}[:x, :y, :x],
 ordinal3 = [10.0, 20.0, 30.0],
 ordinal4 = [1.0, 0.0, -1.0],
 nominal = CategoricalValue{String,UInt32}[&quot;Your father&quot;, &quot;he&quot;, &quot;is&quot;],)</code></pre><p>See also <a href="#MLJModels.OneHotEncoder"><code>OneHotEncoder</code></a>, <a href="#MLJModels.ContinuousEncoder"><code>ContinuousEncoder</code></a>.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJModels.OneHotEncoder" href="#MLJModels.OneHotEncoder"><code>MLJModels.OneHotEncoder</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">OneHotEncoder</code></pre><p>A model type for constructing a one-hot encoder, based on <a href="https://github.com/alan-turing-institute/MLJModels.jl">MLJModels.jl</a>, and implementing the MLJ model interface.</p><p>From MLJ, the type can be imported using</p><pre><code class="language-none">OneHotEncoder = @load OneHotEncoder pkg=MLJModels</code></pre><p>Do <code>model = OneHotEncoder()</code> to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in <code>OneHotEncoder(features=...)</code>.</p><p>Use this model to one-hot encode the <code>Multiclass</code> and <code>OrderedFactor</code> features (columns) of some table, leaving other columns unchanged.</p><p>New data to be transformed may lack features present in the fit data, but no <em>new</em> features can be present.</p><p><strong>Warning:</strong> This transformer assumes that <code>levels(col)</code> for any <code>Multiclass</code> or <code>OrderedFactor</code> column, <code>col</code>, is the same for training data and new data to be transformed.</p><p>To ensure <em>all</em> features are transformed into <code>Continuous</code> features, or dropped, use <a href="#MLJModels.ContinuousEncoder"><code>ContinuousEncoder</code></a> instead.</p><p><strong>Training data</strong></p><p>In MLJ or MLJBase, bind an instance <code>model</code> to data with</p><pre><code class="language-none">mach = machine(model, X)</code></pre><p>where</p><ul><li><code>X</code>: any Tables.jl compatible table. Columns can be of mixed type but only those with element scitype <code>Multiclass</code> or <code>OrderedFactor</code> can be encoded. Check column scitypes with <code>schema(X)</code>.</li></ul><p>Train the machine using <code>fit!(mach, rows=...)</code>.</p><p><strong>Hyper-parameters</strong></p><ul><li><p><code>features</code>: a vector of symbols (column names). If empty (default) then all <code>Multiclass</code> and <code>OrderedFactor</code> features are encoded. Otherwise, encoding is further restricted to the specified features (<code>ignore=false</code>) or the unspecified features (<code>ignore=true</code>). This default behavior can be modified by the <code>ordered_factor</code> flag.</p></li><li><p><code>ordered_factor=false</code>: when <code>true</code>, <code>OrderedFactor</code> features are universally excluded</p></li><li><p><code>drop_last=true</code>: whether to drop the column corresponding to the final class of encoded features. For example, a three-class feature is spawned into three new features if <code>drop_last=false</code>, but just two features otherwise.</p></li></ul><p><strong>Fitted parameters</strong></p><p>The fields of <code>fitted_params(mach).fitresult</code> are:</p><ul><li><p><code>all_features</code>: names of all features encountered in training</p></li><li><p><code>fitted_levels_given_feature</code>: dictionary of the levels associated with each feature encoded, keyed on the feature name</p></li><li><p><code>ref_name_pairs_given_feature</code>: dictionary of pairs <code>r =&gt; ftr</code> (such as <code>0x00000001 =&gt; :grad__A</code>) where <code>r</code> is a CategoricalArrays.jl reference integer representing a level, and <code>ftr</code> the corresponding new feature name; the dictionary is keyed on the names of features that are encoded</p></li></ul><p><strong>Warning:</strong> <code>fitted_params(mach)</code> does not have a standard form and  may change in the future.</p><p><strong>Report</strong></p><p>The fields of <code>report(mach)</code> are:</p><ul><li><p><code>features_to_be_encoded</code>: names of input features to be encoded</p></li><li><p><code>new_features</code>: names of all output features</p></li></ul><p><strong>Example</strong></p><pre><code class="language-none">using MLJ

X = (name=categorical([&quot;Danesh&quot;, &quot;Lee&quot;, &quot;Mary&quot;, &quot;John&quot;]),
     grade=categorical([&quot;A&quot;, &quot;B&quot;, &quot;A&quot;, &quot;C&quot;], ordered=true),
     height=[1.85, 1.67, 1.5, 1.67],
     n_devices=[3, 2, 4, 3])

julia&gt; schema(X)
┌───────────┬──────────────────┐
│ names     │ scitypes         │
├───────────┼──────────────────┤
│ name      │ Multiclass{4}    │
│ grade     │ OrderedFactor{3} │
│ height    │ Continuous       │
│ n_devices │ Count            │
└───────────┴──────────────────┘

hot = OneHotEncoder(drop_last=true)
mach = fit!(machine(hot, X))
W = transform(mach, X)

julia&gt; schema(W)
┌──────────────┬────────────┐
│ names        │ scitypes   │
├──────────────┼────────────┤
│ name__Danesh │ Continuous │
│ name__John   │ Continuous │
│ name__Lee    │ Continuous │
│ grade__A     │ Continuous │
│ grade__B     │ Continuous │
│ height       │ Continuous │
│ n_devices    │ Count      │
└──────────────┴────────────┘</code></pre><p>See also <a href="#MLJModels.ContinuousEncoder"><code>ContinuousEncoder</code></a>.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJModels.ContinuousEncoder" href="#MLJModels.ContinuousEncoder"><code>MLJModels.ContinuousEncoder</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">ContinuousEncoder</code></pre><p>A model type for constructing a continuous encoder, based on <a href="https://github.com/alan-turing-institute/MLJModels.jl">MLJModels.jl</a>, and implementing the MLJ model interface.</p><p>From MLJ, the type can be imported using</p><pre><code class="language-none">ContinuousEncoder = @load ContinuousEncoder pkg=MLJModels</code></pre><p>Do <code>model = ContinuousEncoder()</code> to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in <code>ContinuousEncoder(drop_last=...)</code>.</p><p>Use this model to arrange all features (columns) of a table to have <code>Continuous</code> element scitype, by applying the following protocol to each feature <code>ftr</code>:</p><ul><li><p>If <code>ftr</code> is already <code>Continuous</code> retain it.</p></li><li><p>If <code>ftr</code> is <code>Multiclass</code>, one-hot encode it.</p></li><li><p>If <code>ftr</code> is <code>OrderedFactor</code>, replace it with <code>coerce(ftr, Continuous)</code> (vector of floating point integers), unless <code>ordered_factors=false</code> is specified, in which case one-hot encode it.</p></li><li><p>If <code>ftr</code> is <code>Count</code>, replace it with <code>coerce(ftr, Continuous)</code>.</p></li><li><p>If <code>ftr</code> has some other element scitype, or was not observed in fitting the encoder, drop it from the table.</p></li></ul><p><strong>Warning:</strong> This transformer assumes that <code>levels(col)</code> for any <code>Multiclass</code> or <code>OrderedFactor</code> column, <code>col</code>, is the same for training data and new data to be transformed.</p><p>To selectively one-hot-encode categorical features (without dropping columns) use <a href="#MLJModels.OneHotEncoder"><code>OneHotEncoder</code></a> instead.</p><p><strong>Training data</strong></p><p>In MLJ or MLJBase, bind an instance <code>model</code> to data with</p><pre><code class="language-none">mach = machine(model, X)</code></pre><p>where</p><ul><li><code>X</code>: any Tables.jl compatible table. Columns can be of mixed type but only those with element scitype <code>Multiclass</code> or <code>OrderedFactor</code> can be encoded. Check column scitypes with <code>schema(X)</code>.</li></ul><p>Train the machine using <code>fit!(mach, rows=...)</code>.</p><p><strong>Hyper-parameters</strong></p><ul><li><p><code>drop_last=true</code>: whether to drop the column corresponding to the final class of one-hot encoded features. For example, a three-class feature is spawned into three new features if <code>drop_last=false</code>, but two just features otherwise.</p></li><li><p><code>one_hot_ordered_factors=false</code>: whether to one-hot any feature with <code>OrderedFactor</code> element scitype, or to instead coerce it directly to a (single) <code>Continuous</code> feature using the order</p></li></ul><p><strong>Fitted parameters</strong></p><p>The fields of <code>fitted_params(mach)</code> are:</p><ul><li><p><code>features_to_keep</code>: names of features that will not be dropped from the table</p></li><li><p><code>one_hot_encoder</code>: the <code>OneHotEncoder</code> model instance for handling the one-hot encoding</p></li><li><p><code>one_hot_encoder_fitresult</code>: the fitted parameters of the <code>OneHotEncoder</code> model</p></li></ul><p><strong>Report</strong></p><ul><li><p><code>features_to_keep</code>: names of input features that will not be dropped from the table</p></li><li><p><code>new_features</code>: names of all output features</p></li></ul><p><strong>Example</strong></p><pre><code class="language-julia">X = (name=categorical([&quot;Danesh&quot;, &quot;Lee&quot;, &quot;Mary&quot;, &quot;John&quot;]),
     grade=categorical([&quot;A&quot;, &quot;B&quot;, &quot;A&quot;, &quot;C&quot;], ordered=true),
     height=[1.85, 1.67, 1.5, 1.67],
     n_devices=[3, 2, 4, 3],
     comments=[&quot;the force&quot;, &quot;be&quot;, &quot;with you&quot;, &quot;too&quot;])

julia&gt; schema(X)
┌───────────┬──────────────────┐
│ names     │ scitypes         │
├───────────┼──────────────────┤
│ name      │ Multiclass{4}    │
│ grade     │ OrderedFactor{3} │
│ height    │ Continuous       │
│ n_devices │ Count            │
│ comments  │ Textual          │
└───────────┴──────────────────┘

encoder = ContinuousEncoder(drop_last=true)
mach = fit!(machine(encoder, X))
W = transform(mach, X)

julia&gt; schema(W)
┌──────────────┬────────────┐
│ names        │ scitypes   │
├──────────────┼────────────┤
│ name__Danesh │ Continuous │
│ name__John   │ Continuous │
│ name__Lee    │ Continuous │
│ grade        │ Continuous │
│ height       │ Continuous │
│ n_devices    │ Continuous │
└──────────────┴────────────┘

julia&gt; setdiff(schema(X).names, report(mach).features_to_keep) # dropped features
1-element Vector{Symbol}:
 :comments
</code></pre><p>See also <a href="#MLJModels.OneHotEncoder"><code>OneHotEncoder</code></a></p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJModels.FillImputer" href="#MLJModels.FillImputer"><code>MLJModels.FillImputer</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">FillImputer</code></pre><p>A model type for constructing a fill imputer, based on <a href="https://github.com/alan-turing-institute/MLJModels.jl">MLJModels.jl</a>, and implementing the MLJ model interface.</p><p>From MLJ, the type can be imported using</p><pre><code class="language-none">FillImputer = @load FillImputer pkg=MLJModels</code></pre><p>Do <code>model = FillImputer()</code> to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in <code>FillImputer(features=...)</code>.</p><p>Use this model to impute <code>missing</code> values in tabular data. A fixed &quot;filler&quot; value is learned from the training data, one for each column of the table.</p><p>For imputing missing values in a vector, use <a href="#MLJModels.UnivariateFillImputer"><code>UnivariateFillImputer</code></a> instead.</p><p><strong>Training data</strong></p><p>In MLJ or MLJBase, bind an instance <code>model</code> to data with</p><pre><code class="language-none">mach = machine(model, X)</code></pre><p>where</p><ul><li><code>X</code>: any table of input features (eg, a <code>DataFrame</code>) whose columns each have element scitypes <code>Union{Missing, T}</code>, where <code>T</code> is a subtype of <code>Continuous</code>, <code>Multiclass</code>, <code>OrderedFactor</code> or <code>Count</code>. Check scitypes with <code>schema(X)</code>.</li></ul><p>Train the machine using <code>fit!(mach, rows=...)</code>.</p><p><strong>Hyper-parameters</strong></p><ul><li><p><code>features</code>: a vector of names of features (symbols) for which imputation is to be attempted; default is empty, which is interpreted as &quot;impute all&quot;.</p></li><li><p><code>continuous_fill</code>: function or other callable to determine value to be imputed in the case of <code>Continuous</code> (abstract float) data; default is to apply <code>median</code> after skipping <code>missing</code> values</p></li><li><p><code>count_fill</code>: function or other callable to determine value to be imputed in the case of <code>Count</code> (integer) data; default is to apply rounded <code>median</code> after skipping <code>missing</code> values</p></li><li><p><code>finite_fill</code>: function or other callable to determine value to be imputed in the case of <code>Multiclass</code> or <code>OrderedFactor</code> data (categorical vectors); default is to apply <code>mode</code> after skipping <code>missing</code> values</p></li></ul><p><strong>Operations</strong></p><ul><li><code>transform(mach, Xnew)</code>: return <code>Xnew</code> with missing values imputed with the fill values learned when fitting <code>mach</code></li></ul><p><strong>Fitted parameters</strong></p><p>The fields of <code>fitted_params(mach)</code> are:</p><ul><li><p><code>features_seen_in_fit</code>: the names of features (columns) encountered during training</p></li><li><p><code>univariate_transformer</code>: the univariate model applied to determine   the fillers (it&#39;s fields contain the functions defining the filler computations)</p></li><li><p><code>filler_given_feature</code>: dictionary of filler values, keyed on feature (column) names</p></li></ul><p><strong>Examples</strong></p><pre><code class="language-none">using MLJ
imputer = FillImputer()

X = (a = [1.0, 2.0, missing, 3.0, missing],
     b = coerce([&quot;y&quot;, &quot;n&quot;, &quot;y&quot;, missing, &quot;y&quot;], Multiclass),
     c = [1, 1, 2, missing, 3])

schema(X)
julia&gt; schema(X)
┌───────┬───────────────────────────────┐
│ names │ scitypes                      │
├───────┼───────────────────────────────┤
│ a     │ Union{Missing, Continuous}    │
│ b     │ Union{Missing, Multiclass{2}} │
│ c     │ Union{Missing, Count}         │
└───────┴───────────────────────────────┘

mach = machine(imputer, X)
fit!(mach)

julia&gt; fitted_params(mach).filler_given_feature
(filler = 2.0,)

julia&gt; fitted_params(mach).filler_given_feature
Dict{Symbol, Any} with 3 entries:
  :a =&gt; 2.0
  :b =&gt; &quot;y&quot;
  :c =&gt; 2

julia&gt; transform(mach, X)
(a = [1.0, 2.0, 2.0, 3.0, 2.0],
 b = CategoricalValue{String, UInt32}[&quot;y&quot;, &quot;n&quot;, &quot;y&quot;, &quot;y&quot;, &quot;y&quot;],
 c = [1, 1, 2, 2, 3],)</code></pre><p>See also <a href="#MLJModels.UnivariateFillImputer"><code>UnivariateFillImputer</code></a>.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJModels.UnivariateFillImputer" href="#MLJModels.UnivariateFillImputer"><code>MLJModels.UnivariateFillImputer</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">UnivariateFillImputer</code></pre><p>A model type for constructing a single variable fill imputer, based on <a href="https://github.com/alan-turing-institute/MLJModels.jl">MLJModels.jl</a>, and implementing the MLJ model interface.</p><p>From MLJ, the type can be imported using</p><pre><code class="language-none">UnivariateFillImputer = @load UnivariateFillImputer pkg=MLJModels</code></pre><p>Do <code>model = UnivariateFillImputer()</code> to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in <code>UnivariateFillImputer(continuous_fill=...)</code>.</p><p>Use this model to imputing <code>missing</code> values in a vector with a fixed value learned from the non-missing values of training vector.</p><p>For imputing missing values in tabular data, use <a href="#MLJModels.FillImputer"><code>FillImputer</code></a> instead.</p><p><strong>Training data</strong></p><p>In MLJ or MLJBase, bind an instance <code>model</code> to data with</p><pre><code class="language-none">mach = machine(model, x)</code></pre><p>where</p><ul><li><code>x</code>: any abstract vector with element scitype <code>Union{Missing, T}</code> where <code>T</code> is a subtype of <code>Continuous</code>, <code>Multiclass</code>, <code>OrderedFactor</code> or <code>Count</code>; check scitype using <code>scitype(x)</code></li></ul><p>Train the machine using <code>fit!(mach, rows=...)</code>.</p><p><strong>Hyper-parameters</strong></p><ul><li><p><code>continuous_fill</code>: function or other callable to determine value to be imputed in the case of <code>Continuous</code> (abstract float) data; default is to apply <code>median</code> after skipping <code>missing</code> values</p></li><li><p><code>count_fill</code>: function or other callable to determine value to be imputed in the case of <code>Count</code> (integer) data; default is to apply rounded <code>median</code> after skipping <code>missing</code> values</p></li><li><p><code>finite_fill</code>: function or other callable to determine value to be imputed in the case of <code>Multiclass</code> or <code>OrderedFactor</code> data (categorical vectors); default is to apply <code>mode</code> after skipping <code>missing</code> values</p></li></ul><p><strong>Operations</strong></p><ul><li><code>transform(mach, xnew)</code>: return <code>xnew</code> with missing values imputed with the fill values learned when fitting <code>mach</code></li></ul><p><strong>Fitted parameters</strong></p><p>The fields of <code>fitted_params(mach)</code> are:</p><ul><li><code>filler</code>: the fill value to be imputed in all new data</li></ul><p><strong>Examples</strong></p><pre><code class="language-none">using MLJ
imputer = UnivariateFillImputer()

x_continuous = [1.0, 2.0, missing, 3.0]
x_multiclass = coerce([&quot;y&quot;, &quot;n&quot;, &quot;y&quot;, missing, &quot;y&quot;], Multiclass)
x_count = [1, 1, 1, 2, missing, 3, 3]

mach = machine(imputer, x_continuous)
fit!(mach)

julia&gt; fitted_params(mach)
(filler = 2.0,)

julia&gt; transform(mach, [missing, missing, 101.0])
3-element Vector{Float64}:
 2.0
 2.0
 101.0

mach2 = machine(imputer, x_multiclass) |&gt; fit!

julia&gt; transform(mach2, x_multiclass)
5-element CategoricalArray{String,1,UInt32}:
 &quot;y&quot;
 &quot;n&quot;
 &quot;y&quot;
 &quot;y&quot;
 &quot;y&quot;

mach3 = machine(imputer, x_count) |&gt; fit!

julia&gt; transform(mach3, [missing, missing, 5])
3-element Vector{Int64}:
 2
 2
 5</code></pre><p>For imputing tabular data, use <a href="#MLJModels.FillImputer"><code>FillImputer</code></a>.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJModels.FeatureSelector" href="#MLJModels.FeatureSelector"><code>MLJModels.FeatureSelector</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">FeatureSelector</code></pre><p>A model type for constructing a feature selector, based on <a href="https://github.com/alan-turing-institute/MLJModels.jl">MLJModels.jl</a>, and implementing the MLJ model interface.</p><p>From MLJ, the type can be imported using</p><pre><code class="language-none">FeatureSelector = @load FeatureSelector pkg=MLJModels</code></pre><p>Do <code>model = FeatureSelector()</code> to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in <code>FeatureSelector(features=...)</code>.</p><p>Use this model to select features (columns) of a table, usually as part of a model <code>Pipeline</code>.</p><p><strong>Training data</strong></p><p>In MLJ or MLJBase, bind an instance <code>model</code> to data with</p><pre><code class="language-none">mach = machine(model, X)</code></pre><p>where</p><ul><li><code>X</code>: any table of input features, where &quot;table&quot; is in the sense of Tables.jl</li></ul><p>Train the machine using <code>fit!(mach, rows=...)</code>.</p><p><strong>Hyper-parameters</strong></p><ul><li><p><code>features</code>: one of the following, with the behavior indicated:</p><ul><li><p><code>[]</code> (empty, the default): filter out all features (columns) which were not encountered in training</p></li><li><p>non-empty vector of feature names (symbols): keep only the specified features (<code>ignore=false</code>) or keep only unspecified features (<code>ignore=true</code>)</p></li><li><p>function or other callable: keep a feature if the callable returns <code>true</code> on its name. For example, specifying <code>FeatureSelector(features = name -&gt; name in [:x1, :x3], ignore = true)</code> has the same effect as <code>FeatureSelector(features = [:x1, :x3], ignore = true)</code>, namely to select all features, with the exception of <code>:x1</code> and <code>:x3</code>.</p></li></ul></li><li><p><code>ignore</code>: whether to ignore or keep specified <code>features</code>, as explained above</p></li></ul><p><strong>Operations</strong></p><ul><li><code>transform(mach, Xnew)</code>: select features from the table <code>Xnew</code> as specified by the model, taking features seen during training into account, if relevant</li></ul><p><strong>Fitted parameters</strong></p><p>The fields of <code>fitted_params(mach)</code> are:</p><ul><li><code>features_to_keep</code>: the features that will be selected</li></ul><p><strong>Example</strong></p><pre><code class="language-none">using MLJ

X = (ordinal1 = [1, 2, 3],
     ordinal2 = coerce([&quot;x&quot;, &quot;y&quot;, &quot;x&quot;], OrderedFactor),
     ordinal3 = [10.0, 20.0, 30.0],
     ordinal4 = [-20.0, -30.0, -40.0],
     nominal = coerce([&quot;Your father&quot;, &quot;he&quot;, &quot;is&quot;], Multiclass));

selector = FeatureSelector(features=[:ordinal3, ], ignore=true);

julia&gt; transform(fit!(machine(selector, X)), X)
(ordinal1 = [1, 2, 3],
 ordinal2 = CategoricalValue{Symbol,UInt32}[&quot;x&quot;, &quot;y&quot;, &quot;x&quot;],
 ordinal4 = [-20.0, -30.0, -40.0],
 nominal = CategoricalValue{String,UInt32}[&quot;Your father&quot;, &quot;he&quot;, &quot;is&quot;],)
</code></pre></div></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJModels.UnivariateBoxCoxTransformer" href="#MLJModels.UnivariateBoxCoxTransformer"><code>MLJModels.UnivariateBoxCoxTransformer</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">UnivariateBoxCoxTransformer</code></pre><p>A model type for constructing a single variable Box-Cox transformer, based on <a href="https://github.com/alan-turing-institute/MLJModels.jl">MLJModels.jl</a>, and implementing the MLJ model interface.</p><p>From MLJ, the type can be imported using</p><pre><code class="language-none">UnivariateBoxCoxTransformer = @load UnivariateBoxCoxTransformer pkg=MLJModels</code></pre><p>Do <code>model = UnivariateBoxCoxTransformer()</code> to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in <code>UnivariateBoxCoxTransformer(n=...)</code>.</p><p>Box-Cox transformations attempt to make data look more normally distributed. This can improve performance and assist in the interpretation of models which suppose that data is generated by a normal distribution.</p><p>A Box-Cox transformation (with shift) is of the form</p><pre><code class="language-none">x -&gt; ((x + c)^λ - 1)/λ</code></pre><p>for some constant <code>c</code> and real <code>λ</code>, unless <code>λ = 0</code>, in which case the above is replaced with</p><pre><code class="language-none">x -&gt; log(x + c)</code></pre><p>Given user-specified hyper-parameters <code>n::Integer</code> and <code>shift::Bool</code>, the present implementation learns the parameters <code>c</code> and <code>λ</code> from the training data as follows: If <code>shift=true</code> and zeros are encountered in the data, then <code>c</code> is set to <code>0.2</code> times the data mean.  If there are no zeros, then no shift is applied. Finally, <code>n</code> different values of <code>λ</code> between <code>-0.4</code> and <code>3</code> are considered, with <code>λ</code> fixed to the value maximizing normality of the transformed data.</p><p><em>Reference:</em> <a href="https://en.wikipedia.org/wiki/Power_transform">Wikipedia entry for power  transform</a>.</p><p><strong>Training data</strong></p><p>In MLJ or MLJBase, bind an instance <code>model</code> to data with</p><pre><code class="language-none">mach = machine(model, x)</code></pre><p>where</p><ul><li><code>x</code>: any abstract vector with element scitype <code>Continuous</code>; check the scitype with <code>scitype(x)</code></li></ul><p>Train the machine using <code>fit!(mach, rows=...)</code>.</p><p><strong>Hyper-parameters</strong></p><ul><li><p><code>n=171</code>: number of values of the exponent <code>λ</code> to try</p></li><li><p><code>shift=false</code>: whether to include a preliminary constant translation in transformations, in the presence of zeros</p></li></ul><p><strong>Operations</strong></p><ul><li><p><code>transform(mach, xnew)</code>: apply the Box-Cox transformation learned when fitting <code>mach</code></p></li><li><p><code>inverse_transform(mach, z)</code>: reconstruct the vector <code>z</code> whose transformation learned by <code>mach</code> is <code>z</code></p></li></ul><p><strong>Fitted parameters</strong></p><p>The fields of <code>fitted_params(mach)</code> are:</p><ul><li><p><code>λ</code>: the learned Box-Cox exponent</p></li><li><p><code>c</code>: the learned shift</p></li></ul><p><strong>Examples</strong></p><pre><code class="language-none">using MLJ
using UnicodePlots
using Random
Random.seed!(123)

transf = UnivariateBoxCoxTransformer()

x = randn(1000).^2

mach = machine(transf, x)
fit!(mach)

z = transform(mach, x)

julia&gt; histogram(x)
                ┌                                        ┐
   [ 0.0,  2.0) ┤███████████████████████████████████  848
   [ 2.0,  4.0) ┤████▌ 109
   [ 4.0,  6.0) ┤█▍ 33
   [ 6.0,  8.0) ┤▍ 7
   [ 8.0, 10.0) ┤▏ 2
   [10.0, 12.0) ┤  0
   [12.0, 14.0) ┤▏ 1
                └                                        ┘
                                 Frequency

julia&gt; histogram(z)
                ┌                                        ┐
   [-5.0, -4.0) ┤█▎ 8
   [-4.0, -3.0) ┤████████▊ 64
   [-3.0, -2.0) ┤█████████████████████▊ 159
   [-2.0, -1.0) ┤█████████████████████████████▊ 216
   [-1.0,  0.0) ┤███████████████████████████████████  254
   [ 0.0,  1.0) ┤█████████████████████████▊ 188
   [ 1.0,  2.0) ┤████████████▍ 90
   [ 2.0,  3.0) ┤██▊ 20
   [ 3.0,  4.0) ┤▎ 1
                └                                        ┘
                                 Frequency
</code></pre></div></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJModels.UnivariateDiscretizer" href="#MLJModels.UnivariateDiscretizer"><code>MLJModels.UnivariateDiscretizer</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">UnivariateDiscretizer</code></pre><p>A model type for constructing a single variable discretizer, based on <a href="https://github.com/alan-turing-institute/MLJModels.jl">MLJModels.jl</a>, and implementing the MLJ model interface.</p><p>From MLJ, the type can be imported using</p><pre><code class="language-none">UnivariateDiscretizer = @load UnivariateDiscretizer pkg=MLJModels</code></pre><p>Do <code>model = UnivariateDiscretizer()</code> to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in <code>UnivariateDiscretizer(n_classes=...)</code>.</p><p>Discretization converts a <code>Continuous</code> vector into an <code>OrderedFactor</code> vector. In particular, the output is a <code>CategoricalVector</code> (whose reference type is optimized).</p><p>The transformation is chosen so that the vector on which the transformer is fit has, in transformed form, an approximately uniform distribution of values. Specifically, if <code>n_classes</code> is the level of discretization, then <code>2*n_classes - 1</code> ordered quantiles are computed, the odd quantiles being used for transforming (discretization) and the even quantiles for inverse transforming.</p><p><strong>Training data</strong></p><p>In MLJ or MLJBase, bind an instance <code>model</code> to data with</p><pre><code class="language-none">mach = machine(model, x)</code></pre><p>where</p><ul><li><code>x</code>: any abstract vector with <code>Continuous</code> element scitype; check scitype with <code>scitype(x)</code>.</li></ul><p>Train the machine using <code>fit!(mach, rows=...)</code>.</p><p><strong>Hyper-parameters</strong></p><ul><li><code>n_classes</code>: number of discrete classes in the output</li></ul><p><strong>Operations</strong></p><ul><li><p><code>transform(mach, xnew)</code>: discretize <code>xnew</code> according to the discretization learned when fitting <code>mach</code></p></li><li><p><code>inverse_transform(mach, z)</code>: attempt to reconstruct from <code>z</code> a vector that transforms to give <code>z</code></p></li></ul><p><strong>Fitted parameters</strong></p><p>The fields of <code>fitted_params(mach).fitesult</code> include:</p><ul><li><p><code>odd_quantiles</code>: quantiles used for transforming (length is <code>n_classes - 1</code>)</p></li><li><p><code>even_quantiles</code>: quantiles used for inverse transforming (length is <code>n_classes</code>)</p></li></ul><p><strong>Warning.</strong> <code>fitted_params(mach)</code> does not have a standard form and  may change in the future.</p><p><strong>Example</strong></p><pre><code class="language-none">using MLJ
using Random
Random.seed!(123)

discretizer = UnivariateDiscretizer(n_classes=100)
mach = machine(discretizer, randn(1000))
fit!(mach)

julia&gt; x = rand(5)
5-element Vector{Float64}:
 0.6342070799721164
 0.8681793651724181
 0.43780421808821424
 0.5740792503574783
 0.22444170437768007

julia&gt; z = transform(mach, x)
5-element CategoricalArrays.CategoricalArray{UInt8,1,UInt8}:
 0x49
 0x50
 0x43
 0x47
 0x3a5

julia&gt; x_approx = inverse_transform(mach, z)
5-element Vector{Float64}:
 0.6333797607904535
 0.855839325856769
 0.433203047224622
 0.5662624832429449
 0.222065923759177</code></pre></div></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJModels.UnivariateTimeTypeToContinuous" href="#MLJModels.UnivariateTimeTypeToContinuous"><code>MLJModels.UnivariateTimeTypeToContinuous</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">UnivariateTimeTypeToContinuous</code></pre><p>A model type for constructing a single variable transformer that creates continuous representations of temporally typed data, based on <a href="https://github.com/alan-turing-institute/MLJModels.jl">MLJModels.jl</a>, and implementing the MLJ model interface.</p><p>From MLJ, the type can be imported using</p><pre><code class="language-none">UnivariateTimeTypeToContinuous = @load UnivariateTimeTypeToContinuous pkg=MLJModels</code></pre><p>Do <code>model = UnivariateTimeTypeToContinuous()</code> to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in <code>UnivariateTimeTypeToContinuous(zero_time=...)</code>.</p><p>Use this model to convert vectors with a <code>TimeType</code> element type to vectors of <code>Float64</code> type (<code>Continuous</code> element scitype).</p><p><strong>Training data</strong></p><p>In MLJ or MLJBase, bind an instance <code>model</code> to data with</p><pre><code class="language-none">mach = machine(model, x)</code></pre><p>where</p><ul><li><code>x</code>: any abstract vector whose element type is a subtype of <code>Dates.TimeType</code></li></ul><p>Train the machine using <code>fit!(mach, rows=...)</code>.</p><p><strong>Hyper-parameters</strong></p><ul><li><p><code>zero_time</code>: the time that is to correspond to 0.0 under transformations, with the type coinciding with the training data element type. If unspecified, the earliest time encountered in training is used.</p></li><li><p><code>step::Period=Hour(24)</code>: time interval to correspond to one unit under transformation</p></li></ul><p><strong>Operations</strong></p><ul><li><code>transform(mach, xnew)</code>: apply the encoding inferred when <code>mach</code> was fit</li></ul><p><strong>Fitted parameters</strong></p><p><code>fitted_params(mach).fitresult</code> is the tuple <code>(zero_time, step)</code> actually used in transformations, which may differ from the user-specified hyper-parameters.</p><p><strong>Example</strong></p><pre><code class="language-none">using MLJ
using Dates

x = [Date(2001, 1, 1) + Day(i) for i in 0:4]

encoder = UnivariateTimeTypeToContinuous(zero_time=Date(2000, 1, 1),
                                         step=Week(1))

mach = machine(encoder, x)
fit!(mach)
julia&gt; transform(mach, x)
5-element Vector{Float64}:
 52.285714285714285
 52.42857142857143
 52.57142857142857
 52.714285714285715
 52.857142</code></pre></div></section></article><h2 id="Static-transformers"><a class="docs-heading-anchor" href="#Static-transformers">Static transformers</a><a id="Static-transformers-1"></a><a class="docs-heading-anchor-permalink" href="#Static-transformers" title="Permalink"></a></h2><p>The main use-case for static transformers is for insertion into <a href="../linear_pipelines/#Linear-Pipelines">Linear Pipelines</a> or other exported learning networks (see <a href="../composing_models/#Composing-Models">Composing Models</a>). If a static transformer has no hyper-parameters, it is tantamount to an ordinary function. An ordinary function can be inserted directly into a pipeline; the situation for learning networks is only slightly more complicated; see <a href="../composing_models/#Static-operations-on-nodes">Static operations on nodes</a>.</p><p>The following example defines a new model type <code>Averager</code> to perform the weighted average of two vectors (target predictions, for example). We suppose the weighting is normalized, and therefore controlled by a single hyper-parameter, <code>mix</code>.</p><pre><code class="language-julia">mutable struct Averager &lt;: Static
    mix::Float64
end

MLJ.transform(a::Averager, _, y1, y2) = (1 - a.mix)*y1 + a.mix*y2</code></pre><p><em>Important.</em> Note the sub-typing <code>&lt;: Static</code>.</p><p>Such static transformers with (unlearned) parameters can have arbitrarily many inputs, but only one output. In the single input case an <code>inverse_transform</code> can also be defined. Since they have no real learned parameters, you bind a static transformer to a machine without specifying training arguments.</p><pre><code class="language-julia">mach = machine(Averager(0.5)) |&gt; fit!
transform(mach, [1, 2, 3], [3, 2, 1])</code></pre><pre class="documenter-example-output">3-element Vector{Float64}:
 2.0
 2.0
 2.0</pre><p>Let&#39;s see how we can include our <code>Averager</code> in a learning network (see <a href="../composing_models/#Composing-Models">Composing Models</a>) to mix the predictions of two regressors, with one-hot encoding of the inputs:</p><pre><code class="language-julia">X = source()
y = source()

ridge = (@load RidgeRegressor pkg=MultivariateStats)()
knn = (@load KNNRegressor)()
averager = Averager(0.5)

hotM = machine(OneHotEncoder(), X)
W = transform(hotM, X) # one-hot encode the input

ridgeM = machine(ridge, W, y)
y1 = predict(ridgeM, W)

knnM = machine(knn, W, y)
y2 = predict(knnM, W)

averagerM= machine(averager)
yhat = transform(averagerM, y1, y2)</code></pre><pre class="documenter-example-output">Node{Machine{Averager,…}}
  args:
    1:	Node{Machine{RidgeRegressor,…}}
    2:	Node{Machine{KNNRegressor,…}}
  formula:
    transform(
        Machine{Averager,…}, 
        predict(
            Machine{RidgeRegressor,…}, 
            transform(
                Machine{OneHotEncoder,…}, 
                Source @696)),
        predict(
            Machine{KNNRegressor,…}, 
            transform(
                Machine{OneHotEncoder,…}, 
                Source @696)))</pre><p>Now we export to obtain a <code>Deterministic</code> composite model and then instantiate composite model</p><pre><code class="language-julia">learning_mach = machine(Deterministic(), X, y; predict=yhat)
Machine{DeterministicSurrogate} @772 trained 0 times.
  args:
    1:	Source @415 ⏎ `Unknown`
    2:	Source @389 ⏎ `Unknown`


@from_network learning_mach struct DoubleRegressor
       regressor1=ridge
       regressor2=knn
       averager=averager
       end

composite = DoubleRegressor()
julia&gt; composite = DoubleRegressor()
DoubleRegressor(
    regressor1 = RidgeRegressor(
            lambda = 1.0),
    regressor2 = KNNRegressor(
            K = 5,
            algorithm = :kdtree,
            metric = Distances.Euclidean(0.0),
            leafsize = 10,
            reorder = true,
            weights = :uniform),
    averager = Averager(
            mix = 0.5)) @301
</code></pre><p>which can be can be evaluated like any other model:</p><pre><code class="language-julia">composite.averager.mix = 0.25 # adjust mix from default of 0.5
julia&gt; evaluate(composite, (@load_reduced_ames)..., measure=rms)
Evaluating over 6 folds: 100%[=========================] Time: 0:00:00
┌───────────┬───────────────┬────────────────────────────────────────────────────────┐
│ _.measure │ _.measurement │ _.per_fold                                             │
├───────────┼───────────────┼────────────────────────────────────────────────────────┤
│ rms       │ 26800.0       │ [21400.0, 23700.0, 26800.0, 25900.0, 30800.0, 30700.0] │
└───────────┴───────────────┴────────────────────────────────────────────────────────┘
_.per_observation = [missing]
_.fitted_params_per_fold = [ … ]
_.report_per_fold = [ … ]</code></pre><h2 id="Transformers-that-also-predict"><a class="docs-heading-anchor" href="#Transformers-that-also-predict">Transformers that also predict</a><a id="Transformers-that-also-predict-1"></a><a class="docs-heading-anchor-permalink" href="#Transformers-that-also-predict" title="Permalink"></a></h2><p>Some clustering algorithms learn to label data by identifying a collection of &quot;centroids&quot; in the training data. Any new input observation is labeled with the cluster to which it is closest (this is the output of <code>predict</code>) while the vector of all distances from the centroids defines a lower-dimensional representation of the observation (the output of <code>transform</code>). In the following example a K-means clustering algorithm assigns one of three labels 1, 2, 3 to the input features of the iris data set and compares them with the actual species recorded in the target (not seen by the algorithm).</p><pre><code class="language-julia">import Random.seed!
seed!(123)

X, y = @load_iris;
KMeans = @load KMeans pkg=ParallelKMeans
kmeans = KMeans()
mach = machine(kmeans, X) |&gt; fit!

# transforming:
Xsmall = transform(mach);
selectrows(Xsmall, 1:4) |&gt; pretty
julia&gt; selectrows(Xsmall, 1:4) |&gt; pretty
┌─────────────────────┬────────────────────┬────────────────────┐
│ x1                  │ x2                 │ x3                 │
│ Float64             │ Float64            │ Float64            │
│ Continuous          │ Continuous         │ Continuous         │
├─────────────────────┼────────────────────┼────────────────────┤
│ 0.0215920000000267  │ 25.314260355029603 │ 11.645232464391299 │
│ 0.19199200000001326 │ 25.882721893491123 │ 11.489658693899486 │
│ 0.1699920000000077  │ 27.58656804733728  │ 12.674412792260142 │
│ 0.26919199999998966 │ 26.28656804733727  │ 11.64392098898145  │
└─────────────────────┴────────────────────┴────────────────────┘

# predicting:
yhat = predict(mach);
compare = zip(yhat, y) |&gt; collect;
compare[1:8]
8-element Array{Tuple{CategoricalValue{Int64,UInt32},CategoricalString{UInt32}},1}:
 (1, &quot;setosa&quot;)
 (1, &quot;setosa&quot;)
 (1, &quot;setosa&quot;)
 (1, &quot;setosa&quot;)
 (1, &quot;setosa&quot;)
 (1, &quot;setosa&quot;)
 (1, &quot;setosa&quot;)
 (1, &quot;setosa&quot;)

compare[51:58]
8-element Array{Tuple{CategoricalValue{Int64,UInt32},CategoricalString{UInt32}},1}:
 (2, &quot;versicolor&quot;)
 (3, &quot;versicolor&quot;)
 (2, &quot;versicolor&quot;)
 (3, &quot;versicolor&quot;)
 (3, &quot;versicolor&quot;)
 (3, &quot;versicolor&quot;)
 (3, &quot;versicolor&quot;)
 (3, &quot;versicolor&quot;)

compare[101:108]
8-element Array{Tuple{CategoricalValue{Int64,UInt32},CategoricalString{UInt32}},1}:
 (2, &quot;virginica&quot;)
 (3, &quot;virginica&quot;)
 (2, &quot;virginica&quot;)
 (2, &quot;virginica&quot;)
 (2, &quot;virginica&quot;)
 (2, &quot;virginica&quot;)
 (3, &quot;virginica&quot;)
 (2, &quot;virginica&quot;)</code></pre></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../preparing_data/">« Preparing Data</a><a class="docs-footer-nextpage" href="../more_on_probabilistic_predictors/">More on Probablistic Predictors »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Tuesday 19 April 2022 22:30">Tuesday 19 April 2022</span>. Using Julia version 1.7.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
