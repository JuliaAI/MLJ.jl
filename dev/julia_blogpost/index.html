<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Julia BlogPost · MLJ</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.svg" alt="MLJ logo"/></a><div class="docs-package-name"><span class="docs-autofit">MLJ</span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Introduction</a></li><li><a class="tocitem" href="../getting_started/">Getting Started</a></li><li><a class="tocitem" href="../common_mlj_workflows/">Common MLJ Workflows</a></li><li><a class="tocitem" href="../working_with_categorical_data/">Working with Categorical Data</a></li><li><a class="tocitem" href="../model_search/">Model Search</a></li><li><a class="tocitem" href="../machines/">Machines</a></li><li><a class="tocitem" href="../evaluating_model_performance/">Evaluating Model Performance</a></li><li><a class="tocitem" href="../performance_measures/">Performance Measures</a></li><li><a class="tocitem" href="../tuning_models/">Tuning Models</a></li><li><a class="tocitem" href="../learning_curves/">Learning Curves</a></li><li><a class="tocitem" href="../transformers/">Transformers and other unsupervised models</a></li><li><a class="tocitem" href="../composing_models/">Composing Models</a></li><li><a class="tocitem" href="../homogeneous_ensembles/">Homogeneous Ensembles</a></li><li><a class="tocitem" href="../generating_synthetic_data/">Generating Synthetic Data</a></li><li><a class="tocitem" href="../openml_integration/">OpenML Integration</a></li><li><a class="tocitem" href="../simple_user_defined_models/">Simple User Defined Models</a></li><li><a class="tocitem" href="../quick_start_guide_to_adding_models/">Quick-Start Guide to Adding Models</a></li><li><a class="tocitem" href="../adding_models_for_general_use/">Adding Models for General Use</a></li><li><a class="tocitem" href="../benchmarking/">Benchmarking</a></li><li><a class="tocitem" href="../internals/">Internals</a></li><li><a class="tocitem" href="../list_of_supported_models/">List of Supported Models</a></li><li><a class="tocitem" href="../glossary/">Glossary</a></li><li><a class="tocitem" href="../mlj_cheatsheet/">MLJ Cheatsheet</a></li><li><a class="tocitem" href="../frequently_asked_questions/">FAQ</a></li><li class="is-active"><a class="tocitem" href>Julia BlogPost</a><ul class="internal"><li><a class="tocitem" href="#Introducing-MLJ-1"><span>Introducing MLJ</span></a></li><li><a class="tocitem" href="#MLJ-features-1"><span>MLJ features</span></a></li><li><a class="tocitem" href="#Learning-networks-1"><span>Learning networks</span></a></li><li><a class="tocitem" href="#Building-a-simple-network-1"><span>Building a simple network</span></a></li><li><a class="tocitem" href="#Exporting-and-retraining-1"><span>Exporting and retraining</span></a></li><li><a class="tocitem" href="#Just-&quot;Write-the-math!&quot;-1"><span>Just &quot;Write the math!&quot;</span></a></li><li><a class="tocitem" href="#Invitation-to-the-community-1"><span>Invitation to the community</span></a></li></ul></li><li><a class="tocitem" href="../acceleration_and_parallelism/">Acceleration and Parallelism</a></li><li><a class="tocitem" href="../api/">Index of Methods</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Julia BlogPost</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Julia BlogPost</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/alan-turing-institute/MLJ.jl/blob/master/docs/src/julia_blogpost.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><div class="admonition is-warning"><header class="admonition-header">Old post</header><div class="admonition-body"><p>This post is quite old. For a newer overview of the design of MLJ, see <a href="https://github.com/alan-turing-institute/MLJ.jl/blob/master/paper/paper.md">here</a></p></div></div><h1 id="Beyond-machine-learning-pipelines-with-MLJ-1"><a class="docs-heading-anchor" href="#Beyond-machine-learning-pipelines-with-MLJ-1">Beyond machine learning pipelines with MLJ</a><a class="docs-heading-anchor-permalink" href="#Beyond-machine-learning-pipelines-with-MLJ-1" title="Permalink"></a></h1><p>Anthony Blaom, Diego Arenas, Franz Kiraly, Yiannis Simillides, Sebastian Vollmer</p><p><strong>May 1st, 2019.</strong> Blog post also posted on the <a href="https://julialang.org/blog/2019/05/beyond-ml-pipelines-with-mlj">Julia Language Blog</a></p><table><tr><th style="text-align: right"><img src="../img/learningcurves.png" alt/></th><th style="text-align: right"><img src="../img/heatmap.png" alt/></th></tr><tr><td style="text-align: right"><img src="../img/wrapped_ridge.png" alt/></td><td style="text-align: right"><img src="../img/MLPackages.png" alt/></td></tr></table><h2 id="Introducing-MLJ-1"><a class="docs-heading-anchor" href="#Introducing-MLJ-1">Introducing MLJ</a><a class="docs-heading-anchor-permalink" href="#Introducing-MLJ-1" title="Permalink"></a></h2><p><a href="https://github.com/alan-turing-institute/MLJ.jl">MLJ</a> is an open-source machine learning toolbox written in pure Julia. It provides a uniform interface for interacting with supervised and unsupervised learning models currently scattered in different Julia packages.</p><p>Building on a earlier proof-of-concept, development began in earnest at <a href="https://www.turing.ac.uk">The Alan Turing Institute</a> in December 2018. In a short time interest grew and the project is now the Institute&#39;s most starred software repository.</p><p>After outlining MLJ&#39;s current functionality, this post introduces MLJ <strong>learning networks</strong>, a super-charged pipelining feature for model composition.</p><p><strong>Quick links:</strong></p><ul><li><p><a href="https://alan-turing-institute.github.io/MLJ.jl/dev/frequently_asked_questions/">MLJ vs ScikitLearn.jl</a>  </p></li><li><p>Video from <a href="https://www.youtube.com/watch?v=CfHkjNmj1eE">London Julia User Group meetup in March 2019</a> (skip to <a href="https://youtu.be/CfHkjNmj1eE?t=21m39s">demo at 21&#39;39</a>) &amp;nbsp;</p></li><li><p><a href="https://alan-turing-institute.github.io/MLJTutorials/">MLJ Tutorials</a></p></li><li><p>Implementing the MLJ interface for a <a href="https://alan-turing-institute.github.io/MLJ.jl/dev/adding_models_for_general_use/">new model</a></p></li><li><p>How to <a href="https://github.com/alan-turing-institute/MLJ.jl/blob/master/CONTRIBUTE.md">contribute</a></p></li><li><p>Julia <a href="http://julialang.slack.com">Slack</a> channel: #mlj.</p></li><li><p>Star&#39;ing us to show support for <a href="https://github.com/alan-turing-institute/MLJ.jl">MLJ</a> would be greatly appreciated!</p></li></ul><h2 id="MLJ-features-1"><a class="docs-heading-anchor" href="#MLJ-features-1">MLJ features</a><a class="docs-heading-anchor-permalink" href="#MLJ-features-1" title="Permalink"></a></h2><p>MLJ already has substantial functionality:</p><ul><li><p><strong>Learning networks.</strong> Flexible model composition beyond traditional pipelines (more on this below).</p></li><li><p><strong>Automatic tuning.</strong> Automated tuning of hyperparameters, including composite models. Tuning implemented as a model wrapper for composition with other meta-algorithms.</p></li><li><p><strong>Homogeneous model ensembling.</strong></p></li><li><p><strong>Registry for model metadata.</strong> Metadata available without loading model code. Basis of a &quot;task&quot; interface and facilitates model composition.</p></li><li><p><strong>Task interface.</strong> Automatically match models to specified learning tasks, to streamline benchmarking and model selection.</p></li><li><p><strong>Clean probabilistic API.</strong> Improves support for Bayesian statistics and probabilistic graphical models.</p></li><li><p><strong>Data container agnostic.</strong> Present and manipulate data in your favorite Tables.jl format.</p></li><li><p><strong>Universal adoption of categorical data types.</strong> Enables model implementations to properly account for classes seen in training but not in evaluation.</p></li></ul><p>Enhancements planned for the near future include integration of Flux.jl <strong>deep learning</strong> models, and <strong>gradient descent tuning</strong> of continuous hyperparameters using automatic differentiation.</p><p>While a relatively small number of machine learning models currently implement the MLJ interface, work in progress aims to wrap models supported by the popular python framework, scikit-learn, as a temporary expedient. For a comparison of the MLJ&#39;s design with the Julia wrap <a href="https://github.com/cstjean/ScikitLearn.jl">ScitLearn.jl</a>, see this <a href="https://github.com/alan-turing-institute/MLJ.jl/blob/master/docs/src/frequently_asked_questions.md">FAQ</a>.</p><h2 id="Learning-networks-1"><a class="docs-heading-anchor" href="#Learning-networks-1">Learning networks</a><a class="docs-heading-anchor-permalink" href="#Learning-networks-1" title="Permalink"></a></h2><p>MLJ&#39;s model composition interface is flexible enough to implement, for example, the <a href="https://www.kdnuggets.com/2017/02/stacking-models-imropved-predictions.html">model stacks</a> popular in data science competitions. To treat examples of this kind, the interface design must account for the fact that information flow in prediction and training modes is different. This can be seen from the following schematic of a simple two-model stack, viewed as a network:</p><p><img src="../img/two_model_stack.png" alt/></p><h2 id="Building-a-simple-network-1"><a class="docs-heading-anchor" href="#Building-a-simple-network-1">Building a simple network</a><a class="docs-heading-anchor-permalink" href="#Building-a-simple-network-1" title="Permalink"></a></h2><p>In MLJ, networks of models are built using a declarative syntax already familiar from basic use of the package. For example, the ordinary syntax for training a decision tree in MLJ, after one-hot encoding the categorical features, looks like this:</p><pre><code class="language-julia">using MLJ
@load DecisionTreeRegressor

# load some data:
task = load_reduced_ames();
X, y = task();

# one-hot encode the inputs, X:
hot_model = OneHotEncoder()
hot = machine(hot_model, X)
fit!(hot)
Xt = transform(hot, X)

# fit a decision tree to the transformed data:
tree_model = DecisionTreeRegressor()
tree = machine(tree_model, Xt, y)
fit!(tree, rows = 1:1300)</code></pre><p>Note that a <em>model</em> in MLJ is just a struct containing hyperparameters. Wrapping a model in data delivers a <em>machine</em> struct, which will additionally record the results of training.</p><p>Without a pipeline, each time we want to present new data for prediction we must first apply one-hot encoding:</p><pre><code class="language-julia">Xnew = X[1301:1400,:];
Xnewt = transform(hot, Xnew);
yhat = predict(tree, Xnewt);
yhat[1:3]
 3-element Array{Float64,1}:
  223956.9999999999
  320142.85714285733
  161227.49999999994</code></pre><p>To build a pipeline one simply wraps the supplied data in source nodes and repeats similar declarations, omitting calls to <code>fit!</code>. The difference now is that each &quot;variable&quot; (e.g., <code>Xt</code>, <code>yhat</code>) is a node of our pipeline, instead of concrete data:</p><pre><code class="language-julia">Xs = source(X)
ys = source(y)

hot = machine(hot_model, Xs)
Xt = transform(hot, Xs);

tree = machine(tree_model, Xt, ys)
yhat = predict(tree, Xt)</code></pre><p>If we like, we can think of a node as <em>dynamic data</em> - &quot;data&quot; because it can be called (indexed) on rows, but &quot;dynamic&quot; because the result depends on the outcome of training events, which in turn depend on hyperparameter values. For example, after fitting the completed pipeline, we can make new predictions like this:</p><pre><code class="language-julia">fit!(yhat, rows=1:1300)
 [ Info: Training NodalMachine @ 1…51.
 [ Info: Spawned 1300 sub-features to one-hot encode feature :Neighborhood.
 [ Info: Spawned 1300 sub-features to one-hot encode feature :MSSubClass.
 [ Info: Training NodalMachine @ 1…17.
 Node @ 1…79 = predict(1…17, transform(1…51, 1…07))

yhat(rows=1301:1302) # to predict on rows of source node
yhat(Xnew)           # to predict on new data
156-element Array{Float64,1}:
 223956.9999999999
 320142.85714285733
 ...</code></pre><h2 id="Exporting-and-retraining-1"><a class="docs-heading-anchor" href="#Exporting-and-retraining-1">Exporting and retraining</a><a class="docs-heading-anchor-permalink" href="#Exporting-and-retraining-1" title="Permalink"></a></h2><p>Once a pipeline like this has been built and tested on sample data, it can be exported as a stand-alone model, ready to be trained on any dataset. For details, see the MLJ <a href="https://alan-turing-institute.github.io/MLJ.jl/dev/learning_networks/">documentation</a>. In the future, Julia macros will allow common architectures (e.g., linear pipelines) to be built in a couple of lines.</p><p>Finally, we mention that MLJ learning networks, and their exported counterparts, are &quot;smart&quot; in the sense that changing a hyperparameter does not trigger retraining of component models upstream of the change:</p><pre><code class="language-julia">tree_model.max_depth = 4
fit!(yhat, rows=1:1300)
 [ Info: Not retraining NodalMachine @ 1…51. It is up-to-date.
 [ Info: Updating NodalMachine @ 1…17.
 Node @ 1…79 = predict(1…17, transform(1…51, 1…07))</code></pre><h2 id="Just-&quot;Write-the-math!&quot;-1"><a class="docs-heading-anchor" href="#Just-&quot;Write-the-math!&quot;-1">Just &quot;Write the math!&quot;</a><a class="docs-heading-anchor-permalink" href="#Just-&quot;Write-the-math!&quot;-1" title="Permalink"></a></h2><p>Because of Julia&#39;s generic programming features, any kind of operation you would normally apply to data (arithmetic, row selection, column concatenation, etc) can be overloaded to work with nodes. In this way, MLJ&#39;s network-building syntax is economical, intuitive and easy to read. In this respect we have been inspired by <a href="https://julialang.org/blog/2017/12/ml&amp;pl">On Machine Learning and Programming Languages</a>.</p><h2 id="Invitation-to-the-community-1"><a class="docs-heading-anchor" href="#Invitation-to-the-community-1">Invitation to the community</a><a class="docs-heading-anchor-permalink" href="#Invitation-to-the-community-1" title="Permalink"></a></h2><p>We now invite the community to try out our newly registered packages, <a href="https://github.com/alan-turing-institute/MLJ.jl">MLJ</a>alongside <a href="https://github.com/alan-turing-institute/MLJModels.jl">MLJModels</a>, and provide any feedback or suggestions you may have going forward. We are also particularly interested in hearing how you would use our package, and what features it may be lacking.</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../frequently_asked_questions/">« FAQ</a><a class="docs-footer-nextpage" href="../acceleration_and_parallelism/">Acceleration and Parallelism »</a></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Friday 27 November 2020 21:29">Friday 27 November 2020</span>. Using Julia version 1.2.0.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
