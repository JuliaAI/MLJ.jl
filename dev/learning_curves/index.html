<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Learning Curves · MLJ</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit">MLJ</span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Getting Started</a></li><li><a class="tocitem" href="../common_mlj_workflows/">Common MLJ Workflows</a></li><li><a class="tocitem" href="../model_search/">Model Search</a></li><li><a class="tocitem" href="../machines/">Machines</a></li><li><a class="tocitem" href="../evaluating_model_performance/">Evaluating Model Performance</a></li><li><a class="tocitem" href="../performance_measures/">Performance Measures</a></li><li><a class="tocitem" href="../tuning_models/">Tuning Models</a></li><li class="is-active"><a class="tocitem" href>Learning Curves</a><ul class="internal"><li><a class="tocitem" href="#API-reference-1"><span>API reference</span></a></li></ul></li><li><a class="tocitem" href="../built_in_transformers/">Built-in Transformers</a></li><li><a class="tocitem" href="../composing_models/">Composing Models</a></li><li><a class="tocitem" href="../homogeneous_ensembles/">Homogeneous Ensembles</a></li><li><a class="tocitem" href="../simple_user_defined_models/">Simple User Defined Models</a></li><li><a class="tocitem" href="../quick_start_guide_to_adding_models/">Quick-Start Guide to Adding Models</a></li><li><a class="tocitem" href="../adding_models_for_general_use/">Adding Models for General Use</a></li><li><a class="tocitem" href="../benchmarking/">Benchmarking</a></li><li><a class="tocitem" href="../internals/">Internals</a></li><li><a class="tocitem" href="../glossary/">Glossary</a></li><li><a class="tocitem" href="../mlj_cheatsheet/">MLJ Cheatsheet</a></li><li><a class="tocitem" href="../NEWS/">MLJ News</a></li><li><a class="tocitem" href="../frequently_asked_questions/">FAQ</a></li><li><a class="tocitem" href="../julia_blogpost/">Julia BlogPost</a></li><li><a class="tocitem" href="../acceleration_and_parallelism/">Acceleration and Parallelism</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Learning Curves</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Learning Curves</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/alan-turing-institute/MLJ.jl/blob/master/docs/src/learning_curves.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Learning-Curves-1"><a class="docs-heading-anchor" href="#Learning-Curves-1">Learning Curves</a><a class="docs-heading-anchor-permalink" href="#Learning-Curves-1" title="Permalink"></a></h1><p>A <em>learning curve</em> in MLJ is a plot of some performance estimate, as a function of some model hyperparameter. This can be useful when tuning a single model hyperparameter, or when deciding how many iterations are required for some iterative model. The <code>learning_curve</code> method does not actually generate a plot, but generates the data needed to do so.</p><p>To generate learning curves you can bind data to a model by instantiating a machine. You can choose to supply all available data, as performance estimates are computed using a resampling strategy, defaulting to <code>Holdout(fraction_train=0.7)</code>.</p><pre><code class="language-julia">X, y = @load_boston;

atom = @load RidgeRegressor pkg=MultivariateStats
ensemble = EnsembleModel(atom=atom, n=1000)
mach = machine(ensemble, X, y)

r_lambda = range(ensemble, :(atom.lambda), lower=10, upper=500, scale=:log10)
curve = MLJ.learning_curve(mach;
                           range=r_lambda,
                           resampling=CV(nfolds=3),
                           measure=mav)
using Plots
plot(curve.parameter_values,
     curve.measurements,
     xlab=curve.parameter_name,
     xscale=curve.parameter_scale,
     ylab = &quot;CV estimate of RMS error&quot;)</code></pre><p><img src="../img/learning_curve42.png" alt/></p><p>In the case the <code>range</code> hyperparameter is the number of iterations in some iterative model, <code>learning_curve</code> will not restart the training from scratch for each new value, unless a non-holdout <code>resampling</code> strategy is specified (and provided the model implements an appropriate <code>update</code> method). To obtain multiple curves (that are distinct) you will need to pass the name of the model random number generator, <code>rng_name</code>, and specify the random number generators to be used using <code>rngs=...</code> (an integer automatically generates the number specified):</p><pre><code class="language-julia">atom.lambda=200
r_n = range(ensemble, :n, lower=1, upper=50)
curves = MLJ.learning_curve(mach;
                             range=r_n,
                             verbosity=0,
                             rng_name=:rng,
                             rngs=4)
plot(curves.parameter_values,
     curves.measurements,
     xlab=curves.parameter_name,
     ylab=&quot;Holdout estimate of RMS error&quot;)</code></pre><p><img src="../img/learning_curve_n.png" alt/></p><h2 id="API-reference-1"><a class="docs-heading-anchor" href="#API-reference-1">API reference</a><a class="docs-heading-anchor-permalink" href="#API-reference-1" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="MLJTuning.learning_curve" href="#MLJTuning.learning_curve"><code>MLJTuning.learning_curve</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">curve = learning_curve(mach; resolution=30,
                             resampling=Holdout(),
                             repeats=1,
                             measure=rms,
                             weights=nothing,
                             operation=predict,
                             range=nothing,
                             acceleration=default_resource(),
                             acceleration_grid=CPU1(),
                             rngs=nothing,
                             rng_name=nothing)</code></pre><p>Given a supervised machine <code>mach</code>, returns a named tuple of objects suitable for generating a plot of performance estimates, as a function of the single hyperparameter specified in <code>range</code>. The tuple <code>curve</code> has the following keys: <code>:parameter_name</code>, <code>:parameter_scale</code>, <code>:parameter_values</code>, <code>:measurements</code>.</p><p>To generate multiple curves for a <code>model</code> with a random number generator (RNG) as a hyperparameter, specify the name of the (possibly nested) RNG field, and a vector <code>rngs</code> of RNG&#39;s, one for each curve. Alternatively, set <code>rngs</code> to the number of curves desired, in which case RNG&#39;s are automatically generated. The individual curve computations can be distributed across multiple processes using <code>acceleration=CPUProcesses()</code>. See the second example below for a demonstration.</p><pre><code class="language-julia">X, y = @load_boston;
atom = @load RidgeRegressor pkg=MultivariateStats
ensemble = EnsembleModel(atom=atom, n=1000)
mach = machine(ensemble, X, y)
r_lambda = range(ensemble, :(atom.lambda), lower=10, upper=500, scale=:log10)
curve = learning_curve(mach; range=r_lambda, resampling=CV(), measure=mav)
using Plots
plot(curve.parameter_values,
     curve.measurements,
     xlab=curve.parameter_name,
     xscale=curve.parameter_scale,
     ylab = &quot;CV estimate of RMS error&quot;)</code></pre><p>If using a <code>Holdout()</code> <code>resampling</code> strategy (with no shuffling) and if the specified hyperparameter is the number of iterations in some iterative model (and that model has an appropriately overloaded <code>MLJBase.update</code> method) then training is not restarted from scratch for each increment of the parameter, ie the model is trained progressively.</p><pre><code class="language-julia">atom.lambda=200
r_n = range(ensemble, :n, lower=1, upper=250)
curves = learning_curve(mach; range=r_n, verbosity=0, rng_name=:rng, rngs=3)
plot!(curves.parameter_values,
     curves.measurements,
     xlab=curves.parameter_name,
     ylab=&quot;Holdout estimate of RMS error&quot;)

</code></pre><pre><code class="language-none">learning_curve(model::Supervised, X, y; kwargs...)
learning_curve(model::Supervised, X, y, w; kwargs...)</code></pre><p>Plot a learning curve (or curves) directly, without first constructing a machine.</p></div></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../tuning_models/">« Tuning Models</a><a class="docs-footer-nextpage" href="../built_in_transformers/">Built-in Transformers »</a></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Thursday 13 February 2020 05:50">Thursday 13 February 2020</span>. Using Julia version 1.2.0.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
