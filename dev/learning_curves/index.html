<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Learning Curves · MLJ</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.svg" alt="MLJ logo"/></a><div class="docs-package-name"><span class="docs-autofit">MLJ</span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../about_mlj/">About MLJ</a></li><li><a class="tocitem" href="../getting_started/">Getting Started</a></li><li><a class="tocitem" href="../common_mlj_workflows/">Common MLJ Workflows</a></li><li><a class="tocitem" href="../working_with_categorical_data/">Working with Categorical Data</a></li><li><a class="tocitem" href="../model_search/">Model Search</a></li><li><a class="tocitem" href="../loading_model_code/">Loading Model Code</a></li><li><a class="tocitem" href="../machines/">Machines</a></li><li><a class="tocitem" href="../evaluating_model_performance/">Evaluating Model Performance</a></li><li><a class="tocitem" href="../performance_measures/">Performance Measures</a></li><li><a class="tocitem" href="../weights/">Weights</a></li><li><a class="tocitem" href="../tuning_models/">Tuning Models</a></li><li class="is-active"><a class="tocitem" href>Learning Curves</a><ul class="internal"><li><a class="tocitem" href="#API-reference"><span>API reference</span></a></li></ul></li><li><a class="tocitem" href="../preparing_data/">Preparing Data</a></li><li><a class="tocitem" href="../transformers/">Transformers and Other Unsupervised models</a></li><li><a class="tocitem" href="../more_on_probabilistic_predictors/">More on Probablistic Predictors</a></li><li><a class="tocitem" href="../composing_models/">Composing Models</a></li><li><a class="tocitem" href="../linear_pipelines/">Linear Pipelines</a></li><li><a class="tocitem" href="../target_transformations/">Target Transformations</a></li><li><a class="tocitem" href="../homogeneous_ensembles/">Homogeneous Ensembles</a></li><li><a class="tocitem" href="../model_stacking/">Model Stacking</a></li><li><a class="tocitem" href="../controlling_iterative_models/">Controlling Iterative Models</a></li><li><a class="tocitem" href="../generating_synthetic_data/">Generating Synthetic Data</a></li><li><a class="tocitem" href="../openml_integration/">OpenML Integration</a></li><li><a class="tocitem" href="../acceleration_and_parallelism/">Acceleration and Parallelism</a></li><li><a class="tocitem" href="../simple_user_defined_models/">Simple User Defined Models</a></li><li><a class="tocitem" href="../quick_start_guide_to_adding_models/">Quick-Start Guide to Adding Models</a></li><li><a class="tocitem" href="../adding_models_for_general_use/">Adding Models for General Use</a></li><li><a class="tocitem" href="../modifying_behavior/">Modifying Behavior</a></li><li><a class="tocitem" href="../internals/">Internals</a></li><li><a class="tocitem" href="../list_of_supported_models/">List of Supported Models</a></li><li><a class="tocitem" href="../third_party_packages/">Third Party Packages</a></li><li><a class="tocitem" href="../glossary/">Glossary</a></li><li><a class="tocitem" href="../mlj_cheatsheet/">MLJ Cheatsheet</a></li><li><a class="tocitem" href="../known_issues/">Known Issues</a></li><li><a class="tocitem" href="../frequently_asked_questions/">FAQ</a></li><li><a class="tocitem" href="../julia_blogpost/">Julia BlogPost</a></li><li><a class="tocitem" href="../api/">Index of Methods</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Learning Curves</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Learning Curves</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/alan-turing-institute/MLJ.jl/blob/master/docs/src/learning_curves.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Learning-Curves"><a class="docs-heading-anchor" href="#Learning-Curves">Learning Curves</a><a id="Learning-Curves-1"></a><a class="docs-heading-anchor-permalink" href="#Learning-Curves" title="Permalink"></a></h1><p>A <em>learning curve</em> in MLJ is a plot of some performance estimate, as a function of some model hyperparameter. This can be useful when tuning a single model hyperparameter, or when deciding how many iterations are required for some iterative model. The <code>learning_curve</code> method does not actually generate a plot, but generates the data needed to do so.</p><p>To generate learning curves you can bind data to a model by instantiating a machine. You can choose to supply all available data, as performance estimates are computed using a resampling strategy, defaulting to <code>Holdout(fraction_train=0.7)</code>.</p><pre><code class="language-julia">using MLJ
X, y = @load_boston;

atom = (@load RidgeRegressor pkg=MLJLinearModels)()
ensemble = EnsembleModel(model=atom, n=1000)
mach = machine(ensemble, X, y)

r_lambda = range(ensemble, :(model.lambda), lower=1e-1, upper=100, scale=:log10)
curve = MLJ.learning_curve(mach;
                           range=r_lambda,
                           resampling=CV(nfolds=3),
                           measure=MeanAbsoluteError())</code></pre><pre class="documenter-example-output">(parameter_name = &quot;model.lambda&quot;,
 parameter_scale = :log10,
 parameter_values = [0.1, 0.12689610031679222, 0.16102620275609392, 0.20433597178569418, 0.25929437974046676, 0.3290344562312668, 0.41753189365604015, 0.5298316906283709, 0.6723357536499337, 0.8531678524172808  …  11.7210229753348, 14.87352107293511, 18.873918221350976, 23.95026619987486, 30.391953823131978, 38.56620421163473, 48.93900918477494, 62.10169418915616, 78.80462815669912, 100.0],
 measurements = [6.602045244188644, 6.561177127509054, 6.491764697458604, 6.358455280917016, 6.2219851984414944, 6.081292996216969, 5.960323489805503, 5.783126251373872, 5.598308185774324, 5.43164436273311  …  4.792565574071091, 4.911144982385355, 5.031127497735947, 5.182514320682574, 5.350666955609704, 5.544994831336472, 5.754714417109406, 5.927610848291631, 6.107438348736913, 6.303621146147894],)</pre><pre><code class="language-julia">using Plots
plot(curve.parameter_values,
     curve.measurements,
     xlab=curve.parameter_name,
     xscale=curve.parameter_scale,
     ylab = &quot;CV estimate of RMS error&quot;)</code></pre><p><img src="../img/learning_curve42.png" alt/></p><p>In the case the <code>range</code> hyperparameter is the number of iterations in some iterative model, <code>learning_curve</code> will not restart the training from scratch for each new value, unless a non-holdout <code>resampling</code> strategy is specified (and provided the model implements an appropriate <code>update</code> method). To obtain multiple curves (that are distinct) you will need to pass the name of the model random number generator, <code>rng_name</code>, and specify the random number generators to be used using <code>rngs=...</code> (an integer automatically generates the number specified):</p><pre><code class="language-julia">atom.lambda= 7.3
r_n = range(ensemble, :n, lower=1, upper=50)
curves = MLJ.learning_curve(mach;
                            range=r_n,
                            measure=MeanAbsoluteError(),
                            verbosity=0,
                            rng_name=:rng,
                            rngs=4)</code></pre><pre class="documenter-example-output">(parameter_name = &quot;n&quot;,
 parameter_scale = :linear,
 parameter_values = [1, 3, 4, 6, 8, 9, 11, 13, 15, 16  …  35, 36, 38, 40, 42, 43, 45, 47, 48, 50],
 measurements = [6.267086768364965 9.016551323507677 8.693213673337342 6.6284243276381645; 7.076439307404187 8.128349359638042 8.372305838710082 7.499646480481914; … ; 7.260210746446452 7.97999778580648 8.314419022423824 7.683557093865239; 7.272776254468539 7.970107680884375 8.310567563409217 7.695817801424128],)</pre><pre><code class="language-julia">plot(curves.parameter_values,
     curves.measurements,
     xlab=curves.parameter_name,
     ylab=&quot;Holdout estimate of RMS error&quot;)</code></pre><p><img src="../img/learning_curve_n.png" alt/></p><h2 id="API-reference"><a class="docs-heading-anchor" href="#API-reference">API reference</a><a id="API-reference-1"></a><a class="docs-heading-anchor-permalink" href="#API-reference" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="MLJTuning.learning_curve" href="#MLJTuning.learning_curve"><code>MLJTuning.learning_curve</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">curve = learning_curve(mach; resolution=30,
                             resampling=Holdout(),
                             repeats=1,
                             measure=default_measure(machine.model),
                             rows=nothing,
                             weights=nothing,
                             operation=nothing,
                             range=nothing,
                             acceleration=default_resource(),
                             acceleration_grid=CPU1(),
                             rngs=nothing,
                             rng_name=nothing)</code></pre><p>Given a supervised machine <code>mach</code>, returns a named tuple of objects suitable for generating a plot of performance estimates, as a function of the single hyperparameter specified in <code>range</code>. The tuple <code>curve</code> has the following keys: <code>:parameter_name</code>, <code>:parameter_scale</code>, <code>:parameter_values</code>, <code>:measurements</code>.</p><p>To generate multiple curves for a <code>model</code> with a random number generator (RNG) as a hyperparameter, specify the name, <code>rng_name</code>, of the (possibly nested) RNG field, and a vector <code>rngs</code> of RNG&#39;s, one for each curve. Alternatively, set <code>rngs</code> to the number of curves desired, in which case RNG&#39;s are automatically generated. The individual curve computations can be distributed across multiple processes using <code>acceleration=CPUProcesses()</code> or <code>acceleration=CPUThreads()</code>. See the second example below for a demonstration.</p><pre><code class="language-julia">X, y = @load_boston;
atom = @load RidgeRegressor pkg=MultivariateStats
ensemble = EnsembleModel(atom=atom, n=1000)
mach = machine(ensemble, X, y)
r_lambda = range(ensemble, :(atom.lambda), lower=10, upper=500, scale=:log10)
curve = learning_curve(mach; range=r_lambda, resampling=CV(), measure=mav)
using Plots
plot(curve.parameter_values,
     curve.measurements,
     xlab=curve.parameter_name,
     xscale=curve.parameter_scale,
     ylab = &quot;CV estimate of RMS error&quot;)</code></pre><p>If using a <code>Holdout()</code> <code>resampling</code> strategy (with no shuffling) and if the specified hyperparameter is the number of iterations in some iterative model (and that model has an appropriately overloaded <code>MLJModelInterface.update</code> method) then training is not restarted from scratch for each increment of the parameter, ie the model is trained progressively.</p><pre><code class="language-julia">atom.lambda=200
r_n = range(ensemble, :n, lower=1, upper=250)
curves = learning_curve(mach; range=r_n, verbosity=0, rng_name=:rng, rngs=3)
plot!(curves.parameter_values,
     curves.measurements,
     xlab=curves.parameter_name,
     ylab=&quot;Holdout estimate of RMS error&quot;)

</code></pre><pre><code class="language-none">learning_curve(model::Supervised, X, y; kwargs...)
learning_curve(model::Supervised, X, y, w; kwargs...)</code></pre><p>Plot a learning curve (or curves) directly, without first constructing a machine.</p><p><strong>Summary of key-word options</strong></p><ul><li><p><code>resolution</code> - number of points generated from <code>range</code> (number model evaluations); default is <code>30</code></p></li><li><p><code>acceleration</code> - parallelization option for passing to <code>evaluate!</code>; an instance of <code>CPU1</code>, <code>CPUProcesses</code> or <code>CPUThreads</code> from the <code>ComputationalResources.jl</code>; default is <code>default_resource()</code></p></li><li><p><code>acceleration_grid</code> - parallelization option for distributing each performancde evaluation</p></li><li><p><code>rngs</code> - for specifying random number generator(s) to be passed to the model (see above)</p></li><li><p><code>rng_name</code> - name of the model hyper-parameter representing a random number generator (see above); possibly nested</p></li></ul><p>Other key-word options are documented at <a href="../tuning_models/#MLJTuning.TunedModel"><code>TunedModel</code></a>.</p></div></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../tuning_models/">« Tuning Models</a><a class="docs-footer-nextpage" href="../preparing_data/">Preparing Data »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Saturday 1 January 2022 03:45">Saturday 1 January 2022</span>. Using Julia version 1.7.1.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
