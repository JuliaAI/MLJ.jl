var documenterSearchIndex = {"docs":
[{"location":"models/LDA_MultivariateStats/#LDA_MultivariateStats","page":"LDA","title":"LDA","text":"LDA\n\nA model type for constructing a linear discriminant analysis model, based on MultivariateStats.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nLDA = @load LDA pkg=MultivariateStats\n\nDo model = LDA() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in LDA(method=...).\n\nMulticlass linear discriminant analysis learns a projection in a space of features to a lower dimensional space, in a way that attempts to preserve as much as possible the degree to which the classes of a discrete target variable can be discriminated. This can be used either for dimension reduction of the features (see transform below) or for probabilistic classification of the target (see predict below).\n\nIn the case of prediction, the class probability for a new observation reflects the proximity of that observation to training observations associated with that class, and how far away the observation is from observations associated with other classes. Specifically, the distances, in the transformed (projected) space, of a new observation, from the centroid of each target class, is computed; the resulting vector of distances, multiplied by minus one, is passed to a softmax function to obtain a class probability prediction. Here \"distance\" is computed using a user-specified distance function.","category":"section"},{"location":"models/LDA_MultivariateStats/#Training-data","page":"LDA","title":"Training data","text":"In MLJ or MLJBase, bind an instance model to data with\n\nmach = machine(model, X, y)\n\nHere:\n\nX is any table of input features (eg, a DataFrame) whose columns are of scitype Continuous; check column scitypes with schema(X).\ny is the target, which can be any AbstractVector whose element scitype is OrderedFactor or Multiclass; check the scitype with scitype(y)\n\nTrain the machine using fit!(mach, rows=...).","category":"section"},{"location":"models/LDA_MultivariateStats/#Hyper-parameters","page":"LDA","title":"Hyper-parameters","text":"method::Symbol=:gevd: The solver, one of :gevd or :whiten methods.\ncov_w::StatsBase.SimpleCovariance(): An estimator for the within-class covariance (used in computing the within-class scatter matrix, Sw). Any robust estimator from CovarianceEstimation.jl can be used.\ncov_b::StatsBase.SimpleCovariance(): The same as cov_w but for the between-class covariance (used in computing the between-class scatter matrix, Sb).\noutdim::Int=0: The output dimension, i.e dimension of the transformed space, automatically set to min(indim, nclasses-1) if equal to 0.\nregcoef::Float64=1e-6: The regularization coefficient. A positive value regcoef*eigmax(Sw) where Sw is the within-class scatter matrix, is added to the diagonal of Sw to improve numerical stability. This can be useful if using the standard covariance estimator.\ndist=Distances.SqEuclidean(): The distance metric to use when performing classification (to compare the distance between a new point and centroids in the transformed space); must be a subtype of Distances.SemiMetric from Distances.jl, e.g., Distances.CosineDist.","category":"section"},{"location":"models/LDA_MultivariateStats/#Operations","page":"LDA","title":"Operations","text":"transform(mach, Xnew): Return a lower dimensional projection of the input Xnew, which should have the same scitype as X above.\npredict(mach, Xnew): Return predictions of the target given features Xnew having the same scitype as X above. Predictions are probabilistic but uncalibrated.\npredict_mode(mach, Xnew): Return the modes of the probabilistic predictions returned above.","category":"section"},{"location":"models/LDA_MultivariateStats/#Fitted-parameters","page":"LDA","title":"Fitted parameters","text":"The fields of fitted_params(mach) are:\n\nclasses: The classes seen during model fitting.\nprojection_matrix: The learned projection matrix, of size (indim, outdim), where indim and outdim are the input and output dimensions respectively (See Report section below).","category":"section"},{"location":"models/LDA_MultivariateStats/#Report","page":"LDA","title":"Report","text":"The fields of report(mach) are:\n\nindim: The dimension of the input space i.e the number of training features.\noutdim: The dimension of the transformed space the model is projected to.\nmean: The mean of the untransformed training data. A vector of length indim.\nnclasses: The number of classes directly observed in the training data (which can be less than the total number of classes in the class pool).\nclass_means: The class-specific means of the training data. A matrix of size (indim, nclasses) with the ith column being the class-mean of the ith class in classes (See fitted params section above).\nclass_weights: The weights (class counts) of each class. A vector of length nclasses with the ith element being the class weight of the ith class in classes. (See fitted params section above.)\nSb: The between class scatter matrix.\nSw: The within class scatter matrix.","category":"section"},{"location":"models/LDA_MultivariateStats/#Examples","page":"LDA","title":"Examples","text":"using MLJ\n\nLDA = @load LDA pkg=MultivariateStats\n\nX, y = @load_iris ## a table and a vector\n\nmodel = LDA()\nmach = machine(model, X, y) |> fit!\n\nXproj = transform(mach, X)\ny_hat = predict(mach, X)\nlabels = predict_mode(mach, X)\n\n\nSee also BayesianLDA, SubspaceLDA, BayesianSubspaceLDA","category":"section"},{"location":"models/NuSVC_LIBSVM/#NuSVC_LIBSVM","page":"NuSVC","title":"NuSVC","text":"NuSVC\n\nA model type for constructing a ν-support vector classifier, based on LIBSVM.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nNuSVC = @load NuSVC pkg=LIBSVM\n\nDo model = NuSVC() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in NuSVC(kernel=...).\n\nThis model is a re-parameterization of the SVC classifier, where nu replaces cost, and is mathematically equivalent to it. The parameter nu allows more direct control over the number of support vectors (see under \"Hyper-parameters\").\n\nThis model always predicts actual class labels. For probabilistic predictions, use instead ProbabilisticNuSVC.\n\nReference for algorithm and core C-library: C.-C. Chang and C.-J. Lin (2011): \"LIBSVM: a library for support vector machines.\" ACM Transactions on Intelligent Systems and Technology, 2(3):27:1–27:27. Updated at https://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf. ","category":"section"},{"location":"models/NuSVC_LIBSVM/#Training-data","page":"NuSVC","title":"Training data","text":"In MLJ or MLJBase, bind an instance model to data with:\n\nmach = machine(model, X, y)\n\nwhere\n\nX: any table of input features (eg, a DataFrame) whose columns each have Continuous element scitype; check column scitypes with schema(X)\ny: is the target, which can be any AbstractVector whose element scitype is <:OrderedFactor or <:Multiclass; check the scitype with scitype(y)\n\nTrain the machine using fit!(mach, rows=...).","category":"section"},{"location":"models/NuSVC_LIBSVM/#Hyper-parameters","page":"NuSVC","title":"Hyper-parameters","text":"kernel=LIBSVM.Kernel.RadialBasis: either an object that can be called, as in kernel(x1, x2), or one of the built-in kernels from the LIBSVM.jl package listed below.  Here x1 and x2 are vectors whose lengths match the number of columns of the training data X (see \"Examples\" below).\nLIBSVM.Kernel.Linear: (x1, x2) -> x1'*x2\nLIBSVM.Kernel.Polynomial: (x1, x2) -> gamma*x1'*x2 + coef0)^degree\nLIBSVM.Kernel.RadialBasis: (x1, x2) -> (exp(-gamma*norm(x1 - x2)^2))\nLIBSVM.Kernel.Sigmoid: (x1, x2) - > tanh(gamma*x1'*x2 + coef0)\nHere gamma, coef0, degree are other hyper-parameters. Serialization of models with user-defined kernels comes with some restrictions. See LIVSVM.jl issue91\ngamma = 0.0: kernel parameter (see above); if gamma==-1.0 then gamma = 1/nfeatures is used in training, where nfeatures is the number of features (columns of X).  If gamma==0.0 then gamma = 1/(var(Tables.matrix(X))*nfeatures) is used. Actual value used appears in the report (see below).\ncoef0 = 0.0: kernel parameter (see above)\ndegree::Int32 = Int32(3): degree in polynomial kernel (see above)\nnu=0.5 (range (0, 1]): An upper bound on the fraction of margin errors and a lower bound of the fraction of support vectors. Denoted ν in the cited paper. Changing nu changes the thickness of the margin (a neighborhood of the decision surface) and a margin error is said to have occurred if a training observation lies on the wrong side of the surface or within the margin.\ncachesize=200.0 cache memory size in MB\ntolerance=0.001: tolerance for the stopping criterion\nshrinking=true: whether to use shrinking heuristics","category":"section"},{"location":"models/NuSVC_LIBSVM/#Operations","page":"NuSVC","title":"Operations","text":"predict(mach, Xnew): return predictions of the target given features Xnew having the same scitype as X above.","category":"section"},{"location":"models/NuSVC_LIBSVM/#Fitted-parameters","page":"NuSVC","title":"Fitted parameters","text":"The fields of fitted_params(mach) are:\n\nlibsvm_model: the trained model object created by the LIBSVM.jl package\nencoding: class encoding used internally by libsvm_model - a dictionary of class labels keyed on the internal integer representation","category":"section"},{"location":"models/NuSVC_LIBSVM/#Report","page":"NuSVC","title":"Report","text":"The fields of report(mach) are:\n\ngamma: actual value of the kernel parameter gamma used in training","category":"section"},{"location":"models/NuSVC_LIBSVM/#Examples","page":"NuSVC","title":"Examples","text":"","category":"section"},{"location":"models/NuSVC_LIBSVM/#Using-a-built-in-kernel","page":"NuSVC","title":"Using a built-in kernel","text":"using MLJ\nimport LIBSVM\n\nNuSVC = @load NuSVC pkg=LIBSVM                 ## model type\nmodel = NuSVC(kernel=LIBSVM.Kernel.Polynomial) ## instance\n\nX, y = @load_iris ## table, vector\nmach = machine(model, X, y) |> fit!\n\nXnew = (sepal_length = [6.4, 7.2, 7.4],\n        sepal_width = [2.8, 3.0, 2.8],\n        petal_length = [5.6, 5.8, 6.1],\n        petal_width = [2.1, 1.6, 1.9],)\n\njulia> yhat = predict(mach, Xnew)\n3-element CategoricalArrays.CategoricalArray{String,1,UInt32}:\n \"virginica\"\n \"virginica\"\n \"virginica\"","category":"section"},{"location":"models/NuSVC_LIBSVM/#User-defined-kernels","page":"NuSVC","title":"User-defined kernels","text":"k(x1, x2) = x1'*x2 ## equivalent to `LIBSVM.Kernel.Linear`\nmodel = NuSVC(kernel=k)\nmach = machine(model, X, y) |> fit!\n\njulia> yhat = predict(mach, Xnew)\n3-element CategoricalArrays.CategoricalArray{String,1,UInt32}:\n \"virginica\"\n \"virginica\"\n \"virginica\"\n\nSee also the classifiers SVC and LinearSVC, LIVSVM.jl and the original C implementation. documentation.","category":"section"},{"location":"models/KMedoidsClusterer_BetaML/#KMedoidsClusterer_BetaML","page":"KMedoidsClusterer","title":"KMedoidsClusterer","text":"mutable struct KMedoidsClusterer <: MLJModelInterface.Unsupervised","category":"section"},{"location":"models/KMedoidsClusterer_BetaML/#Parameters:","page":"KMedoidsClusterer","title":"Parameters:","text":"n_classes::Int64: Number of classes to discriminate the data [def: 3]\ndist::Function: Function to employ as distance. Default to the Euclidean distance. Can be one of the predefined distances (l1_distance, l2_distance, l2squared_distance),  cosine_distance), any user defined function accepting two vectors and returning a scalar or an anonymous function with the same characteristics.\ninitialisation_strategy::String: The computation method of the vector of the initial representatives. One of the following:\n\"random\": randomly in the X space\n\"grid\": using a grid approach\n\"shuffle\": selecting randomly within the available points [default]\n\"given\": using a provided set of initial representatives provided in the initial_representatives parameter\ninitial_representatives::Union{Nothing, Matrix{Float64}}: Provided (K x D) matrix of initial representatives (useful only with initialisation_strategy=\"given\") [default: nothing]\nrng::Random.AbstractRNG: Random Number Generator [deafult: Random.GLOBAL_RNG]\n\nThe K-medoids clustering algorithm with customisable distance function, from the Beta Machine Learning Toolkit (BetaML).\n\nSimilar to K-Means, but the \"representatives\" (the cetroids) are guaranteed to be one of the training points. The algorithm work with any arbitrary distance measure.","category":"section"},{"location":"models/KMedoidsClusterer_BetaML/#Notes:","page":"KMedoidsClusterer","title":"Notes:","text":"data must be numerical\nonline fitting (re-fitting with new data) is supported","category":"section"},{"location":"models/KMedoidsClusterer_BetaML/#Example:","page":"KMedoidsClusterer","title":"Example:","text":"julia> using MLJ\n\njulia> X, y        = @load_iris;\n\njulia> modelType   = @load KMedoidsClusterer pkg = \"BetaML\" verbosity=0\nBetaML.Clustering.KMedoidsClusterer\n\njulia> model       = modelType()\nKMedoidsClusterer(\n  n_classes = 3, \n  dist = BetaML.Clustering.var\"#39#41\"(), \n  initialisation_strategy = \"shuffle\", \n  initial_representatives = nothing, \n  rng = Random._GLOBAL_RNG())\n\njulia> mach        = machine(model, X);\n\njulia> fit!(mach);\n[ Info: Training machine(KMedoidsClusterer(n_classes = 3, …), …).\n\njulia> classes_est = predict(mach, X);\n\njulia> hcat(y,classes_est)\n150×2 CategoricalArrays.CategoricalArray{Union{Int64, String},2,UInt32}:\n \"setosa\"     3\n \"setosa\"     3\n \"setosa\"     3\n ⋮            \n \"virginica\"  1\n \"virginica\"  1\n \"virginica\"  2","category":"section"},{"location":"models/NeuralNetworkBinaryClassifier_MLJFlux/#NeuralNetworkBinaryClassifier_MLJFlux","page":"NeuralNetworkBinaryClassifier","title":"NeuralNetworkBinaryClassifier","text":"NeuralNetworkBinaryClassifier\n\nA model type for constructing a neural network binary classifier, based on MLJFlux.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nNeuralNetworkBinaryClassifier = @load NeuralNetworkBinaryClassifier pkg=MLJFlux\n\nDo model = NeuralNetworkBinaryClassifier() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in NeuralNetworkBinaryClassifier(builder=...).\n\nNeuralNetworkBinaryClassifier is for training a data-dependent Flux.jl neural network for making probabilistic predictions of a binary (Multiclass{2} or OrderedFactor{2}) target, given a table of Continuous features. Users provide a recipe for constructing  the network, based on properties of the data that is encountered, by specifying  an appropriate builder. See MLJFlux documentation for more on builders.\n\nIn addition to features with Continuous scientific element type, this model supports categorical features in the input table. If present, such features are embedded into dense vectors by the use of an additional EntityEmbedderLayer layer after the input, as described in Entity Embeddings of Categorical Variables by Cheng Guo, Felix Berkhahn arXiv, 2016.","category":"section"},{"location":"models/NeuralNetworkBinaryClassifier_MLJFlux/#Training-data","page":"NeuralNetworkBinaryClassifier","title":"Training data","text":"In MLJ or MLJBase, bind an instance model to data with\n\nmach = machine(model, X, y)\n\nHere:\n\nX provides input features and is either: (i) a Matrix with Continuous element scitype (typically Float32); or (ii) a table of input features (eg, a DataFrame) whose columns have Continuous, Multiclass or OrderedFactor element scitype; check column scitypes with schema(X).  If any Multiclass or OrderedFactor features appear, the constructed network will use an EntityEmbedderLayer layer to transform them into dense vectors. If X is a Matrix, it is assumed that columns correspond to features and rows corresponding to observations.\ny is the target, which can be any AbstractVector whose element scitype is Multiclass{2} or OrderedFactor{2}; check the scitype with scitype(y)\n\nTrain the machine with fit!(mach, rows=...).","category":"section"},{"location":"models/NeuralNetworkBinaryClassifier_MLJFlux/#Hyper-parameters","page":"NeuralNetworkBinaryClassifier","title":"Hyper-parameters","text":"builder=MLJFlux.Short(): An MLJFlux builder that constructs a neural network. Possible  builders include: MLJFlux.Linear, MLJFlux.Short, and MLJFlux.MLP. See  MLJFlux.jl documentation for examples of user-defined builders. See also finaliser  below.\noptimiser::Flux.Adam(): A Flux.Optimise optimiser. The optimiser performs the updating of the weights of the network. For further reference, see the Flux optimiser documentation. To choose a learning rate (the update rate of the optimizer), a good rule of thumb is to start out at 10e-3, and tune using powers of 10 between 1 and 1e-7.\nloss=Flux.binarycrossentropy: The loss function which the network will optimize. Should be a function which can be called in the form loss(yhat, y).  Possible loss functions are listed in the Flux loss function documentation. For a classification task, the most natural loss functions are:\nFlux.binarycrossentropy: Standard binary classification loss, also known as the log loss.\nFlux.logitbinarycrossentropy: Mathematically equal to crossentropy, but numerically more stable than finalising the outputs with σ and then calculating crossentropy. You will need to specify finaliser=identity to remove MLJFlux's default sigmoid finaliser, and understand that the output of predict is then unnormalized (no longer probabilistic).\nFlux.tversky_loss: Used with imbalanced data to give more weight to false negatives.\nFlux.binary_focal_loss: Used with highly imbalanced data. Weights harder examples more than easier examples.\nCurrently MLJ measures are not supported values of loss.\nepochs::Int=10: The duration of training, in epochs. Typically, one epoch represents one pass through the complete the training dataset.\nbatch_size::int=1: the batch size to be used for training, representing the number of samples per update of the network weights. Typically, batch size is between 8 and 512. Increassing batch size may accelerate training if acceleration=CUDALibs() and a GPU is available.\nlambda::Float64=0: The strength of the weight regularization penalty. Can be any value in the range [0, ∞).\nalpha::Float64=0: The L2/L1 mix of regularization, in the range [0, 1]. A value of 0 represents L2 regularization, and a value of 1 represents L1 regularization.\nrng::Union{AbstractRNG, Int64}: The random number generator or seed used during training.\noptimizer_changes_trigger_retraining::Bool=false: Defines what happens when re-fitting a machine if the associated optimiser has changed. If true, the associated machine will retrain from scratch on fit! call, otherwise it will not.\nacceleration::AbstractResource=CPU1(): Defines on what hardware training is done. For Training on GPU, use CUDALibs().\nfinaliser=Flux.σ: The final activation function of the neural network (applied after the network defined by builder). Defaults to Flux.σ.\nembedding_dims: a Dict whose keys are names of categorical features, given as symbols, and whose values are numbers representing the desired dimensionality of the entity embeddings of such features: an integer value of 7, say, sets the embedding dimensionality to 7; a float value of 0.5, say, sets the embedding dimensionality to ceil(0.5 * c), where c is the number of feature levels.  Unspecified feature dimensionality defaults to min(c - 1, 10).","category":"section"},{"location":"models/NeuralNetworkBinaryClassifier_MLJFlux/#Operations","page":"NeuralNetworkBinaryClassifier","title":"Operations","text":"predict(mach, Xnew): return predictions of the target given new features Xnew, which should have the same scitype as X above. Predictions are probabilistic but uncalibrated.\npredict_mode(mach, Xnew): Return the modes of the probabilistic predictions returned above.\ntransform(mach, Xnew): Assuming Xnew has the same schema as X, transform the categorical features of Xnew into dense Continuous vectors using the MLJFlux.EntityEmbedderLayer layer present in the network. Does nothing in case the model was trained on an input X that lacks categorical features.","category":"section"},{"location":"models/NeuralNetworkBinaryClassifier_MLJFlux/#Fitted-parameters","page":"NeuralNetworkBinaryClassifier","title":"Fitted parameters","text":"The fields of fitted_params(mach) are:\n\nchain: The trained \"chain\" (Flux.jl model), namely the series of layers,  functions, and activations which make up the neural network. This includes  the final layer specified by finaliser (eg, softmax).","category":"section"},{"location":"models/NeuralNetworkBinaryClassifier_MLJFlux/#Report","page":"NeuralNetworkBinaryClassifier","title":"Report","text":"The fields of report(mach) are:\n\ntraining_losses: A vector of training losses (penalised if lambda != 0) in  historical order, of length epochs + 1.  The first element is the pre-training loss.","category":"section"},{"location":"models/NeuralNetworkBinaryClassifier_MLJFlux/#Examples","page":"NeuralNetworkBinaryClassifier","title":"Examples","text":"In this example we build a classification model using the Iris dataset. This is a very basic example, using a default builder and no standardization.  For a more advanced illustration, see NeuralNetworkRegressor or ImageClassifier, and examples in the MLJFlux.jl documentation.\n\nusing MLJ, Flux\nimport Optimisers\nimport RDatasets\n\nFirst, we can load the data:\n\nmtcars = RDatasets.dataset(\"datasets\", \"mtcars\");\ny, X = unpack(mtcars, ==(:VS), in([:MPG, :Cyl, :Disp, :HP, :WT, :QSec]));\n\nNote that y is a vector and X a table.\n\ny = categorical(y) ## classifier takes catogorical input\nX_f32 = Float32.(X) ## To match floating point type of the neural network layers\nNeuralNetworkBinaryClassifier = @load NeuralNetworkBinaryClassifier pkg=MLJFlux\nbclf = NeuralNetworkBinaryClassifier()\n\nNext, we can train the model:\n\nmach = machine(bclf, X_f32, y)\nfit!(mach)\n\nWe can train the model in an incremental fashion, altering the learning rate as we go, provided optimizer_changes_trigger_retraining is false (the default). Here, we also change the number of (total) iterations:\n\njulia> bclf.optimiser\nAdam(0.001, (0.9, 0.999), 1.0e-8)\n\nbclf.optimiser = Optimisers.Adam(eta = bclf.optimiser.eta * 2)\nbclf.epochs = bclf.epochs + 5\n\nfit!(mach, verbosity=2) ## trains 5 more epochs\n\nWe can inspect the mean training loss using the cross_entropy function:\n\ntraining_loss = cross_entropy(predict(mach, X_f32), y)\n\nAnd we can access the Flux chain (model) using fitted_params:\n\nchain = fitted_params(mach).chain\n\nFinally, we can see how the out-of-sample performance changes over time, using MLJ's learning_curve function:\n\nr = range(bclf, :epochs, lower=1, upper=200, scale=:log10)\ncurve = learning_curve(\n    bclf,\n    X_f32,\n    y,\n    range=r,\n    resampling=Holdout(fraction_train=0.7),\n    measure=cross_entropy,\n)\nusing Plots\nplot(\n   curve.parameter_values,\n   curve.measurements,\n   xlab=curve.parameter_name,\n   xscale=curve.parameter_scale,\n   ylab = \"Cross Entropy\",\n)\n\n\nSee also ImageClassifier.","category":"section"},{"location":"benchmarking/#Benchmarking","page":"Benchmarking","title":"Benchmarking","text":"This feature not yet available.\n\nCONTRIBUTE.md","category":"section"},{"location":"weights/#Weights","page":"Weights","title":"Weights","text":"In machine learning it is possible to assign each observation an independent significance, or weight, either in training or in performance evaluation, or both.\n\nThere are two kinds of weights in use in MLJ:\n\nper observation weights (also just called weights) refer to weight vectors of the same length as the number of observations\nclass weights refer to dictionaries keyed on the target classes (levels) for use in classification problems","category":"section"},{"location":"weights/#Specifying-weights-in-training","page":"Weights","title":"Specifying weights in training","text":"To specify weights in training you bind the weights to the model along with the data when constructing a machine.  For supervised models the weights are specified last:\n\nKNNRegressor = @load KNNRegressor\nmodel = KNNRegressor()\nX, y = make_regression(10, 3)\nw = rand(length(y))\n\nmach = machine(model, X, y, w) |> fit!\n\nNote that model supports per observation weights if supports_weights(model) is true. To list all such models, do\n\nmodels() do m\n    m.supports_weights\nend\n\nThe model model supports class weights if supports_class_weights(model) is true.","category":"section"},{"location":"weights/#Specifying-weights-in-performance-evaluation","page":"Weights","title":"Specifying weights in performance evaluation","text":"When calling a measure (metric) that supports weights, provide the weights as the last argument, as in\n\n_, y = @load_iris\nŷ = shuffle(y)\nw = Dict(\"versicolor\" => 1, \"setosa\" => 2, \"virginica\"=> 3)\nmacro_f1score(ŷ, y, w)\n\nSome measures also support specification of a class weight dictionary. For details see the StatisticalMeasures.jl tutorial.\n\nTo pass weights to all the measures listed in an evaluate!/evaluate call, use the keyword specifiers weights=... or class_weights=.... For details, see Evaluating Model Performance.","category":"section"},{"location":"models/NeuralNetworkClassifier_MLJFlux/#NeuralNetworkClassifier_MLJFlux","page":"NeuralNetworkClassifier","title":"NeuralNetworkClassifier","text":"NeuralNetworkClassifier\n\nA model type for constructing a neural network classifier, based on MLJFlux.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nNeuralNetworkClassifier = @load NeuralNetworkClassifier pkg=MLJFlux\n\nDo model = NeuralNetworkClassifier() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in NeuralNetworkClassifier(builder=...).\n\nNeuralNetworkClassifier is for training a data-dependent Flux.jl neural network for making probabilistic predictions of a Multiclass or OrderedFactor target, given a table of Continuous features. Users provide a recipe for constructing  the network, based on properties of the data that is encountered, by specifying  an appropriate builder. See MLJFlux documentation for more on builders.\n\nIn addition to features with Continuous scientific element type, this model supports categorical features in the input table. If present, such features are embedded into dense vectors by the use of an additional EntityEmbedderLayer layer after the input, as described in Entity Embeddings of Categorical Variables by Cheng Guo, Felix Berkhahn arXiv, 2016.","category":"section"},{"location":"models/NeuralNetworkClassifier_MLJFlux/#Training-data","page":"NeuralNetworkClassifier","title":"Training data","text":"In MLJ or MLJBase, bind an instance model to data with\n\nmach = machine(model, X, y)\n\nHere:\n\nX provides input features and is either: (i) a Matrix with Continuous element scitype (typically Float32); or (ii) a table of input features (eg, a DataFrame) whose columns have Continuous, Multiclass or OrderedFactor element scitype; check column scitypes with schema(X).  If any Multiclass or OrderedFactor features appear, the constructed network will use an EntityEmbedderLayer layer to transform them into dense vectors. If X is a Matrix, it is assumed that columns correspond to features and rows corresponding to observations.\ny is the target, which can be any AbstractVector whose element scitype is Multiclass or OrderedFactor; check the scitype with scitype(y)\n\nTrain the machine with fit!(mach, rows=...).","category":"section"},{"location":"models/NeuralNetworkClassifier_MLJFlux/#Hyper-parameters","page":"NeuralNetworkClassifier","title":"Hyper-parameters","text":"builder=MLJFlux.Short(): An MLJFlux builder that constructs a neural network. Possible  builders include: MLJFlux.Linear, MLJFlux.Short, and MLJFlux.MLP. See  MLJFlux.jl documentation for examples of user-defined builders. See also finaliser  below.\noptimiser::Optimisers.Adam(): An Optimisers.jl optimiser. The optimiser performs the updating of the weights of the network. To choose a learning rate (the update rate of the optimizer), a good rule of thumb is to start out at 10e-3, and tune using powers of 10 between 1 and 1e-7.\nloss=Flux.crossentropy: The loss function which the network will optimize. Should be a function which can be called in the form loss(yhat, y).  Possible loss functions are listed in the Flux loss function documentation. For a classification task, the most natural loss functions are:\nFlux.crossentropy: Standard multiclass classification loss, also known as the log loss.\nFlux.logitcrossentopy: Mathematically equal to crossentropy, but numerically more stable than finalising the outputs with softmax and then calculating crossentropy. You will need to specify finaliser=identity to remove MLJFlux's default softmax finaliser, and understand that the output of predict is then unnormalized (no longer probabilistic).\nFlux.tversky_loss: Used with imbalanced data to give more weight to false negatives.\nFlux.focal_loss: Used with highly imbalanced data. Weights harder examples more than easier examples.\nCurrently MLJ measures are not supported values of loss.\nepochs::Int=10: The duration of training, in epochs. Typically, one epoch represents one pass through the complete the training dataset.\nbatch_size::int=1: the batch size to be used for training, representing the number of samples per update of the network weights.] Typically, batch size is between 8 and 512. Increassing batch size may accelerate training if acceleration=CUDALibs() and a GPU is available.\nlambda::Float64=0: The strength of the weight regularization penalty. Can be any value in the range [0, ∞). Note the history reports unpenalized losses.\nalpha::Float64=0: The L2/L1 mix of regularization, in the range [0, 1]. A value of 0 represents L2 regularization, and a value of 1 represents L1 regularization.\nrng::Union{AbstractRNG, Int64}: The random number generator or seed used during training. The default is Random.default_rng().\noptimizer_changes_trigger_retraining::Bool=false: Defines what happens when re-fitting a machine if the associated optimiser has changed. If true, the associated machine will retrain from scratch on fit! call, otherwise it will not.\nacceleration::AbstractResource=CPU1(): Defines on what hardware training is done. For Training on GPU, use CUDALibs().\nfinaliser=Flux.softmax: The final activation function of the neural network (applied after the network defined by builder). Defaults to Flux.softmax.\nembedding_dims: a Dict whose keys are names of categorical features, given as symbols, and whose values are numbers representing the desired dimensionality of the entity embeddings of such features: an integer value of 7, say, sets the embedding dimensionality to 7; a float value of 0.5, say, sets the embedding dimensionality to ceil(0.5 * c), where c is the number of feature levels.  Unspecified feature dimensionality defaults to min(c - 1, 10).","category":"section"},{"location":"models/NeuralNetworkClassifier_MLJFlux/#Operations","page":"NeuralNetworkClassifier","title":"Operations","text":"predict(mach, Xnew): return predictions of the target given new features Xnew, which should have the same scitype as X above. Predictions are probabilistic but uncalibrated.\npredict_mode(mach, Xnew): Return the modes of the probabilistic predictions returned above.\ntransform(mach, Xnew): Assuming Xnew has the same schema as X, transform the categorical features of Xnew into dense Continuous vectors using the MLJFlux.EntityEmbedderLayer layer present in the network. Does nothing in case the model was trained on an input X that lacks categorical features.","category":"section"},{"location":"models/NeuralNetworkClassifier_MLJFlux/#Fitted-parameters","page":"NeuralNetworkClassifier","title":"Fitted parameters","text":"The fields of fitted_params(mach) are:\n\nchain: The trained \"chain\" (Flux.jl model), namely the series of layers,  functions, and activations which make up the neural network. This includes  the final layer specified by finaliser (eg, softmax).","category":"section"},{"location":"models/NeuralNetworkClassifier_MLJFlux/#Report","page":"NeuralNetworkClassifier","title":"Report","text":"The fields of report(mach) are:\n\ntraining_losses: A vector of training losses (penalised if lambda != 0) in  historical order, of length epochs + 1.  The first element is the pre-training loss.","category":"section"},{"location":"models/NeuralNetworkClassifier_MLJFlux/#Examples","page":"NeuralNetworkClassifier","title":"Examples","text":"In this example we build a classification model using the Iris dataset. This is a very basic example, using a default builder and no standardization.  For a more advanced illustration, see NeuralNetworkRegressor or ImageClassifier, and examples in the MLJFlux.jl documentation.\n\nusing MLJ\nusing Flux\nimport RDatasets\nimport Optimisers\n\nFirst, we can load the data:\n\niris = RDatasets.dataset(\"datasets\", \"iris\");\ny, X = unpack(iris, ==(:Species), rng=123); ## a vector and a table\nNeuralNetworkClassifier = @load NeuralNetworkClassifier pkg=MLJFlux\nclf = NeuralNetworkClassifier()\n\nNext, we can train the model:\n\nmach = machine(clf, X, y)\nfit!(mach)\n\nWe can train the model in an incremental fashion, altering the learning rate as we go, provided optimizer_changes_trigger_retraining is false (the default). Here, we also change the number of (total) iterations:\n\nclf.optimiser = Optimisers.Adam(clf.optimiser.eta * 2)\nclf.epochs = clf.epochs + 5\n\nfit!(mach, verbosity=2) ## trains 5 more epochs\n\nWe can inspect the mean training loss using the cross_entropy function:\n\ntraining_loss = cross_entropy(predict(mach, X), y)\n\nAnd we can access the Flux chain (model) using fitted_params:\n\nchain = fitted_params(mach).chain\n\nFinally, we can see how the out-of-sample performance changes over time, using MLJ's learning_curve function:\n\nr = range(clf, :epochs, lower=1, upper=200, scale=:log10)\ncurve = learning_curve(clf, X, y,\n                     range=r,\n                     resampling=Holdout(fraction_train=0.7),\n                     measure=cross_entropy)\nusing Plots\nplot(curve.parameter_values,\n     curve.measurements,\n     xlab=curve.parameter_name,\n     xscale=curve.parameter_scale,\n     ylab = \"Cross Entropy\")\n\n\nSee also ImageClassifier, NeuralNetworkBinaryClassifier.","category":"section"},{"location":"models/HBOSDetector_OutlierDetectionPython/#HBOSDetector_OutlierDetectionPython","page":"HBOSDetector","title":"HBOSDetector","text":"HBOSDetector(n_bins = 10,\n                alpha = 0.1,\n                tol = 0.5)\n\nhttps://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.hbos","category":"section"},{"location":"models/RecursiveFeatureElimination_FeatureSelection/#RecursiveFeatureElimination_FeatureSelection","page":"RecursiveFeatureElimination","title":"RecursiveFeatureElimination","text":"RecursiveFeatureElimination(model; n_features=0, step=1)\n\nThis model implements a recursive feature elimination algorithm for feature selection. It recursively removes features, training a base model on the remaining features and evaluating their importance until the desired number of features is selected.","category":"section"},{"location":"models/RecursiveFeatureElimination_FeatureSelection/#Training-data","page":"RecursiveFeatureElimination","title":"Training data","text":"In MLJ or MLJBase, bind an instance rfe_model to data with\n\nmach = machine(rfe_model, X, y)\n\nOR, if the base model supports weights, as\n\nmach = machine(rfe_model, X, y, w)\n\nHere:\n\nX is any table of input features (eg, a DataFrame) whose columns are of the scitype as that required by the base model; check column scitypes with schema(X) and column scitypes required by base model with input_scitype(basemodel).\ny is the target, which can be any table of responses whose element scitype is   Continuous or Finite depending on the target_scitype required by the base model;   check the scitype with scitype(y).\nw is the observation weights which can either be nothing(default) or an AbstractVector whoose element scitype is Count or Continuous. This is different from weights kernel which is an hyperparameter to the model, see below.\n\nTrain the machine using fit!(mach, rows=...).","category":"section"},{"location":"models/RecursiveFeatureElimination_FeatureSelection/#Hyper-parameters","page":"RecursiveFeatureElimination","title":"Hyper-parameters","text":"model: A base model with a fit method that provides information on feature feature importance (i.e reports_feature_importances(model) == true)\nn_features::Real = 0: The number of features to select. If 0, half of the features are selected. If a positive integer, the parameter is the absolute number of features to select. If a real number between 0 and 1, it is the fraction of features to select.\nstep::Real=1: If the value of step is at least 1, it signifies the quantity of features to eliminate in each iteration. Conversely, if step falls strictly within the range of 0.0 to 1.0, it denotes the proportion (rounded down) of features to remove during each iteration.","category":"section"},{"location":"models/RecursiveFeatureElimination_FeatureSelection/#Operations","page":"RecursiveFeatureElimination","title":"Operations","text":"transform(mach, X): transform the input table X into a new table containing only columns corresponding to features accepted by the RFE algorithm.\npredict(mach, X): transform the input table X into a new table same as in transform(mach, X) above and predict using the fitted base model on the transformed table.","category":"section"},{"location":"models/RecursiveFeatureElimination_FeatureSelection/#Fitted-parameters","page":"RecursiveFeatureElimination","title":"Fitted parameters","text":"The fields of fitted_params(mach) are:\n\nfeatures_left: names of features remaining after recursive feature elimination.\nmodel_fitresult: fitted parameters of the base model.","category":"section"},{"location":"models/RecursiveFeatureElimination_FeatureSelection/#Report","page":"RecursiveFeatureElimination","title":"Report","text":"The fields of report(mach) are:\n\nscores: dictionary of scores for each feature in the training dataset. The model deems highly scored variables more significant.\nmodel_report: report for the fitted base model.","category":"section"},{"location":"models/RecursiveFeatureElimination_FeatureSelection/#Examples","page":"RecursiveFeatureElimination","title":"Examples","text":"The following example assumes you have MLJDecisionTreeInterface in the active package ennvironment.\n\nusing MLJ\n\nRandomForestRegressor = @load RandomForestRegressor pkg=DecisionTree\n\n## Creates a dataset where the target only depends on the first 5 columns of the input table.\nA = rand(50, 10);\ny = 10 .* sin.(\n        pi .* A[:, 1] .* A[:, 2]\n    ) + 20 .* (A[:, 3] .- 0.5).^ 2 .+ 10 .* A[:, 4] .+ 5 * A[:, 5];\nX = MLJ.table(A);\n\n## fit a rfe model:\nrf = RandomForestRegressor()\nselector = RecursiveFeatureElimination(rf, n_features=2)\nmach = machine(selector, X, y)\nfit!(mach)\n\n## view the feature importances\nfeature_importances(mach)\n\n## predict using the base model trained on the reduced feature set:\nXnew = MLJ.table(rand(50, 10));\npredict(mach, Xnew)\n\n## transform data with all features to the reduced feature set:\ntransform(mach, Xnew)","category":"section"},{"location":"models/DBSCAN_Clustering/#DBSCAN_Clustering","page":"DBSCAN","title":"DBSCAN","text":"DBSCAN\n\nA model type for constructing a DBSCAN clusterer (density-based spatial clustering of applications with noise), based on Clustering.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nDBSCAN = @load DBSCAN pkg=Clustering\n\nDo model = DBSCAN() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in DBSCAN(radius=...).\n\nDBSCAN is a clustering algorithm that groups together points that are closely packed together (points with many nearby neighbors), marking as outliers points that lie alone in low-density regions (whose nearest neighbors are too far away). More information is available at the Clustering.jl documentation. Use predict to get cluster assignments. Point types - core, boundary or noise - are accessed from the machine report (see below).\n\nThis is a static implementation, i.e., it does not generalize to new data instances, and there is no training data. For clusterers that do generalize, see KMeans or KMedoids.\n\nIn MLJ or MLJBase, create a machine with\n\nmach = machine(model)","category":"section"},{"location":"models/DBSCAN_Clustering/#Hyper-parameters","page":"DBSCAN","title":"Hyper-parameters","text":"radius=1.0: query radius.\nleafsize=20: number of points binned in each leaf node of the nearest neighbor k-d tree.\nmin_neighbors=1: minimum number of a core point neighbors.\nmin_cluster_size=1: minimum number of points in a valid cluster.","category":"section"},{"location":"models/DBSCAN_Clustering/#Operations","page":"DBSCAN","title":"Operations","text":"predict(mach, X): return cluster label assignments, as an unordered CategoricalVector. Here X is any table of input features (eg, a DataFrame) whose columns are of scitype Continuous; check column scitypes with schema(X). Note that points of type noise will always get a label of 0.","category":"section"},{"location":"models/DBSCAN_Clustering/#Report","page":"DBSCAN","title":"Report","text":"After calling predict(mach), the fields of report(mach)  are:\n\npoint_types: A CategoricalVector with the DBSCAN point type classification, one element per row of X. Elements are either 'C' (core), 'B' (boundary), or 'N' (noise).\nnclusters: The number of clusters (excluding the noise \"cluster\")\ncluster_labels: The unique list of cluster labels\nclusters: A vector of Clustering.DbscanCluster objects from Clustering.jl, which have these fields:\nsize: number of points in a cluster (core + boundary)\ncore_indices: indices of points in the cluster core\nboundary_indices: indices of points on the cluster boundary","category":"section"},{"location":"models/DBSCAN_Clustering/#Examples","page":"DBSCAN","title":"Examples","text":"using MLJ\n\nX, labels  = make_moons(400, noise=0.09, rng=1) ## synthetic data with 2 clusters; X\ny = map(labels) do label\n    label == 0 ? \"cookie\" : \"monster\"\nend;\ny = coerce(y, Multiclass);\n\nDBSCAN = @load DBSCAN pkg=Clustering\nmodel = DBSCAN(radius=0.13, min_cluster_size=5)\nmach = machine(model)\n\n## compute and output cluster assignments for observations in `X`:\nyhat = predict(mach, X)\n\n## get DBSCAN point types:\nreport(mach).point_types\nreport(mach).nclusters\n\n## compare cluster labels with actual labels:\ncompare = zip(yhat, y) |> collect;\ncompare[1:10] ## clusters align with classes\n\n## visualize clusters, noise in red:\npoints = zip(X.x1, X.x2) |> collect\ncolors = map(yhat) do i\n   i == 0 ? :red :\n   i == 1 ? :blue :\n   i == 2 ? :green :\n   i == 3 ? :yellow :\n   :black\nend\nusing Plots\nscatter(points, color=colors)","category":"section"},{"location":"glossary/#Glossary","page":"Glossary","title":"Glossary","text":"Note: This glossary includes some detail intended mainly for MLJ developers.","category":"section"},{"location":"glossary/#Basics","page":"Glossary","title":"Basics","text":"","category":"section"},{"location":"glossary/#hyperparameters","page":"Glossary","title":"hyperparameters","text":"Parameters on which some learning algorithm depends, specified before the algorithm is applied, and where learning is interpreted in the broadest sense. For example, PCA feature reduction is a \"preprocessing\" transformation \"learning\" a projection from training data, governed by a dimension hyperparameter. Hyperparameters in our sense may specify configuration (eg, number of parallel processes) even when this does not affect the end-product of learning. (But we exclude verbosity level.)","category":"section"},{"location":"glossary/#model-(object-of-abstract-type-Model)","page":"Glossary","title":"model (object of abstract type Model)","text":"Object collecting together hyperpameters of a single algorithm.  Models are classified either as supervised or unsupervised models (eg, \"transformers\"), with corresponding subtypes Supervised <: Model and Unsupervised <: Model.","category":"section"},{"location":"glossary/#fitresult-(type-generally-defined-outside-of-MLJ)","page":"Glossary","title":"fitresult (type generally defined outside of MLJ)","text":"Also known as \"learned\" or \"fitted\" parameters, these are \"weights\", \"coefficients\", or similar parameters learned by an algorithm, after adopting the prescribed hyper-parameters. For example, decision trees of a random forest, the coefficients and intercept of a linear model, or the projection matrices of a PCA dimension-reduction algorithm.","category":"section"},{"location":"glossary/#operation","page":"Glossary","title":"operation","text":"Data-manipulating operations (methods) using some fitresult. For supervised learners, the predict, predict_mean, predict_median, or predict_mode methods; for transformers, the transform or inverse_transform method. An operation may also refer to an ordinary data-manipulating method that does not depend on a fit-result (e.g., a broadcasted logarithm) which is then called static operation for clarity. An operation that is not static is dynamic.","category":"section"},{"location":"glossary/#machine-(object-of-type-Machine)","page":"Glossary","title":"machine (object of type Machine)","text":"An object consisting of:\n\nA model\nA fit-result (undefined until training)\nTraining arguments (one for each data argument of the model's associated fit method). A training argument is data used for training (subsampled by specifying rows=... in fit!) but also in evaluation (subsampled by specifying rows=... in predict, predict_mean, etc). Generally, there are two training arguments for supervised models, and just one for unsupervised models. Each argument is either a Source node, wrapping concrete data supplied to the machine constructor, or a Node, in the case of a learning network (see below). Both kinds of nodes can be called with an optional rows=... keyword argument to (lazily) return concrete data.\n\nIn addition, machines store \"report\" metadata, for recording algorithm-specific statistics of training (eg, an internal estimate of generalization error, feature importances); and they cache information allowing the fit-result to be updated without repeating unnecessary information.\n\nMachines are trained by calls to a fit! method which may be passed an optional argument specifying the rows of data to be used in training.\n\nFor more, see the Machines section.","category":"section"},{"location":"glossary/#Learning-Networks-and-Composite-Models","page":"Glossary","title":"Learning Networks and Composite Models","text":"Note: Multiple machines in a learning network may share the same model, and multiple learning nodes may share the same machine.","category":"section"},{"location":"glossary/#source-node-(object-of-type-Source)","page":"Glossary","title":"source node (object of type Source)","text":"A container for training data and point of entry for new data in a learning network (see below).","category":"section"},{"location":"glossary/#node-(object-of-type-Node)","page":"Glossary","title":"node (object of type Node)","text":"Essentially a machine (whose arguments are possibly other nodes) wrapped in an associated operation (e.g., predict or inverse_transform). It consists primarily of:\n\nAn operation, static or dynamic.\nA machine, or nothing if the operation is static.\nUpstream connections to other nodes, specified by a list of arguments (one for each argument of the operation). These are the arguments on which the operation \"acts\" when the node N is called, as in N().","category":"section"},{"location":"glossary/#learning-network","page":"Glossary","title":"learning network","text":"A directed acyclic graph implicit in the connections of a collection of source(s) and nodes. ","category":"section"},{"location":"glossary/#wrapper","page":"Glossary","title":"wrapper","text":"Any model with one or more other models as hyper-parameters.","category":"section"},{"location":"glossary/#composite-model","page":"Glossary","title":"composite model","text":"Any wrapper, or any learning network, \"exported\" as a model (see Composing Models).","category":"section"},{"location":"models/ProbabilisticSGDClassifier_MLJScikitLearnInterface/#ProbabilisticSGDClassifier_MLJScikitLearnInterface","page":"ProbabilisticSGDClassifier","title":"ProbabilisticSGDClassifier","text":"ProbabilisticSGDClassifier\n\nA model type for constructing a probabilistic sgd classifier, based on MLJScikitLearnInterface.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nProbabilisticSGDClassifier = @load ProbabilisticSGDClassifier pkg=MLJScikitLearnInterface\n\nDo model = ProbabilisticSGDClassifier() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in ProbabilisticSGDClassifier(loss=...).","category":"section"},{"location":"models/ProbabilisticSGDClassifier_MLJScikitLearnInterface/#Hyper-parameters","page":"ProbabilisticSGDClassifier","title":"Hyper-parameters","text":"loss = log_loss\npenalty = l2\nalpha = 0.0001\nl1_ratio = 0.15\nfit_intercept = true\nmax_iter = 1000\ntol = 0.001\nshuffle = true\nverbose = 0\nepsilon = 0.1\nn_jobs = nothing\nrandom_state = nothing\nlearning_rate = optimal\neta0 = 0.0\npower_t = 0.5\nearly_stopping = false\nvalidation_fraction = 0.1\nn_iter_no_change = 5\nclass_weight = nothing\nwarm_start = false\naverage = false","category":"section"},{"location":"models/HuberRegressor_MLJScikitLearnInterface/#HuberRegressor_MLJScikitLearnInterface","page":"HuberRegressor","title":"HuberRegressor","text":"HuberRegressor\n\nA model type for constructing a Huber regressor, based on MLJScikitLearnInterface.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nHuberRegressor = @load HuberRegressor pkg=MLJScikitLearnInterface\n\nDo model = HuberRegressor() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in HuberRegressor(epsilon=...).","category":"section"},{"location":"models/HuberRegressor_MLJScikitLearnInterface/#Hyper-parameters","page":"HuberRegressor","title":"Hyper-parameters","text":"epsilon = 1.35\nmax_iter = 100\nalpha = 0.0001\nwarm_start = false\nfit_intercept = true\ntol = 1.0e-5","category":"section"},{"location":"models/EpsilonSVR_LIBSVM/#EpsilonSVR_LIBSVM","page":"EpsilonSVR","title":"EpsilonSVR","text":"EpsilonSVR\n\nA model type for constructing a ϵ-support vector regressor, based on LIBSVM.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nEpsilonSVR = @load EpsilonSVR pkg=LIBSVM\n\nDo model = EpsilonSVR() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in EpsilonSVR(kernel=...).\n\nReference for algorithm and core C-library: C.-C. Chang and C.-J. Lin (2011): \"LIBSVM: a library for support vector machines.\" ACM Transactions on Intelligent Systems and Technology, 2(3):27:1–27:27. Updated at https://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf. \n\nThis model is an adaptation of the classifier SVC to regression, but has an additional parameter epsilon (denoted ϵ in the cited reference).","category":"section"},{"location":"models/EpsilonSVR_LIBSVM/#Training-data","page":"EpsilonSVR","title":"Training data","text":"In MLJ or MLJBase, bind an instance model to data with:\n\nmach = machine(model, X, y)\n\nwhere\n\nX: any table of input features (eg, a DataFrame) whose columns each have Continuous element scitype; check column scitypes with schema(X)\ny: is the target, which can be any AbstractVector whose element scitype is Continuous; check the scitype with scitype(y)\n\nTrain the machine using fit!(mach, rows=...).","category":"section"},{"location":"models/EpsilonSVR_LIBSVM/#Hyper-parameters","page":"EpsilonSVR","title":"Hyper-parameters","text":"kernel=LIBSVM.Kernel.RadialBasis: either an object that can be called, as in kernel(x1, x2), or one of the built-in kernels from the LIBSVM.jl package listed below.  Here x1 and x2 are vectors whose lengths match the number of columns of the training data X (see \"Examples\" below).\nLIBSVM.Kernel.Linear: (x1, x2) -> x1'*x2\nLIBSVM.Kernel.Polynomial: (x1, x2) -> gamma*x1'*x2 + coef0)^degree\nLIBSVM.Kernel.RadialBasis: (x1, x2) -> (exp(-gamma*norm(x1 - x2)^2))\nLIBSVM.Kernel.Sigmoid: (x1, x2) - > tanh(gamma*x1'*x2 + coef0)\nHere gamma, coef0, degree are other hyper-parameters. Serialization of models with user-defined kernels comes with some restrictions. See LIVSVM.jl issue91\ngamma = 0.0: kernel parameter (see above); if gamma==-1.0 then gamma = 1/nfeatures is used in training, where nfeatures is the number of features (columns of X).  If gamma==0.0 then gamma = 1/(var(Tables.matrix(X))*nfeatures) is used. Actual value used appears in the report (see below).\ncoef0 = 0.0: kernel parameter (see above)\ndegree::Int32 = Int32(3): degree in polynomial kernel (see above)\ncost=1.0 (range (0, Inf)): the parameter denoted C in the cited reference; for greater regularization, decrease cost\nepsilon=0.1 (range (0, Inf)): the parameter denoted ϵ in the cited reference; epsilon is the thickness of the penalty-free neighborhood of the graph of the prediction function (\"slab\" or \"tube\"). Specifically, a data point (x, y) incurs no training loss unless it is outside this neighborhood; the further away it is from the this neighborhood, the greater the loss penalty.\ncachesize=200.0 cache memory size in MB\ntolerance=0.001: tolerance for the stopping criterion\nshrinking=true: whether to use shrinking heuristics","category":"section"},{"location":"models/EpsilonSVR_LIBSVM/#Operations","page":"EpsilonSVR","title":"Operations","text":"predict(mach, Xnew): return predictions of the target given features Xnew having the same scitype as X above.","category":"section"},{"location":"models/EpsilonSVR_LIBSVM/#Fitted-parameters","page":"EpsilonSVR","title":"Fitted parameters","text":"The fields of fitted_params(mach) are:\n\nlibsvm_model: the trained model object created by the LIBSVM.jl package","category":"section"},{"location":"models/EpsilonSVR_LIBSVM/#Report","page":"EpsilonSVR","title":"Report","text":"The fields of report(mach) are:\n\ngamma: actual value of the kernel parameter gamma used in training","category":"section"},{"location":"models/EpsilonSVR_LIBSVM/#Examples","page":"EpsilonSVR","title":"Examples","text":"","category":"section"},{"location":"models/EpsilonSVR_LIBSVM/#Using-a-built-in-kernel","page":"EpsilonSVR","title":"Using a built-in kernel","text":"using MLJ\nimport LIBSVM\n\nEpsilonSVR = @load EpsilonSVR pkg=LIBSVM            ## model type\nmodel = EpsilonSVR(kernel=LIBSVM.Kernel.Polynomial) ## instance\n\nX, y = make_regression(rng=123) ## table, vector\nmach = machine(model, X, y) |> fit!\n\nXnew, _ = make_regression(3, rng=123)\n\njulia> yhat = predict(mach, Xnew)\n3-element Vector{Float64}:\n  0.2512132502584155\n  0.007340201523624579\n -0.2482949812264707","category":"section"},{"location":"models/EpsilonSVR_LIBSVM/#User-defined-kernels","page":"EpsilonSVR","title":"User-defined kernels","text":"k(x1, x2) = x1'*x2 ## equivalent to `LIBSVM.Kernel.Linear`\nmodel = EpsilonSVR(kernel=k)\nmach = machine(model, X, y) |> fit!\n\njulia> yhat = predict(mach, Xnew)\n3-element Vector{Float64}:\n  1.1121225361666656\n  0.04667702229741916\n -0.6958148424680672\n\nSee also NuSVR, LIVSVM.jl and the original C implementation documentation.","category":"section"},{"location":"models/EvoSplineRegressor_EvoLinear/#EvoSplineRegressor_EvoLinear","page":"EvoSplineRegressor","title":"EvoSplineRegressor","text":"EvoSplineRegressor(; kwargs...)\n\nA model type for constructing a EvoSplineRegressor, based on EvoLinear.jl, and implementing both an internal API and the MLJ model interface.","category":"section"},{"location":"models/EvoSplineRegressor_EvoLinear/#Keyword-arguments","page":"EvoSplineRegressor","title":"Keyword arguments","text":"loss=:mse: loss function to be minimised.    Can be one of:\n:mse\n:logistic\n:poisson\n:gamma\n:tweedie\nnrounds=10: maximum number of training rounds.\neta=1: Learning rate. Typically in the range [1e-2, 1].\nL1=0: Regularization penalty applied by shrinking to 0 weight update if update is < L1. No penalty if update > L1. Results in sparse feature selection. Typically in the [0, 1] range on normalized features.\nL2=0: Regularization penalty applied to the squared of the weight update value. Restricts large parameter values. Typically in the [0, 1] range on normalized features.\nrng=123: random seed. Not used at the moment.\nupdater=:all: training method. Only :all is supported at the moment. Gradients for each feature are computed simultaneously, then bias is updated based on all features update.\ndevice=:cpu: Only :cpu is supported at the moment.","category":"section"},{"location":"models/EvoSplineRegressor_EvoLinear/#Internal-API","page":"EvoSplineRegressor","title":"Internal API","text":"Do config = EvoSplineRegressor() to construct an hyper-parameter struct with default hyper-parameters. Provide keyword arguments as listed above to override defaults, for example:\n\nEvoSplineRegressor(loss=:logistic, L1=1e-3, L2=1e-2, nrounds=100)","category":"section"},{"location":"models/EvoSplineRegressor_EvoLinear/#Training-model","page":"EvoSplineRegressor","title":"Training model","text":"A model is built using fit:\n\nconfig = EvoSplineRegressor()\nm = fit(config; x, y, w)","category":"section"},{"location":"models/EvoSplineRegressor_EvoLinear/#Inference","page":"EvoSplineRegressor","title":"Inference","text":"Fitted results is an EvoLinearModel which acts as a prediction function when passed a features matrix as argument.  \n\npreds = m(x)","category":"section"},{"location":"models/EvoSplineRegressor_EvoLinear/#MLJ-Interface","page":"EvoSplineRegressor","title":"MLJ Interface","text":"From MLJ, the type can be imported using:\n\nEvoSplineRegressor = @load EvoSplineRegressor pkg=EvoLinear\n\nDo model = EvoLinearRegressor() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in EvoSplineRegressor(loss=...).","category":"section"},{"location":"models/EvoSplineRegressor_EvoLinear/#Training-model-2","page":"EvoSplineRegressor","title":"Training model","text":"In MLJ or MLJBase, bind an instance model to data with mach = machine(model, X, y) where: \n\nX: any table of input features (eg, a DataFrame) whose columns each have one of the following element scitypes: Continuous, Count, or <:OrderedFactor; check column scitypes with schema(X)\ny: is the target, which can be any AbstractVector whose element scitype is <:Continuous; check the scitype with scitype(y)\n\nTrain the machine using fit!(mach, rows=...).","category":"section"},{"location":"models/EvoSplineRegressor_EvoLinear/#Operations","page":"EvoSplineRegressor","title":"Operations","text":"predict(mach, Xnew): return predictions of the target given\n\nfeatures Xnew having the same scitype as X above. Predictions   are deterministic.","category":"section"},{"location":"models/EvoSplineRegressor_EvoLinear/#Fitted-parameters","page":"EvoSplineRegressor","title":"Fitted parameters","text":"The fields of fitted_params(mach) are:\n\n:fitresult: the SplineModel object returned by EvoSplineRegressor fitting algorithm.","category":"section"},{"location":"models/EvoSplineRegressor_EvoLinear/#Report","page":"EvoSplineRegressor","title":"Report","text":"The fields of report(mach) are:\n\n:coef: Vector of coefficients (βs) associated to each of the features.\n:bias: Value of the bias.\n:names: Names of each of the features.","category":"section"},{"location":"models/RandomForestRegressor_BetaML/#RandomForestRegressor_BetaML","page":"RandomForestRegressor","title":"RandomForestRegressor","text":"mutable struct RandomForestRegressor <: MLJModelInterface.Deterministic\n\nA simple Random Forest model for regression with support for Missing data, from the Beta Machine Learning Toolkit (BetaML).","category":"section"},{"location":"models/RandomForestRegressor_BetaML/#Hyperparameters:","page":"RandomForestRegressor","title":"Hyperparameters:","text":"n_trees::Int64: Number of (decision) trees in the forest [def: 30]\nmax_depth::Int64: The maximum depth the tree is allowed to reach. When this is reached the node is forced to become a leaf [def: 0, i.e. no limits]\nmin_gain::Float64: The minimum information gain to allow for a node's partition [def: 0]\nmin_records::Int64: The minimum number of records a node must holds to consider for a partition of it [def: 2]\nmax_features::Int64: The maximum number of (random) features to consider at each partitioning [def: 0, i.e. square root of the data dimension]\nsplitting_criterion::Function: This is the name of the function to be used to compute the information gain of a specific partition. This is done by measuring the difference betwwen the \"impurity\" of the labels of the parent node with those of the two child nodes, weighted by the respective number of items. [def: variance]. Either variance or a custom function. It can also be an anonymous function.\nβ::Float64: Parameter that regulate the weights of the scoring of each tree, to be (optionally) used in prediction based on the error of the individual trees computed on the records on which trees have not been trained. Higher values favour \"better\" trees, but too high values will cause overfitting [def: 0, i.e. uniform weigths]\nrng::Random.AbstractRNG: A Random Number Generator to be used in stochastic parts of the code [deafult: Random.GLOBAL_RNG]","category":"section"},{"location":"models/RandomForestRegressor_BetaML/#Example:","page":"RandomForestRegressor","title":"Example:","text":"julia> using MLJ\n\njulia> X, y        = @load_boston;\n\njulia> modelType   = @load RandomForestRegressor pkg = \"BetaML\" verbosity=0\nBetaML.Trees.RandomForestRegressor\n\njulia> model       = modelType()\nRandomForestRegressor(\n  n_trees = 30, \n  max_depth = 0, \n  min_gain = 0.0, \n  min_records = 2, \n  max_features = 0, \n  splitting_criterion = BetaML.Utils.variance, \n  β = 0.0, \n  rng = Random._GLOBAL_RNG())\n\njulia> mach        = machine(model, X, y);\n\njulia> fit!(mach);\n[ Info: Training machine(RandomForestRegressor(n_trees = 30, …), …).\n\njulia> ŷ           = predict(mach, X);\n\njulia> hcat(y,ŷ)\n506×2 Matrix{Float64}:\n 24.0  25.8433\n 21.6  22.4317\n 34.7  35.5742\n 33.4  33.9233\n  ⋮    \n 23.9  24.42\n 22.0  22.4433\n 11.9  15.5833","category":"section"},{"location":"models/KMeans_ParallelKMeans/#KMeans_ParallelKMeans","page":"KMeans","title":"KMeans","text":"Parallel & lightning fast implementation of all available variants of the KMeans clustering algorithm                              in native Julia. Compatible with Julia 1.3+","category":"section"},{"location":"models/BisectingKMeans_MLJScikitLearnInterface/#BisectingKMeans_MLJScikitLearnInterface","page":"BisectingKMeans","title":"BisectingKMeans","text":"BisectingKMeans\n\nA model type for constructing a bisecting k means, based on MLJScikitLearnInterface.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nBisectingKMeans = @load BisectingKMeans pkg=MLJScikitLearnInterface\n\nDo model = BisectingKMeans() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in BisectingKMeans(n_clusters=...).\n\nBisecting K-Means clustering.","category":"section"},{"location":"logging_workflows/#Logging-Workflows","page":"Logging Workflows","title":"Logging Workflows","text":"Currently the following workflows can log their outcomes to an external machine learning tracking platform, such as MLflow:\n\nEstimating model performance using evaluate/evaluate!.\nModel tuning, using the TunedModel wrapper, as described under Tuning Models.\n\nTo enable logging one must create a logger object for the relevant tracking platform, and either:\n\nProvide logger as an explicit keyword argument in the workflow, as in evaluate(...; logger=...) or TunedModel(...; logger=...); or\nSet a global default logger with the call default_logger(logger).\n\nMLJ logging examples are given in the MLJFlow.jl documentation.","category":"section"},{"location":"logging_workflows/#Supported-tracking-platforms","page":"Logging Workflows","title":"Supported tracking platforms","text":"MLflow is natively supported by MLJ. You will still need to install MLflow itself, and separately launch an MLflow service; see the MLflow docs on how to do this. The service can immediately be wrapped to create a logger object, as demonstrated in the MLJFlow.jl documentation.\n\nwarning: Warning\nMLJFlow.jl is a new package still under active development and should be regarded as experimental. At this time, breaking changes to MLJFlow.jl will not necessarily trigger new breaking releases of MLJ.jl.","category":"section"},{"location":"models/EntityEmbedder_MLJFlux/#EntityEmbedder_MLJFlux","page":"EntityEmbedder","title":"EntityEmbedder","text":"EntityEmbedder(; model=supervised_mljflux_model)\n\nWrapper for a MLJFlux supervised model, to convert it to a transformer. Such transformers are still presented a target variable in training, but they behave as transformers in MLJ pipelines. They are entity embedding transformers, in the sense of the article, \"Entity Embeddings of Categorical Variables\" by Cheng Guo, Felix Berkhahn.","category":"section"},{"location":"models/EntityEmbedder_MLJFlux/#Training-data","page":"EntityEmbedder","title":"Training data","text":"In MLJ (or MLJBase) bind an instance unsupervised model to data with\n\nmach = machine(embed_model, X, y)\n\nHere:\n\nembed_model is an instance of EntityEmbedder, which wraps a supervised MLJFlux model, model, which must be an instance of one of these: MLJFlux.NeuralNetworkClassifier, NeuralNetworkBinaryClassifier, MLJFlux.NeuralNetworkRegressor,MLJFlux.MultitargetNeuralNetworkRegressor.\nX is any table of input features supported by the model being wrapped. Features to be transformed must have element scitype Multiclass or OrderedFactor. Use schema(X) to check scitypes.\ny is the target, which can be any AbstractVector supported by the model being wrapped.\n\nTrain the machine using fit!(mach).","category":"section"},{"location":"models/EntityEmbedder_MLJFlux/#Examples","page":"EntityEmbedder","title":"Examples","text":"In the following example we wrap a NeuralNetworkClassifier as an EntityEmbedder, so that it can be used to supply continuously encoded features to a nearest neighbor model, which does not support categorical features.\n\nusing MLJ\n\n## Setup some data\nN = 400\nX = (\n  a = rand(Float32, N),\n  b = categorical(rand(\"abcde\", N)),\n  c = categorical(rand(\"ABCDEFGHIJ\", N), ordered = true),\n)\n\ny = categorical(rand(\"YN\", N));\n\n## Initiate model\nEntityEmbedder = @load EntityEmbedder pkg=MLJFlux\n\n## Flux model to do learn the entity embeddings:\nNeuralNetworkClassifier = @load NeuralNetworkClassifier pkg=MLJFlux\n\n## Other supervised model type, requiring `Continuous` features:\nKNNClassifier = @load KNNClassifier pkg=NearestNeighborModels\n\n## Instantiate the models:\nclf = NeuralNetworkClassifier(embedding_dims=Dict(:b => 2, :c => 3))\nemb = EntityEmbedder(clf)\n\n## For illustrative purposes, train the embedder on its own:\nmach = machine(emb, X, y)\nfit!(mach)\nXnew = transform(mach, X)\n\n## And compare feature scitypes:\nschema(X)\nschema(Xnew)\n\n## Now construct the pipeline:\npipe = emb |> KNNClassifier()\n\n## And train it to make predictions:\nmach = machine(pipe, X, y)\nfit!(mach)\npredict(mach, X)[1:3]\n\nIt is to be emphasized that the NeuralNertworkClassifier is only being used to learn entity embeddings, not to make predictions, which here are made by KNNClassifier().\n\nSee also NeuralNetworkClassifier, NeuralNetworkRegressor","category":"section"},{"location":"models/ComplementNBClassifier_MLJScikitLearnInterface/#ComplementNBClassifier_MLJScikitLearnInterface","page":"ComplementNBClassifier","title":"ComplementNBClassifier","text":"ComplementNBClassifier\n\nA model type for constructing a Complement naive Bayes classifier, based on MLJScikitLearnInterface.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nComplementNBClassifier = @load ComplementNBClassifier pkg=MLJScikitLearnInterface\n\nDo model = ComplementNBClassifier() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in ComplementNBClassifier(alpha=...).\n\nSimilar to MultinomialNBClassifier but with more robust assumptions. Suited for imbalanced datasets.","category":"section"},{"location":"models/RobustRegressor_MLJLinearModels/#RobustRegressor_MLJLinearModels","page":"RobustRegressor","title":"RobustRegressor","text":"RobustRegressor\n\nA model type for constructing a robust regressor, based on MLJLinearModels.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nRobustRegressor = @load RobustRegressor pkg=MLJLinearModels\n\nDo model = RobustRegressor() to construct an instance with default hyper-parameters.\n\nRobust regression is a linear model with objective function\n\n$\n\n∑ρ(Xθ - y) + n⋅λ|θ|₂² + n⋅γ|θ|₁ $\n\nwhere ρ is a robust loss function (e.g. the Huber function) and n is the number of observations.\n\nIf scale_penalty_with_samples = false the objective function is instead\n\n$\n\n∑ρ(Xθ - y) + λ|θ|₂² + γ|θ|₁ $\n\n.\n\nDifferent solver options exist, as indicated under \"Hyperparameters\" below. ","category":"section"},{"location":"models/RobustRegressor_MLJLinearModels/#Training-data","page":"RobustRegressor","title":"Training data","text":"In MLJ or MLJBase, bind an instance model to data with\n\nmach = machine(model, X, y)\n\nwhere:\n\nX is any table of input features (eg, a DataFrame) whose columns have Continuous scitype; check column scitypes with schema(X)\ny is the target, which can be any AbstractVector whose element scitype is Continuous; check the scitype with scitype(y)\n\nTrain the machine using fit!(mach, rows=...).","category":"section"},{"location":"models/RobustRegressor_MLJLinearModels/#Hyperparameters","page":"RobustRegressor","title":"Hyperparameters","text":"rho::MLJLinearModels.RobustRho: the type of robust loss, which can be any instance of     MLJLinearModels.L where L is one of: AndrewsRho,     BisquareRho, FairRho, HuberRho, LogisticRho,     QuantileRho, TalwarRho, HuberRho, TalwarRho.  Default: HuberRho(0.1)\nlambda::Real: strength of the regularizer if penalty is :l2 or :l1.     Strength of the L2 regularizer if penalty is :en. Default: 1.0\ngamma::Real: strength of the L1 regularizer if penalty is :en. Default: 0.0\npenalty::Union{String, Symbol}: the penalty to use, either :l2, :l1, :en (elastic net) or :none. Default: :l2\nfit_intercept::Bool: whether to fit the intercept or not. Default: true\npenalize_intercept::Bool: whether to penalize the intercept. Default: false\nscale_penalty_with_samples::Bool: whether to scale the penalty with the number of observations. Default: true\nsolver::Union{Nothing, MLJLinearModels.Solver}: some instance of MLJLinearModels.S where S is one of: LBFGS, IWLSCG, Newton, NewtonCG, if penalty = :l2, and ProxGrad otherwise.\nIf solver = nothing (default) then LBFGS() is used, if penalty = :l2, and otherwise ProxGrad(accel=true) (FISTA) is used.\nSolver aliases: FISTA(; kwargs...) = ProxGrad(accel=true, kwargs...), ISTA(; kwargs...) = ProxGrad(accel=false, kwargs...) Default: nothing","category":"section"},{"location":"models/RobustRegressor_MLJLinearModels/#Example","page":"RobustRegressor","title":"Example","text":"using MLJ\nX, y = make_regression()\nmach = fit!(machine(RobustRegressor(), X, y))\npredict(mach, X)\nfitted_params(mach)\n\nSee also HuberRegressor, QuantileRegressor.","category":"section"},{"location":"controlling_iterative_models/#Controlling-Iterative-Models","page":"Controlling Iterative Models","title":"Controlling Iterative Models","text":"Iterative supervised machine learning models are usually trained until an out-of-sample estimate of the performance satisfies some stopping criterion, such as k consecutive deteriorations of the performance (see Patience below). A more sophisticated kind of control might dynamically mutate parameters, such as a learning rate, in response to the behavior of these estimates.\n\nSome iterative model implementations enable some form of automated control, with the method and options for doing so varying from model to model. But sometimes it is up to the user to arrange control, which in the crudest case reduces to manually experimenting with the iteration parameter.\n\nIn response to this ad hoc state of affairs, MLJ provides a uniform and feature-rich interface for controlling any iterative model that exposes its iteration parameter as a hyper-parameter, and which implements the \"warm restart\" behavior described in Machines.","category":"section"},{"location":"controlling_iterative_models/#Basic-use","page":"Controlling Iterative Models","title":"Basic use","text":"As in Tuning Models, iteration control in MLJ is implemented as a model wrapper, which allows composition with other meta-algorithms. Ordinarily, the wrapped model behaves just like the original model, but with the training occurring on a subset of the provided data (to allow computation of an out-of-sample loss) and with the iteration parameter automatically determined by the controls specified in the wrapper.\n\nBy setting retrain=true one can ask that the wrapped model retrain on all supplied data, after learning the appropriate number of iterations from the controlled training phase:\n\nusing MLJ\n\nX, y = make_moons(100, rng=123, noise=0.5)\nEvoTreeClassifier = @load EvoTreeClassifier verbosity=0\n\niterated_model = IteratedModel(model=EvoTreeClassifier(rng=123, eta=0.005),\n                               resampling=Holdout(),\n                               measures=log_loss,\n                               controls=[Step(5),\n                                         Patience(2),\n                                         NumberLimit(100)],\n                               retrain=true)\n\nmach = machine(iterated_model, X, y)\nnothing # hide\n\nfit!(mach)\n\nAs detailed under IteratedModel below, the specified controls are repeatedly applied in sequence to a training machine, constructed under the hood, until one of the controls triggers a stop. Here Step(5) means \"Compute 5 more iterations\" (in this case starting from none); Patience(2) means \"Stop at the end of the control cycle if there have been 2 consecutive drops in the log loss\"; and NumberLimit(100) is a safeguard ensuring a stop after 100 control cycles (500 iterations). See Controls provided below for a complete list.\n\nBecause iteration is implemented as a wrapper, the \"self-iterating\" model can be evaluated using cross-validation, say, and the number of iterations on each fold will generally be different:\n\ne = evaluate!(mach, resampling=CV(nfolds=3), measure=log_loss, verbosity=0);\nmap(e.report_per_fold) do r\n    r.n_iterations\nend\n\nAlternatively, one might wrap the self-iterating model in a tuning strategy, using TunedModel; see Tuning Models. In this way, the optimization of some other hyper-parameter is realized simultaneously with that of the iteration parameter, which will frequently be more efficient than a direct two-parameter search.","category":"section"},{"location":"controlling_iterative_models/#Controls-provided","page":"Controlling Iterative Models","title":"Controls provided","text":"In the table below, mach is the training machine being iterated, constructed by binding the supplied data to the model specified in the IteratedModel wrapper, but trained in each iteration on a subset of the data, according to the value of the resampling hyper-parameter of the wrapper (using all data if resampling=nothing).\n\ncontrol description can trigger a stop\n  \nStep(n=1) Train model for n more iterations no\nTimeLimit(t=0.5) Stop after t hours yes\nNumberLimit(n=100) Stop after n applications of the control yes\nNumberSinceBest(n=6) Stop when best loss occurred n control applications ago yes\nInvalidValue() Stop when NaN, Inf or -Inf loss/training loss encountered yes\nThreshold(value=0.0) Stop when loss < value yes\nGL(alpha=2.0) † Stop after the \"generalization loss (GL)\" exceeds alpha yes\nPQ(alpha=0.75, k=5) † Stop after \"progress-modified GL\" exceeds alpha yes\nPatience(n=5) † Stop after n consecutive loss increases yes\nWarmup(c; n=1) Wait for n loss updates before checking criteria c no\nInfo(f=identity) Log to Info the value of f(mach), where mach is current machine no\nWarn(predicate; f=\"\") Log to Warn the value of f or f(mach), if predicate(mach) holds no\nError(predicate; f=\"\") Log to Error the value of f or f(mach), if predicate(mach) holds and then stop yes\nCallback(f=mach->nothing) Call f(mach) yes\nWithNumberDo(f=n->@info(n)) Call f(n + 1) where n is the number of complete control cycles so far yes\nWithIterationsDo(f=i->@info(\"iterations: $i\")) Call f(i), where i is total number of iterations yes\nWithLossDo(f=x->@info(\"loss: $x\")) Call f(loss) where loss is the current loss yes\nWithTrainingLossesDo(f=v->@info(v)) Call f(v) where v is the current batch of training losses yes\nWithEvaluationDo(f->e->@info(\"evaluation: $e)) Call f(e) where e is the current performance evaluation object yes\nWithFittedParamsDo(f->fp->@info(\"fitted_params: $fp)) Call f(fp) where fp is fitted parameters of training machine yes\nWithReportDo(f->e->@info(\"report: $e)) Call f(r) where r is the training machine report yes\nWithModelDo(f->m->@info(\"model: $m)) Call f(m) where m is the model, which may be mutated by f yes\nWithMachineDo(f->mach->@info(\"report: $mach)) Call f(mach) wher mach is the training machine in its current state yes\nSave(filename=\"machine.jls\") Save current training machine to machine1.jls, machine2.jsl, etc yes\n\nTable 1. Atomic controls. Some advanced options are omitted.\n\n† For more on these controls see Prechelt, Lutz  (1998):  \"Early Stopping - But When?\", in Neural Networks: Tricks of the  Trade, ed. G. Orr, Springer.\n\nStopping option. All the following controls trigger a stop if the provided function f returns true and stop_if_true=true is specified in the constructor: Callback, WithNumberDo, WithLossDo, WithTrainingLossesDo.\n\nThere are also three control wrappers to modify a control's behavior:\n\nwrapper description\nIterationControl.skip(control, predicate=1) Apply control every predicate applications of the control wrapper (can also be a function; see doc-string)\nIterationControl.louder(control, by=1) Increase the verbosity level of control by the specified value (negative values lower verbosity)\nIterationControl.with_state_do(control; f=...) Apply control and call f(x) where x is the internal state of control; useful for debugging. Default f logs state to Info. Warning: internal control state is not yet part of the public API.\nIterationControl.composite(controls...) Apply each control in controls in sequence; used internally by IterationControl.jl\n\nTable 2. Wrapped controls","category":"section"},{"location":"controlling_iterative_models/#Using-training-losses,-and-controlling-model-tuning","page":"Controlling Iterative Models","title":"Using training losses, and controlling model tuning","text":"Some iterative models report a training loss, as a byproduct of a fit! call and these can be used in two ways:\n\nTo supplement an out-of-sample estimate of the loss in deciding when to stop, as in the PQ stopping criterion (see Prechelt, Lutz (1998))); or\nAs a (generally less reliable) substitute for an out-of-sample loss, when wishing to train exclusively on all supplied data.\n\nTo have IteratedModel bind all data to the training machine and use training losses in place of an out-of-sample loss, specify resampling=nothing. To check if MyFavoriteIterativeModel reports training losses, load the model code and inspect supports_training_losses(MyFavoriteIterativeModel) (or do info(\"MyFavoriteIterativeModel\"))","category":"section"},{"location":"controlling_iterative_models/#Controlling-model-tuning","page":"Controlling Iterative Models","title":"Controlling model tuning","text":"An example of scenario 2 occurs when controlling hyperparameter optimization (model tuning). Recall that MLJ's TunedModel wrapper is implemented as an iterative model. Moreover, this wrapper reports, as a training loss, the lowest value of the optimization objective function so far (typically the lowest value of an out-of-sample loss, or -1 times an out-of-sample score). One may want to simply end the hyperparameter search when this value meets the NumberSinceBest stopping criterion discussed below, say, rather than introducing an extra layer of resampling to first \"learn\" the optimal value of the iteration parameter.\n\nIn the following example, we conduct a RandomSearch for the optimal value of the regularization parameter lambda in a RidgeRegressor using 6-fold cross-validation. By wrapping our \"self-tuning\" version of the regressor as an IteratedModel, with resampling=nothing and NumberSinceBest(20) in the controls, we terminate the search when the number of lambda values tested since the previous best cross-validation loss reaches 20.\n\nusing MLJ\n\nX, y = @load_boston;\nRidgeRegressor = @load RidgeRegressor pkg=MLJLinearModels verbosity=0\nmodel = RidgeRegressor()\nr = range(model, :lambda, lower=-1, upper=2, scale=x->10^x)\nself_tuning_model = TunedModel(model=model,\n                               tuning=RandomSearch(rng=123),\n                               resampling=CV(nfolds=6),\n                               range=r,\n                               measure=mae);\niterated_model = IteratedModel(model=self_tuning_model,\n                               resampling=nothing,\n                               control=[Step(1), NumberSinceBest(20), NumberLimit(1000)])\nmach = machine(iterated_model, X, y)\nnothing # hide\n\nfit!(mach)\n\nreport(mach).model_report.best_model\n\nWe can use mach here to directly obtain predictions using the optimal model (trained on all data), as in\n\npredict(mach, selectrows(X, 1:4))","category":"section"},{"location":"controlling_iterative_models/#Custom-controls","page":"Controlling Iterative Models","title":"Custom controls","text":"Under the hood, control in MLJIteration is implemented using IterationControl.jl. Rather than iterating a training machine directly, we iterate a wrapped version of this object, which includes other information that a control may want to access, such as the MLJ evaluation object. This information is summarized under The training machine wrapper below.\n\nControls must implement two update! methods, one for initializing the control's state on the first application of the control (this state being external to the control struct) and one for all subsequent control applications, which generally updates the state as well. There are two optional methods: done, for specifying conditions triggering a stop, and takedown for specifying actions to perform at the end of controlled training.\n\nWe summarize the training algorithm, as it relates to controls, after giving a simple example.","category":"section"},{"location":"controlling_iterative_models/#Example-1-Non-uniform-iteration-steps","page":"Controlling Iterative Models","title":"Example 1 - Non-uniform iteration steps","text":"Below we define a control, IterateFromList(list), to train, on each application of the control, until the iteration count reaches the next value in a user-specified list, triggering a stop when the list is exhausted. For example, to train on iteration counts on a log scale, one might use IterateFromList([round(Int, 10^x) for x in range(1, 2, length=10)].\n\nIn the code, wrapper is an object that wraps the training machine (see above). The variable n is a counter for control cycles (unused in this example).\n\nimport IterationControl # or MLJ.IterationControl\n\nstruct IterateFromList\n    list::Vector{<:Int} # list of iteration parameter values\n    IterateFromList(v) = new(unique(sort(v)))\nend\n\nfunction IterationControl.update!(control::IterateFromList, wrapper, verbosity, n)\n    Δi = control.list[1]\n    verbosity > 1 && @info \"Training $Δi more iterations. \"\n    MLJIteration.train!(wrapper, Δi) # trains the training machine\n    return (index = 2, )\nend\n\nfunction IterationControl.update!(control::IterateFromList, wrapper, verbosity, n, state)\n    index = state.positioin_in_list\n    Δi = control.list[i] - wrapper.n_iterations\n    verbosity > 1 && @info \"Training $Δi more iterations. \"\n    MLJIteration.train!(wrapper, Δi)\n    return (index = index + 1, )\nend\n\nThe first update method will be called the first time the control is applied, returning an initialized state = (index = 2,), which is passed to the second update method, which is called on subsequent control applications (and which returns the updated state).\n\nA done method articulates the criterion for stopping:\n\nIterationControl.done(control::IterateFromList, state) =\n    state.index > length(control.list)\n\nFor the sake of illustration, we'll implement a takedown method; its return value is included in the IteratedModel report:\n\nIterationControl.takedown(control::IterateFromList, verbosity, state)\n    verbosity > 1 && = @info \"Stepped through these values of the \"*\n                              \"iteration parameter: $(control.list)\"\n    return (iteration_values=control.list, )\nend","category":"section"},{"location":"controlling_iterative_models/#The-training-machine-wrapper","page":"Controlling Iterative Models","title":"The training machine wrapper","text":"A training machine wrapper has these properties:\n\nwrapper.machine - the training machine, type Machine\nwrapper.model   - the mutable atomic model, coinciding with wrapper.machine.model\nwrapper.n_cycles - the number IterationControl.train!(wrapper, _) calls so far; generally the current control cycle count\nwrapper.n_iterations - the total number of iterations applied to the model so far\nwrapper.Δiterations - the number of iterations applied in the last IterationControl.train!(wrapper, _) call\nwrapper.loss - the out-of-sample loss (based on the first measure in measures)\nwrapper.training_losses - the last batch of training losses (if reported by model), an abstract vector of length wrapper.Δiteration.\nwrapper.evaluation - the complete MLJ performance evaluation object, which has the following properties: measure, measurement, per_fold, per_observation, fitted_params_per_fold, report_per_fold (here there is only one fold). For further details, see Evaluating Model Performance.","category":"section"},{"location":"controlling_iterative_models/#The-training-algorithm","page":"Controlling Iterative Models","title":"The training algorithm","text":"Here now is a simplified description of the training of an IteratedModel. First, the atomic model is bound in a machine - the training machine above - to a subset of the supplied data, and then wrapped in an object called wrapper below. To train the training machine machine for i more iterations, and update the other data in the wrapper, requires the call MLJIteration.train!(wrapper, i). Only controls can make this call (e.g., Step(...), or IterateFromList(...) above). If we assume for simplicity there is only a single control, called control, then training proceeds as follows:\n\nn = 1 # initialize control cycle counter\nstate = update!(control, wrapper, verbosity, n)\nfinished = done(control, state)\n\n# subsequent training events:\nwhile !finished\n    n += 1\n    state = update!(control, wrapper, verbosity, n, state)\n    finished = done(control, state)\nend\n\n# finalization:\nreturn takedown(control, verbosity, state)","category":"section"},{"location":"controlling_iterative_models/#Example-2-Cyclic-learning-rates","page":"Controlling Iterative Models","title":"Example 2 - Cyclic learning rates","text":"The control below implements a triangular cyclic learning rate policy, close to that described in L. N. Smith (2019): \"Cyclical Learning Rates for Training Neural Networks,\" 2017 IEEE Winter Conference on Applications of Computer Vision (WACV), Santa Rosa, CA, USA, pp. 464-472. [In that paper learning rates are mutated (slowly) during each training iteration (epoch) while here mutations can occur once per iteration of the model, at most.]\n\nFor the sake of illustration, we suppose the iterative model, model, specified in the IteratedModel constructor, has a field called :learning_parameter, and that mutating this parameter does not trigger cold-restarts.\n\nstruct CycleLearningRate{F<:AbstractFloat}\n    stepsize::Int\n    lower::F\n    upper::F\nend\n\n# return one cycle of learning rate values:\nfunction one_cycle(c::CycleLearningRate)\n    rise = range(c.lower, c.upper, length=c.stepsize + 1)\n    fall = reverse(rise)\n    return vcat(rise[1:end - 1], fall[1:end - 1])\nend\n\nfunction IterationControl.update!(control::CycleLearningRate,\n                                  wrapper,\n                                  verbosity,\n                                  n,\n                                  state = (learning_rates=nothing, ))\n    rates = n == 0 ? one_cycle(control) : state.learning_rates\n    index = mod(n, length(rates)) + 1\n    r = rates[index]\n    verbosity > 1 && @info \"learning rate: $r\"\n    wrapper.model.iteration_control = r\n    return (learning_rates = rates,)\nend","category":"section"},{"location":"controlling_iterative_models/#API-Reference","page":"Controlling Iterative Models","title":"API Reference","text":"","category":"section"},{"location":"controlling_iterative_models/#Controls","page":"Controlling Iterative Models","title":"Controls","text":"","category":"section"},{"location":"controlling_iterative_models/#Control-wrappers","page":"Controlling Iterative Models","title":"Control wrappers","text":"","category":"section"},{"location":"controlling_iterative_models/#MLJIteration.IteratedModel","page":"Controlling Iterative Models","title":"MLJIteration.IteratedModel","text":"IteratedModel(model;\n    controls=MLJIteration.DEFAULT_CONTROLS,\n    resampling=Holdout(),\n    measure=nothing,\n    retrain=false,\n    advanced_options...,\n)\n\nWrap the specified supervised model in the specified iteration controls. Here model should support iteration, which is true if (iteration_parameter(model) is different from nothing.\n\nAvailable controls: Step(), Info(), Warn(), Error(), Callback(), WithLossDo(), WithTrainingLossesDo(), WithNumberDo(), Data(), Disjunction(), GL(), InvalidValue(), Never(), NotANumber(), NumberLimit(), NumberSinceBest(), PQ(), Patience(), Threshold(), TimeLimit(), Warmup(), WithIterationsDo(), WithEvaluationDo(), WithFittedParamsDo(), WithReportDo(), WithMachineDo(), WithModelDo(), CycleLearningRate() and Save().\n\nimportant: Important\nTo make out-of-sample losses available to the controls, the wrapped model is only trained on part of the data, as iteration proceeds. The user may want to force retraining on all data after controlled iteration has finished by specifying retrain=true. See also \"Training\", and the retrain option, under \"Extended help\" below.\n\nExtended help\n\nOptions\n\ncontrols=Any[Step(1), Patience(5), GL(2.0), TimeLimit(Dates.Millisecond(108000)), InvalidValue()]: Controls are summarized at the \"Controls-provided\" section, but query individual doc-strings for details and advanced options. For creating your own controls, refer to the \"Custom controls\" section.\nresampling=Holdout(fraction_train=0.7): The default resampling holds back 30% of data for computing an out-of-sample estimate of performance (the \"loss\") for loss-based controls such as WithLossDo. Specify resampling=nothing if all data is to be used for controlled iteration, with each out-of-sample loss replaced by the most recent training loss, assuming this is made available by the model (supports_training_losses(model) == true). If the model does not report a training loss, you can use resampling=InSample() instead. Otherwise, resampling must have type Holdout or be a vector with one element of the form (train_indices, test_indices).\nmeasure=nothing: StatisticalMeasures.jl compatible measure for estimating model performance (the \"loss\", but the orientation is immaterial - i.e., this could be a score). Inferred by default. Ignored if resampling=nothing.\nretrain=false: If retrain=true or resampling=nothing, iterated_model behaves exactly like the original model but with the iteration parameter automatically selected (\"learned\"). That is, the model is retrained on all available data, using the same number of iterations, once controlled iteration has stopped. This is typically desired if wrapping the iterated model further, or when inserting in a pipeline or other composite model. If retrain=false (default) and resampling isa Holdout, then iterated_model behaves like the original model trained on a subset of the provided data.\nweights=nothing: per-observation weights to be passed to measure where supported; if unspecified, these are understood to be uniform.\nclass_weights=nothing: class-weights to be passed to measure where supported; if unspecified, these are understood to be uniform.\noperation=nothing: Operation, such as predict or predict_mode, for computing target values, or proxy target values, for consumption by measure; automatically inferred by default.\ncheck_measure=true: Specify false to override checks on measure for compatibility with the training data.\niteration_parameter=nothing: A symbol, such as :epochs, naming the iteration parameter of model; inferred by default. Note that the actual value of the iteration parameter in the supplied model is ignored; only the value of an internal clone is mutated during training the wrapped model.\ncache=true: Whether or not model-specific representations of data are cached in between iteration parameter increments; specify cache=false to prioritize memory over speed.\n\nTraining\n\nTraining an instance iterated_model of IteratedModel on some data (by binding to a machine and calling fit!, for example) performs the following actions:\n\nAssuming resampling !== nothing, the data is split into train and test sets, according to the specified resampling strategy.\nA clone of the wrapped model, model is bound to the train data in an internal machine, train_mach. If resampling === nothing, all data is used instead. This machine is the object to which controls are applied. For example, Callback(fitted_params |> print) will print the value of fitted_params(train_mach).\nThe iteration parameter of the clone is set to 0.\nThe specified controls are repeatedly applied to train_mach in sequence, until one of the controls triggers a stop. Loss-based controls (eg, Patience(), GL(), Threshold(0.001)) use an out-of-sample loss, obtained by applying measure to predictions and the test target values. (Specifically, these predictions are those returned by operation(train_mach).)  If resampling === nothing then the most recent training loss is used instead. Some controls require both out-of-sample and training losses (eg, PQ()).\nOnce a stop has been triggered, a clone of model is bound to all data in a machine called mach_production below, unless retrain == false (true by default) or resampling === nothing, in which case mach_production coincides with train_mach.\n\nPrediction\n\nCalling predict(mach, Xnew) in the example above returns predict(mach_production, Xnew). Similar similar statements hold for predict_mean, predict_mode, predict_median.\n\nControls that mutate parameters\n\nA control is permitted to mutate the fields (hyper-parameters) of train_mach.model (the clone of model). For example, to mutate a learning rate one might use the control\n\nCallback(mach -> mach.model.eta = 1.05*mach.model.eta)\n\nHowever, unless model supports warm restarts with respect to changes in that parameter, this will trigger retraining of train_mach from scratch, with a different training outcome, which is not recommended.\n\nWarm restarts\n\nIn the following example, the second fit! call will not restart training of the internal train_mach, assuming model supports warm restarts:\n\niterated_model = IteratedModel(\n    model,\n    controls = [Step(1), NumberLimit(100)],\n)\nmach = machine(iterated_model, X, y)\nfit!(mach) # train for 100 iterations\niterated_model.controls = [Step(1), NumberLimit(50)],\nfit!(mach) # train for an *extra* 50 iterations\n\nMore generally, if iterated_model is mutated and fit!(mach) is called again, then a warm restart is attempted if the only parameters to change are model or controls or both.\n\nSpecifically, train_mach.model is mutated to match the current value of iterated_model.model and the iteration parameter of the latter is updated to the last value used in the preceding fit!(mach) call. Then repeated application of the (updated) controls begin anew.\n\n\n\n\n\n","category":"function"},{"location":"controlling_iterative_models/#IterationControl.Step","page":"Controlling Iterative Models","title":"IterationControl.Step","text":"Step(; n=1)\n\nAn iteration control, as in, Step(2). \n\nTrain for n more iterations. Will never trigger a stop. \n\n\n\n\n\n","category":"type"},{"location":"controlling_iterative_models/#EarlyStopping.TimeLimit","page":"Controlling Iterative Models","title":"EarlyStopping.TimeLimit","text":"TimeLimit(; t=0.5)\n\nAn early stopping criterion for loss-reporting iterative algorithms. \n\nStopping is triggered after t hours have elapsed since the stopping criterion was initiated.\n\nAny Julia built-in Real type can be used for t. Subtypes of Period may also be used, as in TimeLimit(t=Minute(30)).\n\nInternally, t is rounded to nearest millisecond. ``\n\n\n\n\n\n","category":"type"},{"location":"controlling_iterative_models/#EarlyStopping.NumberLimit","page":"Controlling Iterative Models","title":"EarlyStopping.NumberLimit","text":"NumberLimit(; n=100)\n\nAn early stopping criterion for loss-reporting iterative algorithms. \n\nA stop is triggered by n consecutive loss updates, excluding \"training\" loss updates.\n\nIf wrapped in a stopper::EarlyStopper, this is the number of calls to done!(stopper).\n\n\n\n\n\n","category":"type"},{"location":"controlling_iterative_models/#EarlyStopping.NumberSinceBest","page":"Controlling Iterative Models","title":"EarlyStopping.NumberSinceBest","text":"NumberSinceBest(; n=6)\n\nAn early stopping criterion for loss-reporting iterative algorithms. \n\nA stop is triggered when the number of calls to the control, since the lowest value of the loss so far, is n.\n\nFor a customizable loss-based stopping criterion, use WithLossDo or WithTrainingLossesDo with the stop_if_true=true option. \n\n\n\n\n\n","category":"type"},{"location":"controlling_iterative_models/#EarlyStopping.InvalidValue","page":"Controlling Iterative Models","title":"EarlyStopping.InvalidValue","text":"InvalidValue()\n\nAn early stopping criterion for loss-reporting iterative algorithms. \n\nStop if a loss (or training loss) is NaN, Inf or -Inf (or, more precisely, if isnan(loss) or isinf(loss) is true).\n\nFor a customizable loss-based stopping criterion, use WithLossDo or WithTrainingLossesDo with the stop_if_true=true option. \n\n\n\n\n\n","category":"type"},{"location":"controlling_iterative_models/#EarlyStopping.Threshold","page":"Controlling Iterative Models","title":"EarlyStopping.Threshold","text":"Threshold(; value=0.0)\n\nAn early stopping criterion for loss-reporting iterative algorithms. \n\nA stop is triggered as soon as the loss drops below value.\n\nFor a customizable loss-based stopping criterion, use WithLossDo or WithTrainingLossesDo with the stop_if_true=true option. \n\n\n\n\n\n","category":"type"},{"location":"controlling_iterative_models/#EarlyStopping.GL","page":"Controlling Iterative Models","title":"EarlyStopping.GL","text":"GL(; alpha=2.0)\n\nAn early stopping criterion for loss-reporting iterative algorithms. \n\nA stop is triggered when the (rescaled) generalization loss exceeds the threshold alpha.\n\nTerminology. Suppose E_1 E_2  E_t are a sequence of losses, for example, out-of-sample estimates of the loss associated with some iterative machine learning algorithm. Then the generalization loss at time t, is given by\n\nGL_t = 100 (E_t - E_opt) over E_opt\n\nwhere E_opt is the minimum value of the sequence.\n\nReference: Prechelt, Lutz (1998): \"Early Stopping- But When?\", in Neural Networks: Tricks of the Trade, ed. G. Orr, Springer..\n\n\n\n\n\n","category":"type"},{"location":"controlling_iterative_models/#EarlyStopping.PQ","page":"Controlling Iterative Models","title":"EarlyStopping.PQ","text":"PQ(; alpha=0.75, k=5, tol=eps(Float64))\n\nA stopping criterion for training iterative supervised learners.\n\nA stop is triggered when Prechelt's progress-modified generalization loss exceeds the threshold PQ_T  alpha, or if the training progress drops below P_j  tol. Here k is the number of training (in-sample) losses used to estimate the training progress.\n\nContext and explanation of terminology\n\nThe training progress at time j is defined by\n\nP_j = 1000 M - mm\n\nwhere M is the mean of the last k training losses F_1 F_2  F_k and m is the minimum value of those losses.\n\nThe progress-modified generalization loss at time t is then given by\n\nPQ_t = GL_t  P_t\n\nwhere GL_t is the generalization loss at time t; see GL.\n\nPQ will stop when the following are true:\n\nAt least k training samples have been collected via done!(c::PQ, loss; training = true) or update_training(c::PQ, loss, state)\nThe last update was an out-of-sample update. (done!(::PQ, loss; training=true) is always false)\nThe progress-modified generalization loss exceeds the threshold PQ_t  alpha OR the training progress stalls P_j  tol.\n\nReference: Prechelt, Lutz (1998): \"Early Stopping- But When?\", in Neural Networks: Tricks of the Trade, ed. G. Orr, Springer..\n\n\n\n\n\n","category":"type"},{"location":"controlling_iterative_models/#EarlyStopping.Patience","page":"Controlling Iterative Models","title":"EarlyStopping.Patience","text":"Patience(; n=5)\n\nAn early stopping criterion for loss-reporting iterative algorithms. \n\nA stop is triggered by n consecutive increases in the loss.\n\nDenoted \"UPs\" in Prechelt, Lutz (1998): \"Early Stopping- But When?\", in Neural Networks: Tricks of the Trade, ed. G. Orr, Springer..\n\nFor a customizable loss-based stopping criterion, use WithLossDo or WithTrainingLossesDo with the stop_if_true=true option. \n\n\n\n\n\n","category":"type"},{"location":"controlling_iterative_models/#EarlyStopping.Warmup","page":"Controlling Iterative Models","title":"EarlyStopping.Warmup","text":"Warmup(c::StoppingCriterion, n)\n\nWait for n updates before checking stopping criterion c\n\n\n\n\n\n","category":"type"},{"location":"controlling_iterative_models/#IterationControl.Info","page":"Controlling Iterative Models","title":"IterationControl.Info","text":"Info(f=identity)\n\nAn iteration control, as in, Info(my_loss_function). \n\nLog to Info the value of f(m), where m is the object being iterated. If IterativeControl.expose(m) has been overloaded, then log f(expose(m)) instead.\n\nCan be suppressed by setting the global verbosity level sufficiently low. \n\nSee also Warn, Error. \n\n\n\n\n\n","category":"type"},{"location":"controlling_iterative_models/#IterationControl.Warn","page":"Controlling Iterative Models","title":"IterationControl.Warn","text":"Warn(predicate; f=\"\")\n\nAn iteration control, as in, Warn(m -> length(m.cache) > 100, f=\"Memory low\"). \n\nIf predicate(m) is true, then log to Warn the value of f (or f(IterationControl.expose(m)) if f is a function). Here m is the object being iterated.\n\nCan be suppressed by setting the global verbosity level sufficiently low.\n\nSee also Info, Error. \n\n\n\n\n\n","category":"type"},{"location":"controlling_iterative_models/#IterationControl.Error","page":"Controlling Iterative Models","title":"IterationControl.Error","text":"Error(predicate; f=\"\", exception=nothing))\n\nAn iteration control, as in, Error(m -> isnan(m.bias), f=\"Bias overflow!\"). \n\nIf predicate(m) is true, then log at the Error level the value of f (or f(IterationControl.expose(m)) if f is a function) and stop iteration at the end of the current control cycle. Here m is the object being iterated.\n\nSpecify exception=... to throw an immediate execption, without waiting to the end of the control cycle.\n\nSee also Info, Warn. \n\n\n\n\n\n","category":"type"},{"location":"controlling_iterative_models/#IterationControl.Callback","page":"Controlling Iterative Models","title":"IterationControl.Callback","text":"Callback(f=_->nothing, stop_if_true=false, stop_message=nothing, raw=false)\n\nAn iteration control, as in, Callback(m->put!(v, my_loss_function(m)). \n\nCall f(IterationControl.expose(m)), where m is the object being iterated, unless raw=true, in which case call f(m) (guaranteed if expose has not been overloaded.) If stop_if_true is true, then trigger an early stop if the value returned by f is true, logging the stop_message if specified. \n\n\n\n\n\n","category":"type"},{"location":"controlling_iterative_models/#IterationControl.WithNumberDo","page":"Controlling Iterative Models","title":"IterationControl.WithNumberDo","text":"WithNumberDo(f=n->@info(\"number: $n\"), stop_if_true=false, stop_message=nothing)\n\nAn iteration control, as in, WithNumberDo(n->put!(my_channel, n)). \n\nCall f(n + 1), where n is the number of complete control cycles. of the control (so, n = 1, 2, 3, ..., unless control is wrapped in a IterationControl.skip)`.\n\nIf stop_if_true is true, then trigger an early stop if the value returned by f is true, logging the stop_message if specified. \n\n\n\n\n\n","category":"type"},{"location":"controlling_iterative_models/#MLJIteration.WithIterationsDo","page":"Controlling Iterative Models","title":"MLJIteration.WithIterationsDo","text":"WithIterationsDo(f=x->@info(\"iterations: $x\"), stop_if_true=false, stop_message=nothing)\n\nAn iteration control, as in, WithIterationsDo(x->put!(my_channel, x)). \n\nCall f(x), where x is the current number of model iterations (generally more than the number of control cycles). If stop_if_true is true, then trigger an early stop if the value returned by f is true, logging the stop_message if specified. \n\n\n\n\n\n","category":"type"},{"location":"controlling_iterative_models/#IterationControl.WithLossDo","page":"Controlling Iterative Models","title":"IterationControl.WithLossDo","text":"WithLossDo(f=x->@info(\"loss: $x\"), stop_if_true=false, stop_message=nothing)\n\nAn iteration control, as in, WithLossDo(x->put!(my_losses, x)). \n\nCall f(loss), where loss is current loss.\n\nIf stop_if_true is true, then trigger an early stop if the value returned by f is true, logging the stop_message if specified. \n\n\n\n\n\n","category":"type"},{"location":"controlling_iterative_models/#IterationControl.WithTrainingLossesDo","page":"Controlling Iterative Models","title":"IterationControl.WithTrainingLossesDo","text":"WithTrainingLossesDo(f=v->@info(\"training: $v\"), stop_if_true=false, stop_message=nothing)\n\nAn iteration control, as in, WithTrainingLossesDo(v->put!(my_losses, last(v)). \n\nCall f(training_losses), where training_losses is the vector of most recent batch of training losses.\n\nIf stop_if_true is true, then trigger an early stop if the value returned by f is true, logging the stop_message if specified. \n\n\n\n\n\n","category":"type"},{"location":"controlling_iterative_models/#MLJIteration.WithEvaluationDo","page":"Controlling Iterative Models","title":"MLJIteration.WithEvaluationDo","text":"WithEvaluationDo(f=x->@info(\"evaluation: $x\"), stop_if_true=false, stop_message=nothing)\n\nAn iteration control, as in, WithEvaluationDo(x->put!(my_channel, x)). \n\nCall f(x), where x is the latest performance evaluation, as returned by evaluate!(train_mach, resampling=..., ...). Not valid if resampling=nothing. If stop_if_true is true, then trigger an early stop if the value returned by f is true, logging the stop_message if specified. \n\n\n\n\n\n","category":"type"},{"location":"controlling_iterative_models/#MLJIteration.WithFittedParamsDo","page":"Controlling Iterative Models","title":"MLJIteration.WithFittedParamsDo","text":"WithFittedParamsDo(f=x->@info(\"fitted_params: $x\"), stop_if_true=false, stop_message=nothing)\n\nAn iteration control, as in, WithFittedParamsDo(x->put!(my_channel, x)). \n\nCall f(x), where x = fitted_params(mach) is the fitted parameters of the training machine, mach, in its current state. If stop_if_true is true, then trigger an early stop if the value returned by f is true, logging the stop_message if specified. \n\n\n\n\n\n","category":"type"},{"location":"controlling_iterative_models/#MLJIteration.WithReportDo","page":"Controlling Iterative Models","title":"MLJIteration.WithReportDo","text":"WithReportDo(f=x->@info(\"report: $x\"), stop_if_true=false, stop_message=nothing)\n\nAn iteration control, as in, WithReportDo(x->put!(my_channel, x)). \n\nCall f(x), where x = report(mach) is the report associated with the training machine, mach,  in its current state. If stop_if_true is true, then trigger an early stop if the value returned by f is true, logging the stop_message if specified. \n\n\n\n\n\n","category":"type"},{"location":"controlling_iterative_models/#MLJIteration.WithModelDo","page":"Controlling Iterative Models","title":"MLJIteration.WithModelDo","text":"WithModelDo(f=x->@info(\"model: $x\"), stop_if_true=false, stop_message=nothing)\n\nAn iteration control, as in, WithModelDo(x->put!(my_channel, x)). \n\nCall f(x), where x is the model associated with the training machine; f may mutate x, as in f(x) = (x.learning_rate *= 0.9). If stop_if_true is true, then trigger an early stop if the value returned by f is true, logging the stop_message if specified. \n\n\n\n\n\n","category":"type"},{"location":"controlling_iterative_models/#MLJIteration.WithMachineDo","page":"Controlling Iterative Models","title":"MLJIteration.WithMachineDo","text":"WithMachineDo(f=x->@info(\"machine: $x\"), stop_if_true=false, stop_message=nothing)\n\nAn iteration control, as in, WithMachineDo(x->put!(my_channel, x)). \n\nCall f(x), where x is the training machine in its current state. If stop_if_true is true, then trigger an early stop if the value returned by f is true, logging the stop_message if specified. \n\n\n\n\n\n","category":"type"},{"location":"controlling_iterative_models/#MLJIteration.Save","page":"Controlling Iterative Models","title":"MLJIteration.Save","text":"Save(filename=\"machine.jls\")\n\nAn iteration control, as in, Save(\"run3/machine.jls\"). \n\nSave the current state of the machine being iterated to disk, using the provided filename, decorated with a number, as in \"run3/machine42.jls\". The default behaviour uses the Serialization module but this can be changed by setting the method=save_fn(::String, ::Any) argument where save_fn is any serialization method. For more on what is meant by \"the machine being iterated\", see IteratedModel.\n\n\n\n\n\n","category":"type"},{"location":"controlling_iterative_models/#IterationControl.skip","page":"Controlling Iterative Models","title":"IterationControl.skip","text":"IterationControl.skip(control, predicate=1)\n\nAn iteration control wrapper.\n\nIf predicate is an integer, k: Apply control on every k calls to apply the wrapped control, starting with the kth call.\n\nIf predicate is a function: Apply control as usual when predicate(n + 1) is true but otherwise skip. Here n is the number of control cycles applied so far.\n\n\n\n\n\n","category":"function"},{"location":"controlling_iterative_models/#IterationControl.louder","page":"Controlling Iterative Models","title":"IterationControl.louder","text":"IterationControl.louder(control, by=1)\n\nWrap control to make in more (or less) verbose. The same as control, but as if the global verbosity were increased by the value by.\n\n\n\n\n\n","category":"function"},{"location":"controlling_iterative_models/#IterationControl.with_state_do","page":"Controlling Iterative Models","title":"IterationControl.with_state_do","text":"IterationControl.with_state_do(control,\n                              f=x->@info \"$(typeof(control)) state: $x\")\n\nWrap control to give access to it's internal state. Acts exactly like control except that f is called on the internal state of control. If f is not specified, the control type and state are logged to Info at every update (useful for debugging new controls).\n\nWarning. The internal state of a control is not yet considered part of the public interface and could change between in any pre 1.0 release of IterationControl.jl.\n\n\n\n\n\n","category":"function"},{"location":"controlling_iterative_models/#IterationControl.composite","page":"Controlling Iterative Models","title":"IterationControl.composite","text":"composite(controls...)\n\nConstruct an iteration control that applies the specified controls in sequence.\n\n\n\n\n\n","category":"function"},{"location":"models/SODDetector_OutlierDetectionPython/#SODDetector_OutlierDetectionPython","page":"SODDetector","title":"SODDetector","text":"SODDetector(n_neighbors = 5,\n               ref_set = 10,\n               alpha = 0.8)\n\nhttps://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.sod","category":"section"},{"location":"models/RandomUndersampler_Imbalance/#RandomUndersampler_Imbalance","page":"RandomUndersampler","title":"RandomUndersampler","text":"Initiate a random undersampling model with the given hyper-parameters.\n\nRandomUndersampler\n\nA model type for constructing a random undersampler, based on Imbalance.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nRandomUndersampler = @load RandomUndersampler pkg=Imbalance\n\nDo model = RandomUndersampler() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in RandomUndersampler(ratios=...).\n\nRandomUndersampler implements naive undersampling by randomly removing existing observations. ","category":"section"},{"location":"models/RandomUndersampler_Imbalance/#Training-data","page":"RandomUndersampler","title":"Training data","text":"In MLJ or MLJBase, wrap the model in a machine by     mach = machine(model)\n\nThere is no need to provide any data here because the model is a static transformer.\n\nLikewise, there is no need to fit!(mach). \n\nFor default values of the hyper-parameters, model can be constructed by     model = RandomUndersampler()","category":"section"},{"location":"models/RandomUndersampler_Imbalance/#Hyperparameters","page":"RandomUndersampler","title":"Hyperparameters","text":"ratios=1.0: A parameter that controls the amount of undersampling to be done for each class\nCan be a float and in this case each class will be undersampled to the size of the minority class times the float. By default, all classes are undersampled to the size of the minority class\nCan be a dictionary mapping each class label to the float ratio for that class\nrng::Union{AbstractRNG, Integer}=default_rng(): Either an AbstractRNG object or an Integer    seed to be used with Xoshiro if the Julia VERSION supports it. Otherwise, uses MersenneTwister`.","category":"section"},{"location":"models/RandomUndersampler_Imbalance/#Transform-Inputs","page":"RandomUndersampler","title":"Transform Inputs","text":"X: A matrix of real numbers or a table with element scitypes that subtype Union{Finite, Infinite}.     Elements in nominal columns should subtype Finite (i.e., have scitype OrderedFactor or Multiclass) and    elements in continuous columns should subtype Infinite (i.e., have scitype Count or Continuous).\ny: An abstract vector of labels (e.g., strings) that correspond to the observations in X","category":"section"},{"location":"models/RandomUndersampler_Imbalance/#Transform-Outputs","page":"RandomUndersampler","title":"Transform Outputs","text":"X_under: A matrix or table that includes the data after undersampling    depending on whether the input X is a matrix or table respectively\ny_under: An abstract vector of labels corresponding to X_under","category":"section"},{"location":"models/RandomUndersampler_Imbalance/#Operations","page":"RandomUndersampler","title":"Operations","text":"transform(mach, X, y): resample the data X and y using RandomUndersampler, returning both the new and original observations","category":"section"},{"location":"models/RandomUndersampler_Imbalance/#Example","page":"RandomUndersampler","title":"Example","text":"using MLJ\nimport Imbalance\n\n## set probability of each class\nclass_probs = [0.5, 0.2, 0.3]                         \nnum_rows, num_continuous_feats = 100, 5\n## generate a table and categorical vector accordingly\nX, y = Imbalance.generate_imbalanced_data(num_rows, num_continuous_feats; \n                                class_probs, rng=42)   \n\njulia> Imbalance.checkbalance(y; ref=\"minority\")\n 1: ▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 19 (100.0%) \n 2: ▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 33 (173.7%) \n 0: ▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 48 (252.6%) \n\n## load RandomUndersampler\nRandomUndersampler = @load RandomUndersampler pkg=Imbalance\n\n## wrap the model in a machine\nundersampler = RandomUndersampler(ratios=Dict(0=>1.0, 1=> 1.0, 2=>1.0), \n               rng=42)\nmach = machine(undersampler)\n\n## provide the data to transform (there is nothing to fit)\nX_under, y_under = transform(mach, X, y)\n                                      \njulia> Imbalance.checkbalance(y_under; ref=\"minority\")\n0: ▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 19 (100.0%) \n2: ▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 19 (100.0%) \n1: ▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 19 (100.0%) ","category":"section"},{"location":"models/RandomForestRegressor_MLJScikitLearnInterface/#RandomForestRegressor_MLJScikitLearnInterface","page":"RandomForestRegressor","title":"RandomForestRegressor","text":"RandomForestRegressor\n\nA model type for constructing a random forest regressor, based on MLJScikitLearnInterface.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nRandomForestRegressor = @load RandomForestRegressor pkg=MLJScikitLearnInterface\n\nDo model = RandomForestRegressor() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in RandomForestRegressor(n_estimators=...).\n\nA random forest is a meta estimator that fits a number of  classifying decision trees on various sub-samples of the  dataset and uses averaging to improve the predictive accuracy  and control over-fitting. The sub-sample size is controlled  with the max_samples parameter if bootstrap=True (default),  otherwise the whole dataset is used to build each tree.","category":"section"},{"location":"composing_models/#Composing-Models","page":"Composing Models","title":"Composing Models","text":"Three common ways of combining multiple models together have out-of-the-box implementations in MLJ:\n\nLinear Pipelines (Pipeline)- for unbranching chains that take the output of one model (e.g., dimension reduction, such as PCA) and make it the input of the next model in the chain (e.g., a classification model, such as EvoTreeClassifier). To include transformations of the target variable in a supervised pipeline model, see Target Transformations.\nHomogeneous Ensembles (EnsembleModel) - for blending the predictions of multiple supervised models all of the same type, but which receive different views of the training data to reduce overall variance. The technique implemented here is known as observation bagging. \nModel Stacking - (Stack) for combining the predictions of a smaller number of models of possibly different types, with the help of an adjudicating model.\n\nAdditionally, more complicated model compositions are possible using:\n\nLearning Networks - \"blueprints\" for combining models in flexible ways; these are simple transformations of your existing workflows which can be \"exported\" to define new, stand-alone model types.","category":"section"},{"location":"models/UnivariateTimeTypeToContinuous_MLJTransforms/#UnivariateTimeTypeToContinuous_MLJTransforms","page":"UnivariateTimeTypeToContinuous","title":"UnivariateTimeTypeToContinuous","text":"UnivariateTimeTypeToContinuous\n\nA model type for constructing a single variable transformer that creates continuous representations of temporally typed data, based on MLJTransforms.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nUnivariateTimeTypeToContinuous = @load UnivariateTimeTypeToContinuous pkg=MLJTransforms\n\nDo model = UnivariateTimeTypeToContinuous() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in UnivariateTimeTypeToContinuous(zero_time=...).\n\nUse this model to convert vectors with a TimeType element type to vectors of Float64 type (Continuous element scitype).","category":"section"},{"location":"models/UnivariateTimeTypeToContinuous_MLJTransforms/#Training-data","page":"UnivariateTimeTypeToContinuous","title":"Training data","text":"In MLJ or MLJBase, bind an instance model to data with\n\nmach = machine(model, x)\n\nwhere\n\nx: any abstract vector whose element type is a subtype of Dates.TimeType\n\nTrain the machine using fit!(mach, rows=...).","category":"section"},{"location":"models/UnivariateTimeTypeToContinuous_MLJTransforms/#Hyper-parameters","page":"UnivariateTimeTypeToContinuous","title":"Hyper-parameters","text":"zero_time: the time that is to correspond to 0.0 under transformations, with the type coinciding with the training data element type. If unspecified, the earliest time encountered in training is used.\nstep::Period=Hour(24): time interval to correspond to one unit under transformation","category":"section"},{"location":"models/UnivariateTimeTypeToContinuous_MLJTransforms/#Operations","page":"UnivariateTimeTypeToContinuous","title":"Operations","text":"transform(mach, xnew): apply the encoding inferred when mach was fit","category":"section"},{"location":"models/UnivariateTimeTypeToContinuous_MLJTransforms/#Fitted-parameters","page":"UnivariateTimeTypeToContinuous","title":"Fitted parameters","text":"fitted_params(mach).fitresult is the tuple (zero_time, step) actually used in transformations, which may differ from the user-specified hyper-parameters.","category":"section"},{"location":"models/UnivariateTimeTypeToContinuous_MLJTransforms/#Example","page":"UnivariateTimeTypeToContinuous","title":"Example","text":"using MLJ\nusing Dates\n\nx = [Date(2001, 1, 1) + Day(i) for i in 0:4]\n\nencoder = UnivariateTimeTypeToContinuous(zero_time=Date(2000, 1, 1),\n                                         step=Week(1))\n\nmach = machine(encoder, x)\nfit!(mach)\njulia> transform(mach, x)\n5-element Vector{Float64}:\n 52.285714285714285\n 52.42857142857143\n 52.57142857142857\n 52.714285714285715\n 52.857142","category":"section"},{"location":"models/OPTICS_MLJScikitLearnInterface/#OPTICS_MLJScikitLearnInterface","page":"OPTICS","title":"OPTICS","text":"OPTICS\n\nA model type for constructing a optics, based on MLJScikitLearnInterface.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nOPTICS = @load OPTICS pkg=MLJScikitLearnInterface\n\nDo model = OPTICS() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in OPTICS(min_samples=...).\n\nOPTICS (Ordering Points To Identify the Clustering Structure), closely related to `DBSCAN', finds core sample of high density and expands clusters from them. Unlike DBSCAN, keeps cluster hierarchy for a variable neighborhood radius. Better suited for usage on large datasets than the current sklearn implementation of DBSCAN.","category":"section"},{"location":"models/BalancedBaggingClassifier_MLJBalancing/#BalancedBaggingClassifier_MLJBalancing","page":"BalancedBaggingClassifier","title":"BalancedBaggingClassifier","text":"BalancedBaggingClassifier\n\nA model type for constructing a balanced bagging classifier, based on MLJBalancing.jl.\n\nFrom MLJ, the type can be imported using\n\nBalancedBaggingClassifier = @load BalancedBaggingClassifier pkg=MLJBalancing\n\nConstruct an instance with default hyper-parameters using the syntax bagging_model = BalancedBaggingClassifier(model=...)\n\nGiven a probablistic classifier.BalancedBaggingClassifier performs bagging by undersampling only majority data in each bag so that its includes as much samples as in the minority data. This is proposed with an Adaboost classifier where the output scores are averaged in the paper Xu-Ying Liu, Jianxin Wu, & Zhi-Hua Zhou. (2009). Exploratory Undersampling for Class-Imbalance Learning. IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics), 39 (2), 539–5501","category":"section"},{"location":"models/BalancedBaggingClassifier_MLJBalancing/#Training-data","page":"BalancedBaggingClassifier","title":"Training data","text":"In MLJ or MLJBase, bind an instance model to data with\n\nmach = machine(model, X, y)\n\nwhere\n\nX: input features of a form supported by the model being wrapped (typically a table, e.g., DataFrame,   with Continuous columns will be supported, as a minimum)\ny: the binary target, which can be any AbstractVector where length(unique(y)) == 2\n\nTrain the machine with fit!(mach, rows=...).","category":"section"},{"location":"models/BalancedBaggingClassifier_MLJBalancing/#Hyperparameters","page":"BalancedBaggingClassifier","title":"Hyperparameters","text":"model::Probabilistic: The classifier to use to train on each bag.\nT::Integer=0: The number of bags to be used in the ensemble. If not given, will be set as   the ratio between the frequency of the majority and minority classes. Can be later found in report(mach).\nrng::Union{AbstractRNG, Integer}=default_rng(): Either an AbstractRNG object or an Integer    seed to be used with Xoshiro if Julia VERSION>=1.7. Otherwise, uses MersenneTwister`.","category":"section"},{"location":"models/BalancedBaggingClassifier_MLJBalancing/#Operations","page":"BalancedBaggingClassifier","title":"Operations","text":"predict(mach, Xnew): return predictions of the target given\n\nfeatures Xnew having the same scitype as X above. Predictions are probabilistic, but uncalibrated.\n\npredict_mode(mach, Xnew): return the mode of each prediction above","category":"section"},{"location":"models/BalancedBaggingClassifier_MLJBalancing/#Example","page":"BalancedBaggingClassifier","title":"Example","text":"using MLJ\nusing Imbalance\n\n## Load base classifier and BalancedBaggingClassifier\nBalancedBaggingClassifier = @load BalancedBaggingClassifier pkg=MLJBalancing\nLogisticClassifier = @load LogisticClassifier pkg=MLJLinearModels verbosity=0\n\n## Construct the base classifier and use it to construct a BalancedBaggingClassifier\nlogistic_model = LogisticClassifier()\nmodel = BalancedBaggingClassifier(model=logistic_model, T=5)\n\n## Load the data and train the BalancedBaggingClassifier\nX, y = Imbalance.generate_imbalanced_data(100, 5; num_vals_per_category = [3, 2],\n                                            class_probs = [0.9, 0.1],\n                                            type = \"ColTable\",\n                                            rng=42)\njulia> Imbalance.checkbalance(y)\n1: ▇▇▇▇▇▇▇▇▇▇ 16 (19.0%)\n0: ▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 84 (100.0%)\n\nmach = machine(model, X, y) |> fit!\n\n## Predict using the trained model\n\nyhat = predict(mach, X)     ## probabilistic predictions\npredict_mode(mach, X)       ## point predictions","category":"section"},{"location":"internals/#internals_section","page":"Internals","title":"Internals","text":"","category":"section"},{"location":"internals/#The-machine-interface,-simplified","page":"Internals","title":"The machine interface, simplified","text":"The following is a simplified description of the Machine interface. It predates the introduction of an optional data front-end for models (see Implementing a data front-end). See also the Glossary","category":"section"},{"location":"internals/#The-Machine-type","page":"Internals","title":"The Machine type","text":"mutable struct Machine{M<Model}\n\n    model::M\n    fitresult\n    cache\n    args::Tuple    # e.g., (X, y) for supervised models\n    report\n    previous_rows # remember the last rows used\n\n    function Machine{M}(model::M, args...) where M<:Model\n        machine = new{M}(model)\n        machine.args = args\n        machine.report = Dict{Symbol,Any}()\n        return machine\n    end\n\nend","category":"section"},{"location":"internals/#Constructor","page":"Internals","title":"Constructor","text":"machine(model::M, Xtable, y) = Machine{M}(model, Xtable, y)","category":"section"},{"location":"internals/#fit!-and-predict/transform","page":"Internals","title":"fit! and predict/transform","text":"function fit!(mach::Machine; rows=nothing, force=false, verbosity=1)\n\n    warning = clean!(mach.model)\n    isempty(warning) || verbosity < 0 || @warn warning\n\n    if rows === nothing\n        rows = (:)\n    end\n\n    rows_have_changed  = (!isdefined(mach, :previous_rows) ||\n            rows != mach.previous_rows)\n\n    args = [MLJ.selectrows(arg, rows) for arg in mach.args]\n\n    if !isdefined(mach, :fitresult) || rows_have_changed || force\n        mach.fitresult, mach.cache, report =\n            fit(mach.model, verbosity, args...)\n    else # call `update`:\n        mach.fitresult, mach.cache, report =\n            update(mach.model, verbosity, mach.fitresult, mach.cache, args...)\n    end\n\n    if rows_have_changed\n        mach.previous_rows = deepcopy(rows)\n    end\n\n    if report !== nothing\n        merge!(mach.report, report)\n    end\n\n    return mach\n\nend\n\nfunction predict(machine::Machine{<:Supervised}, Xnew)\n    if isdefined(machine, :fitresult)\n        return predict(machine.model, machine.fitresult, Xnew))\n    else\n        throw(error(\"$machine is not trained and so cannot predict.\"))\n    end\nend\n\nfunction transform(machine::Machine{<:Unsupervised}, Xnew)\n    if isdefined(machine, :fitresult)\n        return transform(machine.model, machine.fitresult, Xnew))\n    else\n        throw(error(\"$machine is not trained and so cannot transform.\"))\n    end\nend","category":"section"},{"location":"models/MultitargetLinearRegressor_MultivariateStats/#MultitargetLinearRegressor_MultivariateStats","page":"MultitargetLinearRegressor","title":"MultitargetLinearRegressor","text":"MultitargetLinearRegressor\n\nA model type for constructing a multitarget linear regressor, based on MultivariateStats.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nMultitargetLinearRegressor = @load MultitargetLinearRegressor pkg=MultivariateStats\n\nDo model = MultitargetLinearRegressor() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in MultitargetLinearRegressor(bias=...).\n\nMultitargetLinearRegressor assumes the target variable is vector-valued with continuous components.  It trains a linear prediction function using the least squares algorithm. Options exist to specify a bias term.","category":"section"},{"location":"models/MultitargetLinearRegressor_MultivariateStats/#Training-data","page":"MultitargetLinearRegressor","title":"Training data","text":"In MLJ or MLJBase, bind an instance model to data with\n\nmach = machine(model, X, y)\n\nHere:\n\nX is any table of input features (eg, a DataFrame) whose columns are of scitype    Continuous; check column scitypes with schema(X).\ny is the target, which can be any table of responses whose element scitype is    Continuous; check the scitype with scitype(y).\n\nTrain the machine using fit!(mach, rows=...).","category":"section"},{"location":"models/MultitargetLinearRegressor_MultivariateStats/#Hyper-parameters","page":"MultitargetLinearRegressor","title":"Hyper-parameters","text":"bias=true: Include the bias term if true, otherwise fit without bias term.","category":"section"},{"location":"models/MultitargetLinearRegressor_MultivariateStats/#Operations","page":"MultitargetLinearRegressor","title":"Operations","text":"predict(mach, Xnew): Return predictions of the target given new features Xnew,    which should have the same scitype as X above.","category":"section"},{"location":"models/MultitargetLinearRegressor_MultivariateStats/#Fitted-parameters","page":"MultitargetLinearRegressor","title":"Fitted parameters","text":"The fields of fitted_params(mach) are:\n\ncoefficients: The linear coefficients determined by the model.\nintercept: The intercept determined by the model.","category":"section"},{"location":"models/MultitargetLinearRegressor_MultivariateStats/#Examples","page":"MultitargetLinearRegressor","title":"Examples","text":"using MLJ\nusing DataFrames\n\nLinearRegressor = @load MultitargetLinearRegressor pkg=MultivariateStats\nlinear_regressor = LinearRegressor()\n\nX, y = make_regression(100, 9; n_targets = 2) ## a table and a table (synthetic data)\n\nmach = machine(linear_regressor, X, y) |> fit!\n\nXnew, _ = make_regression(3, 9)\nyhat = predict(mach, Xnew) ## new predictions\n\nSee also LinearRegressor, RidgeRegressor, MultitargetRidgeRegressor","category":"section"},{"location":"models/CDDetector_OutlierDetectionPython/#CDDetector_OutlierDetectionPython","page":"CDDetector","title":"CDDetector","text":"CDDetector(whitening = true,\n              rule_of_thumb = false)\n\nhttps://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.cd","category":"section"},{"location":"models/ConstantRegressor_MLJModels/#ConstantRegressor_MLJModels","page":"ConstantRegressor","title":"ConstantRegressor","text":"ConstantRegressor\n\nThis \"dummy\" probabilistic predictor always returns the same distribution, irrespective of the provided input pattern. The distribution returned is the one of the type specified that best fits the training target data. Use predict_mean or predict_median to predict the mean or median values instead. If not specified, a normal distribution is fit.\n\nAlmost any reasonable model is expected to outperform ConstantRegressor which is used almost exclusively for testing and establishing performance baselines.\n\nIn MLJ (or MLJModels) do model = ConstantRegressor() or model = ConstantRegressor(distribution=...) to construct a model instance.","category":"section"},{"location":"models/ConstantRegressor_MLJModels/#Training-data","page":"ConstantRegressor","title":"Training data","text":"In MLJ (or MLJBase) bind an instance model to data with\n\nmach = machine(model, X, y)\n\nHere:\n\nX is any table of input features (eg, a DataFrame)\ny is the target, which can be any AbstractVector whose element scitype is Continuous; check the scitype with schema(y)\n\nTrain the machine using fit!(mach, rows=...).","category":"section"},{"location":"models/ConstantRegressor_MLJModels/#Hyper-parameters","page":"ConstantRegressor","title":"Hyper-parameters","text":"distribution_type=Distributions.Normal: The distribution to be fit to the target data. Must be a subtype of Distributions.ContinuousUnivariateDistribution.","category":"section"},{"location":"models/ConstantRegressor_MLJModels/#Operations","page":"ConstantRegressor","title":"Operations","text":"predict(mach, Xnew): Return predictions of the target given features Xnew (which for this model are ignored). Predictions are probabilistic.\npredict_mean(mach, Xnew): Return instead the means of the probabilistic predictions returned above.\npredict_median(mach, Xnew): Return instead the medians of the probabilistic predictions returned above.","category":"section"},{"location":"models/ConstantRegressor_MLJModels/#Fitted-parameters","page":"ConstantRegressor","title":"Fitted parameters","text":"The fields of fitted_params(mach) are:\n\ntarget_distribution: The distribution fit to the supplied target data.","category":"section"},{"location":"models/ConstantRegressor_MLJModels/#Examples","page":"ConstantRegressor","title":"Examples","text":"using MLJ\n\nX, y = make_regression(10, 2) ## synthetic data: a table and vector\nregressor = ConstantRegressor()\nmach = machine(regressor, X, y) |> fit!\n\nfitted_params(mach)\n\nXnew, _ = make_regression(3, 2)\npredict(mach, Xnew)\npredict_mean(mach, Xnew)\n\n\nSee also ConstantClassifier","category":"section"},{"location":"models/ElasticNetRegressor_MLJScikitLearnInterface/#ElasticNetRegressor_MLJScikitLearnInterface","page":"ElasticNetRegressor","title":"ElasticNetRegressor","text":"ElasticNetRegressor\n\nA model type for constructing a elastic net regressor, based on MLJScikitLearnInterface.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nElasticNetRegressor = @load ElasticNetRegressor pkg=MLJScikitLearnInterface\n\nDo model = ElasticNetRegressor() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in ElasticNetRegressor(alpha=...).","category":"section"},{"location":"models/ElasticNetRegressor_MLJScikitLearnInterface/#Hyper-parameters","page":"ElasticNetRegressor","title":"Hyper-parameters","text":"alpha = 1.0\nl1_ratio = 0.5\nfit_intercept = true\nprecompute = false\nmax_iter = 1000\ncopy_X = true\ntol = 0.0001\nwarm_start = false\npositive = false\nrandom_state = nothing\nselection = cyclic","category":"section"},{"location":"models/SubspaceLDA_MultivariateStats/#SubspaceLDA_MultivariateStats","page":"SubspaceLDA","title":"SubspaceLDA","text":"SubspaceLDA\n\nA model type for constructing a subpace LDA model, based on MultivariateStats.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nSubspaceLDA = @load SubspaceLDA pkg=MultivariateStats\n\nDo model = SubspaceLDA() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in SubspaceLDA(normalize=...).\n\nMulticlass subspace linear discriminant analysis (LDA) is a variation on ordinary LDA suitable for high dimensional data, as it avoids storing scatter matrices. For details, refer the MultivariateStats.jl documentation.\n\nIn addition to dimension reduction (using transform) probabilistic classification is provided (using predict).  In the case of classification, the class probability for a new observation reflects the proximity of that observation to training observations associated with that class, and how far away the observation is from observations associated with other classes. Specifically, the distances, in the transformed (projected) space, of a new observation, from the centroid of each target class, is computed; the resulting vector of distances, multiplied by minus one, is passed to a softmax function to obtain a class probability prediction. Here \"distance\" is computed using a user-specified distance function.","category":"section"},{"location":"models/SubspaceLDA_MultivariateStats/#Training-data","page":"SubspaceLDA","title":"Training data","text":"In MLJ or MLJBase, bind an instance model to data with\n\nmach = machine(model, X, y)\n\nHere:\n\nX is any table of input features (eg, a DataFrame) whose columns are of scitype Continuous; check column scitypes with schema(X).\ny is the target, which can be any AbstractVector whose element scitype is OrderedFactor or Multiclass; check the scitype with scitype(y).\n\nTrain the machine using fit!(mach, rows=...).","category":"section"},{"location":"models/SubspaceLDA_MultivariateStats/#Hyper-parameters","page":"SubspaceLDA","title":"Hyper-parameters","text":"normalize=true: Option to normalize the between class variance for the number of observations in each class, one of true or false.\noutdim: the ouput dimension, automatically set to min(indim, nclasses-1) if equal to 0. If a non-zero outdim is passed, then the actual output dimension used is min(rank, outdim) where rank is the rank of the within-class covariance matrix.\ndist=Distances.SqEuclidean(): The distance metric to use when performing classification (to compare the distance between a new point and centroids in the transformed space); must be a subtype of Distances.SemiMetric from Distances.jl, e.g., Distances.CosineDist.","category":"section"},{"location":"models/SubspaceLDA_MultivariateStats/#Operations","page":"SubspaceLDA","title":"Operations","text":"transform(mach, Xnew): Return a lower dimensional projection of the input Xnew, which should have the same scitype as X above.\npredict(mach, Xnew): Return predictions of the target given features Xnew, which should have same scitype as X above. Predictions are probabilistic but uncalibrated.\npredict_mode(mach, Xnew): Return the modes of the probabilistic predictions returned above.","category":"section"},{"location":"models/SubspaceLDA_MultivariateStats/#Fitted-parameters","page":"SubspaceLDA","title":"Fitted parameters","text":"The fields of fitted_params(mach) are:\n\nclasses: The classes seen during model fitting.\nprojection_matrix: The learned projection matrix, of size (indim, outdim), where indim and outdim are the input and output dimensions respectively (See Report section below).","category":"section"},{"location":"models/SubspaceLDA_MultivariateStats/#Report","page":"SubspaceLDA","title":"Report","text":"The fields of report(mach) are:\n\nindim: The dimension of the input space i.e the number of training features.\noutdim: The dimension of the transformed space the model is projected to.\nmean: The mean of the untransformed training data. A vector of length indim.\nnclasses: The number of classes directly observed in the training data (which can be less than the total number of classes in the class pool)\n\nclass_means: The class-specific means of the training data. A matrix of size   (indim, nclasses) with the ith column being the class-mean of the ith class in   classes (See fitted params section above).\n\nclass_weights: The weights (class counts) of each class. A vector of length nclasses with the ith element being the class weight of the ith class in classes. (See fitted params section above.)\nexplained_variance_ratio: The ratio of explained variance to total variance. Each dimension corresponds to an eigenvalue.","category":"section"},{"location":"models/SubspaceLDA_MultivariateStats/#Examples","page":"SubspaceLDA","title":"Examples","text":"using MLJ\n\nSubspaceLDA = @load SubspaceLDA pkg=MultivariateStats\n\nX, y = @load_iris ## a table and a vector\n\nmodel = SubspaceLDA()\nmach = machine(model, X, y) |> fit!\n\nXproj = transform(mach, X)\ny_hat = predict(mach, X)\nlabels = predict_mode(mach, X)\n\nSee also LDA, BayesianLDA, BayesianSubspaceLDA","category":"section"},{"location":"generating_synthetic_data/#Generating-Synthetic-Data","page":"Generating Synthetic Data","title":"Generating Synthetic Data","text":"Here synthetic data means artificially generated data, with no reference to a \"real world\" data set. Not to be confused \"fake data\" obtained by resampling from a distribution fit to some actual real data.\n\nMLJ has a set of functions - make_blobs, make_circles, make_moons and make_regression (closely resembling functions in scikit-learn of the same name) - for generating synthetic data sets. These are useful for testing machine learning models (e.g., testing user-defined composite models; see Composing Models)","category":"section"},{"location":"generating_synthetic_data/#Generating-Gaussian-blobs","page":"Generating Synthetic Data","title":"Generating Gaussian blobs","text":"using MLJ, DataFrames\nX, y = make_blobs(100, 3; centers=2, cluster_std=[1.0, 3.0])\ndfBlobs = DataFrame(X)\ndfBlobs.y = y\nfirst(dfBlobs, 3)\n\nusing VegaLite\ndfBlobs |> @vlplot(:point, x=:x1, y=:x2, color = :\"y:n\") \n\n(Image: svg)\n\ndfBlobs |> @vlplot(:point, x=:x1, y=:x3, color = :\"y:n\") \n\n(Image: svg)","category":"section"},{"location":"generating_synthetic_data/#Generating-concentric-circles","page":"Generating Synthetic Data","title":"Generating concentric circles","text":"using MLJ, DataFrames\nX, y = make_circles(100; noise=0.05, factor=0.3)\ndfCircles = DataFrame(X)\ndfCircles.y = y\nfirst(dfCircles, 3)\n\nusing VegaLite\ndfCircles |> @vlplot(:circle, x=:x1, y=:x2, color = :\"y:n\") \n\n(Image: svg)","category":"section"},{"location":"generating_synthetic_data/#Sampling-from-two-interleaved-half-circles","page":"Generating Synthetic Data","title":"Sampling from two interleaved half-circles","text":"using MLJ, DataFrames\nX, y = make_moons(100; noise=0.05)\ndfHalfCircles = DataFrame(X)\ndfHalfCircles.y = y\nfirst(dfHalfCircles, 3)\n\nusing VegaLite\ndfHalfCircles |> @vlplot(:circle, x=:x1, y=:x2, color = :\"y:n\") \n\n(Image: svg)","category":"section"},{"location":"generating_synthetic_data/#Regression-data-generated-from-noisy-linear-models","page":"Generating Synthetic Data","title":"Regression data generated from noisy linear models","text":"using MLJ, DataFrames\nX, y = make_regression(100, 5; noise=0.5, sparse=0.2, outliers=0.1)\ndfRegression = DataFrame(X)\ndfRegression.y = y\nfirst(dfRegression, 3)","category":"section"},{"location":"generating_synthetic_data/#MLJBase.make_blobs","page":"Generating Synthetic Data","title":"MLJBase.make_blobs","text":"X, y = make_blobs(n=100, p=2; kwargs...)\n\nGenerate Gaussian blobs for clustering and classification problems.\n\nReturn value\n\nBy default, a table X with p columns (features) and n rows (observations), together with a corresponding vector of n Multiclass target observations y, indicating blob membership.\n\nKeyword arguments\n\nshuffle=true: whether to shuffle the resulting points,\ncenters=3: either a number of centers or a c x p matrix with c pre-determined centers,\ncluster_std=1.0: the standard deviation(s) of each blob,\ncenter_box=(-10. => 10.): the limits of the p-dimensional cube within which the cluster centers are drawn if they are not provided,\neltype=Float64: machine type of points (any subtype of  AbstractFloat).\nrng=Random.GLOBAL_RNG: any AbstractRNG object, or integer to seed a MersenneTwister (for reproducibility).\nas_table=true: whether to return the points as a table (true) or a matrix (false). If false the target y has integer element type. \n\nExample\n\nX, y = make_blobs(100, 3; centers=2, cluster_std=[1.0, 3.0])\n\n\n\n\n\n","category":"function"},{"location":"generating_synthetic_data/#MLJBase.make_circles","page":"Generating Synthetic Data","title":"MLJBase.make_circles","text":"X, y = make_circles(n=100; kwargs...)\n\nGenerate n labeled points close to two concentric circles for classification and clustering models.\n\nReturn value\n\nBy default, a table X with 2 columns and n rows (observations), together with a corresponding vector of n Multiclass target observations y. The target is either 0 or 1, corresponding to membership to the smaller or larger circle, respectively.\n\nKeyword arguments\n\nshuffle=true: whether to shuffle the resulting points,\nnoise=0: standard deviation of the Gaussian noise added to the data,\nfactor=0.8: ratio of the smaller radius over the larger one,\neltype=Float64: machine type of points (any subtype of  AbstractFloat).\nrng=Random.GLOBAL_RNG: any AbstractRNG object, or integer to seed a MersenneTwister (for reproducibility).\nas_table=true: whether to return the points as a table (true) or a matrix (false). If false the target y has integer element type. \n\nExample\n\nX, y = make_circles(100; noise=0.5, factor=0.3)\n\n\n\n\n\n","category":"function"},{"location":"generating_synthetic_data/#MLJBase.make_moons","page":"Generating Synthetic Data","title":"MLJBase.make_moons","text":"make_moons(n::Int=100; kwargs...)\n\nGenerates labeled two-dimensional points lying close to two interleaved semi-circles, for use with classification and clustering models.\n\nReturn value\n\nBy default, a table X with 2 columns and n rows (observations), together with a corresponding vector of n Multiclass target observations y. The target is either 0 or 1, corresponding to membership to the left or right semi-circle.\n\nKeyword arguments\n\nshuffle=true: whether to shuffle the resulting points,\nnoise=0.1: standard deviation of the Gaussian noise added to the data,\nxshift=1.0: horizontal translation of the second center with respect to the first one.\nyshift=0.3: vertical translation of the second center with respect to the first one.  \neltype=Float64: machine type of points (any subtype of  AbstractFloat).\nrng=Random.GLOBAL_RNG: any AbstractRNG object, or integer to seed a MersenneTwister (for reproducibility).\nas_table=true: whether to return the points as a table (true) or a matrix (false). If false the target y has integer element type. \n\nExample\n\nX, y = make_moons(100; noise=0.5)\n\n\n\n\n\n","category":"function"},{"location":"generating_synthetic_data/#MLJBase.make_regression","page":"Generating Synthetic Data","title":"MLJBase.make_regression","text":"make_regression(n, p; kwargs...)\n\nGenerate Gaussian input features and a linear response with Gaussian noise, for use with regression models.\n\nReturn value\n\nBy default, a tuple (X, y) where table X has p columns and n rows (observations), together with a corresponding vector of n Continuous target observations y.\n\nKeywords\n\nintercept=true: Whether to generate data from a model with intercept.\nn_targets=1: Number of columns in the target.\nsparse=0: Proportion of the generating weight vector that is sparse.\nnoise=0.1: Standard deviation of the Gaussian noise added to the response (target).\noutliers=0: Proportion of the response vector to make as outliers by adding a random quantity with high variance. (Only applied if binary is false.)\nas_table=true: Whether X (and y, if n_targets > 1) should be a table or a matrix.\neltype=Float64: Element type for X and y. Must subtype AbstractFloat.\nbinary=false: Whether the target should be binarized (via a sigmoid).\neltype=Float64: machine type of points (any subtype of  AbstractFloat).\nrng=Random.GLOBAL_RNG: any AbstractRNG object, or integer to seed a MersenneTwister (for reproducibility).\nas_table=true: whether to return the points as a table (true) or a matrix (false). \n\nExample\n\nX, y = make_regression(100, 5; noise=0.5, sparse=0.2, outliers=0.1)\n\n\n\n\n\n","category":"function"},{"location":"models/FeatureAgglomeration_MLJScikitLearnInterface/#FeatureAgglomeration_MLJScikitLearnInterface","page":"FeatureAgglomeration","title":"FeatureAgglomeration","text":"FeatureAgglomeration\n\nA model type for constructing a feature agglomeration, based on MLJScikitLearnInterface.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nFeatureAgglomeration = @load FeatureAgglomeration pkg=MLJScikitLearnInterface\n\nDo model = FeatureAgglomeration() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in FeatureAgglomeration(n_clusters=...).\n\nSimilar to AgglomerativeClustering, but recursively merges features instead of samples.\"","category":"section"},{"location":"models/SVMRegressor_MLJScikitLearnInterface/#SVMRegressor_MLJScikitLearnInterface","page":"SVMRegressor","title":"SVMRegressor","text":"SVMRegressor\n\nA model type for constructing a epsilon-support vector regressor, based on MLJScikitLearnInterface.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nSVMRegressor = @load SVMRegressor pkg=MLJScikitLearnInterface\n\nDo model = SVMRegressor() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in SVMRegressor(kernel=...).","category":"section"},{"location":"models/SVMRegressor_MLJScikitLearnInterface/#Hyper-parameters","page":"SVMRegressor","title":"Hyper-parameters","text":"kernel = rbf\ndegree = 3\ngamma = scale\ncoef0 = 0.0\ntol = 0.001\nC = 1.0\nepsilon = 0.1\nshrinking = true\ncache_size = 200\nmax_iter = -1","category":"section"},{"location":"models/SimpleImputer_BetaML/#SimpleImputer_BetaML","page":"SimpleImputer","title":"SimpleImputer","text":"mutable struct SimpleImputer <: MLJModelInterface.Unsupervised\n\nImpute missing values using feature (column) mean, with optional record normalisation (using l-norm norms), from the Beta Machine Learning Toolkit (BetaML).","category":"section"},{"location":"models/SimpleImputer_BetaML/#Hyperparameters:","page":"SimpleImputer","title":"Hyperparameters:","text":"statistic::Function: The descriptive statistic of the column (feature) to use as imputed value [def: mean]\nnorm::Union{Nothing, Int64}: Normalise the feature mean by l-norm norm of the records [default: nothing]. Use it (e.g. norm=1 to use the l-1 norm) if the records are highly heterogeneus (e.g. quantity exports of different countries).","category":"section"},{"location":"models/SimpleImputer_BetaML/#Example:","page":"SimpleImputer","title":"Example:","text":"julia> using MLJ\n\njulia> X = [1 10.5;1.5 missing; 1.8 8; 1.7 15; 3.2 40; missing missing; 3.3 38; missing -2.3; 5.2 -2.4] |> table ;\n\njulia> modelType   = @load SimpleImputer  pkg = \"BetaML\" verbosity=0\nBetaML.Imputation.SimpleImputer\n\njulia> model     = modelType(norm=1)\nSimpleImputer(\n  statistic = Statistics.mean, \n  norm = 1)\n\njulia> mach      = machine(model, X);\n\njulia> fit!(mach);\n[ Info: Training machine(SimpleImputer(statistic = mean, …), …).\n\njulia> X_full       = transform(mach) |> MLJ.matrix\n9×2 Matrix{Float64}:\n 1.0        10.5\n 1.5         0.295466\n 1.8         8.0\n 1.7        15.0\n 3.2        40.0\n 0.280952    1.69524\n 3.3        38.0\n 0.0750839  -2.3\n 5.2        -2.4","category":"section"},{"location":"models/GaussianNBClassifier_MLJScikitLearnInterface/#GaussianNBClassifier_MLJScikitLearnInterface","page":"GaussianNBClassifier","title":"GaussianNBClassifier","text":"GaussianNBClassifier\n\nA model type for constructing a Gaussian naive Bayes classifier, based on MLJScikitLearnInterface.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nGaussianNBClassifier = @load GaussianNBClassifier pkg=MLJScikitLearnInterface\n\nDo model = GaussianNBClassifier() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in GaussianNBClassifier(priors=...).","category":"section"},{"location":"models/GaussianNBClassifier_MLJScikitLearnInterface/#Hyper-parameters","page":"GaussianNBClassifier","title":"Hyper-parameters","text":"priors = nothing\nvar_smoothing = 1.0e-9","category":"section"},{"location":"models/GaussianNBClassifier_NaiveBayes/#GaussianNBClassifier_NaiveBayes","page":"GaussianNBClassifier","title":"GaussianNBClassifier","text":"GaussianNBClassifier\n\nA model type for constructing a Gaussian naive Bayes classifier, based on NaiveBayes.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nGaussianNBClassifier = @load GaussianNBClassifier pkg=NaiveBayes\n\nDo model = GaussianNBClassifier() to construct an instance with default hyper-parameters. \n\nGiven each class taken on by the target variable y, it is supposed that the conditional probability distribution for the input variables X is a multivariate Gaussian. The mean and covariance of these Gaussian distributions are estimated using maximum likelihood, and a probability distribution for y given X is deduced by applying Bayes' rule. The required marginal for y is estimated using class frequency in the training data.\n\nImportant. The name \"naive Bayes classifier\" is perhaps misleading. Since we are learning the full multivariate Gaussian distributions for X given y, we are not applying the usual naive Bayes independence condition, which would amount to forcing the covariance matrix to be diagonal.","category":"section"},{"location":"models/GaussianNBClassifier_NaiveBayes/#Training-data","page":"GaussianNBClassifier","title":"Training data","text":"In MLJ or MLJBase, bind an instance model to data with\n\nmach = machine(model, X, y)\n\nHere:\n\nX is any table of input features (eg, a DataFrame) whose columns are of scitype Continuous; check the column scitypes with schema(X)\ny is the target, which can be any AbstractVector whose element scitype is Finite; check the scitype with schema(y)\n\nTrain the machine using fit!(mach, rows=...).","category":"section"},{"location":"models/GaussianNBClassifier_NaiveBayes/#Operations","page":"GaussianNBClassifier","title":"Operations","text":"predict(mach, Xnew): return predictions of the target given new features Xnew, which should have the same scitype as X above. Predictions are probabilistic.\npredict_mode(mach, Xnew): Return the mode of above predictions.","category":"section"},{"location":"models/GaussianNBClassifier_NaiveBayes/#Fitted-parameters","page":"GaussianNBClassifier","title":"Fitted parameters","text":"The fields of fitted_params(mach) are:\n\nc_counts: A dictionary containing the observed count of each input class.\nc_stats: A dictionary containing observed statistics on each input class. Each class is represented by a DataStats object, with the following fields:\nn_vars: The number of variables used to describe the class's behavior.\nn_obs: The number of times the class is observed.\nobs_axis: The axis along which the observations were computed.\ngaussians: A per class dictionary of Gaussians, each representing the distribution of the class. Represented with type Distributions.MvNormal from the Distributions.jl package.\nn_obs: The total number of observations in the training data.","category":"section"},{"location":"models/GaussianNBClassifier_NaiveBayes/#Examples","page":"GaussianNBClassifier","title":"Examples","text":"using MLJ\nGaussianNB = @load GaussianNBClassifier pkg=NaiveBayes\n\nX, y = @load_iris\nclf = GaussianNB()\nmach = machine(clf, X, y) |> fit!\n\nfitted_params(mach)\n\npreds = predict(mach, X) ## probabilistic predictions\npreds[1]\npredict_mode(mach, X) ## point predictions\n\nSee also MultinomialNBClassifier","category":"section"},{"location":"models/UnivariateBoxCoxTransformer_MLJTransforms/#UnivariateBoxCoxTransformer_MLJTransforms","page":"UnivariateBoxCoxTransformer","title":"UnivariateBoxCoxTransformer","text":"UnivariateBoxCoxTransformer\n\nA model type for constructing a single variable Box-Cox transformer, based on MLJTransforms.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nUnivariateBoxCoxTransformer = @load UnivariateBoxCoxTransformer pkg=MLJTransforms\n\nDo model = UnivariateBoxCoxTransformer() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in UnivariateBoxCoxTransformer(n=...).\n\nBox-Cox transformations attempt to make data look more normally distributed. This can improve performance and assist in the interpretation of models which suppose that data is generated by a normal distribution.\n\nA Box-Cox transformation (with shift) is of the form\n\nx -> ((x + c)^λ - 1)/λ\n\nfor some constant c and real λ, unless λ = 0, in which case the above is replaced with\n\nx -> log(x + c)\n\nGiven user-specified hyper-parameters n::Integer and shift::Bool, the present implementation learns the parameters c and λ from the training data as follows: If shift=true and zeros are encountered in the data, then c is set to 0.2 times the data mean.  If there are no zeros, then no shift is applied. Finally, n different values of λ between -0.4 and 3 are considered, with λ fixed to the value maximizing normality of the transformed data.\n\nReference: Wikipedia entry for power  transform.","category":"section"},{"location":"models/UnivariateBoxCoxTransformer_MLJTransforms/#Training-data","page":"UnivariateBoxCoxTransformer","title":"Training data","text":"In MLJ or MLJBase, bind an instance model to data with\n\nmach = machine(model, x)\n\nwhere\n\nx: any abstract vector with element scitype Continuous; check the scitype with scitype(x)\n\nTrain the machine using fit!(mach, rows=...).","category":"section"},{"location":"models/UnivariateBoxCoxTransformer_MLJTransforms/#Hyper-parameters","page":"UnivariateBoxCoxTransformer","title":"Hyper-parameters","text":"n=171: number of values of the exponent λ to try\nshift=false: whether to include a preliminary constant translation in transformations, in the presence of zeros","category":"section"},{"location":"models/UnivariateBoxCoxTransformer_MLJTransforms/#Operations","page":"UnivariateBoxCoxTransformer","title":"Operations","text":"transform(mach, xnew): apply the Box-Cox transformation learned when fitting mach\ninverse_transform(mach, z): reconstruct the vector z whose transformation learned by mach is z","category":"section"},{"location":"models/UnivariateBoxCoxTransformer_MLJTransforms/#Fitted-parameters","page":"UnivariateBoxCoxTransformer","title":"Fitted parameters","text":"The fields of fitted_params(mach) are:\n\nλ: the learned Box-Cox exponent\nc: the learned shift","category":"section"},{"location":"models/UnivariateBoxCoxTransformer_MLJTransforms/#Examples","page":"UnivariateBoxCoxTransformer","title":"Examples","text":"using MLJ\nusing UnicodePlots\nusing Random\nRandom.seed!(123)\n\ntransf = UnivariateBoxCoxTransformer()\n\nx = randn(1000).^2\n\nmach = machine(transf, x)\nfit!(mach)\n\nz = transform(mach, x)\n\njulia> histogram(x)\n                ┌                                        ┐\n   [ 0.0,  2.0) ┤███████████████████████████████████  848\n   [ 2.0,  4.0) ┤████▌ 109\n   [ 4.0,  6.0) ┤█▍ 33\n   [ 6.0,  8.0) ┤▍ 7\n   [ 8.0, 10.0) ┤▏ 2\n   [10.0, 12.0) ┤  0\n   [12.0, 14.0) ┤▏ 1\n                └                                        ┘\n                                 Frequency\n\njulia> histogram(z)\n                ┌                                        ┐\n   [-5.0, -4.0) ┤█▎ 8\n   [-4.0, -3.0) ┤████████▊ 64\n   [-3.0, -2.0) ┤█████████████████████▊ 159\n   [-2.0, -1.0) ┤█████████████████████████████▊ 216\n   [-1.0,  0.0) ┤███████████████████████████████████  254\n   [ 0.0,  1.0) ┤█████████████████████████▊ 188\n   [ 1.0,  2.0) ┤████████████▍ 90\n   [ 2.0,  3.0) ┤██▊ 20\n   [ 3.0,  4.0) ┤▎ 1\n                └                                        ┘\n                                 Frequency\n","category":"section"},{"location":"models/Resampler_MLJBase/#Resampler_MLJBase","page":"Resampler","title":"Resampler","text":"resampler = Resampler(\n    model=ConstantRegressor(),\n    resampling=CV(),\n    measure=nothing,\n    weights=nothing,\n    class_weights=nothing\n    operation=predict,\n    repeats = 1,\n    acceleration=default_resource(),\n    check_measure=true,\n    per_observation=true,\n    logger=default_logger(),\n    compact=false,\n)\n\nPrivate method. Use at own risk.\n\nResampling model wrapper, used internally by the fit method of TunedModel instances and IteratedModel instances. See evaluate! for meaning of the options. Not intended for use by general user, who will ordinarily use evaluate! directly.\n\nGiven a machine mach = machine(resampler, args...) one obtains a performance evaluation of the specified model, performed according to the prescribed resampling strategy and other parameters, using data args..., by calling fit!(mach) followed by evaluate(mach).\n\nOn subsequent calls to fit!(mach) new train/test pairs of row indices are only regenerated if resampling, repeats or cache fields of resampler have changed. The evolution of an RNG field of resampler does not constitute a change (== for MLJType objects is not sensitive to such changes; see is_same_except).\n\nIf there is single train/test pair, then warm-restart behavior of the wrapped model resampler.model will extend to warm-restart behaviour of the wrapper resampler, with respect to mutations of the wrapped model.\n\nThe sample weights are passed to the specified performance measures that support weights for evaluation. These weights are not to be confused with any weights bound to a Resampler instance in a machine, used for training the wrapped model when supported.\n\nThe sample class_weights are passed to the specified performance measures that support per-class weights for evaluation. These weights are not to be confused with any weights bound to a Resampler instance in a machine, used for training the wrapped model when supported.","category":"section"},{"location":"models/CatBoostRegressor_CatBoost/#CatBoostRegressor_CatBoost","page":"CatBoostRegressor","title":"CatBoostRegressor","text":"CatBoostRegressor\n\nA model type for constructing a CatBoost regressor, based on CatBoost.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nCatBoostRegressor = @load CatBoostRegressor pkg=CatBoost\n\nDo model = CatBoostRegressor() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in CatBoostRegressor(iterations=...).","category":"section"},{"location":"models/CatBoostRegressor_CatBoost/#Training-data","page":"CatBoostRegressor","title":"Training data","text":"In MLJ or MLJBase, bind an instance model to data with\n\nmach = machine(model, X, y)\n\nwhere\n\nX: any table of input features (eg, a DataFrame) whose columns each have one of the following element scitypes: Continuous, Count, Finite, Textual; check column scitypes with schema(X). Textual columns will be passed to catboost as text_features, Multiclass columns will be passed to catboost as cat_features, and OrderedFactor columns will be converted to integers.\ny: the target, which can be any AbstractVector whose element scitype is Continuous; check the scitype with scitype(y)\n\nTrain the machine with fit!(mach, rows=...).","category":"section"},{"location":"models/CatBoostRegressor_CatBoost/#Hyper-parameters","page":"CatBoostRegressor","title":"Hyper-parameters","text":"More details on the catboost hyperparameters, here are the Python docs:  https://catboost.ai/en/docs/concepts/python-reference_catboostclassifier#parameters","category":"section"},{"location":"models/CatBoostRegressor_CatBoost/#Operations","page":"CatBoostRegressor","title":"Operations","text":"predict(mach, Xnew): probabilistic predictions of the target given new features Xnew having the same scitype as X above.","category":"section"},{"location":"models/CatBoostRegressor_CatBoost/#Accessor-functions","page":"CatBoostRegressor","title":"Accessor functions","text":"feature_importances(mach): return vector of feature importances, in the form of   feature::Symbol => importance::Real pairs","category":"section"},{"location":"models/CatBoostRegressor_CatBoost/#Fitted-parameters","page":"CatBoostRegressor","title":"Fitted parameters","text":"The fields of fitted_params(mach) are:\n\nmodel: The Python CatBoostRegressor model","category":"section"},{"location":"models/CatBoostRegressor_CatBoost/#Report","page":"CatBoostRegressor","title":"Report","text":"The fields of report(mach) are:\n\nfeature_importances: Vector{Pair{Symbol, Float64}} of feature importances","category":"section"},{"location":"models/CatBoostRegressor_CatBoost/#Examples","page":"CatBoostRegressor","title":"Examples","text":"using CatBoost.MLJCatBoostInterface\nusing MLJ\n\nX = (\n    duration = [1.5, 4.1, 5.0, 6.7], \n    n_phone_calls = [4, 5, 6, 7], \n    department = coerce([\"acc\", \"ops\", \"acc\", \"ops\"], Multiclass), \n)\ny = [2.0, 4.0, 6.0, 7.0]\n\nmodel = CatBoostRegressor(iterations=5)\nmach = machine(model, X, y)\nfit!(mach)\npreds = predict(mach, X)\n\nSee also catboost and the unwrapped model type CatBoost.CatBoostRegressor.","category":"section"},{"location":"models/RidgeClassifier_MLJScikitLearnInterface/#RidgeClassifier_MLJScikitLearnInterface","page":"RidgeClassifier","title":"RidgeClassifier","text":"RidgeClassifier\n\nA model type for constructing a ridge regression classifier, based on MLJScikitLearnInterface.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nRidgeClassifier = @load RidgeClassifier pkg=MLJScikitLearnInterface\n\nDo model = RidgeClassifier() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in RidgeClassifier(alpha=...).","category":"section"},{"location":"models/RidgeClassifier_MLJScikitLearnInterface/#Hyper-parameters","page":"RidgeClassifier","title":"Hyper-parameters","text":"alpha = 1.0\nfit_intercept = true\ncopy_X = true\nmax_iter = nothing\ntol = 0.001\nclass_weight = nothing\nsolver = auto\nrandom_state = nothing","category":"section"},{"location":"feature_selection/#Feature-Selection","page":"Feature Selection","title":"Feature Selection","text":"For manually selecting features by hand, use the FeatureSelector transformer.","category":"section"},{"location":"feature_selection/#Recursive-feature-elimination","page":"Feature Selection","title":"Recursive feature elimination","text":"Supervised models that report feature importances can be wrapped using RecursiveFeatureElmination method, to carry out recursive feature elimination. A model (or model type) m reports feature importances if reports_feature_importances(m) is true.\n\nSee the FeatureSelection.jl documentation for examples, including recursive feature elimination with cross-validation to learn the optimal number of features to retain.","category":"section"},{"location":"feature_selection/#Reference","page":"Feature Selection","title":"Reference","text":"","category":"section"},{"location":"feature_selection/#FeatureSelection.FeatureSelector","page":"Feature Selection","title":"FeatureSelection.FeatureSelector","text":"FeatureSelector\n\nA model type for constructing a feature selector, based on unknown.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nFeatureSelector = @load FeatureSelector pkg=unknown\n\nDo model = FeatureSelector() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in FeatureSelector(features=...).\n\nUse this model to select features (columns) of a table, usually as part of a model Pipeline.\n\nTraining data\n\nIn MLJ or MLJBase, bind an instance model to data with\n\nmach = machine(model, X)\n\nwhere\n\nX: any table of input features, where \"table\" is in the sense of Tables.jl\n\nTrain the machine using fit!(mach, rows=...).\n\nHyper-parameters\n\nfeatures: one of the following, with the behavior indicated:\n[] (empty, the default): filter out all features (columns) which were not encountered in training\nnon-empty vector of feature names (symbols): keep only the specified features (ignore=false) or keep only unspecified features (ignore=true)\nfunction or other callable: keep a feature if the callable returns true on its name. For example, specifying FeatureSelector(features = name -> name in [:x1, :x3], ignore = true) has the same effect as FeatureSelector(features = [:x1, :x3], ignore = true), namely to select all features, with the exception of :x1 and :x3.\nignore: whether to ignore or keep specified features, as explained above\n\nOperations\n\ntransform(mach, Xnew): select features from the table Xnew as specified by the model, taking features seen during training into account, if relevant\n\nFitted parameters\n\nThe fields of fitted_params(mach) are:\n\nfeatures_to_keep: the features that will be selected\n\nExample\n\nusing MLJ\n\nX = (ordinal1 = [1, 2, 3],\n     ordinal2 = coerce([\"x\", \"y\", \"x\"], OrderedFactor),\n     ordinal3 = [10.0, 20.0, 30.0],\n     ordinal4 = [-20.0, -30.0, -40.0],\n     nominal = coerce([\"Your father\", \"he\", \"is\"], Multiclass));\n\nselector = FeatureSelector(features=[:ordinal3, ], ignore=true);\n\njulia> transform(fit!(machine(selector, X)), X)\n(ordinal1 = [1, 2, 3],\n ordinal2 = CategoricalValue{Symbol,UInt32}[\"x\", \"y\", \"x\"],\n ordinal4 = [-20.0, -30.0, -40.0],\n nominal = CategoricalValue{String,UInt32}[\"Your father\", \"he\", \"is\"],)\n\n\n\n\n\n\n","category":"type"},{"location":"feature_selection/#FeatureSelection.RecursiveFeatureElimination","page":"Feature Selection","title":"FeatureSelection.RecursiveFeatureElimination","text":"RecursiveFeatureElimination(model; n_features=0, step=1)\n\nThis model implements a recursive feature elimination algorithm for feature selection. It recursively removes features, training a base model on the remaining features and evaluating their importance until the desired number of features is selected.\n\nTraining data\n\nIn MLJ or MLJBase, bind an instance rfe_model to data with\n\nmach = machine(rfe_model, X, y)\n\nOR, if the base model supports weights, as\n\nmach = machine(rfe_model, X, y, w)\n\nHere:\n\nX is any table of input features (eg, a DataFrame) whose columns are of the scitype as that required by the base model; check column scitypes with schema(X) and column scitypes required by base model with input_scitype(basemodel).\ny is the target, which can be any table of responses whose element scitype is   Continuous or Finite depending on the target_scitype required by the base model;   check the scitype with scitype(y).\nw is the observation weights which can either be nothing(default) or an AbstractVector whoose element scitype is Count or Continuous. This is different from weights kernel which is an hyperparameter to the model, see below.\n\nTrain the machine using fit!(mach, rows=...).\n\nHyper-parameters\n\nmodel: A base model with a fit method that provides information on feature feature importance (i.e reports_feature_importances(model) == true)\nn_features::Real = 0: The number of features to select. If 0, half of the features are selected. If a positive integer, the parameter is the absolute number of features to select. If a real number between 0 and 1, it is the fraction of features to select.\nstep::Real=1: If the value of step is at least 1, it signifies the quantity of features to eliminate in each iteration. Conversely, if step falls strictly within the range of 0.0 to 1.0, it denotes the proportion (rounded down) of features to remove during each iteration.\n\nOperations\n\ntransform(mach, X): transform the input table X into a new table containing only columns corresponding to features accepted by the RFE algorithm.\npredict(mach, X): transform the input table X into a new table same as in transform(mach, X) above and predict using the fitted base model on the transformed table.\n\nFitted parameters\n\nThe fields of fitted_params(mach) are:\n\nfeatures_left: names of features remaining after recursive feature elimination.\nmodel_fitresult: fitted parameters of the base model.\n\nReport\n\nThe fields of report(mach) are:\n\nscores: dictionary of scores for each feature in the training dataset. The model deems highly scored variables more significant.\nmodel_report: report for the fitted base model.\n\nExamples\n\nThe following example assumes you have MLJDecisionTreeInterface in the active package ennvironment.\n\nusing MLJ\n\nRandomForestRegressor = @load RandomForestRegressor pkg=DecisionTree\n\n# Creates a dataset where the target only depends on the first 5 columns of the input table.\nA = rand(50, 10);\ny = 10 .* sin.(\n        pi .* A[:, 1] .* A[:, 2]\n    ) + 20 .* (A[:, 3] .- 0.5).^ 2 .+ 10 .* A[:, 4] .+ 5 * A[:, 5];\nX = MLJ.table(A);\n\n# fit a rfe model:\nrf = RandomForestRegressor()\nselector = RecursiveFeatureElimination(rf, n_features=2)\nmach = machine(selector, X, y)\nfit!(mach)\n\n# view the feature importances\nfeature_importances(mach)\n\n# predict using the base model trained on the reduced feature set:\nXnew = MLJ.table(rand(50, 10));\npredict(mach, Xnew)\n\n# transform data with all features to the reduced feature set:\ntransform(mach, Xnew)\n\n\n\n\n\n","category":"function"},{"location":"models/LassoRegressor_MLJScikitLearnInterface/#LassoRegressor_MLJScikitLearnInterface","page":"LassoRegressor","title":"LassoRegressor","text":"LassoRegressor\n\nA model type for constructing a lasso regressor, based on MLJScikitLearnInterface.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nLassoRegressor = @load LassoRegressor pkg=MLJScikitLearnInterface\n\nDo model = LassoRegressor() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in LassoRegressor(alpha=...).","category":"section"},{"location":"models/LassoRegressor_MLJScikitLearnInterface/#Hyper-parameters","page":"LassoRegressor","title":"Hyper-parameters","text":"alpha = 1.0\nfit_intercept = true\nprecompute = false\ncopy_X = true\nmax_iter = 1000\ntol = 0.0001\nwarm_start = false\npositive = false\nrandom_state = nothing\nselection = cyclic","category":"section"},{"location":"models/KDEDetector_OutlierDetectionPython/#KDEDetector_OutlierDetectionPython","page":"KDEDetector","title":"KDEDetector","text":"KDEDetector(bandwidth=1.0,\n               algorithm=\"auto\",\n               leaf_size=30,\n               metric=\"minkowski\",\n               metric_params=None)\n\nhttps://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.kde","category":"section"},{"location":"models/ConstantClassifier_MLJModels/#ConstantClassifier_MLJModels","page":"ConstantClassifier","title":"ConstantClassifier","text":"ConstantClassifier\n\nThis \"dummy\" probabilistic predictor always returns the same distribution, irrespective of the provided input pattern. The distribution d returned is the UnivariateFinite distribution based on frequency of classes observed in the training target data. So, pdf(d, level) is the number of times the training target takes on the value level. Use predict_mode instead of predict to obtain the training target mode instead. For more on the UnivariateFinite type, see the CategoricalDistributions.jl package.\n\nAlmost any reasonable model is expected to outperform ConstantClassifier, which is used almost exclusively for testing and establishing performance baselines.\n\nIn MLJ (or MLJModels) do model = ConstantClassifier() to construct an instance.","category":"section"},{"location":"models/ConstantClassifier_MLJModels/#Training-data","page":"ConstantClassifier","title":"Training data","text":"In MLJ or MLJBase, bind an instance model to data with\n\nmach = machine(model, X, y)\n\nHere:\n\nX is any table of input features (eg, a DataFrame)\ny is the target, which can be any AbstractVector whose element scitype is Finite; check the scitype with schema(y)\n\nTrain the machine using fit!(mach, rows=...).","category":"section"},{"location":"models/ConstantClassifier_MLJModels/#Hyper-parameters","page":"ConstantClassifier","title":"Hyper-parameters","text":"None.","category":"section"},{"location":"models/ConstantClassifier_MLJModels/#Operations","page":"ConstantClassifier","title":"Operations","text":"predict(mach, Xnew): Return predictions of the target given features Xnew (which for this model are ignored). Predictions are probabilistic.\npredict_mode(mach, Xnew): Return the mode of the probabilistic predictions returned above.","category":"section"},{"location":"models/ConstantClassifier_MLJModels/#Fitted-parameters","page":"ConstantClassifier","title":"Fitted parameters","text":"The fields of fitted_params(mach) are:\n\ntarget_distribution: The distribution fit to the supplied target data.","category":"section"},{"location":"models/ConstantClassifier_MLJModels/#Examples","page":"ConstantClassifier","title":"Examples","text":"using MLJ\n\nclf = ConstantClassifier()\n\nX, y = @load_crabs ## a table and a categorical vector\nmach = machine(clf, X, y) |> fit!\n\nfitted_params(mach)\n\nXnew = (;FL = [8.1, 24.8, 7.2],\n        RW = [5.1, 25.7, 6.4],\n        CL = [15.9, 46.7, 14.3],\n        CW = [18.7, 59.7, 12.2],\n        BD = [6.2, 23.6, 8.4],)\n\n## probabilistic predictions:\nyhat = predict(mach, Xnew)\nyhat[1]\n\n## raw probabilities:\npdf.(yhat, \"B\")\n\n## probability matrix:\nL = levels(y)\npdf(yhat, L)\n\n## point predictions:\npredict_mode(mach, Xnew)\n\nSee also ConstantRegressor","category":"section"},{"location":"models/ClusterUndersampler_Imbalance/#ClusterUndersampler_Imbalance","page":"ClusterUndersampler","title":"ClusterUndersampler","text":"Initiate a cluster undersampling model with the given hyper-parameters.\n\nClusterUndersampler\n\nA model type for constructing a cluster undersampler, based on Imbalance.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nClusterUndersampler = @load ClusterUndersampler pkg=Imbalance\n\nDo model = ClusterUndersampler() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in ClusterUndersampler(mode=...).\n\nClusterUndersampler implements clustering undersampling as presented in Wei-Chao, L., Chih-Fong, T., Ya-Han, H., & Jing-Shang, J. (2017).    Clustering-based undersampling in class-imbalanced data. Information Sciences, 409–410, 17–26. with K-means as   the clustering algorithm.","category":"section"},{"location":"models/ClusterUndersampler_Imbalance/#Training-data","page":"ClusterUndersampler","title":"Training data","text":"In MLJ or MLJBase, wrap the model in a machine by \tmach = machine(model)\n\nThere is no need to provide any data here because the model is a static transformer.\n\nLikewise, there is no need to fit!(mach). \n\nFor default values of the hyper-parameters, model can be constructed with model = ClusterUndersampler().","category":"section"},{"location":"models/ClusterUndersampler_Imbalance/#Hyperparameters","page":"ClusterUndersampler","title":"Hyperparameters","text":"mode::AbstractString=\"nearest: If \"center\" then the undersampled data will consist of the centriods of\n\neach cluster found; if `\"nearest\"` then it will consist of the nearest neighbor of each centroid.\n\nratios=1.0: A parameter that controls the amount of undersampling to be done for each class\nCan be a float and in this case each class will be undersampled to the size of the minority class times the float. By default, all classes are undersampled to the size of the minority class\nCan be a dictionary mapping each class label to the float ratio for that class\nmaxiter::Integer=100: Maximum number of iterations to run K-means\nrng::Integer=42: Random number generator seed. Must be an integer.","category":"section"},{"location":"models/ClusterUndersampler_Imbalance/#Transform-Inputs","page":"ClusterUndersampler","title":"Transform Inputs","text":"X: A matrix or table of floats where each row is an observation from the dataset\ny: An abstract vector of labels (e.g., strings) that correspond to the observations in X","category":"section"},{"location":"models/ClusterUndersampler_Imbalance/#Transform-Outputs","page":"ClusterUndersampler","title":"Transform Outputs","text":"X_under: A matrix or table that includes the data after undersampling    depending on whether the input X is a matrix or table respectively\ny_under: An abstract vector of labels corresponding to X_under","category":"section"},{"location":"models/ClusterUndersampler_Imbalance/#Operations","page":"ClusterUndersampler","title":"Operations","text":"transform(mach, X, y): resample the data X and y using ClusterUndersampler, returning the undersampled versions","category":"section"},{"location":"models/ClusterUndersampler_Imbalance/#Example","page":"ClusterUndersampler","title":"Example","text":"using MLJ\nimport Imbalance\n\n## set probability of each class\nclass_probs = [0.5, 0.2, 0.3]                         \nnum_rows, num_continuous_feats = 100, 5\n## generate a table and categorical vector accordingly\nX, y = Imbalance.generate_imbalanced_data(num_rows, num_continuous_feats; \n                                class_probs, rng=42)   \n                                                    \njulia> Imbalance.checkbalance(y; ref=\"minority\")\n 1: ▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 19 (100.0%) \n 2: ▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 33 (173.7%) \n 0: ▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 48 (252.6%) \n\n## load cluster_undersampling\nClusterUndersampler = @load ClusterUndersampler pkg=Imbalance\n\n## wrap the model in a machine\nundersampler = ClusterUndersampler(mode=\"nearest\", \n                                   ratios=Dict(0=>1.0, 1=> 1.0, 2=>1.0), rng=42)\nmach = machine(undersampler)\n\n## provide the data to transform (there is nothing to fit)\nX_under, y_under = transform(mach, X, y)\n\n                                       \njulia> Imbalance.checkbalance(y_under; ref=\"minority\")\n0: ▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 19 (100.0%) \n2: ▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 19 (100.0%) \n1: ▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 19 (100.0%)","category":"section"},{"location":"models/MultitargetRidgeRegressor_MultivariateStats/#MultitargetRidgeRegressor_MultivariateStats","page":"MultitargetRidgeRegressor","title":"MultitargetRidgeRegressor","text":"MultitargetRidgeRegressor\n\nA model type for constructing a multitarget ridge regressor, based on MultivariateStats.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nMultitargetRidgeRegressor = @load MultitargetRidgeRegressor pkg=MultivariateStats\n\nDo model = MultitargetRidgeRegressor() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in MultitargetRidgeRegressor(lambda=...).\n\nMulti-target ridge regression adds a quadratic penalty term to multi-target least squares regression, for regularization. Ridge regression is particularly useful in the case of multicollinearity. In this case, the output represents a response vector. Options exist to specify a bias term, and to adjust the strength of the penalty term.","category":"section"},{"location":"models/MultitargetRidgeRegressor_MultivariateStats/#Training-data","page":"MultitargetRidgeRegressor","title":"Training data","text":"In MLJ or MLJBase, bind an instance model to data with\n\nmach = machine(model, X, y)\n\nHere:\n\nX is any table of input features (eg, a DataFrame) whose columns are of scitype    Continuous; check column scitypes with schema(X).\ny is the target, which can be any table of responses whose element scitype is    Continuous; check the scitype with scitype(y).\n\nTrain the machine using fit!(mach, rows=...).","category":"section"},{"location":"models/MultitargetRidgeRegressor_MultivariateStats/#Hyper-parameters","page":"MultitargetRidgeRegressor","title":"Hyper-parameters","text":"lambda=1.0: Is the non-negative parameter for the regularization strength. If lambda    is 0, ridge regression is equivalent to linear least squares regression, and as lambda    approaches infinity, all the linear coefficients approach 0.\nbias=true: Include the bias term if true, otherwise fit without bias term.","category":"section"},{"location":"models/MultitargetRidgeRegressor_MultivariateStats/#Operations","page":"MultitargetRidgeRegressor","title":"Operations","text":"predict(mach, Xnew): Return predictions of the target given new features Xnew, which    should have the same scitype as X above.","category":"section"},{"location":"models/MultitargetRidgeRegressor_MultivariateStats/#Fitted-parameters","page":"MultitargetRidgeRegressor","title":"Fitted parameters","text":"The fields of fitted_params(mach) are:\n\ncoefficients: The linear coefficients determined by the model.\nintercept: The intercept determined by the model.","category":"section"},{"location":"models/MultitargetRidgeRegressor_MultivariateStats/#Examples","page":"MultitargetRidgeRegressor","title":"Examples","text":"using MLJ\nusing DataFrames\n\nRidgeRegressor = @load MultitargetRidgeRegressor pkg=MultivariateStats\n\nX, y = make_regression(100, 6; n_targets = 2)  ## a table and a table (synthetic data)\n\nridge_regressor = RidgeRegressor(lambda=1.5)\nmach = machine(ridge_regressor, X, y) |> fit!\n\nXnew, _ = make_regression(3, 6)\nyhat = predict(mach, Xnew) ## new predictions\n\nSee also LinearRegressor, MultitargetLinearRegressor, RidgeRegressor","category":"section"},{"location":"frequently_asked_questions/#Frequently-Asked-Questions","page":"FAQ","title":"Frequently Asked Questions","text":"","category":"section"},{"location":"frequently_asked_questions/#Julia-already-has-a-great-machine-learning-toolbox,-ScitkitLearn.jl.-Why-MLJ?","page":"FAQ","title":"Julia already has a great machine learning toolbox, ScitkitLearn.jl. Why MLJ?","text":"An alternative machine learning toolbox for Julia users is ScikitLearn.jl. Initially intended as a Julia wrapper for the popular python library scikit-learn, ML algorithms written in Julia can also implement the ScikitLearn.jl API. Meta-algorithms (systematic tuning, pipelining, etc) remain python wrapped code, however.\n\nWhile ScikitLearn.jl provides the Julia user with access to a mature and large library of machine learning models, the scikit-learn API on which it is modeled, dating back to 2007, is not likely to evolve significantly in the future. MLJ enjoys (or will enjoy) several features that should make it an attractive alternative in the longer term:\n\nOne language. ScikitLearn.jl wraps Python code, which in turn wraps C code for performance-critical routines. A Julia machine learning algorithm that implements the MLJ model interface is 100% Julia. Writing code in Julia is almost as fast as Python and well-written Julia code runs almost as fast as C. Additionally, a single language design provides superior interoperability. For example, one can implement: (i) gradient-descent tuning of hyperparameters, using automatic differentiation libraries such as Flux.jl; and (ii) GPU performance boosts without major code refactoring, using CuArrays.jl.\nRegistry for model metadata. In ScikitLearn.jl the list of available models, as well as model metadata (whether a model handles categorical inputs, whether it can make probabilistic predictions, etc) must be gleaned from the documentation. In MLJ, this information is more structured and is accessible to MLJ via a searchable model registry (without the models needing to be loaded).\nFlexible API for model composition. Pipelines in scikit-learn are more of an afterthought than an integral part of the original design. By contrast, MLJ's user-interaction API was predicated on the requirements of a flexible \"learning network\" API, one that allows models to be connected in essentially arbitrary ways (such as Wolpert model stacks). Networks can be built and tested in stages before being exported as first-class stand-alone models. Networks feature \"smart\" training (only necessary components are retrained after parameter changes) and will eventually be trainable using a DAG scheduler.\nClean probabilistic API. The scikit-learn API does not specify a universal standard for the form of probabilistic predictions. By fixing a probabilistic API along the lines of the skpro project, MLJ aims to improve support for Bayesian statistics and probabilistic graphical models.\nUniversal adoption of categorical data types. Python's scientific array library NumPy has no dedicated data type for representing categorical data (i.e., no type that tracks the pool of all possible classes). Generally, scikit-learn models deal with this by requiring data to be relabeled as integers. However, the naive user trains a model on relabeled categorical data only to discover that evaluation on a test set crashes their code because a categorical feature takes on a value not observed in training. MLJ mitigates such issues by insisting on the use of categorical data types, and by insisting that MLJ model implementations preserve the class pools. If, for example, a training target contains classes in the pool that do not appear in the training set, a probabilistic prediction will nevertheless predict a distribution whose support includes the missing class, but which is appropriately weighted with probability zero.\n\nFinally, we note that a large number of ScikitLearn.jl models are now wrapped for use in MLJ.","category":"section"},{"location":"models/AffinityPropagation_MLJScikitLearnInterface/#AffinityPropagation_MLJScikitLearnInterface","page":"AffinityPropagation","title":"AffinityPropagation","text":"AffinityPropagation\n\nA model type for constructing a Affinity Propagation Clustering of data, based on MLJScikitLearnInterface.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nAffinityPropagation = @load AffinityPropagation pkg=MLJScikitLearnInterface\n\nDo model = AffinityPropagation() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in AffinityPropagation(damping=...).","category":"section"},{"location":"models/AffinityPropagation_MLJScikitLearnInterface/#Hyper-parameters","page":"AffinityPropagation","title":"Hyper-parameters","text":"damping = 0.5\nmax_iter = 200\nconvergence_iter = 15\ncopy = true\npreference = nothing\naffinity = euclidean\nverbose = false","category":"section"},{"location":"models/LogisticCVClassifier_MLJScikitLearnInterface/#LogisticCVClassifier_MLJScikitLearnInterface","page":"LogisticCVClassifier","title":"LogisticCVClassifier","text":"LogisticCVClassifier\n\nA model type for constructing a logistic regression classifier with built-in cross-validation, based on MLJScikitLearnInterface.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nLogisticCVClassifier = @load LogisticCVClassifier pkg=MLJScikitLearnInterface\n\nDo model = LogisticCVClassifier() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in LogisticCVClassifier(Cs=...).","category":"section"},{"location":"models/LogisticCVClassifier_MLJScikitLearnInterface/#Hyper-parameters","page":"LogisticCVClassifier","title":"Hyper-parameters","text":"Cs = 10\nfit_intercept = true\ncv = 5\ndual = false\npenalty = l2\nscoring = nothing\nsolver = lbfgs\ntol = 0.0001\nmax_iter = 100\nclass_weight = nothing\nn_jobs = nothing\nverbose = 0\nrefit = true\nintercept_scaling = 1.0\nmulti_class = auto\nrandom_state = nothing\nl1_ratios = nothing","category":"section"},{"location":"models/ROSE_Imbalance/#ROSE_Imbalance","page":"ROSE","title":"ROSE","text":"Initiate a ROSE model with the given hyper-parameters.\n\nROSE\n\nA model type for constructing a rose, based on Imbalance.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nROSE = @load ROSE pkg=Imbalance\n\nDo model = ROSE() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in ROSE(s=...).\n\nROSE implements the ROSE (Random Oversampling Examples) algorithm to      correct for class imbalance as in G Menardi, N. Torelli, “Training and assessing      classification rules with imbalanced data,”      Data Mining and Knowledge Discovery, 28(1), pp.92-122, 2014.","category":"section"},{"location":"models/ROSE_Imbalance/#Training-data","page":"ROSE","title":"Training data","text":"In MLJ or MLJBase, wrap the model in a machine by     mach = machine(model)\n\nThere is no need to provide any data here because the model is a static transformer.\n\nLikewise, there is no need to fit!(mach). \n\nFor default values of the hyper-parameters, model can be constructed by     model = ROSE()","category":"section"},{"location":"models/ROSE_Imbalance/#Hyperparameters","page":"ROSE","title":"Hyperparameters","text":"s::float: A parameter that proportionally controls the bandwidth of the Gaussian kernel\nratios=1.0: A parameter that controls the amount of oversampling to be done for each class\nCan be a float and in this case each class will be oversampled to the size of the majority class times the float. By default, all classes are oversampled to the size of the majority class\nCan be a dictionary mapping each class label to the float ratio for that class\nrng::Union{AbstractRNG, Integer}=default_rng(): Either an AbstractRNG object or an Integer    seed to be used with Xoshiro if the Julia VERSION supports it. Otherwise, uses MersenneTwister`.","category":"section"},{"location":"models/ROSE_Imbalance/#Transform-Inputs","page":"ROSE","title":"Transform Inputs","text":"X: A matrix or table of floats where each row is an observation from the dataset\ny: An abstract vector of labels (e.g., strings) that correspond to the observations in X","category":"section"},{"location":"models/ROSE_Imbalance/#Transform-Outputs","page":"ROSE","title":"Transform Outputs","text":"Xover: A matrix or table that includes original data and the new observations    due to oversampling. depending on whether the input X is a matrix or table respectively\nyover: An abstract vector of labels corresponding to Xover","category":"section"},{"location":"models/ROSE_Imbalance/#Operations","page":"ROSE","title":"Operations","text":"transform(mach, X, y): resample the data X and y using ROSE, returning both the new and original observations","category":"section"},{"location":"models/ROSE_Imbalance/#Example","page":"ROSE","title":"Example","text":"using MLJ\nimport Imbalance\n\n## set probability of each class\nclass_probs = [0.5, 0.2, 0.3]                         \nnum_rows, num_continuous_feats = 100, 5\n## generate a table and categorical vector accordingly\nX, y = Imbalance.generate_imbalanced_data(num_rows, num_continuous_feats; \n                                class_probs, rng=42)  \n\njulia> Imbalance.checkbalance(y)\n1: ▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 19 (39.6%) \n2: ▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 33 (68.8%) \n0: ▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 48 (100.0%) \n\n## load ROSE\nROSE = @load ROSE pkg=Imbalance\n\n## wrap the model in a machine\noversampler = ROSE(s=0.3, ratios=Dict(0=>1.0, 1=> 0.9, 2=>0.8), rng=42)\nmach = machine(oversampler)\n\n## provide the data to transform (there is nothing to fit)\nXover, yover = transform(mach, X, y)\n\njulia> Imbalance.checkbalance(yover)\n2: ▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 38 (79.2%) \n1: ▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 43 (89.6%) \n0: ▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 48 (100.0%) ","category":"section"},{"location":"thresholding_probabilistic_predictors/#Thresholding-Probabilistic-Predictors","page":"Thresholding Probabilistic Predictors","title":"Thresholding Probabilistic Predictors","text":"Although one can call predict_mode on a probabilistic binary classifier to get deterministic predictions, a more flexible strategy is to wrap the model using BinaryThresholdPredictor, as this allows the user to specify the threshold probability for predicting a positive class. This wrapping converts a probabilistic classifier into a deterministic one.\n\nThe positive class is always the second class returned when calling levels on the training target y.","category":"section"},{"location":"thresholding_probabilistic_predictors/#MLJModels.BinaryThresholdPredictor","page":"Thresholding Probabilistic Predictors","title":"MLJModels.BinaryThresholdPredictor","text":"BinaryThresholdPredictor(model; threshold=0.5)\n\nWrap the Probabilistic model, model, assumed to support binary classification, as a Deterministic model, by applying the specified threshold to the positive class probability. In addition to conventional supervised classifiers, it can also be applied to outlier detection models that predict normalized scores - in the form of appropriate UnivariateFinite distributions - that is, models that subtype AbstractProbabilisticUnsupervisedDetector or AbstractProbabilisticSupervisedDetector.\n\nBy convention the positive class is the second class returned by levels(y), where y is the target.\n\nIf threshold=0.5 then calling predict on the wrapped model is equivalent to calling predict_mode on the atomic model.\n\nExample\n\nBelow is an application to the well-known Pima Indian diabetes dataset, including optimization of the threshold parameter, with a high balanced accuracy the objective. The target class distribution is 500 positives to 268 negatives.\n\nLoading the data:\n\nusing MLJ, Random\nrng = Xoshiro(123)\n\ndiabetes = OpenML.load(43582)\noutcome, X = unpack(diabetes, ==(:Outcome), rng=rng);\ny = coerce(Int.(outcome), OrderedFactor);\n\nChoosing a probabilistic classifier:\n\nEvoTreesClassifier = @load EvoTreesClassifier\nprob_predictor = EvoTreesClassifier()\n\nWrapping in TunedModel to get a deterministic classifier with threshold as a new hyperparameter:\n\npoint_predictor = BinaryThresholdPredictor(prob_predictor, threshold=0.6)\nXnew, _ = make_moons(3, rng=rng)\nmach = machine(point_predictor, X, y) |> fit!\npredict(mach, X)[1:3] # [0, 0, 0]\n\nEstimating performance:\n\nbalanced = BalancedAccuracy(adjusted=true)\ne = evaluate!(mach, resampling=CV(nfolds=6), measures=[balanced, accuracy])\ne.measurement[1] # 0.405 ± 0.089\n\nWrapping in tuning strategy to learn threshold that maximizes balanced accuracy:\n\nr = range(point_predictor, :threshold, lower=0.1, upper=0.9)\ntuned_point_predictor = TunedModel(\n    point_predictor,\n    tuning=RandomSearch(rng=rng),\n    resampling=CV(nfolds=6),\n    range = r,\n    measure=balanced,\n    n=30,\n)\nmach2 = machine(tuned_point_predictor, X, y) |> fit!\noptimized_point_predictor = report(mach2).best_model\noptimized_point_predictor.threshold # 0.260\npredict(mach2, X)[1:3] # [1, 1, 0]\n\nEstimating the performance of the auto-thresholding model (nested resampling here):\n\ne = evaluate!(mach2, resampling=CV(nfolds=6), measure=[balanced, accuracy])\ne.measurement[1] # 0.477 ± 0.110\n\n\n\n\n\n","category":"type"},{"location":"models/UnivariateDiscretizer_MLJTransforms/#UnivariateDiscretizer_MLJTransforms","page":"UnivariateDiscretizer","title":"UnivariateDiscretizer","text":"UnivariateDiscretizer\n\nA model type for constructing a single variable discretizer, based on MLJTransforms.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nUnivariateDiscretizer = @load UnivariateDiscretizer pkg=MLJTransforms\n\nDo model = UnivariateDiscretizer() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in UnivariateDiscretizer(n_classes=...).\n\nDiscretization converts a Continuous vector into an OrderedFactor vector. In particular, the output is a CategoricalVector (whose reference type is optimized).\n\nThe transformation is chosen so that the vector on which the transformer is fit has, in transformed form, an approximately uniform distribution of values. Specifically, if n_classes is the level of discretization, then 2*n_classes - 1 ordered quantiles are computed, the odd quantiles being used for transforming (discretization) and the even quantiles for inverse transforming.","category":"section"},{"location":"models/UnivariateDiscretizer_MLJTransforms/#Training-data","page":"UnivariateDiscretizer","title":"Training data","text":"In MLJ or MLJBase, bind an instance model to data with\n\nmach = machine(model, x)\n\nwhere\n\nx: any abstract vector with Continuous element scitype; check scitype with scitype(x).\n\nTrain the machine using fit!(mach, rows=...).","category":"section"},{"location":"models/UnivariateDiscretizer_MLJTransforms/#Hyper-parameters","page":"UnivariateDiscretizer","title":"Hyper-parameters","text":"n_classes: number of discrete classes in the output","category":"section"},{"location":"models/UnivariateDiscretizer_MLJTransforms/#Operations","page":"UnivariateDiscretizer","title":"Operations","text":"transform(mach, xnew): discretize xnew according to the discretization learned when fitting mach\ninverse_transform(mach, z): attempt to reconstruct from z a vector that transforms to give z","category":"section"},{"location":"models/UnivariateDiscretizer_MLJTransforms/#Fitted-parameters","page":"UnivariateDiscretizer","title":"Fitted parameters","text":"The fields of fitted_params(mach).fitesult include:\n\nodd_quantiles: quantiles used for transforming (length is n_classes - 1)\neven_quantiles: quantiles used for inverse transforming (length is n_classes)","category":"section"},{"location":"models/UnivariateDiscretizer_MLJTransforms/#Example","page":"UnivariateDiscretizer","title":"Example","text":"using MLJ\nusing Random\nRandom.seed!(123)\n\ndiscretizer = UnivariateDiscretizer(n_classes=100)\nmach = machine(discretizer, randn(1000))\nfit!(mach)\n\njulia> x = rand(5)\n5-element Vector{Float64}:\n 0.8585244609846809\n 0.37541692370451396\n 0.6767070590395461\n 0.9208844241267105\n 0.7064611415680901\n\njulia> z = transform(mach, x)\n5-element CategoricalArrays.CategoricalArray{UInt8,1,UInt8}:\n 0x52\n 0x42\n 0x4d\n 0x54\n 0x4e\n\nx_approx = inverse_transform(mach, z)\njulia> x - x_approx\n5-element Vector{Float64}:\n 0.008224506144777322\n 0.012731354778359405\n 0.0056265330571125816\n 0.005738175684445124\n 0.006835652575801987","category":"section"},{"location":"simple_user_defined_models/#Simple-User-Defined-Models","page":"Simple User Defined Models","title":"Simple User Defined Models","text":"To quickly implement a new supervised model in MLJ, it suffices to:\n\nDefine a mutable struct to store hyperparameters. This is either a subtype of Probabilistic or Deterministic, depending on whether probabilistic or ordinary point predictions are intended. This struct is the model.\nDefine a fit method, dispatched on the model, returning learned parameters, also known as the fitresult.\nDefine a predict method, dispatched on the model, and the fitresult, to return predictions on new patterns.\n\nIn the examples below, the training input X of fit, and the new input Xnew passed to predict, are tables. Each training target y is an AbstractVector.\n\nThe predictions returned by predict have the same form as y for deterministic models, but are Vectors of distributions for probabilistic models.\n\nAdvanced model functionality not addressed here includes: (i) optional update method to avoid redundant calculations when calling fit! on machines a second time; (ii) reporting extra training-related statistics; (iii) exposing model-specific functionality; (iv) checking the scientific type of data passed to your model in machine construction; and (iv) checking the validity of hyperparameter values. All this is described in Adding Models for General Use.\n\nFor an unsupervised model, implement transform and, optionally, inverse_transform using the same signature at predict below.","category":"section"},{"location":"simple_user_defined_models/#A-simple-deterministic-regressor","page":"Simple User Defined Models","title":"A simple deterministic regressor","text":"Here's a quick-and-dirty implementation of a ridge regressor with no intercept:\n\nusing MLJ; color_off() # hide\nimport MLJBase\nusing LinearAlgebra\n\nmutable struct MyRegressor <: MLJBase.Deterministic\n    lambda::Float64\nend\nMyRegressor(; lambda=0.1) = MyRegressor(lambda)\n\n# fit returns coefficients minimizing a penalized rms loss function:\nfunction MLJBase.fit(model::MyRegressor, verbosity, X, y)\n    x = MLJBase.matrix(X)                     # convert table to matrix\n    fitresult = (x'x + model.lambda*I)\\(x'y)  # the coefficients\n    cache = nothing\n    report = nothing\n    return fitresult, cache, report\nend\n\n# predict uses coefficients to make a new prediction:\nMLJBase.predict(::MyRegressor, fitresult, Xnew) = MLJBase.matrix(Xnew) * fitresult\nnothing # hide\n\nAfter loading this code, all MLJ's basic meta-algorithms can be applied to MyRegressor:\n\nusing MLJ # hide\nX, y = @load_boston;\nmodel = MyRegressor(lambda=1.0)\nregressor = machine(model, X, y)\nevaluate!(regressor, resampling=CV(), measure=rms, verbosity=0)","category":"section"},{"location":"simple_user_defined_models/#A-simple-probabilistic-classifier","page":"Simple User Defined Models","title":"A simple probabilistic classifier","text":"The following probabilistic model simply fits a probability distribution to the MultiClass training target (i.e., ignores X) and returns this pdf for any new pattern:\n\nusing MLJ # hide\nimport MLJBase\nimport Distributions\n\nstruct MyClassifier <: MLJBase.Probabilistic\nend\n\n# `fit` ignores the inputs X and returns the training target y\n# probability distribution:\nfunction MLJBase.fit(model::MyClassifier, verbosity, X, y)\n    fitresult = Distributions.fit(MLJBase.UnivariateFinite, y)\n    cache = nothing\n    report = nothing\n    return fitresult, cache, report\nend\n\n# `predict` returns the passed fitresult (pdf) for all new patterns:\nMLJBase.predict(model::MyClassifier, fitresult, Xnew) =\n    [fitresult for r in 1:nrows(Xnew)]\n\nX, y = @load_iris;\nmach = machine(MyClassifier(), X, y) |> fit!;\npredict(mach, selectrows(X, 1:2))","category":"section"},{"location":"models/BayesianRidgeRegressor_MLJScikitLearnInterface/#BayesianRidgeRegressor_MLJScikitLearnInterface","page":"BayesianRidgeRegressor","title":"BayesianRidgeRegressor","text":"BayesianRidgeRegressor\n\nA model type for constructing a Bayesian ridge regressor, based on MLJScikitLearnInterface.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nBayesianRidgeRegressor = @load BayesianRidgeRegressor pkg=MLJScikitLearnInterface\n\nDo model = BayesianRidgeRegressor() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in BayesianRidgeRegressor(max_iter=...).","category":"section"},{"location":"models/BayesianRidgeRegressor_MLJScikitLearnInterface/#Hyper-parameters","page":"BayesianRidgeRegressor","title":"Hyper-parameters","text":"max_iter = 300\ntol = 0.001\nalpha_1 = 1.0e-6\nalpha_2 = 1.0e-6\nlambda_1 = 1.0e-6\nlambda_2 = 1.0e-6\ncompute_score = false\nfit_intercept = true\ncopy_X = true\nverbose = false","category":"section"},{"location":"models/RidgeCVClassifier_MLJScikitLearnInterface/#RidgeCVClassifier_MLJScikitLearnInterface","page":"RidgeCVClassifier","title":"RidgeCVClassifier","text":"RidgeCVClassifier\n\nA model type for constructing a ridge regression classifier with built-in cross-validation, based on MLJScikitLearnInterface.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nRidgeCVClassifier = @load RidgeCVClassifier pkg=MLJScikitLearnInterface\n\nDo model = RidgeCVClassifier() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in RidgeCVClassifier(alphas=...).","category":"section"},{"location":"models/RidgeCVClassifier_MLJScikitLearnInterface/#Hyper-parameters","page":"RidgeCVClassifier","title":"Hyper-parameters","text":"alphas = [0.1, 1.0, 10.0]\nfit_intercept = true\nscoring = nothing\ncv = 5\nclass_weight = nothing\nstore_cv_values = false","category":"section"},{"location":"models/ICA_MultivariateStats/#ICA_MultivariateStats","page":"ICA","title":"ICA","text":"ICA\n\nA model type for constructing a independent component analysis model, based on MultivariateStats.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nICA = @load ICA pkg=MultivariateStats\n\nDo model = ICA() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in ICA(outdim=...).\n\nIndependent component analysis is a computational technique for separating a multivariate signal into additive subcomponents, with the assumption that the subcomponents are non-Gaussian and independent from each other.","category":"section"},{"location":"models/ICA_MultivariateStats/#Training-data","page":"ICA","title":"Training data","text":"In MLJ or MLJBase, bind an instance model to data with\n\nmach = machine(model, X)\n\nHere:\n\nX is any table of input features (eg, a DataFrame) whose columns are of scitype Continuous; check column scitypes with schema(X).\n\nTrain the machine using fit!(mach, rows=...).","category":"section"},{"location":"models/ICA_MultivariateStats/#Hyper-parameters","page":"ICA","title":"Hyper-parameters","text":"outdim::Int=0: The number of independent components to recover, set automatically if 0.\nalg::Symbol=:fastica: The algorithm to use (only :fastica is supported at the moment).\nfun::Symbol=:tanh: The approximate neg-entropy function, one of :tanh, :gaus.\ndo_whiten::Bool=true: Whether or not to perform pre-whitening.\nmaxiter::Int=100: The maximum number of iterations.\ntol::Real=1e-6: The convergence tolerance for change in the unmixing matrix W.\nmean::Union{Nothing, Real, Vector{Float64}}=nothing: mean to use, if nothing (default) centering is computed and applied, if zero, no centering; otherwise a vector of means can be passed.\nwinit::Union{Nothing,Matrix{<:Real}}=nothing: Initial guess for the unmixing matrix W: either an empty matrix (for random initialization of W), a matrix of size m × k (if do_whiten is true), or a matrix of size m × k. Here m is the number of components (columns) of the input.","category":"section"},{"location":"models/ICA_MultivariateStats/#Operations","page":"ICA","title":"Operations","text":"transform(mach, Xnew): Return the component-separated version of input Xnew, which should have the same scitype as X above.","category":"section"},{"location":"models/ICA_MultivariateStats/#Fitted-parameters","page":"ICA","title":"Fitted parameters","text":"The fields of fitted_params(mach) are:\n\nprojection: The estimated component matrix.\nmean: The estimated mean vector.","category":"section"},{"location":"models/ICA_MultivariateStats/#Report","page":"ICA","title":"Report","text":"The fields of report(mach) are:\n\nindim: Dimension (number of columns) of the training data and new data to be transformed.\noutdim: Dimension of transformed data.\nmean: The mean of the untransformed training data, of length indim.","category":"section"},{"location":"models/ICA_MultivariateStats/#Examples","page":"ICA","title":"Examples","text":"using MLJ\n\nICA = @load ICA pkg=MultivariateStats\n\ntimes = range(0, 8, length=2000)\n\nsine_wave = sin.(2*times)\nsquare_wave = sign.(sin.(3*times))\nsawtooth_wave = map(t -> mod(2t, 2) - 1, times)\nsignals = hcat(sine_wave, square_wave, sawtooth_wave)\nnoisy_signals = signals + 0.2*randn(size(signals))\n\nmixing_matrix = [ 1 1 1; 0.5 2 1; 1.5 1 2]\nX = MLJ.table(noisy_signals*mixing_matrix)\n\nmodel = ICA(outdim = 3, tol=0.1)\nmach = machine(model, X) |> fit!\n\nX_unmixed = transform(mach, X)\n\nusing Plots\n\nplot(X.x2)\nplot(X.x2)\nplot(X.x3)\n\nplot(X_unmixed.x1)\nplot(X_unmixed.x2)\nplot(X_unmixed.x3)\n\n\nSee also PCA, KernelPCA, FactorAnalysis, PPCA","category":"section"},{"location":"models/LarsCVRegressor_MLJScikitLearnInterface/#LarsCVRegressor_MLJScikitLearnInterface","page":"LarsCVRegressor","title":"LarsCVRegressor","text":"LarsCVRegressor\n\nA model type for constructing a least angle regressor with built-in cross-validation, based on MLJScikitLearnInterface.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nLarsCVRegressor = @load LarsCVRegressor pkg=MLJScikitLearnInterface\n\nDo model = LarsCVRegressor() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in LarsCVRegressor(fit_intercept=...).","category":"section"},{"location":"models/LarsCVRegressor_MLJScikitLearnInterface/#Hyper-parameters","page":"LarsCVRegressor","title":"Hyper-parameters","text":"fit_intercept = true\nverbose = false\nmax_iter = 500\nprecompute = auto\ncv = 5\nmax_n_alphas = 1000\nn_jobs = nothing\neps = 2.220446049250313e-16\ncopy_X = true","category":"section"},{"location":"models/LogisticClassifier_MLJLinearModels/#LogisticClassifier_MLJLinearModels","page":"LogisticClassifier","title":"LogisticClassifier","text":"LogisticClassifier\n\nA model type for constructing a logistic classifier, based on MLJLinearModels.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nLogisticClassifier = @load LogisticClassifier pkg=MLJLinearModels\n\nDo model = LogisticClassifier() to construct an instance with default hyper-parameters.\n\nThis model is more commonly known as \"logistic regression\". It is a standard classifier for both binary and multiclass classification.  The objective function applies either a logistic loss (binary target) or multinomial (softmax) loss, and has a mixed L1/L2 penalty:\n\n$\n\nL(y, Xθ) + n⋅λ|θ|₂²/2 + n⋅γ|θ|₁ $\n\n.\n\nHere L is either MLJLinearModels.LogisticLoss or MLJLinearModels.MultiClassLoss, λ and γ indicate the strength of the L2 (resp. L1) regularization components and n is the number of training observations.\n\nWith scale_penalty_with_samples = false the objective function is instead\n\n$\n\nL(y, Xθ) + λ|θ|₂²/2 + γ|θ|₁ $\n\n.","category":"section"},{"location":"models/LogisticClassifier_MLJLinearModels/#Training-data","page":"LogisticClassifier","title":"Training data","text":"In MLJ or MLJBase, bind an instance model to data with\n\nmach = machine(model, X, y)\n\nwhere:\n\nX is any table of input features (eg, a DataFrame) whose columns have Continuous scitype; check column scitypes with schema(X)\ny is the target, which can be any AbstractVector whose element scitype is <:OrderedFactor or <:Multiclass; check the scitype with scitype(y)\n\nTrain the machine using fit!(mach, rows=...).","category":"section"},{"location":"models/LogisticClassifier_MLJLinearModels/#Hyperparameters","page":"LogisticClassifier","title":"Hyperparameters","text":"lambda::Real: strength of the regularizer if penalty is :l2 or :l1 and strength of the L2     regularizer if penalty is :en. Default: eps()\ngamma::Real: strength of the L1 regularizer if penalty is :en. Default: 0.0\npenalty::Union{String, Symbol}: the penalty to use, either :l2, :l1, :en (elastic net) or :none. Default: :l2\nfit_intercept::Bool: whether to fit the intercept or not. Default: true\npenalize_intercept::Bool: whether to penalize the intercept. Default: false\nscale_penalty_with_samples::Bool: whether to scale the penalty with the number of samples. Default: true\nsolver::Union{Nothing, MLJLinearModels.Solver}: some instance of MLJLinearModels.S where S is one of: LBFGS, Newton, NewtonCG, ProxGrad; but subject to the following restrictions:\nIf penalty = :l2, ProxGrad is disallowed. Otherwise, ProxGrad is the only option.\nUnless scitype(y) <: Finite{2} (binary target) Newton is disallowed.\nIf solver = nothing (default) then ProxGrad(accel=true) (FISTA) is used, unless gamma = 0, in which case LBFGS() is used.\nSolver aliases: FISTA(; kwargs...) = ProxGrad(accel=true, kwargs...), ISTA(; kwargs...) = ProxGrad(accel=false, kwargs...) Default: nothing","category":"section"},{"location":"models/LogisticClassifier_MLJLinearModels/#Example","page":"LogisticClassifier","title":"Example","text":"using MLJ\nX, y = make_blobs(centers = 2)\nmach = fit!(machine(LogisticClassifier(), X, y))\npredict(mach, X)\nfitted_params(mach)\n\nSee also MultinomialClassifier.","category":"section"},{"location":"models/BaggingRegressor_MLJScikitLearnInterface/#BaggingRegressor_MLJScikitLearnInterface","page":"BaggingRegressor","title":"BaggingRegressor","text":"BaggingRegressor\n\nA model type for constructing a bagging ensemble regressor, based on MLJScikitLearnInterface.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nBaggingRegressor = @load BaggingRegressor pkg=MLJScikitLearnInterface\n\nDo model = BaggingRegressor() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in BaggingRegressor(estimator=...).\n\nA Bagging regressor is an ensemble meta-estimator that fits base  regressors each on random subsets of the original dataset and then  aggregate their individual predictions (either by voting or by  averaging) to form a final prediction. Such a meta-estimator can  typically be used as a way to reduce the variance of a black-box  estimator (e.g., a decision tree), by introducing randomization  into its construction procedure and then making an ensemble out  of it.","category":"section"},{"location":"models/KNNClassifier_NearestNeighborModels/#KNNClassifier_NearestNeighborModels","page":"KNNClassifier","title":"KNNClassifier","text":"KNNClassifier\n\nA model type for constructing a K-nearest neighbor classifier, based on NearestNeighborModels.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nKNNClassifier = @load KNNClassifier pkg=NearestNeighborModels\n\nDo model = KNNClassifier() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in KNNClassifier(K=...).\n\nKNNClassifier implements K-Nearest Neighbors classifier  which is non-parametric algorithm that predicts a discrete class distribution associated  with a new point by taking a vote over the classes of the k-nearest points. Each neighbor  vote is assigned a weight based on proximity of the neighbor point to the test point  according to a specified distance metric.\n\nFor more information about the weighting kernels, see the paper by Geler et.al  Comparison of different weighting schemes for the kNN classifier on time-series data. ","category":"section"},{"location":"models/KNNClassifier_NearestNeighborModels/#Training-data","page":"KNNClassifier","title":"Training data","text":"In MLJ or MLJBase, bind an instance model to data with\n\nmach = machine(model, X, y)\n\nOR\n\nmach = machine(model, X, y, w)\n\nHere:\n\nX is any table of input features (eg, a DataFrame) whose columns are of scitype Continuous; check column scitypes with schema(X).\ny is the target, which can be any AbstractVector whose element scitype is <:Finite (<:Multiclass or <:OrderedFactor will do); check the scitype with scitype(y)\nw is the observation weights which can either be nothing (default) or an  AbstractVector whose element scitype is Count or Continuous. This is  different from weights kernel which is a model hyperparameter, see below.\n\nTrain the machine using fit!(mach, rows=...).","category":"section"},{"location":"models/KNNClassifier_NearestNeighborModels/#Hyper-parameters","page":"KNNClassifier","title":"Hyper-parameters","text":"K::Int=5 : number of neighbors\nalgorithm::Symbol = :kdtree : one of (:kdtree, :brutetree, :balltree)\nmetric::Metric = Euclidean() : any Metric from    Distances.jl for the    distance between points. For algorithm = :kdtree only metrics which are    instances of Distances.UnionMinkowskiMetric are supported.\nleafsize::Int = algorithm == 10 : determines the number of points    at which to stop splitting the tree. This option is ignored and always taken as 0    for algorithm = :brutetree, since brutetree isn't actually a tree.\nreorder::Bool = true : if true then points which are close in    distance are placed close in memory. In this case, a copy of the original data    will be made so that the original data is left unmodified. Setting this to true    can significantly improve performance of the specified algorithm    (except :brutetree). This option is ignored and always taken as false for    algorithm = :brutetree.\nweights::KNNKernel=Uniform() : kernel used in assigning weights to the    k-nearest neighbors for each observation. An instance of one of the types in    list_kernels(). User-defined weighting functions can be passed by wrapping the    function in a UserDefinedKernel kernel (do ?NearestNeighborModels.UserDefinedKernel for more    info). If observation weights w are passed during machine construction then the    weight assigned to each neighbor vote is the product of the kernel generated    weight for that neighbor and the corresponding observation weight.","category":"section"},{"location":"models/KNNClassifier_NearestNeighborModels/#Operations","page":"KNNClassifier","title":"Operations","text":"predict(mach, Xnew): Return predictions of the target given features Xnew, which should have same scitype as X above. Predictions are probabilistic but uncalibrated.\npredict_mode(mach, Xnew): Return the modes of the probabilistic predictions returned above.","category":"section"},{"location":"models/KNNClassifier_NearestNeighborModels/#Fitted-parameters","page":"KNNClassifier","title":"Fitted parameters","text":"The fields of fitted_params(mach) are:\n\ntree: An instance of either KDTree, BruteTree or BallTree depending on the  value of the algorithm hyperparameter (See hyper-parameters section above).  These are data structures that stores the training data with the view of making  quicker nearest neighbor searches on test data points.","category":"section"},{"location":"models/KNNClassifier_NearestNeighborModels/#Examples","page":"KNNClassifier","title":"Examples","text":"using MLJ\nKNNClassifier = @load KNNClassifier pkg=NearestNeighborModels\nX, y = @load_crabs; ## a table and a vector from the crabs dataset\n## view possible kernels\nNearestNeighborModels.list_kernels()\n## KNNClassifier instantiation\nmodel = KNNClassifier(weights = NearestNeighborModels.Inverse())\nmach = machine(model, X, y) |> fit! ## wrap model and required data in an MLJ machine and fit\ny_hat = predict(mach, X)\nlabels = predict_mode(mach, X)\n\n\nSee also MultitargetKNNClassifier","category":"section"},{"location":"models/KMedoids_Clustering/#KMedoids_Clustering","page":"KMedoids","title":"KMedoids","text":"KMedoids\n\nA model type for constructing a K-medoids clusterer, based on Clustering.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nKMedoids = @load KMedoids pkg=Clustering\n\nDo model = KMedoids() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in KMedoids(k=...).\n\nK-medoids is a clustering algorithm that works by finding k data points (called medoids) such that the total distance between each data point and the closest medoid is minimal.","category":"section"},{"location":"models/KMedoids_Clustering/#Training-data","page":"KMedoids","title":"Training data","text":"In MLJ or MLJBase, bind an instance model to data with\n\nmach = machine(model, X)\n\nHere:\n\nX is any table of input features (eg, a DataFrame) whose columns are of scitype Continuous; check column scitypes with schema(X)\n\nTrain the machine using fit!(mach, rows=...).","category":"section"},{"location":"models/KMedoids_Clustering/#Hyper-parameters","page":"KMedoids","title":"Hyper-parameters","text":"k=3: The number of centroids to use in clustering.\nmetric::SemiMetric=Distances.SqEuclidean: The metric used to calculate the clustering. Must have type PreMetric from Distances.jl.\ninit (defaults to :kmpp): how medoids should be initialized, could  be one of the following:\n:kmpp: KMeans++\n:kmenc: K-medoids initialization based on centrality\n:rand: random\nan instance of Clustering.SeedingAlgorithm from Clustering.jl\nan integer vector of length k that provides the indices of points to use as initial medoids.\nSee documentation of Clustering.jl.","category":"section"},{"location":"models/KMedoids_Clustering/#Operations","page":"KMedoids","title":"Operations","text":"predict(mach, Xnew): return cluster label assignments, given new  features Xnew having the same Scitype as X above.\ntransform(mach, Xnew): instead return the mean pairwise distances from  new samples to the cluster centers.","category":"section"},{"location":"models/KMedoids_Clustering/#Fitted-parameters","page":"KMedoids","title":"Fitted parameters","text":"The fields of fitted_params(mach) are:\n\nmedoids: The coordinates of the cluster medoids.","category":"section"},{"location":"models/KMedoids_Clustering/#Report","page":"KMedoids","title":"Report","text":"The fields of report(mach) are:\n\nassignments: The cluster assignments of each point in the training data.\ncluster_labels: The labels assigned to each cluster.","category":"section"},{"location":"models/KMedoids_Clustering/#Examples","page":"KMedoids","title":"Examples","text":"using MLJ\nKMedoids = @load KMedoids pkg=Clustering\n\ntable = load_iris()\ny, X = unpack(table, ==(:target), rng=123)\nmodel = KMedoids(k=3)\nmach = machine(model, X) |> fit!\n\nyhat = predict(mach, X)\n@assert yhat == report(mach).assignments\n\ncompare = zip(yhat, y) |> collect;\ncompare[1:8] ## clusters align with classes\n\ncenter_dists = transform(mach, fitted_params(mach).medoids')\n\n@assert center_dists[1][1] == 0.0\n@assert center_dists[2][2] == 0.0\n@assert center_dists[3][3] == 0.0\n\nSee also KMeans","category":"section"},{"location":"models/RandomWalkOversampler_Imbalance/#RandomWalkOversampler_Imbalance","page":"RandomWalkOversampler","title":"RandomWalkOversampler","text":"Initiate a RandomWalkOversampler model with the given hyper-parameters.\n\nRandomWalkOversampler\n\nA model type for constructing a random walk oversampler, based on Imbalance.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nRandomWalkOversampler = @load RandomWalkOversampler pkg=Imbalance\n\nDo model = RandomWalkOversampler() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in RandomWalkOversampler(ratios=...).\n\nRandomWalkOversampler implements the random walk oversampling algorithm to correct for class imbalance as in     Zhang, H., & Li, M. (2014). RWO-Sampling: A random walk over-sampling approach to imbalanced data classification.      Information Fusion, 25, 4-20.","category":"section"},{"location":"models/RandomWalkOversampler_Imbalance/#Training-data","page":"RandomWalkOversampler","title":"Training data","text":"In MLJ or MLJBase, wrap the model in a machine by\n\nmach = machine(model)\n\nThere is no need to provide any data here because the model is a static transformer.\n\nLikewise, there is no need to fit!(mach).\n\nFor default values of the hyper-parameters, model can be constructed by\n\nmodel = RandomWalkOversampler()","category":"section"},{"location":"models/RandomWalkOversampler_Imbalance/#Hyperparameters","page":"RandomWalkOversampler","title":"Hyperparameters","text":"ratios=1.0: A parameter that controls the amount of oversampling to be done for each class\nCan be a float and in this case each class will be oversampled to the size of the majority class times the float. By default, all classes are oversampled to the size of the majority class\nCan be a dictionary mapping each class label to the float ratio for that class\nrng::Union{AbstractRNG, Integer}=default_rng(): Either an AbstractRNG object or an Integer    seed to be used with Xoshiro if the Julia VERSION supports it. Otherwise, uses MersenneTwister`.","category":"section"},{"location":"models/RandomWalkOversampler_Imbalance/#Transform-Inputs","page":"RandomWalkOversampler","title":"Transform Inputs","text":"X: A table with element scitypes     that subtype Union{Finite, Infinite}. Elements in nominal columns should subtype Finite     (i.e., have scitype OrderedFactor or Multiclass) and\n\n elements in continuous columns should subtype `Infinite` (i.e., have \n [scitype](https://juliaai.github.io/ScientificTypes.jl/) `Count` or `Continuous`).\n\ny: An abstract vector of labels (e.g., strings) that correspond to the observations in X","category":"section"},{"location":"models/RandomWalkOversampler_Imbalance/#Transform-Outputs","page":"RandomWalkOversampler","title":"Transform Outputs","text":"Xover: A matrix or table that includes original data and the new observations    due to oversampling. depending on whether the input X is a matrix or table respectively\nyover: An abstract vector of labels corresponding to Xover","category":"section"},{"location":"models/RandomWalkOversampler_Imbalance/#Operations","page":"RandomWalkOversampler","title":"Operations","text":"transform(mach, X, y): resample the data X and y using RandomWalkOversampler, returning both the new and original observations","category":"section"},{"location":"models/RandomWalkOversampler_Imbalance/#Example","page":"RandomWalkOversampler","title":"Example","text":"using MLJ\nusing ScientificTypes\nimport Imbalance\n\n## set probability of each class\nclass_probs = [0.5, 0.2, 0.3]                         \nnum_rows = 100\nnum_continuous_feats = 3\n## want two categorical features with three and two possible values respectively\nnum_vals_per_category = [3, 2]\n\n## generate a table and categorical vector accordingly\nX, y = Imbalance.generate_imbalanced_data(num_rows, num_continuous_feats; \n                                          class_probs, num_vals_per_category, rng=42)                      \njulia> Imbalance.checkbalance(y)\n1: ▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 19 (39.6%) \n2: ▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 33 (68.8%) \n0: ▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 48 (100.0%) \n\n\njulia> ScientificTypes.schema(X).scitypes\n(Continuous, Continuous, Continuous, Continuous, Continuous)\n## coerce nominal columns to a finite scitype (multiclass or ordered factor)\nX = coerce(X, :Column4=>Multiclass, :Column5=>Multiclass)\n\n## load RandomWalkOversampler model type:\nRandomWalkOversampler = @load RandomWalkOversampler pkg=Imbalance\n\n## oversample the minority classes to  sizes relative to the majority class:\noversampler = RandomWalkOversampler(ratios = Dict(0=>1.0, 1=> 0.9, 2=>0.8), rng = 42)\nmach = machine(oversampler)\nXover, yover = transform(mach, X, y)\n\njulia> Imbalance.checkbalance(yover)\n2: ▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 38 (79.2%) \n1: ▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 43 (89.6%) \n0: ▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 48 (100.0%)","category":"section"},{"location":"models/EvoTreeRegressor_EvoTrees/#EvoTreeRegressor_EvoTrees","page":"EvoTreeRegressor","title":"EvoTreeRegressor","text":"EvoTreeRegressor(;kwargs...)\n\nA model type for constructing a EvoTreeRegressor, based on EvoTrees.jl, and implementing both an internal API and the MLJ model interface.","category":"section"},{"location":"models/EvoTreeRegressor_EvoTrees/#Hyper-parameters","page":"EvoTreeRegressor","title":"Hyper-parameters","text":"loss=:mse:         Loss to be be minimized during training. One of:\n:mse\n:mae\n:logloss\n:gamma\n:tweedie\n:quantile\n:cred_var: experimental credibility-based gains, derived from ratio of spread to process variance.\n:cred_std: experimental credibility-based gains, derived from ratio of spread to process std deviation.\nmetric:     The evaluation metric used to track evaluation data and serves as a basis for early stopping. Supported metrics are: \n:mse:     Mean-squared error. Adapted for general regression models.\n:rmse:    Root-mean-squared error. Adapted for general regression models.\n:mae:     Mean absolute error. Adapted for general regression models.\n:logloss: Adapted for :logistic regression models.\n:poisson: Poisson deviance. Adapted to EvoTreeCount count models.\n:gamma:   Gamma deviance. Adapted to regression problem on Gamma like, positively distributed targets.\n:tweedie: Tweedie deviance. Adapted to regression problem on Tweedie like, positively distributed targets with probability mass at y == 0.\n:quantile: The corresponds to an assymetric absolute error, where residuals are penalized according to alpha / (1-alpha) according to their sign.\n:gini: The normalized Gini between pred and target\nearly_stopping_rounds::Integer: number of consecutive rounds without metric improvement after which fitting in stopped.\nnrounds=100:           Number of rounds. It corresponds to the number of trees that will be sequentially stacked. Must be >= 1.\neta=0.1:              Learning rate. Each tree raw predictions are scaled by eta prior to be added to the stack of predictions. Must be > 0. A lower eta results in slower learning, requiring a higher nrounds but typically improves model performance.\nL2::T=0.0:            L2 regularization factor on aggregate gain. Must be >= 0. Higher L2 can result in a more robust model.\nlambda::T=0.0:        L2 regularization factor on individual gain. Must be >= 0. Higher lambda can result in a more robust model.\ngamma::T=0.0:         Minimum gain improvement needed to perform a node split. Higher gamma can result in a more robust model. Must be >= 0.\nalpha::T=0.5:         Loss specific parameter in the [0, 1] range:                           - :quantile: target quantile for the regression.\nmax_depth=6:          Maximum depth of a tree. Must be >= 1. A tree of depth 1 is made of a single prediction leaf. A complete tree of depth N contains 2^(N - 1) terminal leaves and 2^(N - 1) - 1 split nodes. Compute cost is proportional to 2^max_depth. Typical optimal values are in the 3 to 9 range.\nmin_weight=1.0:       Minimum weight needed in a node to perform a split. Matches the number of observations by default or the sum of weights as provided by the weights vector. Must be > 0.\nrowsample=1.0:        Proportion of rows that are sampled at each iteration to build the tree. Should be in ]0, 1].\ncolsample=1.0:        Proportion of columns / features that are sampled at each iteration to build the tree. Should be in ]0, 1].\nnbins=64:             Number of bins into which each feature is quantized. Buckets are defined based on quantiles, hence resulting in equal weight bins. Should be between 2 and 255.\nmonotone_constraints=Dict{Int, Int}(): Specify monotonic constraints using a dict where the key is the feature index and the value the applicable constraint (-1=decreasing, 0=none, 1=increasing).  Only :linear, :logistic, :gamma and tweedie losses are supported at the moment.\ntree_type=:binary    Tree structure to be used. One of:\n:binary:       Each node of a tree is grown independently. Tree are built depthwise until max depth is reach or if min weight or gain (see gamma) stops further node splits.\n:oblivious:    A common splitting condition is imposed to all nodes of a given depth.\nrng=123:              Either an integer used as a seed to the random number generator or an actual random number generator (::Random.AbstractRNG).\ndevice=:cpu: Hardware device to use for computations. Can be either :cpu or gpu.","category":"section"},{"location":"models/EvoTreeRegressor_EvoTrees/#Internal-API","page":"EvoTreeRegressor","title":"Internal API","text":"Do config = EvoTreeRegressor() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in EvoTreeRegressor(loss=...).","category":"section"},{"location":"models/EvoTreeRegressor_EvoTrees/#Training-model","page":"EvoTreeRegressor","title":"Training model","text":"A model is built using fit_evotree:\n\nmodel = fit_evotree(config; x_train, y_train, kwargs...)","category":"section"},{"location":"models/EvoTreeRegressor_EvoTrees/#Inference","page":"EvoTreeRegressor","title":"Inference","text":"Predictions are obtained using predict which returns a Vector of length nobs:\n\nEvoTrees.predict(model, X)\n\nAlternatively, models act as a functor, returning predictions when called as a function with features as argument:\n\nmodel(X)","category":"section"},{"location":"models/EvoTreeRegressor_EvoTrees/#MLJ-Interface","page":"EvoTreeRegressor","title":"MLJ Interface","text":"From MLJ, the type can be imported using:\n\nEvoTreeRegressor = @load EvoTreeRegressor pkg=EvoTrees\n\nDo model = EvoTreeRegressor() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in EvoTreeRegressor(loss=...).","category":"section"},{"location":"models/EvoTreeRegressor_EvoTrees/#Training-model-2","page":"EvoTreeRegressor","title":"Training model","text":"In MLJ or MLJBase, bind an instance model to data with     mach = machine(model, X, y) where\n\nX: any table of input features (eg, a DataFrame) whose columns each have one of the following element scitypes: Continuous, Count, or <:OrderedFactor; check column scitypes with schema(X)\ny: is the target, which can be any AbstractVector whose element scitype is <:Continuous; check the scitype with scitype(y)\n\nTrain the machine using fit!(mach, rows=...).","category":"section"},{"location":"models/EvoTreeRegressor_EvoTrees/#Operations","page":"EvoTreeRegressor","title":"Operations","text":"predict(mach, Xnew): return predictions of the target given features Xnew having the same scitype as X above. Predictions are deterministic.","category":"section"},{"location":"models/EvoTreeRegressor_EvoTrees/#Fitted-parameters","page":"EvoTreeRegressor","title":"Fitted parameters","text":"The fields of fitted_params(mach) are:\n\n:fitresult: The GBTree object returned by EvoTrees.jl fitting algorithm.","category":"section"},{"location":"models/EvoTreeRegressor_EvoTrees/#Report","page":"EvoTreeRegressor","title":"Report","text":"The fields of report(mach) are:\n\n:features: The names of the features encountered in training.","category":"section"},{"location":"models/EvoTreeRegressor_EvoTrees/#Examples","page":"EvoTreeRegressor","title":"Examples","text":"## Internal API\nusing EvoTrees\nconfig = EvoTreeRegressor(max_depth=5, nbins=32, nrounds=100)\nnobs, nfeats = 1_000, 5\nx_train, y_train = randn(nobs, nfeats), rand(nobs)\nmodel = fit_evotree(config; x_train, y_train)\npreds = EvoTrees.predict(model, x_train)\n\n## MLJ Interface\nusing MLJ\nEvoTreeRegressor = @load EvoTreeRegressor pkg=EvoTrees\nmodel = EvoTreeRegressor(max_depth=5, nbins=32, nrounds=100)\nX, y = @load_boston\nmach = machine(model, X, y) |> fit!\npreds = predict(mach, X)","category":"section"},{"location":"models/KNNDetector_OutlierDetectionNeighbors/#KNNDetector_OutlierDetectionNeighbors","page":"KNNDetector","title":"KNNDetector","text":"KNNDetector(k=5,\n            metric=Euclidean,\n            algorithm=:kdtree,\n            leafsize=10,\n            reorder=true,\n            reduction=:maximum)\n\nCalculate the anomaly score of an instance based on the distance to its k-nearest neighbors.","category":"section"},{"location":"models/KNNDetector_OutlierDetectionNeighbors/#Parameters","page":"KNNDetector","title":"Parameters","text":"k::Integer\n\nNumber of neighbors (must be greater than 0).\n\nmetric::Metric\n\nThis is one of the Metric types defined in the Distances.jl package. It is possible to define your own metrics by creating new types that are subtypes of Metric.\n\nalgorithm::Symbol\n\nOne of (:kdtree, :balltree). In a kdtree, points are recursively split into groups using hyper-planes. Therefore a KDTree only works with axis aligned metrics which are: Euclidean, Chebyshev, Minkowski and Cityblock. A brutetree linearly searches all points in a brute force fashion and works with any Metric. A balltree recursively splits points into groups bounded by hyper-spheres and works with any Metric.\n\nstatic::Union{Bool, Symbol}\n\nOne of (true, false, :auto). Whether the input data for fitting and transform should be statically or dynamically allocated. If true, the data is statically allocated. If false, the data is dynamically allocated. If :auto, the data is dynamically allocated if the product of all dimensions except the last is greater than 100.\n\nleafsize::Int\n\nDetermines at what number of points to stop splitting the tree further. There is a trade-off between traversing the tree and having to evaluate the metric function for increasing number of points.\n\nreorder::Bool\n\nWhile building the tree this will put points close in distance close in memory since this helps with cache locality. In this case, a copy of the original data will be made so that the original data is left unmodified. This can have a significant impact on performance and is by default set to true.\n\nparallel::Bool\n\nParallelize score and predict using all threads available. The number of threads can be set with the JULIA_NUM_THREADS environment variable. Note: fit is not parallel.\n\nreduction::Symbol\n\nOne of (:maximum, :median, :mean). (reduction=:maximum) was proposed by [1]. Angiulli et al. [2] proposed sum to reduce the distances, but mean has been implemented for numerical stability.","category":"section"},{"location":"models/KNNDetector_OutlierDetectionNeighbors/#Examples","page":"KNNDetector","title":"Examples","text":"using OutlierDetection: KNNDetector, fit, transform\ndetector = KNNDetector()\nX = rand(10, 100)\nmodel, result = fit(detector, X; verbosity=0)\ntest_scores = transform(detector, model, X)","category":"section"},{"location":"models/KNNDetector_OutlierDetectionNeighbors/#References","page":"KNNDetector","title":"References","text":"[1] Ramaswamy, Sridhar; Rastogi, Rajeev; Shim, Kyuseok (2000): Efficient Algorithms for Mining Outliers from Large Data Sets.\n\n[2] Angiulli, Fabrizio; Pizzuti, Clara (2002): Fast Outlier Detection in High Dimensional Spaces.","category":"section"},{"location":"models/CardinalityReducer_MLJTransforms/#CardinalityReducer_MLJTransforms","page":"CardinalityReducer","title":"CardinalityReducer","text":"CardinalityReducer\n\nA model type for constructing a cardinality reducer, based on MLJTransforms.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nCardinalityReducer = @load CardinalityReducer pkg=MLJTransforms\n\nDo model = CardinalityReducer() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in CardinalityReducer(features=...).\n\nCardinalityReducer maps any level of a categorical feature that occurs with frequency < min_frequency into a new level (e.g., \"Other\"). This is useful when some categorical features have high cardinality and many levels are infrequent. This assumes that the categorical features have raw types that are in Union{AbstractString, Char, Number}.","category":"section"},{"location":"models/CardinalityReducer_MLJTransforms/#Training-data","page":"CardinalityReducer","title":"Training data","text":"In MLJ (or MLJBase) bind an instance unsupervised model to data with\n\nmach = machine(model, X)\n\nHere:\n\nX is any table of input features (eg, a DataFrame). Features to be transformed must  have element scitype Multiclass or OrderedFactor. Use schema(X) to  check scitypes.\n\nTrain the machine using fit!(mach, rows=...).","category":"section"},{"location":"models/CardinalityReducer_MLJTransforms/#Hyper-parameters","page":"CardinalityReducer","title":"Hyper-parameters","text":"features=[]: A list of names of categorical features given as symbols to exclude or in clude from encoding, according to the value of ignore, or a single symbol (which is treated as a vector with one symbol), or a callable that returns true for features to be included/excluded.\nignore=true: Whether to exclude or include the features given in features\nordered_factor=false: Whether to encode OrderedFactor or ignore them\nmin_frequency::Real=3: Any level of a categorical feature that occurs with frequency < min_frequency will be mapped to a new level. Could be an integer or a float which decides whether raw counts or normalized frequencies are used.\nlabel_for_infrequent::Dict{<:Type, <:Any}()= Dict( AbstractString => \"Other\", Char => 'O', ): A dictionary where the possible values for keys are the types in Char, AbstractString, and Number and each value signifies the new level to map into given a column raw super type. By default, if the raw type of the column subtypes AbstractString then the new value is \"Other\" and if the raw type subtypes Char then the new value is 'O' and if the raw type subtypes Number then the new value is the lowest value in the column - 1.","category":"section"},{"location":"models/CardinalityReducer_MLJTransforms/#Operations","page":"CardinalityReducer","title":"Operations","text":"transform(mach, Xnew): Apply cardinality reduction to selected Multiclass or OrderedFactor features of Xnew specified by hyper-parameters, and return the new table.  Features that are neither Multiclass nor OrderedFactor are always left unchanged.","category":"section"},{"location":"models/CardinalityReducer_MLJTransforms/#Fitted-parameters","page":"CardinalityReducer","title":"Fitted parameters","text":"The fields of fitted_params(mach) are:\n\nnew_cat_given_col_val: A dictionary that maps each level in a categorical feature to a  new level (either itself or the new level specified in label_for_infrequent)","category":"section"},{"location":"models/CardinalityReducer_MLJTransforms/#Report","page":"CardinalityReducer","title":"Report","text":"The fields of report(mach) are:\n\nencoded_features: The subset of the categorical features of X that were encoded","category":"section"},{"location":"models/CardinalityReducer_MLJTransforms/#Examples","page":"CardinalityReducer","title":"Examples","text":"import StatsBase.proportionmap\nusing MLJ\n\n## Define categorical features\nA = [ [\"a\" for i in 1:100]..., \"b\", \"b\", \"b\", \"c\", \"d\"]\nB = [ [0 for i in 1:100]..., 1, 2, 3, 4, 4]\n\n## Combine into a named tuple\nX = (A = A, B = B)\n\n## Coerce A, C, D to multiclass and B to continuous and E to ordinal\nX = coerce(X,\n:A => Multiclass,\n:B => Multiclass\n)\n\nencoder = CardinalityReducer(ordered_factor = false, min_frequency=3)\nmach = fit!(machine(encoder, X))\nXnew = transform(mach, X)\n\njulia> proportionmap(Xnew.A)\nDict{CategoricalArrays.CategoricalValue{String, UInt32}, Float64} with 3 entries:\n  \"Other\" => 0.0190476\n  \"b\"     => 0.0285714\n  \"a\"     => 0.952381\n\njulia> proportionmap(Xnew.B)\nDict{CategoricalArrays.CategoricalValue{Int64, UInt32}, Float64} with 2 entries:\n  0  => 0.952381\n  -1 => 0.047619\n\nSee also FrequencyEncoder","category":"section"},{"location":"models/RANSACRegressor_MLJScikitLearnInterface/#RANSACRegressor_MLJScikitLearnInterface","page":"RANSACRegressor","title":"RANSACRegressor","text":"RANSACRegressor\n\nA model type for constructing a ransac regressor, based on MLJScikitLearnInterface.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nRANSACRegressor = @load RANSACRegressor pkg=MLJScikitLearnInterface\n\nDo model = RANSACRegressor() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in RANSACRegressor(estimator=...).","category":"section"},{"location":"models/RANSACRegressor_MLJScikitLearnInterface/#Hyper-parameters","page":"RANSACRegressor","title":"Hyper-parameters","text":"estimator = nothing\nmin_samples = 5\nresidual_threshold = nothing\nis_data_valid = nothing\nis_model_valid = nothing\nmax_trials = 100\nmax_skips = 9223372036854775807\nstop_n_inliers = 9223372036854775807\nstop_score = Inf\nstop_probability = 0.99\nloss = absolute_error\nrandom_state = nothing","category":"section"},{"location":"models/NuSVR_LIBSVM/#NuSVR_LIBSVM","page":"NuSVR","title":"NuSVR","text":"NuSVR\n\nA model type for constructing a ν-support vector regressor, based on LIBSVM.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nNuSVR = @load NuSVR pkg=LIBSVM\n\nDo model = NuSVR() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in NuSVR(kernel=...).\n\nReference for algorithm and core C-library: C.-C. Chang and C.-J. Lin (2011): \"LIBSVM: a library for support vector machines.\" ACM Transactions on Intelligent Systems and Technology, 2(3):27:1–27:27. Updated at https://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf. \n\nThis model is a re-parameterization of EpsilonSVR in which the epsilon hyper-parameter is replaced with a new parameter nu (denoted ν in the cited reference) which attempts to control the number of support vectors directly.","category":"section"},{"location":"models/NuSVR_LIBSVM/#Training-data","page":"NuSVR","title":"Training data","text":"In MLJ or MLJBase, bind an instance model to data with:\n\nmach = machine(model, X, y)\n\nwhere\n\nX: any table of input features (eg, a DataFrame) whose columns each have Continuous element scitype; check column scitypes with schema(X)\ny: is the target, which can be any AbstractVector whose element scitype is Continuous; check the scitype with scitype(y)\n\nTrain the machine using fit!(mach, rows=...).","category":"section"},{"location":"models/NuSVR_LIBSVM/#Hyper-parameters","page":"NuSVR","title":"Hyper-parameters","text":"kernel=LIBSVM.Kernel.RadialBasis: either an object that can be\ncalled, as in kernel(x1, x2), or one of the built-in kernels from the LIBSVM.jl package listed below.  Here x1 and x2 are vectors whose lengths match the number of columns of the training data X (see \"Examples\" below).\nLIBSVM.Kernel.Linear: (x1, x2) -> x1'*x2\nLIBSVM.Kernel.Polynomial: (x1, x2) -> gamma*x1'*x2 + coef0)^degree\nLIBSVM.Kernel.RadialBasis: (x1, x2) -> (exp(-gamma*norm(x1 - x2)^2))\nLIBSVM.Kernel.Sigmoid: (x1, x2) - > tanh(gamma*x1'*x2 + coef0)\nHere gamma, coef0, degree are other hyper-parameters. Serialization of models with user-defined kernels comes with some restrictions. See LIVSVM.jl issue91\ngamma = 0.0: kernel parameter (see above); if gamma==-1.0 then gamma = 1/nfeatures is used in training, where nfeatures is the number of features (columns of X).  If gamma==0.0 then gamma = 1/(var(Tables.matrix(X))*nfeatures) is used. Actual value used appears in the report (see below).\ncoef0 = 0.0: kernel parameter (see above)\ndegree::Int32 = Int32(3): degree in polynomial kernel (see above)\ncost=1.0 (range (0, Inf)): the parameter denoted C in the cited reference; for greater regularization, decrease cost\nnu=0.5 (range (0, 1]): An upper bound on the fraction of training errors and a lower bound of the fraction of support vectors. Denoted ν in the cited paper. Changing nu changes the thickness of some neighborhood of the graph of the prediction function (\"tube\" or \"slab\") and a training error is said to occur when a data point (x, y) lies outside of that neighborhood.\ncachesize=200.0 cache memory size in MB\ntolerance=0.001: tolerance for the stopping criterion\nshrinking=true: whether to use shrinking heuristics","category":"section"},{"location":"models/NuSVR_LIBSVM/#Operations","page":"NuSVR","title":"Operations","text":"predict(mach, Xnew): return predictions of the target given features Xnew having the same scitype as X above.","category":"section"},{"location":"models/NuSVR_LIBSVM/#Fitted-parameters","page":"NuSVR","title":"Fitted parameters","text":"The fields of fitted_params(mach) are:\n\nlibsvm_model: the trained model object created by the LIBSVM.jl package","category":"section"},{"location":"models/NuSVR_LIBSVM/#Report","page":"NuSVR","title":"Report","text":"The fields of report(mach) are:\n\ngamma: actual value of the kernel parameter gamma used in training","category":"section"},{"location":"models/NuSVR_LIBSVM/#Examples","page":"NuSVR","title":"Examples","text":"","category":"section"},{"location":"models/NuSVR_LIBSVM/#Using-a-built-in-kernel","page":"NuSVR","title":"Using a built-in kernel","text":"using MLJ\nimport LIBSVM\n\nNuSVR = @load NuSVR pkg=LIBSVM                 ## model type\nmodel = NuSVR(kernel=LIBSVM.Kernel.Polynomial) ## instance\n\nX, y = make_regression(rng=123) ## table, vector\nmach = machine(model, X, y) |> fit!\n\nXnew, _ = make_regression(3, rng=123)\n\njulia> yhat = predict(mach, Xnew)\n3-element Vector{Float64}:\n  0.2008156459920009\n  0.1131520519131709\n -0.2076156254934889","category":"section"},{"location":"models/NuSVR_LIBSVM/#User-defined-kernels","page":"NuSVR","title":"User-defined kernels","text":"k(x1, x2) = x1'*x2 ## equivalent to `LIBSVM.Kernel.Linear`\nmodel = NuSVR(kernel=k)\nmach = machine(model, X, y) |> fit!\n\njulia> yhat = predict(mach, Xnew)\n3-element Vector{Float64}:\n  1.1211558175964662\n  0.06677125944808422\n -0.6817578942749346\n\nSee also EpsilonSVR, LIVSVM.jl and the original C implementation documentation.","category":"section"},{"location":"models/CatBoostClassifier_CatBoost/#CatBoostClassifier_CatBoost","page":"CatBoostClassifier","title":"CatBoostClassifier","text":"CatBoostClassifier\n\nA model type for constructing a CatBoost classifier, based on CatBoost.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nCatBoostClassifier = @load CatBoostClassifier pkg=CatBoost\n\nDo model = CatBoostClassifier() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in CatBoostClassifier(iterations=...).","category":"section"},{"location":"models/CatBoostClassifier_CatBoost/#Training-data","page":"CatBoostClassifier","title":"Training data","text":"In MLJ or MLJBase, bind an instance model to data with\n\nmach = machine(model, X, y)\n\nwhere\n\nX: any table of input features (eg, a DataFrame) whose columns each have one of the following element scitypes: Continuous, Count, Finite, Textual; check column scitypes with schema(X). Textual columns will be passed to catboost as text_features, Multiclass columns will be passed to catboost as cat_features, and OrderedFactor columns will be converted to integers.\ny: the target, which can be any AbstractVector whose element scitype is Finite; check the scitype with scitype(y)\n\nTrain the machine with fit!(mach, rows=...).","category":"section"},{"location":"models/CatBoostClassifier_CatBoost/#Hyper-parameters","page":"CatBoostClassifier","title":"Hyper-parameters","text":"More details on the catboost hyperparameters, here are the Python docs:  https://catboost.ai/en/docs/concepts/python-reference_catboostclassifier#parameters","category":"section"},{"location":"models/CatBoostClassifier_CatBoost/#Operations","page":"CatBoostClassifier","title":"Operations","text":"predict(mach, Xnew): probabilistic predictions of the target given new features Xnew having the same scitype as X above.\npredict_mode(mach, Xnew): returns the mode of each of the prediction above.","category":"section"},{"location":"models/CatBoostClassifier_CatBoost/#Accessor-functions","page":"CatBoostClassifier","title":"Accessor functions","text":"feature_importances(mach): return vector of feature importances, in the form of   feature::Symbol => importance::Real pairs","category":"section"},{"location":"models/CatBoostClassifier_CatBoost/#Fitted-parameters","page":"CatBoostClassifier","title":"Fitted parameters","text":"The fields of fitted_params(mach) are:\n\nmodel: The Python CatBoostClassifier model","category":"section"},{"location":"models/CatBoostClassifier_CatBoost/#Report","page":"CatBoostClassifier","title":"Report","text":"The fields of report(mach) are:\n\nfeature_importances: Vector{Pair{Symbol, Float64}} of feature importances","category":"section"},{"location":"models/CatBoostClassifier_CatBoost/#Examples","page":"CatBoostClassifier","title":"Examples","text":"using CatBoost.MLJCatBoostInterface\nusing MLJ\n\nX = (\n    duration = [1.5, 4.1, 5.0, 6.7], \n    n_phone_calls = [4, 5, 6, 7], \n    department = coerce([\"acc\", \"ops\", \"acc\", \"ops\"], Multiclass), \n)\ny = coerce([0, 0, 1, 1], Multiclass)\n\nmodel = CatBoostClassifier(iterations=5)\nmach = machine(model, X, y)\nfit!(mach)\nprobs = predict(mach, X)\npreds = predict_mode(mach, X)\n\nSee also catboost and the unwrapped model type CatBoost.CatBoostClassifier.","category":"section"},{"location":"getting_started/#Getting-Started","page":"Getting Started","title":"Getting Started","text":"For an outline of MLJ's goals and features, see About MLJ.\n\nThis page introduces some MLJ basics, assuming some familiarity with machine learning. For a complete list of other MLJ learning resources, see Learning MLJ.\n\nMLJ collects together the functionality provided by mutliple packages. To learn how to install components separately, run using MLJ; @doc MLJ.\n\nThis section introduces only the most basic MLJ operations and concepts. It assumes MLJ has been successfully installed. See Installation if this is not the case.","category":"section"},{"location":"getting_started/#Choosing-and-evaluating-a-model","page":"Getting Started","title":"Choosing and evaluating a model","text":"The following code loads Fisher's famous iris data set as a named tuple of column vectors:\n\nusing MLJ\niris = load_iris();\nselectrows(iris, 1:3) |> pretty\nschema(iris)\n\nBecause this data format is compatible with Tables.jl (and satisfies Tables.istable(iris) == true) many MLJ methods (such as selectrows, pretty and schema used above) as well as many MLJ models can work with it. However, as most new users are already familiar with the access methods particular to DataFrames (also compatible with Tables.jl) we'll put our data into that format here:\n\nimport DataFrames\niris = DataFrames.DataFrame(iris);\nnothing # hide\n\nNext, let's split the data \"horizontally\" into input and target parts, and specify an RNG seed, to force observations to be shuffled:\n\ny, X = unpack(iris, ==(:target); rng=123);\nfirst(X, 3) |> pretty\n\nThis call to unpack splits off any column with name == to :target into something called y, and all the remaining columns into X.\n\nTo list all models available in MLJ's model registry do models(). Listing the models compatible with the present data:\n\nmodels(matching(X,y))\n\nIn MLJ a model is a struct storing the hyperparameters of the learning algorithm indicated by the struct name (and nothing else). For common problems matching data to models, see Model Search and Preparing Data.\n\nTo see the documentation for DecisionTreeClassifier (without loading its defining code) do\n\ndoc(\"DecisionTreeClassifier\", pkg=\"DecisionTree\")\n\nAssuming the MLJDecisionTreeInterface.jl package is in your load path (see Installation) we can use @load to import the DecisionTreeClassifier model type, which we will bind to Tree:\n\nTree = @load DecisionTreeClassifier pkg=DecisionTree\n\n(In this case, we need to specify pkg=... because multiple packages provide a model type with the name DecisionTreeClassifier.) Now we can instantiate a model with default hyperparameters:\n\ntree = Tree()\n\nImportant: DecisionTree.jl and most other packages implementing machine learning algorithms for use in MLJ are not MLJ dependencies. If such a package is not in your load path you will receive an error explaining how to add the package to your current environment. Alternatively, you can use the interactive macro @iload. For more on importing model types, see Loading Model Code.\n\nOnce instantiated, a model's performance can be evaluated with the evaluate method. Our classifier is a probabilistic predictor (check prediction_type(tree) == :probabilistic) which means we can specify a probabilistic measure (metric) like log_loss, as well deterministic measures like accuracy (which are applied after computing the mode of each prediction):\n\nevaluate(tree, X, y,\n         resampling=CV(shuffle=true),\n         measures=[log_loss, accuracy],\n         verbosity=0)\n\nUnder the hood, evaluate calls lower level functions predict or predict_mode according to the type of measure, as shown in the output. We shall call these operations directly below.\n\nFor more on performance evaluation, see Evaluating Model Performance for details.","category":"section"},{"location":"getting_started/#A-preview-of-data-type-specification-in-MLJ","page":"Getting Started","title":"A preview of data type specification in MLJ","text":"The target y above is a categorical vector, which is appropriate because our model is a decision tree classifier:\n\ntypeof(y)\n\nHowever, MLJ models do not prescribe the machine types for the data they operate on. Rather, they specify a scientific type, which refers to the way data is to be interpreted, as opposed to how it is encoded:\n\ntarget_scitype(tree)\n\nHere Finite is an example of a \"scalar\" scientific type with two subtypes:\n\nsubtypes(Finite)\n\nWe use the scitype function to check how MLJ is going to interpret given data. Our choice of encoding for y works for DecisionTreeClassifier, because we have:\n\nscitype(y)\n\nand Multiclass{3} <: Finite. If we would encode with integers instead, we obtain:\n\nyint = int.(y);\nscitype(yint)\n\nand using yint in place of y in classification problems will fail. See also Working with Categorical Data.\n\nFor more on scientific types, see Data containers and scientific types below.","category":"section"},{"location":"getting_started/#Fit-and-predict","page":"Getting Started","title":"Fit and predict","text":"To illustrate MLJ's fit and predict interface, let's perform our performance evaluations by hand, but using a simple holdout set, instead of cross-validation.\n\nWrapping the model in data creates a machine which will store training outcomes:\n\nmach = machine(tree, X, y)\n\nTraining and testing on a hold-out set:\n\ntrain, test = partition(eachindex(y), 0.7); # 70:30 split\nfit!(mach, rows=train);\nyhat = predict(mach, X[test,:]);\nyhat[3:5]\nlog_loss(yhat, y[test])\n\nNote that log_loss and cross_entropy are aliases for LogLoss() (which can be passed an optional keyword parameter, as in LogLoss(tol=0.001)). For a list of all losses and scores, and their aliases, run measures().\n\nNotice that yhat is a vector of Distribution objects, because DecisionTreeClassifier makes probabilistic predictions. The methods of the Distributions.jl package can be applied to such distributions:\n\nbroadcast(pdf, yhat[3:5], \"virginica\") # predicted probabilities of virginica\nbroadcast(pdf, yhat, y[test])[3:5] # predicted probability of observed class\nmode.(yhat[3:5])\n\nOr, one can explicitly get modes by using predict_mode instead of predict:\n\npredict_mode(mach, X[test[3:5],:])\n\nFinally, we note that pdf() is overloaded to allow the retrieval of probabilities for all levels at once:\n\nL = levels(y)\npdf(yhat[3:5], L)\n\nUnsupervised models have a transform method instead of predict, and may optionally implement an inverse_transform method:\n\nv = Float64[1, 2, 3, 4]\nstand = Standardizer() # this type is built-in\nmach2 = machine(stand, v)\nfit!(mach2)\nw = transform(mach2, v)\ninverse_transform(mach2, w)\n\nMachines have an internal state which allows them to avoid redundant calculations when retrained, in certain conditions - for example when increasing the number of trees in a random forest, or the number of epochs in a neural network. The machine-building syntax also anticipates a more general syntax for composing multiple models, an advanced feature explained in Learning Networks.\n\nThere is a version of evaluate for machines as well as models. This time we'll use a simple holdout strategy as above. (An exclamation point is added to the method name because machines are generally mutated when trained.)\n\nevaluate!(mach, resampling=Holdout(fraction_train=0.7),\n                measures=[log_loss, accuracy],\n                verbosity=0)\n\nChanging a hyperparameter and re-evaluating:\n\ntree.max_depth = 3;\nevaluate!(mach, resampling=Holdout(fraction_train=0.7),\n          measures=[log_loss, accuracy],\n          verbosity=0)","category":"section"},{"location":"getting_started/#Next-steps","page":"Getting Started","title":"Next steps","text":"For next steps, consult the Learning MLJ section. At the least, we recommned you read the remainder of this page before considering serious use of MLJ.","category":"section"},{"location":"getting_started/#Data-containers-and-scientific-types","page":"Getting Started","title":"Data containers and scientific types","text":"The MLJ user should acquaint themselves with some basic assumptions about the form of data expected by MLJ, as outlined below. The basic machine constructors look like this (see also Constructing machines):\n\nmachine(model::Unsupervised, X)\nmachine(model::Supervised, X, y)\n\nEach supervised model in MLJ declares the permitted scientific type of the inputs X and targets y that can be bound to it in the first constructor above, rather than specifying specific machine types (such as Array{Float32, 2}). Similar remarks apply to the input X of an unsupervised model.\n\nScientific types are julia types defined in the package ScientificTypesBase.jl; the package ScientificTypes.jl implements the particular convention used in the MLJ universe for assigning a specific scientific type (interpretation) to each julia object (see the scitype examples below).\n\nThe basic \"scalar\" scientific types are Continuous, Multiclass{N}, OrderedFactor{N}, Count and Textual. Missing and Nothing are also considered scientific types. Be sure you read Scalar scientific types below to guarantee your scalar data is interpreted correctly. Tools exist to coerce the data to have the appropriate scientific type; see ScientificTypes.jl for full details.\n\nAdditionally, most data containers - such as tuples, vectors, matrices and tables - have a scientific type parameterized by scitype of the elements they contain.\n\n(Image: )\n\nFigure 1. Part of the scientific type hierarchy in ScientificTypesBase.jl.\n\nscitype(4.6)\nscitype(42)\nx1 = coerce([\"yes\", \"no\", \"yes\", \"maybe\"], Multiclass);\nscitype(x1)\nX = (x1=x1, x2=rand(4), x3=rand(4))  # a \"column table\"\nscitype(X)","category":"section"},{"location":"getting_started/#Two-dimensional-data","page":"Getting Started","title":"Two-dimensional data","text":"Generally, two-dimensional data in MLJ is expected to be tabular.  All data containers X compatible with the Tables.jl interface and sastisfying Tables.istable(X) == true (most of the formats in this list) have the scientific type Table{K}, where K depends on the scientific types of the columns, which can be individually inspected using schema:\n\nschema(X)","category":"section"},{"location":"getting_started/#Matrix-data","page":"Getting Started","title":"Matrix data","text":"MLJ models expecting a table do not generally accept a matrix instead. However, a matrix can be wrapped as a table, using MLJ.table (or Tables.table from the Tables.jl package):\n\nmatrix_table = MLJ.table(rand(2,3));\nschema(matrix_table)\n\nThe matrix is not copied, only wrapped.  To manifest a table as a matrix, use MLJ.matrix (or Tables.matrix).","category":"section"},{"location":"getting_started/#Observations-correspond-to-rows,-not-columns","page":"Getting Started","title":"Observations correspond to rows, not columns","text":"When supplying models with matrices, or wrapping them in tables, each row should correspond to a different observation. That is, the matrix should be n x p, where n is the number of observations and p the number of features. However, some models may perform better if supplied the adjoint of a p x n matrix instead, and observation resampling is always more efficient in this case.","category":"section"},{"location":"getting_started/#Inputs","page":"Getting Started","title":"Inputs","text":"Since an MLJ model only specifies the scientific type of data, if that type is Table - which is the case for the majority of MLJ models - then any Tables.jl container X is permitted, so long as Tables.istable(X) == true.\n\nSpecifically, the requirement for an arbitrary model's input is scitype(X) <: input_scitype(model).","category":"section"},{"location":"getting_started/#Targets","page":"Getting Started","title":"Targets","text":"The target y expected by MLJ models is generally an AbstractVector. A multivariate target y will generally be a table.\n\nSpecifically, the type requirement for a model target is scitype(y) <: target_scitype(model).","category":"section"},{"location":"getting_started/#Querying-a-model-for-acceptable-data-types","page":"Getting Started","title":"Querying a model for acceptable data types","text":"Given a model instance, one can inspect the admissible scientific types of its input and target, and without loading the code defining the model;\n\ni = info(\"DecisionTreeClassifier\", pkg=\"DecisionTree\")\ni.input_scitype\ni.target_scitype\n\nThis output indicates that any table with Continuous, Count or OrderedFactor columns is acceptable as the input X, and that any vector with element scitype <: Finite is acceptable as the target y.\n\nFor more on matching models to data, see Model Search.","category":"section"},{"location":"getting_started/#Scalar-scientific-types","page":"Getting Started","title":"Scalar scientific types","text":"Models in MLJ will always apply the MLJ convention described in ScientificTypes.jl to decide how to interpret the elements of your container types. Here are the key features of that convention:\n\nAny AbstractFloat is interpreted as Continuous.\nAny Integer is interpreted as Count.\nAny CategoricalValue x, is interpreted as Multiclass or OrderedFactor, depending on the value of isordered(x).\nStrings and Chars are not interpreted as Multiclass or OrderedFactor (they have scitypes Textual and Unknown respectively).\nIn particular, integers (including Bools) cannot be used to represent categorical data. Use the preceding coerce operations to coerce to a Finite scitype.\nThe scientific types of nothing and missing are Nothing and Missing, native types we also regard as scientific.\n\nUse coerce(v, OrderedFactor) or coerce(v, Multiclass) to coerce a vector v of integers, strings or characters to a vector with an appropriate Finite (categorical) scitype.  See also Working with Categorical Data, and the ScientificTypes.jl documentation.","category":"section"},{"location":"getting_started/#Reference","page":"Getting Started","title":"Reference","text":"","category":"section"},{"location":"getting_started/#MLJModelInterface.table","page":"Getting Started","title":"MLJModelInterface.table","text":"table(columntable; prototype=nothing)\n\nConvert a named tuple of vectors or tuples columntable, into a table of the \"preferred sink type\" of prototype. This is often the type of prototype itself, when prototype is a sink; see the Tables.jl documentation. If prototype is not specified, then a named tuple of vectors is returned.\n\ntable(A::AbstractMatrix; names=nothing, prototype=nothing)\n\nWrap an abstract matrix A as a Tables.jl compatible table with the specified column names (a tuple of symbols). If names are not specified, names=(:x1, :x2, ..., :xn) is used, where n=size(A, 2).\n\nIf a prototype is specified, then the matrix is materialized as a table of the preferred sink type of prototype, rather than wrapped. Note that if prototype is not specified, then matrix(table(A)) is essentially a no-op.\n\n\n\n\n\n","category":"function"},{"location":"getting_started/#MLJModelInterface.matrix","page":"Getting Started","title":"MLJModelInterface.matrix","text":"matrix(X; transpose=false)\n\nIf X isa AbstractMatrix, return X or permutedims(X) if transpose=true. Otherwise if X is a Tables.jl compatible table source, convert X into a Matrix.\n\n\n\n\n\n","category":"function"},{"location":"getting_started/#ScientificTypes.scitype","page":"Getting Started","title":"ScientificTypes.scitype","text":"scitype(X)\n\nThe scientific type (interpretation) of X, as distinct from its machine type. Atomic scientific types (Continuous, Multiclass, etc) are mostly abstract types defined in the package ScientificTypesBase.jl. Scientific types do not ordinarily have instances.\n\nExamples\n\njulia> scitype(3.14)\nContinuous\n\njulia> scitype([1, 2, missing])\nAbstractVector{Union{Missing, Count}}\n\njulia> scitype((5, \"beige\"))\nTuple{Count, Textual}\n\njulia> using CategoricalArrays\n\njulia> table = (gender = categorical(['M', 'M', 'F', 'M', 'F']),\n     ndevices = [1, 3, 2, 3, 2])\n\njulia> scitype(table)\nTable{Union{AbstractVector{Count}, AbstractVector{Multiclass{2}}}}\n\n\nColumn scitpes of a table can also be inspected with schema.\n\nThe behavior of scitype is detailed in the ScientificTypes documentation. Key features of the default behavior are:\n\nAbstractFloat has scitype as Continuous <: Infinite.\nAny Integer has scitype as Count <: Infinite.\nAny CategoricalValue x has scitype as Multiclass <: Finite or OrderedFactor <: Finite, depending on the value of isordered(x).\nStrings and Chars do not have scitype Multiclass or OrderedFactor; they have scitypes Textual and Unknown respectively.\nThe scientific types of nothing and missing are Nothing and Missing, Julia types that are also regarded as scientific.\n\nnote: Note\nThird party packages may extend the behavior of scitype: Objects previously having Unknown scitype may no longer do so.\n\nSee also coerce, autotype, schema.\n\n\n\n\n\n","category":"function"},{"location":"getting_started/#ScientificTypes.schema","page":"Getting Started","title":"ScientificTypes.schema","text":"schema(X)\n\nInspect the column types and scitypes of a tabular object. returns nothing if the column types and/or scitypes can't be inspected.\n\nExample\n\nX = (ncalls=[1, 2, 4], mean_delay=[2.0, 5.7, 6.0])\nschema(X)\n\n\n\n\n\n","category":"function"},{"location":"getting_started/#ScientificTypes.coerce","page":"Getting Started","title":"ScientificTypes.coerce","text":"coerce(A, S)\n\nReturn new version of the array A whose scientific element type is S.\n\njulia> v = coerce([3, 7, 5], Continuous)\n3-element Vector{Float64}:\n 3.0\n 7.0\n 5.0\n\njulia> scitype(v)\nAbstractVector{Continuous}\n\n\ncoerce(X, specs...; tight=false, verbosity=1)\n\nGiven a table X, return a copy of X, ensuring that the element scitypes of the columns match the new specification, specs. There are three valid specifications:\n\n(i) one or more column_name=>Scitype pairs:\n\ncoerce(X, col1=>Scitype1, col2=>Scitype2, ... ; verbosity=1)\n\n(ii) one or more OldScitype=>NewScitype pairs (OldScitype covering both the OldScitype and Union{Missing,OldScitype} cases):\n\ncoerce(X, OldScitype1=>NewScitype1, OldScitype2=>NewScitype2, ... ; verbosity=1)\n\n(iii) a dictionary of scientific types keyed on column names:\n\ncoerce(X, d::AbstractDict{<:ColKey, <:Type}; verbosity=1)\n\nwhere ColKey = Union{Symbol,AbstractString}.\n\nExamples\n\nSpecifying  column_name=>Scitype pairs:\n\nusing CategoricalArrays, DataFrames, Tables\nX = DataFrame(name=[\"Siri\", \"Robo\", \"Alexa\", \"Cortana\"],\n              height=[152, missing, 148, 163],\n              rating=[1, 5, 2, 1])\nXc = coerce(X, :name=>Multiclass, :height=>Continuous, :rating=>OrderedFactor)\nschema(Xc).scitypes # (Multiclass, Continuous, OrderedFactor)\n\nSpecifying OldScitype=>NewScitype pairs:\n\nX  = (x = [1, 2, 3],\n      y = rand(3),\n      z = [10, 20, 30])\nXc = coerce(X, Count=>Continuous)\nschema(Xfixed).scitypes # (Continuous, Continuous, Continuous)\n\n\n\n\n\ncoerce(image::AbstractArray{<:Real, N}, I)\n\nGiven an array called image representing one or more images, return a transformed version of the data so as to enforce an appropriate scientific interpretation I:\n\nsingle or collection ? N I scitype of result\nsingle 2 GrayImage GrayImage{W,H}\nsingle 3 ColorImage ColorImage{W,H}\ncollection 3 GrayImage AbstractVector{<:GrayImage}\ncollection 4 (W x H x {1} x C) GrayImage AbstractVector{<:GrayImage}\ncollection 4 ColorImage AbstractVector{<:ColorImage}\n\nimgs = rand(10, 10, 3, 5)\nv = coerce(imgs, ColorImage)\n\njulia> typeof(v)\nVector{Matrix{ColorTypes.RGB{Float64}}}\n\njulia> scitype(v)\nAbstractVector{ColorImage{10, 10}}\n\n\n\n\n\n\n","category":"function"},{"location":"getting_started/#ScientificTypes.autotype","page":"Getting Started","title":"ScientificTypes.autotype","text":"autotype(X; kw...)\n\nReturn a dictionary of suggested scitypes for each column of X, a table or an array based on rules\n\nKwargs\n\nonly_changes=true:       if true, return only a dictionary of the names for                             which applying autotype differs from just using                             the ambient convention. When coercing with                             autotype, only_changes should be true.\nrules=(:few_to_finite,): the set of rules to apply.\n\n\n\n\n\n","category":"function"},{"location":"transformers/#Transformers-and-Other-Unsupervised-Models","page":"Transformers and Other Unsupervised models","title":"Transformers and Other Unsupervised Models","text":"Several unsupervised models used to perform common transformations, such as one-hot encoding, missing value imputation, and categorical encoding, are available in MLJ out-of-the-box (no need to load code with @load). They are detailed in Built-in transformers below.\n\nA transformer is static if it has no learned parameters. While such a transformer is tantamount to an ordinary function, realizing it as an MLJ static transformer (a subtype of Static <: Unsupervised) can be useful, especially if the function depends on parameters the user would like to manipulate (which become hyper-parameters of the model). The necessary syntax for defining your own static transformers is described in Static transformers below.\n\nSome unsupervised models, such as clustering algorithms, have a predict method in addition to a transform method. We give an example of this in Transformers that also predict","category":"section"},{"location":"transformers/#Built-in-transformers","page":"Transformers and Other Unsupervised models","title":"Built-in transformers","text":"For tutorials on the transformers below, refer to the MLJTransforms documentation. \n\nTransformer Brief Description\nStandardizer Transforming columns of numerical features by standardization\nUnivariateBoxCoxTransformer Apply BoxCox transformation given a single vector\nInteractionTransformer Transforming columns of numerical features to create new interaction features\nUnivariateDiscretizer Discretize a continuous vector into an ordered factor\nFillImputer Fill in missing values of features belonging to any scientific type\nUnivariateFillImputer Fill in missing values in a single vector\nUnivariateTimeTypeToContinuous Transform a vector of time type into continuous type\nOneHotEncoder Encode categorical variables into one-hot vectors\nContinuousEncoder Adds type casting functionality to OnehotEncoder\nOrdinalEncoder Encode categorical variables into ordered integers\nFrequencyEncoder Encode categorical variables into their normalized or unormalized frequencies\nTargetEncoder Encode categorical variables into relevant target statistics\nContrastEncoder Allows defining a custom contrast encoder via a contrast matrix\nCardinalityReducer Reduce cardinality of high cardinality categorical features by grouping infrequent categories\nMissingnessEncoder Encode missing values of categorical features into new values","category":"section"},{"location":"transformers/#Static-transformers","page":"Transformers and Other Unsupervised models","title":"Static transformers","text":"A static transformer is a model for transforming data that does not generalize to new data (does not \"learn\") but which nevertheless has hyperparameters. For example, the DBSAN clustering model from Clustering.jl can assign labels to some collection of observations, cannot directly assign a label to some new observation.\n\nThe general user may define their own static models. The main use-case is insertion into a Linear Pipelines some parameter-dependent transformation. (If a static transformer has no hyper-parameters, it is tantamount to an ordinary function. An ordinary function can be inserted directly into a pipeline; the situation for learning networks is only slightly more complicated.\n\nThe following example defines a new model type Averager to perform the weighted average of two vectors (target predictions, for example). We suppose the weighting is normalized, and therefore controlled by a single hyper-parameter, mix.\n\nmutable struct Averager <: Static\n    mix::Float64\nend\n\nMLJ.transform(a::Averager, _, y1, y2) = (1 - a.mix)*y1 + a.mix*y2\n\nImportant. Note the sub-typing <: Static.\n\nSuch static transformers with (unlearned) parameters can have arbitrarily many inputs, but only one output. In the single input case, an inverse_transform can also be defined. Since they have no real learned parameters, you bind a static transformer to a machine without specifying training arguments; there is no need to fit! the machine:\n\nmach = machine(Averager(0.5))\ntransform(mach, [1, 2, 3], [3, 2, 1])\n\nLet's see how we can include our Averager in a learning network to mix the predictions of two regressors, with one-hot encoding of the inputs. Here's two regressors for mixing, and some dummy data for testing our learning network:\n\nridge = (@load RidgeRegressor pkg=MultivariateStats)()\nknn = (@load KNNRegressor)()\n\nimport Random.seed!\nseed!(112)\nX = (\n    x1=coerce(rand(\"ab\", 100), Multiclass),\n    x2=rand(100),\n)\ny = X.x2 + 0.05*rand(100)\nschema(X)\n\nAnd the learning network:\n\nXs = source(X)\nys = source(y)\n\naverager = Averager(0.5)\n\nmach0 = machine(OneHotEncoder(), Xs)\nW = transform(mach0, Xs) # one-hot encode the input\n\nmach1 = machine(ridge, W, ys)\ny1 = predict(mach1, W)\n\nmach2 = machine(knn, W, ys)\ny2 = predict(mach2, W)\n\nmach4= machine(averager)\nyhat = transform(mach4, y1, y2)\n\n# test:\nfit!(yhat)\nXnew = selectrows(X, 1:3)\nyhat(Xnew)\n\nWe next \"export\" the learning network as a standalone composite model type. First we need a struct for the composite model. Since we are restricting to Deterministic component regressors, the composite will also make deterministic predictions, and so gets the supertype DeterministicNetworkComposite:\n\nmutable struct DoubleRegressor <: DeterministicNetworkComposite\n    regressor1\n    regressor2\n    averager\nend\n\nAs described in Learning Networks, we next paste the learning network into a prefit declaration, replace the component models with symbolic placeholders, and add a learning network \"interface\":\n\nimport MLJBase\nfunction MLJBase.prefit(composite::DoubleRegressor, verbosity, X, y)\n    Xs = source(X)\n    ys = source(y)\n\n    mach0 = machine(OneHotEncoder(), Xs)\n    W = transform(mach0, Xs) # one-hot encode the input\n\n    mach1 = machine(:regressor1, W, ys)\n    y1 = predict(mach1, W)\n\n    mach2 = machine(:regressor2, W, ys)\n    y2 = predict(mach2, W)\n\n    mach4= machine(:averager)\n    yhat = transform(mach4, y1, y2)\n\n    # learning network interface:\n    (; predict=yhat)\nend\n\nThe new model type can be evaluated like any other supervised model:\n\nX, y = @load_reduced_ames;\ncomposite = DoubleRegressor(ridge, knn, Averager(0.5))\n\ncomposite.averager.mix = 0.25 # adjust mix from default of 0.5\nevaluate(composite, X, y, measure=l1)\n\nA static transformer can also expose byproducts of the transform computation in the report of any associated machine. See Static transformers for details.","category":"section"},{"location":"transformers/#Transformers-that-also-predict","page":"Transformers and Other Unsupervised models","title":"Transformers that also predict","text":"Some clustering algorithms learn to label data by identifying a collection of \"centroids\" in the training data. Any new input observation is labeled with the cluster to which it is closest (this is the output of predict) while the vector of all distances from the centroids defines a lower-dimensional representation of the observation (the output of transform). In the following example a K-means clustering algorithm assigns one of three labels 1, 2, 3 to the input features of the iris data set and compares them with the actual species recorded in the target (not seen by the algorithm).\n\nimport Random.seed!\nseed!(123)\n\nX, y = @load_iris\nKMeans = @load KMeans pkg=Clustering\nkmeans = KMeans()\nmach = machine(kmeans, X) |> fit!\nnothing # hide\n\nTransforming:\n\nXsmall = transform(mach)\nselectrows(Xsmall, 1:4) |> pretty\n\nPredicting:\n\nyhat = predict(mach)\ncompare = zip(yhat, y) |> collect\n\ncompare[1:8]\n\ncompare[51:58]\n\ncompare[101:108]","category":"section"},{"location":"transformers/#Reference","page":"Transformers and Other Unsupervised models","title":"Reference","text":"","category":"section"},{"location":"transformers/#MLJTransforms.Standardizer","page":"Transformers and Other Unsupervised models","title":"MLJTransforms.Standardizer","text":"Standardizer\n\nA model type for constructing a standardizer, based on MLJTransforms.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nStandardizer = @load Standardizer pkg=MLJTransforms\n\nDo model = Standardizer() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in Standardizer(features=...).\n\nUse this model to standardize (whiten) a Continuous vector, or relevant columns of a table. The rescalings applied by this transformer to new data are always those learned during the training phase, which are generally different from what would actually standardize the new data.\n\nTraining data\n\nIn MLJ or MLJBase, bind an instance model to data with\n\nmach = machine(model, X)\n\nwhere\n\nX: any Tables.jl compatible table or any abstract vector with Continuous element scitype (any abstract float vector). Only features in a table with Continuous scitype can be standardized; check column scitypes with schema(X).\n\nTrain the machine using fit!(mach, rows=...).\n\nHyper-parameters\n\nfeatures: one of the following, with the behavior indicated below:\n[] (empty, the default): standardize all features (columns) having Continuous element scitype\nnon-empty vector of feature names (symbols): standardize only the Continuous features in the vector (if ignore=false) or Continuous features not named in the vector (ignore=true).\nfunction or other callable: standardize a feature if the callable returns true on its name. For example, Standardizer(features = name -> name in [:x1, :x3], ignore = true, count=true) has the same effect as Standardizer(features = [:x1, :x3], ignore = true, count=true), namely to standardize all Continuous and Count features, with the exception of :x1 and :x3.\nNote this behavior is further modified if the ordered_factor or count flags are set to true; see below\nignore=false: whether to ignore or standardize specified features, as explained above\nordered_factor=false: if true, standardize any OrderedFactor feature wherever a Continuous feature would be standardized, as described above\ncount=false: if true, standardize any Count feature wherever a Continuous feature would be standardized, as described above\n\nOperations\n\ntransform(mach, Xnew): return Xnew with relevant features standardized according to the rescalings learned during fitting of mach.\ninverse_transform(mach, Z): apply the inverse transformation to Z, so that inverse_transform(mach, transform(mach, Xnew)) is approximately the same as Xnew; unavailable if ordered_factor or count flags were set to true.\n\nFitted parameters\n\nThe fields of fitted_params(mach) are:\n\nfeatures_fit - the names of features that will be standardized\nmeans - the corresponding untransformed mean values\nstds - the corresponding untransformed standard deviations\n\nReport\n\nThe fields of report(mach) are:\n\nfeatures_fit: the names of features that will be standardized\n\nExamples\n\nusing MLJ\n\nX = (ordinal1 = [1, 2, 3],\n     ordinal2 = coerce([:x, :y, :x], OrderedFactor),\n     ordinal3 = [10.0, 20.0, 30.0],\n     ordinal4 = [-20.0, -30.0, -40.0],\n     nominal = coerce([\"Your father\", \"he\", \"is\"], Multiclass));\n\njulia> schema(X)\n┌──────────┬──────────────────┐\n│ names    │ scitypes         │\n├──────────┼──────────────────┤\n│ ordinal1 │ Count            │\n│ ordinal2 │ OrderedFactor{2} │\n│ ordinal3 │ Continuous       │\n│ ordinal4 │ Continuous       │\n│ nominal  │ Multiclass{3}    │\n└──────────┴──────────────────┘\n\nstand1 = Standardizer();\n\njulia> transform(fit!(machine(stand1, X)), X)\n(ordinal1 = [1, 2, 3],\n ordinal2 = CategoricalValue{Symbol,UInt32}[:x, :y, :x],\n ordinal3 = [-1.0, 0.0, 1.0],\n ordinal4 = [1.0, 0.0, -1.0],\n nominal = CategoricalValue{String,UInt32}[\"Your father\", \"he\", \"is\"],)\n\nstand2 = Standardizer(features=[:ordinal3, ], ignore=true, count=true);\n\njulia> transform(fit!(machine(stand2, X)), X)\n(ordinal1 = [-1.0, 0.0, 1.0],\n ordinal2 = CategoricalValue{Symbol,UInt32}[:x, :y, :x],\n ordinal3 = [10.0, 20.0, 30.0],\n ordinal4 = [1.0, 0.0, -1.0],\n nominal = CategoricalValue{String,UInt32}[\"Your father\", \"he\", \"is\"],)\n\nSee also OneHotEncoder, ContinuousEncoder.\n\n\n\n\n\n","category":"type"},{"location":"transformers/#MLJTransforms.UnivariateBoxCoxTransformer","page":"Transformers and Other Unsupervised models","title":"MLJTransforms.UnivariateBoxCoxTransformer","text":"UnivariateBoxCoxTransformer\n\nA model type for constructing a single variable Box-Cox transformer, based on MLJTransforms.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nUnivariateBoxCoxTransformer = @load UnivariateBoxCoxTransformer pkg=MLJTransforms\n\nDo model = UnivariateBoxCoxTransformer() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in UnivariateBoxCoxTransformer(n=...).\n\nBox-Cox transformations attempt to make data look more normally distributed. This can improve performance and assist in the interpretation of models which suppose that data is generated by a normal distribution.\n\nA Box-Cox transformation (with shift) is of the form\n\nx -> ((x + c)^λ - 1)/λ\n\nfor some constant c and real λ, unless λ = 0, in which case the above is replaced with\n\nx -> log(x + c)\n\nGiven user-specified hyper-parameters n::Integer and shift::Bool, the present implementation learns the parameters c and λ from the training data as follows: If shift=true and zeros are encountered in the data, then c is set to 0.2 times the data mean.  If there are no zeros, then no shift is applied. Finally, n different values of λ between -0.4 and 3 are considered, with λ fixed to the value maximizing normality of the transformed data.\n\nReference: Wikipedia entry for power  transform.\n\nTraining data\n\nIn MLJ or MLJBase, bind an instance model to data with\n\nmach = machine(model, x)\n\nwhere\n\nx: any abstract vector with element scitype Continuous; check the scitype with scitype(x)\n\nTrain the machine using fit!(mach, rows=...).\n\nHyper-parameters\n\nn=171: number of values of the exponent λ to try\nshift=false: whether to include a preliminary constant translation in transformations, in the presence of zeros\n\nOperations\n\ntransform(mach, xnew): apply the Box-Cox transformation learned when fitting mach\ninverse_transform(mach, z): reconstruct the vector z whose transformation learned by mach is z\n\nFitted parameters\n\nThe fields of fitted_params(mach) are:\n\nλ: the learned Box-Cox exponent\nc: the learned shift\n\nExamples\n\nusing MLJ\nusing UnicodePlots\nusing Random\nRandom.seed!(123)\n\ntransf = UnivariateBoxCoxTransformer()\n\nx = randn(1000).^2\n\nmach = machine(transf, x)\nfit!(mach)\n\nz = transform(mach, x)\n\njulia> histogram(x)\n                ┌                                        ┐\n   [ 0.0,  2.0) ┤███████████████████████████████████  848\n   [ 2.0,  4.0) ┤████▌ 109\n   [ 4.0,  6.0) ┤█▍ 33\n   [ 6.0,  8.0) ┤▍ 7\n   [ 8.0, 10.0) ┤▏ 2\n   [10.0, 12.0) ┤  0\n   [12.0, 14.0) ┤▏ 1\n                └                                        ┘\n                                 Frequency\n\njulia> histogram(z)\n                ┌                                        ┐\n   [-5.0, -4.0) ┤█▎ 8\n   [-4.0, -3.0) ┤████████▊ 64\n   [-3.0, -2.0) ┤█████████████████████▊ 159\n   [-2.0, -1.0) ┤█████████████████████████████▊ 216\n   [-1.0,  0.0) ┤███████████████████████████████████  254\n   [ 0.0,  1.0) ┤█████████████████████████▊ 188\n   [ 1.0,  2.0) ┤████████████▍ 90\n   [ 2.0,  3.0) ┤██▊ 20\n   [ 3.0,  4.0) ┤▎ 1\n                └                                        ┘\n                                 Frequency\n\n\n\n\n\n\n","category":"type"},{"location":"transformers/#MLJTransforms.InteractionTransformer","page":"Transformers and Other Unsupervised models","title":"MLJTransforms.InteractionTransformer","text":"InteractionTransformer\n\nA model type for constructing a interaction transformer, based on MLJTransforms.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nInteractionTransformer = @load InteractionTransformer pkg=MLJTransforms\n\nDo model = InteractionTransformer() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in InteractionTransformer(order=...).\n\nGenerates all polynomial interaction terms up to the given order for the subset of chosen columns.  Any column that contains elements with scitype <:Infinite is a valid basis to generate interactions.  If features is not specified, all such columns with scitype <:Infinite in the table are used as a basis.\n\nIn MLJ or MLJBase, you can transform features X with the single call\n\ntransform(machine(model), X)\n\nSee also the example below.\n\nHyper-parameters\n\norder: Maximum order of interactions to be generated.\nfeatures: Restricts interations generation to those columns\n\nOperations\n\ntransform(machine(model), X): Generates polynomial interaction terms out of table X using the hyper-parameters specified in model.\n\nExample\n\nusing MLJ\n\nX = (\n    A = [1, 2, 3],\n    B = [4, 5, 6],\n    C = [7, 8, 9],\n    D = [\"x₁\", \"x₂\", \"x₃\"]\n)\nit = InteractionTransformer(order=3)\nmach = machine(it)\n\njulia> transform(mach, X)\n(A = [1, 2, 3],\n B = [4, 5, 6],\n C = [7, 8, 9],\n D = [\"x₁\", \"x₂\", \"x₃\"],\n A_B = [4, 10, 18],\n A_C = [7, 16, 27],\n B_C = [28, 40, 54],\n A_B_C = [28, 80, 162],)\n\nit = InteractionTransformer(order=2, features=[:A, :B])\nmach = machine(it)\n\njulia> transform(mach, X)\n(A = [1, 2, 3],\n B = [4, 5, 6],\n C = [7, 8, 9],\n D = [\"x₁\", \"x₂\", \"x₃\"],\n A_B = [4, 10, 18],)\n\n\n\n\n\n\n","category":"type"},{"location":"transformers/#MLJTransforms.UnivariateDiscretizer","page":"Transformers and Other Unsupervised models","title":"MLJTransforms.UnivariateDiscretizer","text":"UnivariateDiscretizer\n\nA model type for constructing a single variable discretizer, based on MLJTransforms.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nUnivariateDiscretizer = @load UnivariateDiscretizer pkg=MLJTransforms\n\nDo model = UnivariateDiscretizer() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in UnivariateDiscretizer(n_classes=...).\n\nDiscretization converts a Continuous vector into an OrderedFactor vector. In particular, the output is a CategoricalVector (whose reference type is optimized).\n\nThe transformation is chosen so that the vector on which the transformer is fit has, in transformed form, an approximately uniform distribution of values. Specifically, if n_classes is the level of discretization, then 2*n_classes - 1 ordered quantiles are computed, the odd quantiles being used for transforming (discretization) and the even quantiles for inverse transforming.\n\nTraining data\n\nIn MLJ or MLJBase, bind an instance model to data with\n\nmach = machine(model, x)\n\nwhere\n\nx: any abstract vector with Continuous element scitype; check scitype with scitype(x).\n\nTrain the machine using fit!(mach, rows=...).\n\nHyper-parameters\n\nn_classes: number of discrete classes in the output\n\nOperations\n\ntransform(mach, xnew): discretize xnew according to the discretization learned when fitting mach\ninverse_transform(mach, z): attempt to reconstruct from z a vector that transforms to give z\n\nFitted parameters\n\nThe fields of fitted_params(mach).fitesult include:\n\nodd_quantiles: quantiles used for transforming (length is n_classes - 1)\neven_quantiles: quantiles used for inverse transforming (length is n_classes)\n\nExample\n\nusing MLJ\nusing Random\nRandom.seed!(123)\n\ndiscretizer = UnivariateDiscretizer(n_classes=100)\nmach = machine(discretizer, randn(1000))\nfit!(mach)\n\njulia> x = rand(5)\n5-element Vector{Float64}:\n 0.8585244609846809\n 0.37541692370451396\n 0.6767070590395461\n 0.9208844241267105\n 0.7064611415680901\n\njulia> z = transform(mach, x)\n5-element CategoricalArrays.CategoricalArray{UInt8,1,UInt8}:\n 0x52\n 0x42\n 0x4d\n 0x54\n 0x4e\n\nx_approx = inverse_transform(mach, z)\njulia> x - x_approx\n5-element Vector{Float64}:\n 0.008224506144777322\n 0.012731354778359405\n 0.0056265330571125816\n 0.005738175684445124\n 0.006835652575801987\n\n\n\n\n\n","category":"type"},{"location":"transformers/#MLJTransforms.FillImputer","page":"Transformers and Other Unsupervised models","title":"MLJTransforms.FillImputer","text":"FillImputer\n\nA model type for constructing a fill imputer, based on MLJTransforms.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nFillImputer = @load FillImputer pkg=MLJTransforms\n\nDo model = FillImputer() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in FillImputer(features=...).\n\nUse this model to impute missing values in tabular data. A fixed \"filler\" value is learned from the training data, one for each column of the table.\n\nFor imputing missing values in a vector, use UnivariateFillImputer instead.\n\nTraining data\n\nIn MLJ or MLJBase, bind an instance model to data with\n\nmach = machine(model, X)\n\nwhere\n\nX: any table of input features (eg, a DataFrame) whose features each have element scitypes Union{Missing, T}, where T is a subtype of Continuous, Multiclass, OrderedFactor or Count. Check scitypes with schema(X).\n\nTrain the machine using fit!(mach, rows=...).\n\nHyper-parameters\n\nfeatures: a vector of names of features (symbols) for which imputation is to be attempted; default is empty, which is interpreted as \"impute all\".\ncontinuous_fill: function or other callable to determine value to be imputed in the case of Continuous (abstract float) data; default is to apply median after skipping missing values\ncount_fill: function or other callable to determine value to be imputed in the case of Count (integer) data; default is to apply rounded median after skipping missing values\nfinite_fill: function or other callable to determine value to be imputed in the case of Multiclass or OrderedFactor data (categorical vectors); default is to apply mode after skipping missing values\n\nOperations\n\ntransform(mach, Xnew): return Xnew with missing values imputed with the fill values learned when fitting mach\n\nFitted parameters\n\nThe fields of fitted_params(mach) are:\n\nfeatures_seen_in_fit: the names of features (features) encountered during training\nunivariate_transformer: the univariate model applied to determine   the fillers (it's fields contain the functions defining the filler computations)\nfiller_given_feature: dictionary of filler values, keyed on feature (column) names\n\nExamples\n\nusing MLJ\nimputer = FillImputer()\n\nX = (a = [1.0, 2.0, missing, 3.0, missing],\n     b = coerce([\"y\", \"n\", \"y\", missing, \"y\"], Multiclass),\n     c = [1, 1, 2, missing, 3])\n\nschema(X)\njulia> schema(X)\n┌───────┬───────────────────────────────┐\n│ names │ scitypes                      │\n├───────┼───────────────────────────────┤\n│ a     │ Union{Missing, Continuous}    │\n│ b     │ Union{Missing, Multiclass{2}} │\n│ c     │ Union{Missing, Count}         │\n└───────┴───────────────────────────────┘\n\nmach = machine(imputer, X)\nfit!(mach)\n\njulia> fitted_params(mach).filler_given_feature\n(filler = 2.0,)\n\njulia> fitted_params(mach).filler_given_feature\nDict{Symbol, Any} with 3 entries:\n  :a => 2.0\n  :b => \"y\"\n  :c => 2\n\njulia> transform(mach, X)\n(a = [1.0, 2.0, 2.0, 3.0, 2.0],\n b = CategoricalValue{String, UInt32}[\"y\", \"n\", \"y\", \"y\", \"y\"],\n c = [1, 1, 2, 2, 3],)\n\nSee also UnivariateFillImputer.\n\n\n\n\n\n","category":"type"},{"location":"transformers/#MLJTransforms.UnivariateFillImputer","page":"Transformers and Other Unsupervised models","title":"MLJTransforms.UnivariateFillImputer","text":"UnivariateFillImputer\n\nA model type for constructing a single variable fill imputer, based on MLJTransforms.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nUnivariateFillImputer = @load UnivariateFillImputer pkg=MLJTransforms\n\nDo model = UnivariateFillImputer() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in UnivariateFillImputer(continuous_fill=...).\n\nUse this model to imputing missing values in a vector with a fixed value learned from the non-missing values of training vector.\n\nFor imputing missing values in tabular data, use FillImputer instead.\n\nTraining data\n\nIn MLJ or MLJBase, bind an instance model to data with\n\nmach = machine(model, x)\n\nwhere\n\nx: any abstract vector with element scitype Union{Missing, T} where T is a subtype of Continuous, Multiclass, OrderedFactor or Count; check scitype using scitype(x)\n\nTrain the machine using fit!(mach, rows=...).\n\nHyper-parameters\n\ncontinuous_fill: function or other callable to determine value to be imputed in the case of Continuous (abstract float) data; default is to apply median after skipping missing values\ncount_fill: function or other callable to determine value to be imputed in the case of Count (integer) data; default is to apply rounded median after skipping missing values\nfinite_fill: function or other callable to determine value to be imputed in the case of Multiclass or OrderedFactor data (categorical vectors); default is to apply mode after skipping missing values\n\nOperations\n\ntransform(mach, xnew): return xnew with missing values imputed with the fill values learned when fitting mach\n\nFitted parameters\n\nThe fields of fitted_params(mach) are:\n\nfiller: the fill value to be imputed in all new data\n\nExamples\n\nusing MLJ\nimputer = UnivariateFillImputer()\n\nx_continuous = [1.0, 2.0, missing, 3.0]\nx_multiclass = coerce([\"y\", \"n\", \"y\", missing, \"y\"], Multiclass)\nx_count = [1, 1, 1, 2, missing, 3, 3]\n\nmach = machine(imputer, x_continuous)\nfit!(mach)\n\njulia> fitted_params(mach)\n(filler = 2.0,)\n\njulia> transform(mach, [missing, missing, 101.0])\n3-element Vector{Float64}:\n 2.0\n 2.0\n 101.0\n\nmach2 = machine(imputer, x_multiclass) |> fit!\n\njulia> transform(mach2, x_multiclass)\n5-element CategoricalArray{String,1,UInt32}:\n \"y\"\n \"n\"\n \"y\"\n \"y\"\n \"y\"\n\nmach3 = machine(imputer, x_count) |> fit!\n\njulia> transform(mach3, [missing, missing, 5])\n3-element Vector{Int64}:\n 2\n 2\n 5\n\nFor imputing tabular data, use FillImputer.\n\n\n\n\n\n","category":"type"},{"location":"transformers/#MLJTransforms.UnivariateTimeTypeToContinuous","page":"Transformers and Other Unsupervised models","title":"MLJTransforms.UnivariateTimeTypeToContinuous","text":"UnivariateTimeTypeToContinuous\n\nA model type for constructing a single variable transformer that creates continuous representations of temporally typed data, based on MLJTransforms.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nUnivariateTimeTypeToContinuous = @load UnivariateTimeTypeToContinuous pkg=MLJTransforms\n\nDo model = UnivariateTimeTypeToContinuous() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in UnivariateTimeTypeToContinuous(zero_time=...).\n\nUse this model to convert vectors with a TimeType element type to vectors of Float64 type (Continuous element scitype).\n\nTraining data\n\nIn MLJ or MLJBase, bind an instance model to data with\n\nmach = machine(model, x)\n\nwhere\n\nx: any abstract vector whose element type is a subtype of Dates.TimeType\n\nTrain the machine using fit!(mach, rows=...).\n\nHyper-parameters\n\nzero_time: the time that is to correspond to 0.0 under transformations, with the type coinciding with the training data element type. If unspecified, the earliest time encountered in training is used.\nstep::Period=Hour(24): time interval to correspond to one unit under transformation\n\nOperations\n\ntransform(mach, xnew): apply the encoding inferred when mach was fit\n\nFitted parameters\n\nfitted_params(mach).fitresult is the tuple (zero_time, step) actually used in transformations, which may differ from the user-specified hyper-parameters.\n\nExample\n\nusing MLJ\nusing Dates\n\nx = [Date(2001, 1, 1) + Day(i) for i in 0:4]\n\nencoder = UnivariateTimeTypeToContinuous(zero_time=Date(2000, 1, 1),\n                                         step=Week(1))\n\nmach = machine(encoder, x)\nfit!(mach)\njulia> transform(mach, x)\n5-element Vector{Float64}:\n 52.285714285714285\n 52.42857142857143\n 52.57142857142857\n 52.714285714285715\n 52.857142\n\n\n\n\n\n","category":"type"},{"location":"transformers/#MLJTransforms.OneHotEncoder","page":"Transformers and Other Unsupervised models","title":"MLJTransforms.OneHotEncoder","text":"OneHotEncoder\n\nA model type for constructing a one-hot encoder, based on MLJTransforms.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nOneHotEncoder = @load OneHotEncoder pkg=MLJTransforms\n\nDo model = OneHotEncoder() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in OneHotEncoder(features=...).\n\nUse this model to one-hot encode the Multiclass and OrderedFactor features (columns) of some table, leaving other columns unchanged.\n\nNew data to be transformed may lack features present in the fit data, but no new features can be present.\n\nWarning: This transformer assumes that levels(col) for any Multiclass or OrderedFactor column, col, is the same for training data and new data to be transformed.\n\nTo ensure all features are transformed into Continuous features, or dropped, use ContinuousEncoder instead.\n\nTraining data\n\nIn MLJ or MLJBase, bind an instance model to data with\n\nmach = machine(model, X)\n\nwhere\n\nX: any Tables.jl compatible table. Columns can be of mixed type but only those with element scitype Multiclass or OrderedFactor can be encoded. Check column scitypes with schema(X).\n\nTrain the machine using fit!(mach, rows=...).\n\nHyper-parameters\n\nfeatures: a vector of symbols (feature names). If empty (default) then all Multiclass and OrderedFactor features are encoded. Otherwise, encoding is further restricted to the specified features (ignore=false) or the unspecified features (ignore=true). This default behavior can be modified by the ordered_factor flag.\nordered_factor=false: when true, OrderedFactor features are universally excluded\ndrop_last=false: whether to drop the column corresponding to the final class of encoded features. For example, a three-class feature is spawned into three new features if drop_last=false, but just two features otherwise.\n\nFitted parameters\n\nThe fields of fitted_params(mach) are:\n\nall_features: names of all features encountered in training\nfitted_levels_given_feature: dictionary of the levels associated with each feature encoded, keyed on the feature name\nref_name_pairs_given_feature: dictionary of pairs r => ftr (such as 0x00000001 => :grad__A) where r is a CategoricalArrays.jl reference integer representing a level, and ftr the corresponding new feature name; the dictionary is keyed on the names of features that are encoded\n\nReport\n\nThe fields of report(mach) are:\n\nfeatures_to_be_encoded: names of input features to be encoded\nnew_features: names of all output features\n\nExample\n\nusing MLJ\n\nX = (name=categorical([\"Danesh\", \"Lee\", \"Mary\", \"John\"]),\n     grade=categorical([\"A\", \"B\", \"A\", \"C\"], ordered=true),\n     height=[1.85, 1.67, 1.5, 1.67],\n     n_devices=[3, 2, 4, 3])\n\njulia> schema(X)\n┌───────────┬──────────────────┐\n│ names     │ scitypes         │\n├───────────┼──────────────────┤\n│ name      │ Multiclass{4}    │\n│ grade     │ OrderedFactor{3} │\n│ height    │ Continuous       │\n│ n_devices │ Count            │\n└───────────┴──────────────────┘\n\nhot = OneHotEncoder(drop_last=true)\nmach = fit!(machine(hot, X))\nW = transform(mach, X)\n\njulia> schema(W)\n┌──────────────┬────────────┐\n│ names        │ scitypes   │\n├──────────────┼────────────┤\n│ name__Danesh │ Continuous │\n│ name__John   │ Continuous │\n│ name__Lee    │ Continuous │\n│ grade__A     │ Continuous │\n│ grade__B     │ Continuous │\n│ height       │ Continuous │\n│ n_devices    │ Count      │\n└──────────────┴────────────┘\n\nSee also ContinuousEncoder.\n\n\n\n\n\n","category":"type"},{"location":"transformers/#MLJTransforms.ContinuousEncoder","page":"Transformers and Other Unsupervised models","title":"MLJTransforms.ContinuousEncoder","text":"ContinuousEncoder\n\nA model type for constructing a continuous encoder, based on MLJTransforms.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nContinuousEncoder = @load ContinuousEncoder pkg=MLJTransforms\n\nDo model = ContinuousEncoder() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in ContinuousEncoder(drop_last=...).\n\nUse this model to arrange all features (features) of a table to have Continuous element scitype, by applying the following protocol to each feature ftr:\n\nIf ftr is already Continuous retain it.\nIf ftr is Multiclass, one-hot encode it.\nIf ftr is OrderedFactor, replace it with coerce(ftr, Continuous) (vector of floating point integers), unless ordered_factors=false is specified, in which case one-hot encode it.\nIf ftr is Count, replace it with coerce(ftr, Continuous).\nIf ftr has some other element scitype, or was not observed in fitting the encoder, drop it from the table.\n\nWarning: This transformer assumes that levels(col) for any Multiclass or OrderedFactor column, col, is the same for training data and new data to be transformed.\n\nTo selectively one-hot-encode categorical features (without dropping features) use OneHotEncoder instead.\n\nTraining data\n\nIn MLJ or MLJBase, bind an instance model to data with\n\nmach = machine(model, X)\n\nwhere\n\nX: any Tables.jl compatible table. features can be of mixed type but only those with element scitype Multiclass or OrderedFactor can be encoded. Check column scitypes with schema(X).\n\nTrain the machine using fit!(mach, rows=...).\n\nHyper-parameters\n\ndrop_last=true: whether to drop the column corresponding to the final class of one-hot encoded features. For example, a three-class feature is spawned into three new features if drop_last=false, but two just features otherwise.\none_hot_ordered_factors=false: whether to one-hot any feature with OrderedFactor element scitype, or to instead coerce it directly to a (single) Continuous feature using the order\n\nFitted parameters\n\nThe fields of fitted_params(mach) are:\n\nfeatures_to_keep: names of features that will not be dropped from the table\none_hot_encoder: the OneHotEncoder model instance for handling the one-hot encoding\none_hot_encoder_fitresult: the fitted parameters of the OneHotEncoder model\n\nReport\n\nfeatures_to_keep: names of input features that will not be dropped from the table\nnew_features: names of all output features\n\nExample\n\nX = (name=categorical([\"Danesh\", \"Lee\", \"Mary\", \"John\"]),\n     grade=categorical([\"A\", \"B\", \"A\", \"C\"], ordered=true),\n     height=[1.85, 1.67, 1.5, 1.67],\n     n_devices=[3, 2, 4, 3],\n     comments=[\"the force\", \"be\", \"with you\", \"too\"])\n\njulia> schema(X)\n┌───────────┬──────────────────┐\n│ names     │ scitypes         │\n├───────────┼──────────────────┤\n│ name      │ Multiclass{4}    │\n│ grade     │ OrderedFactor{3} │\n│ height    │ Continuous       │\n│ n_devices │ Count            │\n│ comments  │ Textual          │\n└───────────┴──────────────────┘\n\nencoder = ContinuousEncoder(drop_last=true)\nmach = fit!(machine(encoder, X))\nW = transform(mach, X)\n\njulia> schema(W)\n┌──────────────┬────────────┐\n│ names        │ scitypes   │\n├──────────────┼────────────┤\n│ name__Danesh │ Continuous │\n│ name__John   │ Continuous │\n│ name__Lee    │ Continuous │\n│ grade        │ Continuous │\n│ height       │ Continuous │\n│ n_devices    │ Continuous │\n└──────────────┴────────────┘\n\njulia> setdiff(schema(X).names, report(mach).features_to_keep) # dropped features\n1-element Vector{Symbol}:\n :comments\n\n\nSee also OneHotEncoder\n\n\n\n\n\n","category":"type"},{"location":"transformers/#MLJTransforms.OrdinalEncoder","page":"Transformers and Other Unsupervised models","title":"MLJTransforms.OrdinalEncoder","text":"OrdinalEncoder\n\nA model type for constructing a ordinal encoder, based on MLJTransforms.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nOrdinalEncoder = @load OrdinalEncoder pkg=MLJTransforms\n\nDo model = OrdinalEncoder() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in OrdinalEncoder(features=...).\n\nOrdinalEncoder implements ordinal encoding which replaces the categorical values in the specified     categorical features with integers (ordered arbitrarily). This will create an implicit ordering between     categories which may not be a proper modelling assumption.\n\nTraining data\n\nIn MLJ (or MLJBase) bind an instance unsupervised model to data with\n\nmach = machine(model, X)\n\nHere:\n\nX is any table of input features (eg, a DataFrame). Features to be transformed must  have element scitype Multiclass or OrderedFactor. Use schema(X) to  check scitypes.\n\nTrain the machine using fit!(mach, rows=...).\n\nHyper-parameters\n\nfeatures=[]: A list of names of categorical features given as symbols to exclude or in clude from encoding, according to the value of ignore, or a single symbol (which is treated as a vector with one symbol), or a callable that returns true for features to be included/excluded.\nignore=true: Whether to exclude or include the features given in features\nordered_factor=false: Whether to encode OrderedFactor or ignore them\noutput_type: The numerical concrete type of the encoded features. Default is Float32.\n\nOperations\n\ntransform(mach, Xnew): Apply ordinal encoding to selected Multiclass or OrderedFactor features of Xnew specified by hyper-parameters, and   return the new table.   Features that are neither Multiclass nor OrderedFactor  are always left unchanged.\n\nFitted parameters\n\nThe fields of fitted_params(mach) are:\n\nindex_given_feat_level: A dictionary that maps each level for each column in a subset of the categorical features of X into an integer. \n\nReport\n\nThe fields of report(mach) are:\n\nencoded_features: The subset of the categorical features of X that were encoded\n\nExamples\n\nusing MLJ\n\n# Define categorical features\nA = [\"g\", \"b\", \"g\", \"r\", \"r\",]  \nB = [1.0, 2.0, 3.0, 4.0, 5.0,]\nC = [\"f\", \"f\", \"f\", \"m\", \"f\",]  \nD = [true, false, true, false, true,]\nE = [1, 2, 3, 4, 5,]\n\n# Combine into a named tuple\nX = (A = A, B = B, C = C, D = D, E = E)\n\n# Coerce A, C, D to multiclass and B to continuous and E to ordinal\nX = coerce(X,\n:A => Multiclass,\n:B => Continuous,\n:C => Multiclass,\n:D => Multiclass,\n:E => OrderedFactor,\n)\n\n# Check scitype coercion:\nschema(X)\n\nencoder = OrdinalEncoder(ordered_factor = false)\nmach = fit!(machine(encoder, X))\nXnew = transform(mach, X)\n\njulia > Xnew\n    (A = [2, 1, 2, 3, 3],\n    B = [1.0, 2.0, 3.0, 4.0, 5.0],\n    C = [1, 1, 1, 2, 1],\n    D = [2, 1, 2, 1, 2],\n    E = CategoricalArrays.CategoricalValue{Int64, UInt32}[1, 2, 3, 4, 5],)\n\nSee also TargetEncoder\n\n\n\n\n\n","category":"type"},{"location":"transformers/#MLJTransforms.FrequencyEncoder","page":"Transformers and Other Unsupervised models","title":"MLJTransforms.FrequencyEncoder","text":"FrequencyEncoder\n\nA model type for constructing a frequency encoder, based on MLJTransforms.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nFrequencyEncoder = @load FrequencyEncoder pkg=MLJTransforms\n\nDo model = FrequencyEncoder() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in FrequencyEncoder(features=...).\n\nFrequencyEncoder implements frequency encoding which replaces the categorical values in the specified     categorical features with their (normalized or raw) frequencies of occurrence in the dataset. \n\nTraining data\n\nIn MLJ (or MLJBase) bind an instance unsupervised model to data with\n\nmach = machine(model, X)\n\nHere:\n\nX is any table of input features (eg, a DataFrame). Features to be transformed must  have element scitype Multiclass or OrderedFactor. Use schema(X) to  check scitypes.\n\nTrain the machine using fit!(mach, rows=...).\n\nHyper-parameters\n\nfeatures=[]: A list of names of categorical features given as symbols to exclude or in clude from encoding, according to the value of ignore, or a single symbol (which is treated as a vector with one symbol), or a callable that returns true for features to be included/excluded.\nignore=true: Whether to exclude or include the features given in features\nordered_factor=false: Whether to encode OrderedFactor or ignore them\nnormalize=false: Whether to use normalized frequencies that sum to 1 over category values or to use raw counts.\noutput_type=Float32: The type of the output values. The default is Float32, but you can set it to Float64 or any other type that can hold the frequency values.\n\nOperations\n\ntransform(mach, Xnew): Apply frequency encoding to selected Multiclass or OrderedFactor features of Xnew specified by hyper-parameters, and   return the new table.   Features that are neither Multiclass nor OrderedFactor  are always left unchanged.\n\nFitted parameters\n\nThe fields of fitted_params(mach) are:\n\nstatistic_given_feat_val: A dictionary that maps each level for each column in a subset of the categorical features of X into its frequency.\n\nReport\n\nThe fields of report(mach) are:\n\nencoded_features: The subset of the categorical features of X that were encoded\n\nExamples\n\nusing MLJ\n\n# Define categorical features\nA = [\"g\", \"b\", \"g\", \"r\", \"r\",]  \nB = [1.0, 2.0, 3.0, 4.0, 5.0,]\nC = [\"f\", \"f\", \"f\", \"m\", \"f\",]  \nD = [true, false, true, false, true,]\nE = [1, 2, 3, 4, 5,]\n\n# Combine into a named tuple\nX = (A = A, B = B, C = C, D = D, E = E)\n\n# Coerce A, C, D to multiclass and B to continuous and E to ordinal\nX = coerce(X,\n:A => Multiclass,\n:B => Continuous,\n:C => Multiclass,\n:D => Multiclass,\n:E => OrderedFactor,\n)\n\n# Check scitype coercions:\nschema(X)\n\nencoder = FrequencyEncoder(ordered_factor = false, normalize=true)\nmach = fit!(machine(encoder, X))\nXnew = transform(mach, X)\n\njulia > Xnew\n    (A = [2, 1, 2, 2, 2],\n    B = [1.0, 2.0, 3.0, 4.0, 5.0],\n    C = [4, 4, 4, 1, 4],\n    D = [3, 2, 3, 2, 3],\n    E = CategoricalArrays.CategoricalValue{Int64, UInt32}[1, 2, 3, 4, 5],)\n\nSee also TargetEncoder\n\n\n\n\n\n","category":"type"},{"location":"transformers/#MLJTransforms.TargetEncoder","page":"Transformers and Other Unsupervised models","title":"MLJTransforms.TargetEncoder","text":"TargetEncoder\n\nA model type for constructing a target encoder, based on MLJTransforms.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nTargetEncoder = @load TargetEncoder pkg=MLJTransforms\n\nDo model = TargetEncoder() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in TargetEncoder(features=...).\n\nTargetEncoder implements target encoding as defined in [1] to encode categorical variables     into continuous ones using statistics from the target variable.\n\nTraining data\n\nIn MLJ (or MLJBase) bind an instance model to data with\n\nmach = machine(model, X, y)\n\nHere:\n\nX is any table of input features (eg, a DataFrame). Features to be transformed must  have element scitype Multiclass or OrderedFactor. Use schema(X) to  check scitypes.\n\ny is the target, which can be any AbstractVector whose element scitype is Continuous or Count for regression problems and Multiclass or OrderedFactor for classification problems; check the scitype with schema(y)\n\nTrain the machine using fit!(mach, rows=...).\n\nHyper-parameters\n\nfeatures=[]: A list of names of categorical features given as symbols to exclude or in clude from encoding, according to the value of ignore, or a single symbol (which is treated as a vector with one symbol), or a callable that returns true for features to be included/excluded.\n\nignore=true: Whether to exclude or include the features given in features\n\nordered_factor=false: Whether to encode OrderedFactor or ignore them\n\nλ: Shrinkage hyperparameter used to mix between posterior and prior statistics as described in [1]\nm: An integer hyperparameter to compute shrinkage as described in [1]. If m=:auto then m will be computed using empirical Bayes estimation as described in [1]\n\nOperations\n\ntransform(mach, Xnew): Apply target encoding to selected Multiclass or OrderedFactor features of Xnew specified by hyper-parameters, and return the new table.  Features that are neither Multiclass nor OrderedFactor are always left unchanged.\n\nFitted parameters\n\nThe fields of fitted_params(mach) are:\n\ntask: Whether the task is Classification or Regression\ny_statistic_given_feat_level: A dictionary with the necessary statistics to encode each categorical feature. It maps each level in each categorical feature to a statistic computed over the target.\n\nReport\n\nThe fields of report(mach) are:\n\nencoded_features: The subset of the categorical features of X that were encoded\n\nExamples\n\nusing MLJ\n\n# Define categorical features\nA = [\"g\", \"b\", \"g\", \"r\", \"r\",]\nB = [1.0, 2.0, 3.0, 4.0, 5.0,]\nC = [\"f\", \"f\", \"f\", \"m\", \"f\",]\nD = [true, false, true, false, true,]\nE = [1, 2, 3, 4, 5,]\n\n# Define the target variable\ny = [\"c1\", \"c2\", \"c3\", \"c1\", \"c2\",]\n\n# Combine into a named tuple\nX = (A = A, B = B, C = C, D = D, E = E)\n\n# Coerce A, C, D to multiclass and B to continuous and E to ordinal\nX = coerce(X,\n:A => Multiclass,\n:B => Continuous,\n:C => Multiclass,\n:D => Multiclass,\n:E => OrderedFactor,\n)\ny = coerce(y, Multiclass)\n\nencoder = TargetEncoder(ordered_factor = false, lambda = 1.0, m = 0,)\nmach = fit!(machine(encoder, X, y))\nXnew = transform(mach, X)\n\njulia > schema(Xnew)\n┌───────┬──────────────────┬─────────────────────────────────┐\n│ names │ scitypes         │ types                           │\n├───────┼──────────────────┼─────────────────────────────────┤\n│ A_1   │ Continuous       │ Float64                         │\n│ A_2   │ Continuous       │ Float64                         │\n│ A_3   │ Continuous       │ Float64                         │\n│ B     │ Continuous       │ Float64                         │\n│ C_1   │ Continuous       │ Float64                         │\n│ C_2   │ Continuous       │ Float64                         │\n│ C_3   │ Continuous       │ Float64                         │\n│ D_1   │ Continuous       │ Float64                         │\n│ D_2   │ Continuous       │ Float64                         │\n│ D_3   │ Continuous       │ Float64                         │\n│ E     │ OrderedFactor{5} │ CategoricalValue{Int64, UInt32} │\n└───────┴──────────────────┴─────────────────────────────────┘\n\nReference\n\n[1] Micci-Barreca, Daniele.     “A preprocessing scheme for high-cardinality categorical attributes in classification and prediction problems”     SIGKDD Explor. Newsl. 3, 1 (July 2001), 27–32.\n\nSee also OneHotEncoder\n\n\n\n\n\n","category":"type"},{"location":"transformers/#MLJTransforms.ContrastEncoder","page":"Transformers and Other Unsupervised models","title":"MLJTransforms.ContrastEncoder","text":"ContrastEncoder\n\nA model type for constructing a contrast encoder, based on MLJTransforms.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nContrastEncoder = @load ContrastEncoder pkg=MLJTransforms\n\nDo model = ContrastEncoder() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in ContrastEncoder(features=...).\n\nContrastEncoder implements the following contrast encoding methods for categorical features: dummy, sum, backward/forward difference, and Helmert coding.  More generally, users can specify a custom contrast or hypothesis matrix, and each feature can be encoded using a different method.\n\nTraining data\n\nIn MLJ (or MLJBase) bind an instance unsupervised model to data with\n\nmach = machine(model, X)\n\nHere:\n\nX is any table of input features (eg, a DataFrame). Features to be transformed must  have element scitype Multiclass or OrderedFactor. Use schema(X) to  check scitypes.\n\nTrain the machine using fit!(mach, rows=...).\n\nHyper-parameters\n\nfeatures=[]: A list of names of categorical features given as symbols to exclude or in clude from encoding, according to the value of ignore, or a single symbol (which is treated as a vector with one symbol), or a callable that returns true for features to be included/excluded.\n\nmode=:dummy: The type of encoding to use. Can be one of :contrast, :dummy, :sum, :backward_diff, :forward_diff, :helmert or :hypothesis. If ignore=false (features to be encoded are listed explictly in features), then this can be a vector of the same length as features to specify a different contrast encoding scheme for each feature\nbuildmatrix=nothing: A function or other callable with signature buildmatrix(colname,k), where colname is the name of the feature levels and k is it's length, and which returns contrast or hypothesis matrix with row/column ordering consistent with the ordering of levels(col). Only relevant if mode is :contrast or :hypothesis.\nignore=true: Whether to exclude or include the features given in features\n\nordered_factor=false: Whether to encode OrderedFactor or ignore them\n\nOperations\n\ntransform(mach, Xnew): Apply contrast encoding to selected Multiclass or OrderedFactor features ofXnewspecified by hyper-parameters, and return the new table. Features that are neitherMulticlassnorOrderedFactor` are always left unchanged.\n\nFitted parameters\n\nThe fields of fitted_params(mach) are:\n\nvector_given_value_given_feature: A dictionary that maps each level for each column in a subset of the categorical features of X into its frequency.\n\nReport\n\nThe fields of report(mach) are:\n\nencoded_features: The subset of the categorical features of X that were encoded\n\nExamples\n\nusing MLJ\n\n# Define categorical dataset\nX = (\n    name   = categorical([\"Ben\", \"John\", \"Mary\", \"John\"]),\n    height = [1.85, 1.67, 1.5, 1.67],\n    favnum = categorical([7, 5, 10, 1]),\n    age    = [23, 23, 14, 23],\n)\n\n# Check scitype coercions:\nschema(X)\n\nencoder =  ContrastEncoder(\n    features = [:name, :favnum],\n    ignore = false,\n    mode = [:dummy, :helmert],\n)\nmach = fit!(machine(encoder, X))\nXnew = transform(mach, X)\n\njulia > Xnew\n    (name_John = [1.0, 0.0, 0.0, 0.0],\n    name_Mary = [0.0, 1.0, 0.0, 1.0],\n    height = [1.85, 1.67, 1.5, 1.67],\n    favnum_5 = [0.0, 1.0, 0.0, -1.0],\n    favnum_7 = [2.0, -1.0, 0.0, -1.0],\n    favnum_10 = [-1.0, -1.0, 3.0, -1.0],\n    age = [23, 23, 14, 23],)\n\nSee also OneHotEncoder\n\n\n\n\n\n","category":"type"},{"location":"transformers/#MLJTransforms.CardinalityReducer","page":"Transformers and Other Unsupervised models","title":"MLJTransforms.CardinalityReducer","text":"CardinalityReducer\n\nA model type for constructing a cardinality reducer, based on MLJTransforms.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nCardinalityReducer = @load CardinalityReducer pkg=MLJTransforms\n\nDo model = CardinalityReducer() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in CardinalityReducer(features=...).\n\nCardinalityReducer maps any level of a categorical feature that occurs with frequency < min_frequency into a new level (e.g., \"Other\"). This is useful when some categorical features have high cardinality and many levels are infrequent. This assumes that the categorical features have raw types that are in Union{AbstractString, Char, Number}.\n\nTraining data\n\nIn MLJ (or MLJBase) bind an instance unsupervised model to data with\n\nmach = machine(model, X)\n\nHere:\n\nX is any table of input features (eg, a DataFrame). Features to be transformed must  have element scitype Multiclass or OrderedFactor. Use schema(X) to  check scitypes.\n\nTrain the machine using fit!(mach, rows=...).\n\nHyper-parameters\n\nfeatures=[]: A list of names of categorical features given as symbols to exclude or in clude from encoding, according to the value of ignore, or a single symbol (which is treated as a vector with one symbol), or a callable that returns true for features to be included/excluded.\n\nignore=true: Whether to exclude or include the features given in features\n\nordered_factor=false: Whether to encode OrderedFactor or ignore them\n\nmin_frequency::Real=3: Any level of a categorical feature that occurs with frequency < min_frequency will be mapped to a new level. Could be an integer or a float which decides whether raw counts or normalized frequencies are used.\nlabel_for_infrequent::Dict{<:Type, <:Any}()= Dict( AbstractString => \"Other\", Char => 'O', ): A dictionary where the possible values for keys are the types in Char, AbstractString, and Number and each value signifies the new level to map into given a column raw super type. By default, if the raw type of the column subtypes AbstractString then the new value is \"Other\" and if the raw type subtypes Char then the new value is 'O' and if the raw type subtypes Number then the new value is the lowest value in the column - 1.\n\nOperations\n\ntransform(mach, Xnew): Apply cardinality reduction to selected Multiclass or OrderedFactor features of Xnew specified by hyper-parameters, and return the new table.  Features that are neither Multiclass nor OrderedFactor are always left unchanged.\n\nFitted parameters\n\nThe fields of fitted_params(mach) are:\n\nnew_cat_given_col_val: A dictionary that maps each level in a categorical feature to a  new level (either itself or the new level specified in label_for_infrequent)\n\nReport\n\nThe fields of report(mach) are:\n\nencoded_features: The subset of the categorical features of X that were encoded\n\nExamples\n\nimport StatsBase.proportionmap\nusing MLJ\n\n# Define categorical features\nA = [ [\"a\" for i in 1:100]..., \"b\", \"b\", \"b\", \"c\", \"d\"]\nB = [ [0 for i in 1:100]..., 1, 2, 3, 4, 4]\n\n# Combine into a named tuple\nX = (A = A, B = B)\n\n# Coerce A, C, D to multiclass and B to continuous and E to ordinal\nX = coerce(X,\n:A => Multiclass,\n:B => Multiclass\n)\n\nencoder = CardinalityReducer(ordered_factor = false, min_frequency=3)\nmach = fit!(machine(encoder, X))\nXnew = transform(mach, X)\n\njulia> proportionmap(Xnew.A)\nDict{CategoricalArrays.CategoricalValue{String, UInt32}, Float64} with 3 entries:\n  \"Other\" => 0.0190476\n  \"b\"     => 0.0285714\n  \"a\"     => 0.952381\n\njulia> proportionmap(Xnew.B)\nDict{CategoricalArrays.CategoricalValue{Int64, UInt32}, Float64} with 2 entries:\n  0  => 0.952381\n  -1 => 0.047619\n\nSee also FrequencyEncoder\n\n\n\n\n\n","category":"type"},{"location":"transformers/#MLJTransforms.MissingnessEncoder","page":"Transformers and Other Unsupervised models","title":"MLJTransforms.MissingnessEncoder","text":"MissingnessEncoder\n\nA model type for constructing a missingness encoder, based on MLJTransforms.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nMissingnessEncoder = @load MissingnessEncoder pkg=MLJTransforms\n\nDo model = MissingnessEncoder() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in MissingnessEncoder(features=...).\n\nMissingnessEncoder maps any missing level of a categorical feature into a new level (e.g., \"Missing\").  By this, missingness will be treated as a new level by any subsequent model. This assumes that the categorical features have raw types that are in Char, AbstractString, and Number.\n\nTraining data\n\nIn MLJ (or MLJBase) bind an instance unsupervised model to data with\n\nmach = machine(model, X)\n\nHere:\n\nX is any table of input features (eg, a DataFrame). Features to be transformed must  have element scitype Multiclass or OrderedFactor. Use schema(X) to  check scitypes.\n\nTrain the machine using fit!(mach, rows=...).\n\nHyper-parameters\n\nfeatures=[]: A list of names of categorical features given as symbols to exclude or in clude from encoding, according to the value of ignore, or a single symbol (which is treated as a vector with one symbol), or a callable that returns true for features to be included/excluded.\n\nignore=true: Whether to exclude or include the features given in features\n\nordered_factor=false: Whether to encode OrderedFactor or ignore them\n\nlabel_for_missing::Dict{<:Type, <:Any}()= Dict( AbstractString => \"missing\", Char => 'm', ): A dictionary where the possible values for keys are the types in Char, AbstractString, and Number and where each value signifies the new level to map into given a column raw super type. By default, if the raw type of the column subtypes AbstractString then missing values will be replaced with \"missing\" and if the raw type subtypes Char then the new value is 'm' and if the raw type subtypes Number then the new value is the lowest value in the column - 1.\n\nOperations\n\ntransform(mach, Xnew): Apply cardinality reduction to selected Multiclass or OrderedFactor features of Xnew specified by hyper-parameters, and return the new table.  Features that are neither Multiclass nor OrderedFactor are always left unchanged.\n\nFitted parameters\n\nThe fields of fitted_params(mach) are:\n\nlabel_for_missing_given_feature: A dictionary that for each column, maps missing into some value according to label_for_missing\n\nReport\n\nThe fields of report(mach) are:\n\nencoded_features: The subset of the categorical features of X that were encoded\n\nExamples\n\nimport StatsBase.proportionmap\nusing MLJ\n\n# Define a table with missing values\nXm = (\n    A = categorical([\"Ben\", \"John\", missing, missing, \"Mary\", \"John\", missing]),\n    B = [1.85, 1.67, missing, missing, 1.5, 1.67, missing],\n    C= categorical([7, 5, missing, missing, 10, 0, missing]),\n    D = [23, 23, 44, 66, 14, 23, 11],\n    E = categorical([missing, 'g', 'r', missing, 'r', 'g', 'p'])\n)\n\nencoder = MissingnessEncoder()\nmach = fit!(machine(encoder, Xm))\nXnew = transform(mach, Xm)\n\njulia> Xnew\n(A = [\"Ben\", \"John\", \"missing\", \"missing\", \"Mary\", \"John\", \"missing\"],\n B = Union{Missing, Float64}[1.85, 1.67, missing, missing, 1.5, 1.67, missing],\n C = [7, 5, -1, -1, 10, 0, -1],\n D = [23, 23, 44, 66, 14, 23, 11],\n E = ['m', 'g', 'r', 'm', 'r', 'g', 'p'],)\n\n\nSee also CardinalityReducer\n\n\n\n\n\n","category":"type"},{"location":"models/GaussianProcessRegressor_MLJScikitLearnInterface/#GaussianProcessRegressor_MLJScikitLearnInterface","page":"GaussianProcessRegressor","title":"GaussianProcessRegressor","text":"GaussianProcessRegressor\n\nA model type for constructing a Gaussian process regressor, based on MLJScikitLearnInterface.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nGaussianProcessRegressor = @load GaussianProcessRegressor pkg=MLJScikitLearnInterface\n\nDo model = GaussianProcessRegressor() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in GaussianProcessRegressor(kernel=...).","category":"section"},{"location":"models/GaussianProcessRegressor_MLJScikitLearnInterface/#Hyper-parameters","page":"GaussianProcessRegressor","title":"Hyper-parameters","text":"kernel = nothing\nalpha = 1.0e-10\noptimizer = fmin_l_bfgs_b\nn_restarts_optimizer = 0\nnormalize_y = false\ncopy_X_train = true\nrandom_state = nothing","category":"section"},{"location":"models/MissingnessEncoder_MLJTransforms/#MissingnessEncoder_MLJTransforms","page":"MissingnessEncoder","title":"MissingnessEncoder","text":"MissingnessEncoder\n\nA model type for constructing a missingness encoder, based on MLJTransforms.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nMissingnessEncoder = @load MissingnessEncoder pkg=MLJTransforms\n\nDo model = MissingnessEncoder() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in MissingnessEncoder(features=...).\n\nMissingnessEncoder maps any missing level of a categorical feature into a new level (e.g., \"Missing\").  By this, missingness will be treated as a new level by any subsequent model. This assumes that the categorical features have raw types that are in Char, AbstractString, and Number.","category":"section"},{"location":"models/MissingnessEncoder_MLJTransforms/#Training-data","page":"MissingnessEncoder","title":"Training data","text":"In MLJ (or MLJBase) bind an instance unsupervised model to data with\n\nmach = machine(model, X)\n\nHere:\n\nX is any table of input features (eg, a DataFrame). Features to be transformed must  have element scitype Multiclass or OrderedFactor. Use schema(X) to  check scitypes.\n\nTrain the machine using fit!(mach, rows=...).","category":"section"},{"location":"models/MissingnessEncoder_MLJTransforms/#Hyper-parameters","page":"MissingnessEncoder","title":"Hyper-parameters","text":"features=[]: A list of names of categorical features given as symbols to exclude or in clude from encoding, according to the value of ignore, or a single symbol (which is treated as a vector with one symbol), or a callable that returns true for features to be included/excluded.\nignore=true: Whether to exclude or include the features given in features\nordered_factor=false: Whether to encode OrderedFactor or ignore them\nlabel_for_missing::Dict{<:Type, <:Any}()= Dict( AbstractString => \"missing\", Char => 'm', ): A dictionary where the possible values for keys are the types in Char, AbstractString, and Number and where each value signifies the new level to map into given a column raw super type. By default, if the raw type of the column subtypes AbstractString then missing values will be replaced with \"missing\" and if the raw type subtypes Char then the new value is 'm' and if the raw type subtypes Number then the new value is the lowest value in the column - 1.","category":"section"},{"location":"models/MissingnessEncoder_MLJTransforms/#Operations","page":"MissingnessEncoder","title":"Operations","text":"transform(mach, Xnew): Apply cardinality reduction to selected Multiclass or OrderedFactor features of Xnew specified by hyper-parameters, and return the new table.  Features that are neither Multiclass nor OrderedFactor are always left unchanged.","category":"section"},{"location":"models/MissingnessEncoder_MLJTransforms/#Fitted-parameters","page":"MissingnessEncoder","title":"Fitted parameters","text":"The fields of fitted_params(mach) are:\n\nlabel_for_missing_given_feature: A dictionary that for each column, maps missing into some value according to label_for_missing","category":"section"},{"location":"models/MissingnessEncoder_MLJTransforms/#Report","page":"MissingnessEncoder","title":"Report","text":"The fields of report(mach) are:\n\nencoded_features: The subset of the categorical features of X that were encoded","category":"section"},{"location":"models/MissingnessEncoder_MLJTransforms/#Examples","page":"MissingnessEncoder","title":"Examples","text":"import StatsBase.proportionmap\nusing MLJ\n\n## Define a table with missing values\nXm = (\n    A = categorical([\"Ben\", \"John\", missing, missing, \"Mary\", \"John\", missing]),\n    B = [1.85, 1.67, missing, missing, 1.5, 1.67, missing],\n    C= categorical([7, 5, missing, missing, 10, 0, missing]),\n    D = [23, 23, 44, 66, 14, 23, 11],\n    E = categorical([missing, 'g', 'r', missing, 'r', 'g', 'p'])\n)\n\nencoder = MissingnessEncoder()\nmach = fit!(machine(encoder, Xm))\nXnew = transform(mach, Xm)\n\njulia> Xnew\n(A = [\"Ben\", \"John\", \"missing\", \"missing\", \"Mary\", \"John\", \"missing\"],\n B = Union{Missing, Float64}[1.85, 1.67, missing, missing, 1.5, 1.67, missing],\n C = [7, 5, -1, -1, 10, 0, -1],\n D = [23, 23, 44, 66, 14, 23, 11],\n E = ['m', 'g', 'r', 'm', 'r', 'g', 'p'],)\n\n\nSee also CardinalityReducer","category":"section"},{"location":"models/MeanShift_MLJScikitLearnInterface/#MeanShift_MLJScikitLearnInterface","page":"MeanShift","title":"MeanShift","text":"MeanShift\n\nA model type for constructing a mean shift, based on MLJScikitLearnInterface.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nMeanShift = @load MeanShift pkg=MLJScikitLearnInterface\n\nDo model = MeanShift() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in MeanShift(bandwidth=...).\n\nMean shift clustering using a flat kernel. Mean shift clustering aims to discover \"blobs\" in a smooth density of samples. It is a centroid-based algorithm, which works by updating candidates for centroids to be the mean of the points within a given region. These candidates are then filtered in a post-processing stage to eliminate near-duplicates to form the final set of centroids.\"","category":"section"},{"location":"models/StableRulesRegressor_SIRUS/#StableRulesRegressor_SIRUS","page":"StableRulesRegressor","title":"StableRulesRegressor","text":"StableRulesRegressor\n\nA model type for constructing a stable rules regressor, based on SIRUS.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nStableRulesRegressor = @load StableRulesRegressor pkg=SIRUS\n\nDo model = StableRulesRegressor() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in StableRulesRegressor(rng=...).\n\nStableRulesRegressor implements the explainable rule-based regression model based on a random forest.","category":"section"},{"location":"models/StableRulesRegressor_SIRUS/#Training-data","page":"StableRulesRegressor","title":"Training data","text":"In MLJ or MLJBase, bind an instance model to data with\n\nmach = machine(model, X, y)\n\nwhere\n\nX: any table of input features (eg, a DataFrame) whose columns each have one of the following element scitypes: Continuous, Count, or <:OrderedFactor; check column scitypes with schema(X)\ny: the target, which can be any AbstractVector whose element scitype is <:OrderedFactor or <:Multiclass; check the scitype with scitype(y)\n\nTrain the machine with fit!(mach, rows=...).","category":"section"},{"location":"models/StableRulesRegressor_SIRUS/#Hyperparameters","page":"StableRulesRegressor","title":"Hyperparameters","text":"rng::AbstractRNG=default_rng(): Random number generator.   Using a StableRNG from StableRNGs.jl is advised.\npartial_sampling::Float64=0.7:   Ratio of samples to use in each subset of the data.   The default should be fine for most cases.\nn_trees::Int=1000:   The number of trees to use.   It is advisable to use at least thousand trees to for a better rule selection, and   in turn better predictive performance.\nmax_depth::Int=2:   The depth of the tree.   A lower depth decreases model complexity and can therefore improve accuracy when the sample size is small (reduce overfitting).\nq::Int=10: Number of cutpoints to use per feature.   The default value should be fine for most situations.\nmin_data_in_leaf::Int=5: Minimum number of data points per leaf.\nmax_rules::Int=10:   This is the most important hyperparameter after lambda.   The more rules, the more accurate the model should be.   If this is not the case, tune lambda first.   However, more rules will also decrease model interpretability.   So, it is important to find a good balance here.   In most cases, 10 to 40 rules should provide reasonable accuracy while remaining interpretable.\nlambda::Float64=1.0:   The weights of the final rules are determined via a regularized regression over each rule as a binary feature.   This hyperparameter specifies the strength of the ridge (L2) regularizer.   SIRUS is very sensitive to the choice of this hyperparameter.   Ensure that you try the full range from 10^-4 to 10^4 (e.g., 0.001, 0.01, ..., 100).   When trying the range, one good check is to verify that an increase in max_rules increases performance.   If this is not the case, then try a different value for lambda.","category":"section"},{"location":"models/StableRulesRegressor_SIRUS/#Fitted-parameters","page":"StableRulesRegressor","title":"Fitted parameters","text":"The fields of fitted_params(mach) are:\n\nfitresult: A StableRules object.","category":"section"},{"location":"models/StableRulesRegressor_SIRUS/#Operations","page":"StableRulesRegressor","title":"Operations","text":"predict(mach, Xnew): Return a vector of predictions for each row of Xnew.","category":"section"},{"location":"correcting_class_imbalance/#Correcting-Class-Imbalance","page":"Correcting Class Imbalance","title":"Correcting Class Imbalance","text":"","category":"section"},{"location":"correcting_class_imbalance/#Oversampling-and-undersampling-methods","page":"Correcting Class Imbalance","title":"Oversampling and undersampling methods","text":"Models providing oversampling or undersampling methods, to correct for class imbalance, are listed under Class Imbalance. In particular, several popular algorithms are provided by the Imbalance.jl package, which includes detailed documentation and tutorials.","category":"section"},{"location":"correcting_class_imbalance/#Incorporating-class-imbalance-in-supervised-learning-pipelines","page":"Correcting Class Imbalance","title":"Incorporating class imbalance in supervised learning pipelines","text":"One or more oversampling/undersampling algorithms can be fused with an MLJ classifier using the BalancedModel wrapper. This creates a new classifier which can be treated like any other; resampling to correct for class imbalance, relevant only for training of the atomic classifier, is then carried out internally. If, for example, one applies cross-validation to the wrapped classifier (using evaluate!, say) then this means over/undersampling is then repeated for each training fold automatically.\n\nRefer to the MLJBalancing.jl documentation for further details.","category":"section"},{"location":"correcting_class_imbalance/#MLJBalancing.BalancedModel","page":"Correcting Class Imbalance","title":"MLJBalancing.BalancedModel","text":"BalancedModel(; model=nothing, balancer1=balancer_model1, balancer2=balancer_model2, ...)\nBalancedModel(model;  balancer1=balancer_model1, balancer2=balancer_model2, ...)\n\nGiven a classification model, and one or more balancer models that all implement the MLJModelInterface,     BalancedModel allows constructing a sequential pipeline that wraps an arbitrary number of balancing models     and a classifier together in a sequential pipeline.\n\nOperation\n\nDuring training, data is first passed to balancer1 and the result is passed to balancer2 and so on, the result from the final balancer   is then passed to the classifier for training.\nDuring prediction, the balancers have no effect.\n\nArguments\n\nmodel::Supervised: A classification model that implements the MLJModelInterface.\nbalancer1::Static=...: The first balancer model to pass the data to. This keyword argument can have any name.\nbalancer2::Static=...: The second balancer model to pass the data to. This keyword argument can have any name.\nand so on for an arbitrary number of balancers.\n\nReturns\n\nAn instance of type ProbabilisticBalancedModel or DeterministicBalancedModel, depending on the prediction type of model.\n\nExample\n\nusing MLJ\nusing Imbalance\n\n# generate data\nX, y = Imbalance.generate_imbalanced_data(1000, 5; class_probs=[0.2, 0.3, 0.5])\n\n# prepare classification and balancing models\nSMOTENC = @load SMOTENC pkg=Imbalance verbosity=0\nTomekUndersampler = @load TomekUndersampler pkg=Imbalance verbosity=0\nLogisticClassifier = @load LogisticClassifier pkg=MLJLinearModels verbosity=0\n\noversampler = SMOTENC(k=5, ratios=1.0, rng=42)\nundersampler = TomekUndersampler(min_ratios=0.5, rng=42)\nlogistic_model = LogisticClassifier()\n\n# wrap them in a BalancedModel\nbalanced_model = BalancedModel(model=logistic_model, balancer1=oversampler, balancer2=undersampler)\n\n# now this behaves as a unified model that can be trained, validated, fine-tuned, etc.\nmach = machine(balanced_model, X, y)\nfit!(mach)\n\n\n\n\n\n","category":"function"},{"location":"working_with_categorical_data/#Working-with-Categorical-Data","page":"Working with Categorical Data","title":"Working with Categorical Data","text":"","category":"section"},{"location":"working_with_categorical_data/#Scientific-types-for-discrete-data","page":"Working with Categorical Data","title":"Scientific types for discrete data","text":"Recall that models articulate their data requirements using scientific types (see Getting Started or the ScientificTypes.jl documentation). There are three scientific types discrete data can have: Count, OrderedFactor and Multiclass.","category":"section"},{"location":"working_with_categorical_data/#Count-data","page":"Working with Categorical Data","title":"Count data","text":"In MLJ you cannot use integers to represent (finite) categorical data. Integers are reserved for discrete data you want interpreted as Count <: Infinite:\n\nusing MLJ # hide\nscitype([1, 4, 5, 6])\n\nThe Count scientific type includes things like the number of phone calls, or city populations, and other \"frequency\" data of a generally unbounded nature.\n\nThat said, you may have data that is theoretically Count, but which you coerce to OrderedFactor to enable the use of more models, trusting to your knowledge of how those models work to inform an appropriate interpretation.","category":"section"},{"location":"working_with_categorical_data/#OrderedFactor-and-Multiclass-data","page":"Working with Categorical Data","title":"OrderedFactor and Multiclass data","text":"Other integer data, such as the number of an animal's legs, or number of rooms in homes, are, generally, coerced to OrderedFactor <: Finite. The other categorical scientific type is Multiclass <: Finite, which is for unordered categorical data. Coercing data to one of these two forms is discussed under  Detecting and coercing improperly represented categorical data below.","category":"section"},{"location":"working_with_categorical_data/#Binary-data","page":"Working with Categorical Data","title":"Binary data","text":"There is no separate scientific type for binary data. Binary data is either OrderedFactor{2} if ordered, and Multiclass{2} otherwise. Data with type OrderedFactor{2} is considered to have an intrinsic \"positive\" class, e.g., the outcome of a medical test, and the \"pass/fail\" outcome of an exam. MLJ measures, such as true_positive assume the second class in the ordering is the \"positive\" class. Inspecting and changing order are discussed in the next section.\n\nIf data has type Bool it is considered Count data (as Bool <: Integer) and, generally, users will want to coerce such data to Multiclass or OrderedFactor.","category":"section"},{"location":"working_with_categorical_data/#Detecting-and-coercing-improperly-represented-categorical-data","page":"Working with Categorical Data","title":"Detecting and coercing improperly represented categorical data","text":"One inspects the scientific type of data using scitype as shown above. To inspect all column scientific types in a table simultaneously, use schema. (The scitype(X) of a table X contains a condensed form of this information used in type dispatch; see here.)\n\nimport DataFrames: DataFrame\nX = DataFrame(\n    name = [\"Siri\", \"Robo\", \"Alexa\", \"Cortana\"],\n    gender = [\"male\", \"male\", \"Female\", \"female\"],\n    likes_soup = [true, false, false, true],\n    height = [152, missing, 148, 163],\n    rating = [2, 5, 2, 1],\n    outcome = [\"rejected\", \"accepted\", \"accepted\", \"rejected\"],\n)\nschema(X)\n\nCoercing a single column:\n\nX.outcome = coerce(X.outcome, OrderedFactor)\n\nThe machine type of the result is a CategoricalArray. For more on this type see Under the hood: CategoricalValue and CategoricalArray below.\n\nInspecting the order of the levels:\n\nlevels(X.outcome)\n\nSince we wish to regard \"accepted\" as the positive class, it should appear second, which we correct with the levels! function:\n\nlevels!(X.outcome, [\"rejected\", \"accepted\"])\nlevels(X.outcome)\n\nwarning: Changing levels of categorical data\nThe order of levels should generally be changed early in your data science workflow and then not again. Similar remarks apply to adding levels (which is possible; see the CategorialArrays.jl documentation). MLJ supervised and unsupervised models assume levels and their order do not change.\n\nCoercing all remaining types simultaneously:\n\nXnew = coerce(X, :gender     => Multiclass,\n                 :likes_soup => OrderedFactor,\n                 :height     => Continuous,\n                 :rating     => OrderedFactor)\nschema(Xnew)\n\nFor DataFrames there is also in-place coercion, using coerce!.","category":"section"},{"location":"working_with_categorical_data/#Tracking-all-levels","page":"Working with Categorical Data","title":"Tracking all levels","text":"The key property of vectors of scientific type OrderedFactor and  Multiclass is that the pool of all levels is not lost when separating out one or more elements:\n\nv = Xnew.rating\n\nlevels(v)\n\nlevels(v[1:2])\n\nlevels(v[2])\n\nBy tracking all classes in this way, MLJ avoids common pain points around categorical data, such as evaluating models on an evaluation set, only to crash your code because classes appear there which were not seen during training.\n\nBy drawing test, validation and training data from a common data structure (as described in Getting Started, for example) one ensures that all possible classes of categorical variables are tracked at all times. However, this does not mitigate problems with new production data, if categorical features there are missing classes or contain previously unseen classes.","category":"section"},{"location":"working_with_categorical_data/#New-or-missing-levels-in-production-data","page":"Working with Categorical Data","title":"New or missing levels in production data","text":"warning: Warning\nUnpredictable behavior may result whenever Finite categorical data presents in a production set with different classes (levels) from those presented during training\n\nConsider, for example, the following naive workflow:\n\n# train a one-hot encoder on some data:\nx = coerce([\"black\", \"white\", \"white\", \"black\"], Multiclass)\nX = DataFrame(x=x)\n\nmodel = OneHotEncoder()\nmach = machine(model, X) |> fit!\n\n# one-hot encode new data with missing classes:\nxproduction = coerce([\"white\", \"white\"], Multiclass)\nXproduction = DataFrame(x=xproduction)\nXproduction == X[2:3,:]\n\nSo far, so good. But the following operation throws an error:\n\njulia> transform(mach, Xproduction) == transform(mach, X[2:3,:])\nERROR: Found category level mismatch in feature `x`. Consider using `levels!` to ensure fitted and transforming features have the same category levels.\n\nThe problem here is that levels(X.x) and levels(Xproduction.x) are different:\n\nlevels(X.x)\n\nlevels(Xproduction.x)\n\nThis could be anticipated by the fact that the training and production data have different schema:\n\nschema(X)\n\nschema(Xproduction)\n\nOne fix is to manually correct the levels of the production data:\n\nlevels!(Xproduction.x, levels(x))\ntransform(mach, Xproduction) == transform(mach, X[2:3,:])\n\nAnother solution is to pack all production data with dummy rows based on the training data (subsequently dropped) to ensure there are no missing classes. Currently, MLJ contains no general tooling to check and fix categorical levels in production data (although one can check that training data and production data have the same schema, to ensure the number of classes in categorical data is consistent).","category":"section"},{"location":"working_with_categorical_data/#Extracting-an-integer-representation-of-Finite-data","page":"Working with Categorical Data","title":"Extracting an integer representation of Finite data","text":"Occasionally, you may really want an integer representation of data that currently has scitype Finite. For example, you are a developer wrapping an algorithm from an external package for use in MLJ, and that algorithm uses integer representations. Use the int method for this purpose, and use decoder to construct decoders for reversing the transformation:\n\nv = coerce([\"one\", \"two\", \"three\", \"one\"], OrderedFactor);\nlevels!(v, [\"one\", \"two\", \"three\"]);\nv_int = int(v)\n\nd = decoder(v); # or decoder(v[1])\nd.(v_int)","category":"section"},{"location":"working_with_categorical_data/#Under-the-hood:-CategoricalValue-and-CategoricalArray","page":"Working with Categorical Data","title":"Under the hood: CategoricalValue and CategoricalArray","text":"In MLJ the objects with OrderedFactor or Multiclass scientific type have machine type CategoricalValue, from the CategoricalArrays.jl package. In some sense CategoricalValues are an implementation detail users can ignore for the most part, as shown above. However, you may want some basic understanding of these types, and those implementing MLJ's model interface for new algorithms will have to understand them. For the complete API, see the CategoricalArrays.jl documentation. Here are the basics:\n\nTo construct an OrderedFactor or Multiclass vector directly from raw labels, one uses categorical:\n\nusing CategoricalArrays # hide\nv = categorical(['A', 'B', 'A', 'A', 'C'])\ntypeof(v)\n\n(Equivalent to the idiomatically MLJ v = coerce(['A', 'B', 'A', 'A', 'C']), Multiclass).)\n\nscitype(v)\n\nv = categorical(['A', 'B', 'A', 'A', 'C'], ordered=true, compress=true)\n\nscitype(v)\n\nWhen you index a CategoricalVector you don't get a raw label, but instead an instance of CategoricalValue. As explained above, this value knows the complete pool of levels from the vector from which it came. Use get(val) to extract the raw label from a value val.\n\nDespite the distinction that exists between a value (element) and a label, the two are the same, from the point of == and in:\n\nv[1] == 'A' # true\n'A' in v    # true","category":"section"},{"location":"working_with_categorical_data/#Probabilistic-predictions-of-categorical-data","page":"Working with Categorical Data","title":"Probabilistic predictions of categorical data","text":"Recall from Getting Started that probabilistic classifiers ordinarily predict UnivariateFinite distributions, not raw probabilities (which are instead accessed using the pdf method.) Here's how to construct such a distribution yourself:\n\nv = coerce([\"yes\", \"no\", \"yes\", \"yes\", \"maybe\"], Multiclass)\nd = UnivariateFinite([v[2], v[1]], [0.9, 0.1])\n\nOr, equivalently,\n\nd = UnivariateFinite([\"no\", \"yes\"], [0.9, 0.1], pool=v)\n\nThis distribution tracks all levels, not just the ones to which you have assigned probabilities:\n\npdf(d, \"maybe\")\n\nHowever, pdf(d, \"dunno\") will throw an error.\n\nYou can declare pool=missing, but then \"maybe\" will not be tracked:\n\nd = UnivariateFinite([\"no\", \"yes\"], [0.9, 0.1], pool=missing)\nlevels(d)\n\nTo construct a whole vector of UnivariateFinite distributions, simply give the constructor a matrix of probabilities:\n\nyes_probs = rand(5)\nprobs = hcat(1 .- yes_probs, yes_probs)\nd_vec = UnivariateFinite([\"no\", \"yes\"], probs, pool=v)\n\nOr, equivalently:\n\nd_vec = UnivariateFinite([\"no\", \"yes\"], yes_probs, augment=true, pool=v)\n\nFor more options, see UnivariateFinite.","category":"section"},{"location":"working_with_categorical_data/#Reference","page":"Working with Categorical Data","title":"Reference","text":"","category":"section"},{"location":"working_with_categorical_data/#CategoricalDistributions.UnivariateFinite","page":"Working with Categorical Data","title":"CategoricalDistributions.UnivariateFinite","text":"UnivariateFinite(support, probs; pool=nothing, augmented=false, ordered=false)\n\nConstruct a discrete univariate distribution whose finite support is\nthe elements of the vector `support`, and whose corresponding\nprobabilities are elements of the vector `probs`. Alternatively,\nconstruct an abstract *array* of `UnivariateFinite` distributions by\nchoosing `probs` to be an array of one higher dimension than the array\ngenerated.\n\n`UnivariateFinite` objects can also be constructed from dictionaries; see below. \n\nHere \"probabilities\" need not sum to one. The only requirement is that the probabilities\nhave a common type `T` for which `zero(T)` is defined. In particular, `UnivariateFinite`\nobjects implement arbitrary non-negative, signed, or complex measures over finite sets of\nlabeled points. A `UnivariateDistribution` will be a bona fide probability measure when\nconstructed using the `augment=true` option (see below) or when `fit` to data. And to\nsupport sampling with `rand`, probabilities must have a type that implements `>` and `+`\nand these probabilities must be non-negative, support addition, and not all zero, for\n\nUnless `pool` is specified, `support` should have type\n `AbstractVector{<:CategoricalValue}` and all elements are assumed to\n share the same categorical pool, which may be larger than `support`.\n\n*Important.* All levels of the common pool have associated\nprobabilities, not just those in the specified `support`. However,\nthese probabilities are always zero (see example below).\n\nIf `probs` is a matrix, it should have a column for each class (level) in `support` (or\none less, if `augment=true`). More generally, `probs` will be an array whose size is of\nthe form `(n1, n2, ..., nk, c)`, where `c = length(support)` (or one less, if\n`augment=true`) and the constructor then returns an array of `UnivariateFinite`\ndistributions of size `(n1, n2, ..., nk)`.\n\n\njulia-repl julia> using CategoricalDistributions, CategoricalArrays, Distributions\n\njulia> samples = categorical(['x', 'x', 'y', 'x', 'z']);\n\njulia> Distributions.fit(UnivariateFinite, samples) UnivariateFinite{Multiclass{3}}(x=>0.6, y=>0.2, z=>0.2)            UnivariateFinite{Multiclass{3}}      ┌                                        ┐    x ┤■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■ 0.6    y ┤■■■■■■■■■■■■ 0.2    z ┤■■■■■■■■■■■■ 0.2      └                                        ┘\n\njulia> d = UnivariateFinite([samples[1], samples[end]], [0.1, 0.9]) UnivariateFinite{Multiclass{3}(x=>0.1, z=>0.9)            UnivariateFinite{Multiclass{3}}      ┌                                        ┐    x ┤■■■■ 0.1    z ┤■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■ 0.9      └                                        ┘\n\njulia> rand(d, 3) 3-element Vector{CategoricalValue{Char, UInt32}}:  'z'  'z'  'z'\n\njulia> levels(samples) 3-element CategoricalArray{Char,1,UInt32}:  'x'  'y'  'z'\n\njulia> pdf(d, 'y') 0.0\n\n\n# Specifying a pool\n\nAlternatively, `support` may be a list of raw (non-categorical)\nelements if `pool` is:\n\n- some `CategoricalArray`, `CategoricalValue` or `CategoricalPool`,\n  such that `support` is a subset of `levels(pool)`\n\n- `missing`, in which case a new categorical pool is created which has\n  `support` as its only levels.\n\nIn the last case, specify `ordered=true` if the pool is to be\nconsidered ordered.\n\n\njulia-repl julia> UnivariateFinite(['x', 'z'], [0.1, 0.9], pool=missing, ordered=true) UnivariateFinite{OrderedFactor{2}}(x=>0.1, z=>0.9)          UnivariateFinite{OrderedFactor{2}}      ┌                                        ┐    x ┤■■■■ 0.1    z ┤■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■ 0.9      └                                        ┘\n\njulia> samples = categorical(['x', 'x', 'y', 'x', 'z']); julia> d = UnivariateFinite(['x', 'z'], [0.1, 0.9], pool=samples) UnivariateFinite{Multiclass{3}}(x=>0.1, z=>0.9)\n\n ┌                                        ┐\n\nx ┤■■■■ 0.1    z ┤■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■ 0.9      └                                        ┘\n\njulia> pdf(d, 'y') # allowed as 'y' in levels(samples) 0.0\n\njulia> v = categorical(['x', 'x', 'y', 'x', 'z', 'w']);\n\njulia> probs = rand(100, 3); julia> probs = probs ./ sum(probs, dims=2);\n\njulia> d1 = UnivariateFinite(['x', 'y', 'z'], probs, pool=v) 100-element UnivariateFiniteVector{Multiclass{4}, Char, UInt32, Float64}:  UnivariateFinite{Multiclass{4}}(x=>0.194, y=>0.3, z=>0.505)  UnivariateFinite{Multiclass{4}}(x=>0.727, y=>0.234, z=>0.0391)  UnivariateFinite{Multiclass{4}}(x=>0.674, y=>0.00535, z=>0.321)    ⋮  UnivariateFinite{Multiclass{4}}(x=>0.292, y=>0.339, z=>0.369)\n\n\n# Probability augmentation\n\nIf `augment=true` the provided array is augmented by inserting\nappropriate elements *ahead* of those provided, along the last\ndimension of the array. This means the user only provides probabilities\nfor the classes `c2, c3, ..., cn`. The class `c1` probabilities are\nchosen so that each `UnivariateFinite` distribution in the returned\narray is a bona fide probability distribution.\n\n\njulia-repl julia> UnivariateFinite([0.1, 0.2, 0.3], augment=true, pool=missing) 3-element UnivariateFiniteArray{Multiclass{2}, String, UInt8, Float64, 1}:  UnivariateFinite{Multiclass{2}}(class1=>0.9, class2=>0.1)  UnivariateFinite{Multiclass{2}}(class1=>0.8, class2=>0.2)  UnivariateFinite{Multiclass{2}}(class1=>0.7, class2=>0.3)\n\njulia> d2 = UnivariateFinite(['x', 'y', 'z'], probs[:, 2:end], augment=true, pool=v); julia> pdf(d1, levels(v)) ≈ pdf(d2, levels(v)) true ```\n\n\n\nUnivariateFinite(prob_given_class; pool=nothing, ordered=false)\n\nConstruct a discrete univariate distribution whose finite support is the set of keys of the provided dictionary, prob_given_class, and whose values specify the corresponding probabilities.\n\nThe type requirements on the keys of the dictionary are the same as the elements of support given above with this exception: if non-categorical elements (raw labels) are used as keys, then pool=... must be specified and cannot be missing.\n\nIf the values (probabilities) are arrays instead of scalars, then an abstract array of UnivariateFinite elements is created, with the same size as the array.\n\n\n\n\n\n","category":"type"},{"location":"models/COPODDetector_OutlierDetectionPython/#COPODDetector_OutlierDetectionPython","page":"COPODDetector","title":"COPODDetector","text":"COPODDetector(n_jobs = 1)\n\nhttps://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.copod","category":"section"},{"location":"models/MultitargetNeuralNetworkRegressor_BetaML/#MultitargetNeuralNetworkRegressor_BetaML","page":"MultitargetNeuralNetworkRegressor","title":"MultitargetNeuralNetworkRegressor","text":"mutable struct MultitargetNeuralNetworkRegressor <: MLJModelInterface.Deterministic\n\nA simple but flexible Feedforward Neural Network, from the Beta Machine Learning Toolkit (BetaML) for regression of multiple dimensional targets.","category":"section"},{"location":"models/MultitargetNeuralNetworkRegressor_BetaML/#Parameters:","page":"MultitargetNeuralNetworkRegressor","title":"Parameters:","text":"layers: Array of layer objects [def: nothing, i.e. basic network]. See subtypes(BetaML.AbstractLayer) for supported layers\nloss: Loss (cost) function [def: BetaML.squared_cost].  Should always assume y and ŷ as matrices.\nwarning: Warning\nIf you change the parameter loss, you need to either provide its derivative on the parameter dloss or use autodiff with dloss=nothing.\ndloss: Derivative of the loss function [def: BetaML.dsquared_cost, i.e. use the derivative of the squared cost]. Use nothing for autodiff.\nepochs: Number of epochs, i.e. passages trough the whole training sample [def: 300]\nbatch_size: Size of each individual batch [def: 16]\nopt_alg: The optimisation algorithm to update the gradient at each batch [def: BetaML.ADAM()]. See subtypes(BetaML.OptimisationAlgorithm) for supported optimizers\nshuffle: Whether to randomly shuffle the data at each iteration (epoch) [def: true]\ndescr: An optional title and/or description for this model\ncb: A call back function to provide information during training [def: BetaML.fitting_info]\nrng: Random Number Generator (see FIXEDSEED) [deafult: Random.GLOBAL_RNG]","category":"section"},{"location":"models/MultitargetNeuralNetworkRegressor_BetaML/#Notes:","page":"MultitargetNeuralNetworkRegressor","title":"Notes:","text":"data must be numerical\nthe label should be a n-records by n-dimensions matrix","category":"section"},{"location":"models/MultitargetNeuralNetworkRegressor_BetaML/#Example:","page":"MultitargetNeuralNetworkRegressor","title":"Example:","text":"julia> using MLJ\n\njulia> X, y        = @load_boston;\n\njulia> ydouble     = hcat(y, y .*2  .+5);\n\njulia> modelType   = @load MultitargetNeuralNetworkRegressor pkg = \"BetaML\" verbosity=0\nBetaML.Nn.MultitargetNeuralNetworkRegressor\n\njulia> layers                      = [BetaML.DenseLayer(12,50,f=BetaML.relu),BetaML.DenseLayer(50,50,f=BetaML.relu),BetaML.DenseLayer(50,50,f=BetaML.relu),BetaML.DenseLayer(50,2,f=BetaML.relu)];\n\njulia> model       = modelType(layers=layers,opt_alg=BetaML.ADAM(),epochs=500)\nMultitargetNeuralNetworkRegressor(\n  layers = BetaML.Nn.AbstractLayer[BetaML.Nn.DenseLayer([-0.2591582523441157 -0.027962845131416225 … 0.16044535560124418 -0.12838827994676857; -0.30381834909561184 0.2405495243851402 … -0.2588144861880588 0.09538577909777807; … ; -0.017320292924711156 -0.14042266424603767 … 0.06366999105841187 -0.13419651752478906; 0.07393079961409338 0.24521350531110264 … 0.04256867886217541 -0.0895506802948175], [0.14249427336553644, 0.24719379413682485, -0.25595911822556566, 0.10034088778965933, -0.017086404878505712, 0.21932184025609347, -0.031413516834861266, -0.12569076082247596, -0.18080140982481183, 0.14551901873323253  …  -0.13321995621967364, 0.2436582233332092, 0.0552222336976439, 0.07000814133633904, 0.2280064379660025, -0.28885681475734193, -0.07414214246290696, -0.06783184733650621, -0.055318068046308455, -0.2573488383282579], BetaML.Utils.relu, BetaML.Utils.drelu), BetaML.Nn.DenseLayer([-0.0395424111703751 -0.22531232360829911 … -0.04341228943744482 0.024336206858365517; -0.16481887432946268 0.17798073384748508 … -0.18594039305095766 0.051159225856547474; … ; -0.011639475293705043 -0.02347011206244673 … 0.20508869536159186 -0.1158382446274592; -0.19078069527757857 -0.007487540070740484 … -0.21341165344291158 -0.24158671316310726], [-0.04283623889330032, 0.14924461547060602, -0.17039563392959683, 0.00907774027816255, 0.21738885963113852, -0.06308040225941691, -0.14683286822101105, 0.21726892197970937, 0.19784321784707126, -0.0344988665714947  …  -0.23643089430602846, -0.013560425201427584, 0.05323948910726356, -0.04644175812567475, -0.2350400292671211, 0.09628312383424742, 0.07016420995205697, -0.23266392927140334, -0.18823664451487, 0.2304486691429084], BetaML.Utils.relu, BetaML.Utils.drelu), BetaML.Nn.DenseLayer([-0.11504184627266828 0.08601794194664503 … 0.03843129724045469 -0.18417305624127284; 0.10181551438831654 0.13459759904443674 … 0.11094951365942118 -0.1549466590355218; … ; 0.15279817525427697 0.0846661196058916 … -0.07993619892911122 0.07145402617285884; -0.1614160186346092 -0.13032002335149 … -0.12310552194729624 -0.15915773071049827], [-0.03435885900946367, -0.1198543931290306, 0.008454985905194445, -0.17980887188986966, -0.03557204910359624, 0.19125847393334877, -0.10949700778538696, -0.09343206702591, -0.12229583511781811, -0.09123969069220564  …  0.22119233518322862, 0.2053873143308657, 0.12756489387198222, 0.11567243705173319, -0.20982445664020496, 0.1595157838386987, -0.02087331046544119, -0.20556423263489765, -0.1622837764237961, -0.019220998739847395], BetaML.Utils.relu, BetaML.Utils.drelu), BetaML.Nn.DenseLayer([-0.25796717031347993 0.17579536633402948 … -0.09992960168785256 -0.09426177454620635; -0.026436330246675632 0.18070899284865127 … -0.19310119102392206 -0.06904005900252091], [0.16133004882307822, -0.3061228721091248], BetaML.Utils.relu, BetaML.Utils.drelu)], \n  loss = BetaML.Utils.squared_cost, \n  dloss = BetaML.Utils.dsquared_cost, \n  epochs = 500, \n  batch_size = 32, \n  opt_alg = BetaML.Nn.ADAM(BetaML.Nn.var\"#90#93\"(), 1.0, 0.9, 0.999, 1.0e-8, BetaML.Nn.Learnable[], BetaML.Nn.Learnable[]), \n  shuffle = true, \n  descr = \"\", \n  cb = BetaML.Nn.fitting_info, \n  rng = Random._GLOBAL_RNG())\n\njulia> mach        = machine(model, X, ydouble);\n\njulia> fit!(mach);\n\njulia> ŷdouble    = predict(mach, X);\n\njulia> hcat(ydouble,ŷdouble)\n506×4 Matrix{Float64}:\n 24.0  53.0  28.4624  62.8607\n 21.6  48.2  22.665   49.7401\n 34.7  74.4  31.5602  67.9433\n 33.4  71.8  33.0869  72.4337\n  ⋮                   \n 23.9  52.8  23.3573  50.654\n 22.0  49.0  22.1141  48.5926\n 11.9  28.8  19.9639  45.5823","category":"section"},{"location":"models/MultinomialNBClassifier_MLJScikitLearnInterface/#MultinomialNBClassifier_MLJScikitLearnInterface","page":"MultinomialNBClassifier","title":"MultinomialNBClassifier","text":"MultinomialNBClassifier\n\nA model type for constructing a multinomial naive Bayes classifier, based on MLJScikitLearnInterface.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nMultinomialNBClassifier = @load MultinomialNBClassifier pkg=MLJScikitLearnInterface\n\nDo model = MultinomialNBClassifier() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in MultinomialNBClassifier(alpha=...).\n\nMultinomial naive bayes classifier. It is suitable for classification with discrete features (e.g. word counts for text classification).","category":"section"},{"location":"models/LarsRegressor_MLJScikitLearnInterface/#LarsRegressor_MLJScikitLearnInterface","page":"LarsRegressor","title":"LarsRegressor","text":"LarsRegressor\n\nA model type for constructing a least angle regressor (LARS), based on MLJScikitLearnInterface.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nLarsRegressor = @load LarsRegressor pkg=MLJScikitLearnInterface\n\nDo model = LarsRegressor() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in LarsRegressor(fit_intercept=...).","category":"section"},{"location":"models/LarsRegressor_MLJScikitLearnInterface/#Hyper-parameters","page":"LarsRegressor","title":"Hyper-parameters","text":"fit_intercept = true\nverbose = false\nprecompute = auto\nn_nonzero_coefs = 500\neps = 2.220446049250313e-16\ncopy_X = true\nfit_path = true","category":"section"},{"location":"models/LOFDetector_OutlierDetectionNeighbors/#LOFDetector_OutlierDetectionNeighbors","page":"LOFDetector","title":"LOFDetector","text":"LOFDetector(k = 5,\n            metric = Euclidean(),\n            algorithm = :kdtree,\n            leafsize = 10,\n            reorder = true,\n            parallel = false)\n\nCalculate an anomaly score based on the density of an instance in comparison to its neighbors. This algorithm introduced the notion of local outliers and was developed by Breunig et al., see [1].","category":"section"},{"location":"models/LOFDetector_OutlierDetectionNeighbors/#Parameters","page":"LOFDetector","title":"Parameters","text":"k::Integer\n\nNumber of neighbors (must be greater than 0).\n\nmetric::Metric\n\nThis is one of the Metric types defined in the Distances.jl package. It is possible to define your own metrics by creating new types that are subtypes of Metric.\n\nalgorithm::Symbol\n\nOne of (:kdtree, :balltree). In a kdtree, points are recursively split into groups using hyper-planes. Therefore a KDTree only works with axis aligned metrics which are: Euclidean, Chebyshev, Minkowski and Cityblock. A brutetree linearly searches all points in a brute force fashion and works with any Metric. A balltree recursively splits points into groups bounded by hyper-spheres and works with any Metric.\n\nstatic::Union{Bool, Symbol}\n\nOne of (true, false, :auto). Whether the input data for fitting and transform should be statically or dynamically allocated. If true, the data is statically allocated. If false, the data is dynamically allocated. If :auto, the data is dynamically allocated if the product of all dimensions except the last is greater than 100.\n\nleafsize::Int\n\nDetermines at what number of points to stop splitting the tree further. There is a trade-off between traversing the tree and having to evaluate the metric function for increasing number of points.\n\nreorder::Bool\n\nWhile building the tree this will put points close in distance close in memory since this helps with cache locality. In this case, a copy of the original data will be made so that the original data is left unmodified. This can have a significant impact on performance and is by default set to true.\n\nparallel::Bool\n\nParallelize score and predict using all threads available. The number of threads can be set with the JULIA_NUM_THREADS environment variable. Note: fit is not parallel.","category":"section"},{"location":"models/LOFDetector_OutlierDetectionNeighbors/#Examples","page":"LOFDetector","title":"Examples","text":"using OutlierDetection: LOFDetector, fit, transform\ndetector = LOFDetector()\nX = rand(10, 100)\nmodel, result = fit(detector, X; verbosity=0)\ntest_scores = transform(detector, model, X)","category":"section"},{"location":"models/LOFDetector_OutlierDetectionNeighbors/#References","page":"LOFDetector","title":"References","text":"[1] Breunig, Markus M.; Kriegel, Hans-Peter; Ng, Raymond T.; Sander, Jörg (2000): LOF: Identifying Density-Based Local Outliers.","category":"section"},{"location":"models/AdaBoostClassifier_MLJScikitLearnInterface/#AdaBoostClassifier_MLJScikitLearnInterface","page":"AdaBoostClassifier","title":"AdaBoostClassifier","text":"AdaBoostClassifier\n\nA model type for constructing a ada boost classifier, based on MLJScikitLearnInterface.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nAdaBoostClassifier = @load AdaBoostClassifier pkg=MLJScikitLearnInterface\n\nDo model = AdaBoostClassifier() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in AdaBoostClassifier(estimator=...).\n\nAn AdaBoost  classifier is a meta-estimator that begins by fitting a  classifier on the original dataset and then fits additional copies of  the classifier on the same dataset but where the weights of incorrectly  classified instances are adjusted such that subsequent classifiers  focus more on difficult cases.\n\nThis class implements the algorithm known as AdaBoost-SAMME.","category":"section"},{"location":"models/AffinityPropagation_Clustering/#AffinityPropagation_Clustering","page":"AffinityPropagation","title":"AffinityPropagation","text":"AffinityPropagation\n\nA model type for constructing a Affinity Propagation clusterer, based on Clustering.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nAffinityPropagation = @load AffinityPropagation pkg=Clustering\n\nDo model = AffinityPropagation() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in AffinityPropagation(damp=...).\n\nAffinity Propagation is a clustering algorithm based on the concept of \"message passing\" between data points. More information is available at the Clustering.jl documentation. Use predict to get cluster assignments. Indices of the exemplars, their values, etc, are accessed from the machine report (see below).\n\nThis is a static implementation, i.e., it does not generalize to new data instances, and there is no training data. For clusterers that do generalize, see KMeans or KMedoids.\n\nIn MLJ or MLJBase, create a machine with\n\nmach = machine(model)","category":"section"},{"location":"models/AffinityPropagation_Clustering/#Hyper-parameters","page":"AffinityPropagation","title":"Hyper-parameters","text":"damp = 0.5: damping factor\nmaxiter = 200: maximum number of iteration\ntol = 1e-6: tolerance for converenge\npreference = nothing: the (single float) value of the diagonal elements of the similarity matrix. If unspecified, choose median (negative) similarity of all pairs as mentioned here\nmetric = Distances.SqEuclidean(): metric (see Distances.jl for available metrics)","category":"section"},{"location":"models/AffinityPropagation_Clustering/#Operations","page":"AffinityPropagation","title":"Operations","text":"predict(mach, X): return cluster label assignments, as an unordered CategoricalVector. Here X is any table of input features (eg, a DataFrame) whose columns are of scitype Continuous; check column scitypes with schema(X).","category":"section"},{"location":"models/AffinityPropagation_Clustering/#Report","page":"AffinityPropagation","title":"Report","text":"After calling predict(mach), the fields of report(mach)  are:\n\nexemplars: indices of the data picked as exemplars in X\ncenters: positions of the exemplars in the feature space\ncluster_labels: labels of clusters given to each datum in X\niterations: the number of iteration run by the algorithm\nconverged: whether or not the algorithm converges by the maximum iteration","category":"section"},{"location":"models/AffinityPropagation_Clustering/#Examples","page":"AffinityPropagation","title":"Examples","text":"using MLJ\n\nX, labels = make_moons(400, noise=0.9, rng=1)\n\nAffinityPropagation = @load AffinityPropagation pkg=Clustering\nmodel = AffinityPropagation(preference=-10.0)\nmach = machine(model)\n\n## compute and output cluster assignments for observations in `X`:\nyhat = predict(mach, X)\n\n## Get the positions of the exemplars\nreport(mach).centers\n\n## Plot clustering result\nusing GLMakie\nscatter(MLJ.matrix(X)', color=yhat.refs)","category":"section"},{"location":"models/SVMLinearClassifier_MLJScikitLearnInterface/#SVMLinearClassifier_MLJScikitLearnInterface","page":"SVMLinearClassifier","title":"SVMLinearClassifier","text":"SVMLinearClassifier\n\nA model type for constructing a linear support vector classifier, based on MLJScikitLearnInterface.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nSVMLinearClassifier = @load SVMLinearClassifier pkg=MLJScikitLearnInterface\n\nDo model = SVMLinearClassifier() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in SVMLinearClassifier(penalty=...).","category":"section"},{"location":"models/SVMLinearClassifier_MLJScikitLearnInterface/#Hyper-parameters","page":"SVMLinearClassifier","title":"Hyper-parameters","text":"penalty = l2\nloss = squared_hinge\ndual = true\ntol = 0.0001\nC = 1.0\nmulti_class = ovr\nfit_intercept = true\nintercept_scaling = 1.0\nrandom_state = nothing\nmax_iter = 1000","category":"section"},{"location":"models/StableForestClassifier_SIRUS/#StableForestClassifier_SIRUS","page":"StableForestClassifier","title":"StableForestClassifier","text":"StableForestClassifier\n\nA model type for constructing a stable forest classifier, based on SIRUS.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nStableForestClassifier = @load StableForestClassifier pkg=SIRUS\n\nDo model = StableForestClassifier() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in StableForestClassifier(rng=...).\n\nStableForestClassifier implements the random forest classifier with a stabilized forest structure (Bénard et al., 2021). This stabilization increases stability when extracting rules. The impact on the predictive accuracy compared to standard random forests should be relatively small.\n\nnote: Note\nJust like normal random forests, this model is not easily explainable. If you are interested in an explainable model, use the StableRulesClassifier or StableRulesRegressor.","category":"section"},{"location":"models/StableForestClassifier_SIRUS/#Training-data","page":"StableForestClassifier","title":"Training data","text":"In MLJ or MLJBase, bind an instance model to data with\n\nmach = machine(model, X, y)\n\nwhere\n\nX: any table of input features (eg, a DataFrame) whose columns each have one of the following element scitypes: Continuous, Count, or <:OrderedFactor; check column scitypes with schema(X)\ny: the target, which can be any AbstractVector whose element scitype is <:OrderedFactor or <:Multiclass; check the scitype with scitype(y)\n\nTrain the machine with fit!(mach, rows=...).","category":"section"},{"location":"models/StableForestClassifier_SIRUS/#Hyperparameters","page":"StableForestClassifier","title":"Hyperparameters","text":"rng::AbstractRNG=default_rng(): Random number generator.   Using a StableRNG from StableRNGs.jl is advised.\npartial_sampling::Float64=0.7:   Ratio of samples to use in each subset of the data.   The default should be fine for most cases.\nn_trees::Int=1000:   The number of trees to use.   It is advisable to use at least thousand trees to for a better rule selection, and   in turn better predictive performance.\nmax_depth::Int=2:   The depth of the tree.   A lower depth decreases model complexity and can therefore improve accuracy when the sample size is small (reduce overfitting).\nq::Int=10: Number of cutpoints to use per feature.   The default value should be fine for most situations.\nmin_data_in_leaf::Int=5: Minimum number of data points per leaf.","category":"section"},{"location":"models/StableForestClassifier_SIRUS/#Fitted-parameters","page":"StableForestClassifier","title":"Fitted parameters","text":"The fields of fitted_params(mach) are:\n\nfitresult: A StableForest object.","category":"section"},{"location":"models/StableForestClassifier_SIRUS/#Operations","page":"StableForestClassifier","title":"Operations","text":"predict(mach, Xnew): Return a vector of predictions for each row of Xnew.","category":"section"},{"location":"models/TunedModel_MLJTuning/#TunedModel_MLJTuning","page":"TunedModel","title":"TunedModel","text":"tuned_model = TunedModel(; model=<model to be mutated>,\n                         tuning=RandomSearch(),\n                         resampling=Holdout(),\n                         range=nothing,\n                         measure=nothing,\n                         n=default_n(tuning, range),\n                         operation=nothing,\n                         other_options...)\n\nConstruct a model wrapper for hyper-parameter optimization of a supervised learner, specifying the tuning strategy and model whose hyper-parameters are to be mutated.\n\ntuned_model = TunedModel(; models=<models to be compared>,\n                         resampling=Holdout(),\n                         measure=nothing,\n                         n=length(models),\n                         operation=nothing,\n                         other_options...)\n\nConstruct a wrapper for multiple models, for selection of an optimal one (equivalent to specifying tuning=Explicit() and range=models above). Elements of the iterator models need not have a common type, but they must all be Deterministic or all be Probabilistic and this is not checked but inferred from the first element generated.\n\nSee below for a complete list of options.","category":"section"},{"location":"models/TunedModel_MLJTuning/#Training","page":"TunedModel","title":"Training","text":"Calling fit!(mach) on a machine mach=machine(tuned_model, X, y) or mach=machine(tuned_model, X, y, w) will:\n\nInstigate a search, over clones of model, with the hyperparameter mutations specified by range, for a model optimizing the specified measure, using performance evaluations carried out using the specified tuning strategy and resampling strategy. In the case models is explictly listed, the search is instead over the models generated by the iterator models.\nFit an internal machine, based on the optimal model fitted_params(mach).best_model, wrapping the optimal model object in all the provided data X, y(, w). Calling predict(mach, Xnew) then returns predictions on Xnew of this internal machine. The final train can be supressed by setting train_best=false.","category":"section"},{"location":"models/TunedModel_MLJTuning/#Search-space","page":"TunedModel","title":"Search space","text":"The range objects supported depend on the tuning strategy specified. Query the strategy docstring for details. To optimize over an explicit list v of models of the same type, use strategy=Explicit() and specify model=v[1] and range=v.\n\nThe number of models searched is specified by n. If unspecified, then MLJTuning.default_n(tuning, range) is used. When n is increased and fit!(mach) called again, the old search history is re-instated and the search continues where it left off.","category":"section"},{"location":"models/TunedModel_MLJTuning/#Measures-(metrics)","page":"TunedModel","title":"Measures (metrics)","text":"If more than one measure is specified, then only the first is optimized (unless strategy is multi-objective) but the performance against every measure specified will be computed and reported in report(mach).best_performance and other relevant attributes of the generated report. Options exist to pass per-observation weights or class weights to measures; see below.\n\nImportant. If a custom measure, my_measure is used, and the measure is a score, rather than a loss, be sure to check that MLJ.orientation(my_measure) == :score to ensure maximization of the measure, rather than minimization. Override an incorrect value with MLJ.orientation(::typeof(my_measure)) = :score.","category":"section"},{"location":"models/TunedModel_MLJTuning/#Accessing-the-fitted-parameters-and-other-training-(tuning)-outcomes","page":"TunedModel","title":"Accessing the fitted parameters and other training (tuning) outcomes","text":"A Plots.jl plot of performance estimates is returned by plot(mach) or heatmap(mach).\n\nOnce a tuning machine mach has bee trained as above, then fitted_params(mach) has these keys/values:\n\nkey value\nbest_model optimal model instance\nbest_fitted_params learned parameters of the optimal model\n\nThe named tuple report(mach) includes these keys/values:\n\nkey value\nbest_model optimal model instance\nbest_history_entry corresponding entry in the history, including performance estimate\nbest_report report generated by fitting the optimal model to all data\nhistory tuning strategy-specific history of all evaluations\n\nplus other key/value pairs specific to the tuning strategy.\n\nEach element of history is a property-accessible object with these properties:\n\nkey value\nmeasure vector of measures (metrics)\nmeasurement vector of measurements, one per measure\nper_fold vector of vectors of unaggregated per-fold measurements\nevaluation full PerformanceEvaluation/CompactPerformaceEvaluation object","category":"section"},{"location":"models/TunedModel_MLJTuning/#Complete-list-of-key-word-options","page":"TunedModel","title":"Complete list of key-word options","text":"model: Supervised model prototype that is cloned and mutated to generate models for evaluation\nmodels: Alternatively, an iterator of MLJ models to be explicitly evaluated. These may have varying types.\ntuning=RandomSearch(): tuning strategy to be applied (eg, Grid()). See the Tuning Models section of the MLJ manual for a complete list of options.\nresampling=Holdout(): resampling strategy (eg, Holdout(), CV()), StratifiedCV()) to be applied in performance evaluations\nmeasure: measure or measures to be applied in performance evaluations; only the first used in optimization (unless the strategy is multi-objective) but all reported to the history\nweights: per-observation weights to be passed the measure(s) in performance evaluations, where supported. Check support with supports_weights(measure).\nclass_weights: class weights to be passed the measure(s) in performance evaluations, where supported. Check support with supports_class_weights(measure).\nrepeats=1: for generating train/test sets multiple times in resampling (\"Monte Carlo\" resampling); see evaluate! for details\noperation/operations - One of predict, predict_mean, predict_mode, predict_median, or predict_joint, or a vector of these of the same length as measure/measures. Automatically inferred if left unspecified.\nrange: range object; tuning strategy documentation describes supported types\nselection_heuristic: the rule determining how the best model is decided. According to the default heuristic, NaiveSelection(), measure (or the first element of measure) is evaluated for each resample and these per-fold measurements are aggregrated. The model with the lowest (resp. highest) aggregate is chosen if the measure is a :loss (resp. a :score).\nn: number of iterations (ie, models to be evaluated); set by tuning strategy if left unspecified\ntrain_best=true: whether to train the optimal model\nacceleration=default_resource(): mode of parallelization for tuning strategies that support this\nacceleration_resampling=CPU1(): mode of parallelization for resampling\ncheck_measure=true: whether to check measure is compatible with the specified model and operation)\ncache=true: whether to cache model-specific representations of user-suplied data; set to false to conserve memory. Speed gains likely limited to the case resampling isa Holdout.\ncompact_history=true: whether to write CompactPerformanceEvaluation](@ref) or regular PerformanceEvaluation objects to the history (accessed via the :evaluation key); the compact form excludes some fields to conserve memory.\nlogger=default_logger(): a logger for externally reporting model performance evaluations, such as an MLJFlow.Logger instance. On startup, default_logger()=nothing; use default_logger(logger) to set a global logger.","category":"section"},{"location":"models/RidgeCVRegressor_MLJScikitLearnInterface/#RidgeCVRegressor_MLJScikitLearnInterface","page":"RidgeCVRegressor","title":"RidgeCVRegressor","text":"RidgeCVRegressor\n\nA model type for constructing a ridge regressor with built-in cross-validation, based on MLJScikitLearnInterface.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nRidgeCVRegressor = @load RidgeCVRegressor pkg=MLJScikitLearnInterface\n\nDo model = RidgeCVRegressor() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in RidgeCVRegressor(alphas=...).","category":"section"},{"location":"models/RidgeCVRegressor_MLJScikitLearnInterface/#Hyper-parameters","page":"RidgeCVRegressor","title":"Hyper-parameters","text":"alphas = (0.1, 1.0, 10.0)\nfit_intercept = true\nscoring = nothing\ncv = 5\ngcv_mode = nothing\nstore_cv_values = false","category":"section"},{"location":"models/CountTransformer_MLJText/#CountTransformer_MLJText","page":"CountTransformer","title":"CountTransformer","text":"CountTransformer\n\nA model type for constructing a count transformer, based on MLJText.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nCountTransformer = @load CountTransformer pkg=MLJText\n\nDo model = CountTransformer() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in CountTransformer(max_doc_freq=...).\n\nThe transformer converts a collection of documents, tokenized or pre-parsed as bags of words/ngrams, to a matrix of term counts.","category":"section"},{"location":"models/CountTransformer_MLJText/#Training-data","page":"CountTransformer","title":"Training data","text":"In MLJ or MLJBase, bind an instance model to data with\n\nmach = machine(model, X)\n\nHere:\n\nX is any vector whose elements are either tokenized documents or bags of words/ngrams. Specifically, each element is one of the following:\nA vector of abstract strings (tokens), e.g., [\"I\", \"like\", \"Sam\", \".\", \"Sam\", \"is\", \"nice\", \".\"] (scitype AbstractVector{Textual})\nA dictionary of counts, indexed on abstract strings, e.g., Dict(\"I\"=>1, \"Sam\"=>2, \"Sam is\"=>1) (scitype Multiset{Textual}})\nA dictionary of counts, indexed on plain ngrams, e.g., Dict((\"I\",)=>1, (\"Sam\",)=>2, (\"I\", \"Sam\")=>1) (scitype Multiset{<:NTuple{N,Textual} where N}); here a plain ngram is a tuple of abstract strings.\n\nTrain the machine using fit!(mach, rows=...).","category":"section"},{"location":"models/CountTransformer_MLJText/#Hyper-parameters","page":"CountTransformer","title":"Hyper-parameters","text":"max_doc_freq=1.0: Restricts the vocabulary that the transformer will consider. Terms that occur in > max_doc_freq documents will not be considered by the transformer. For example, if max_doc_freq is set to 0.9, terms that are in more than 90% of the documents will be removed.\nmin_doc_freq=0.0: Restricts the vocabulary that the transformer will consider. Terms that occur in < max_doc_freq documents will not be considered by the transformer. A value of 0.01 means that only terms that are at least in 1% of the documents will be included.","category":"section"},{"location":"models/CountTransformer_MLJText/#Operations","page":"CountTransformer","title":"Operations","text":"transform(mach, Xnew): Based on the vocabulary learned in training, return the matrix of counts for Xnew, a vector of the same form as X above. The matrix has size (n, p), where n = length(Xnew) and p the size of the vocabulary. Tokens/ngrams not appearing in the learned vocabulary are scored zero.","category":"section"},{"location":"models/CountTransformer_MLJText/#Fitted-parameters","page":"CountTransformer","title":"Fitted parameters","text":"The fields of fitted_params(mach) are:\n\nvocab: A vector containing the string used in the transformer's vocabulary.","category":"section"},{"location":"models/CountTransformer_MLJText/#Examples","page":"CountTransformer","title":"Examples","text":"CountTransformer accepts a variety of inputs. The example below transforms tokenized documents:\n\nusing MLJ\nimport TextAnalysis\n\nCountTransformer = @load CountTransformer pkg=MLJText\n\ndocs = [\"Hi my name is Sam.\", \"How are you today?\"]\ncount_transformer = CountTransformer()\n\njulia> tokenized_docs = TextAnalysis.tokenize.(docs)\n2-element Vector{Vector{String}}:\n [\"Hi\", \"my\", \"name\", \"is\", \"Sam\", \".\"]\n [\"How\", \"are\", \"you\", \"today\", \"?\"]\n\nmach = machine(count_transformer, tokenized_docs)\nfit!(mach)\n\nfitted_params(mach)\n\ntfidf_mat = transform(mach, tokenized_docs)\n\nAlternatively, one can provide documents pre-parsed as ngrams counts:\n\nusing MLJ\nimport TextAnalysis\n\ndocs = [\"Hi my name is Sam.\", \"How are you today?\"]\ncorpus = TextAnalysis.Corpus(TextAnalysis.NGramDocument.(docs, 1, 2))\nngram_docs = TextAnalysis.ngrams.(corpus)\n\njulia> ngram_docs[1]\nDict{AbstractString, Int64} with 11 entries:\n  \"is\"      => 1\n  \"my\"      => 1\n  \"name\"    => 1\n  \".\"       => 1\n  \"Hi\"      => 1\n  \"Sam\"     => 1\n  \"my name\" => 1\n  \"Hi my\"   => 1\n  \"name is\" => 1\n  \"Sam .\"   => 1\n  \"is Sam\"  => 1\n\ncount_transformer = CountTransformer()\nmach = machine(count_transformer, ngram_docs)\nMLJ.fit!(mach)\nfitted_params(mach)\n\ntfidf_mat = transform(mach, ngram_docs)\n\nSee also TfidfTransformer, BM25Transformer","category":"section"},{"location":"models/RandomForestRegressor_DecisionTree/#RandomForestRegressor_DecisionTree","page":"RandomForestRegressor","title":"RandomForestRegressor","text":"RandomForestRegressor\n\nA model type for constructing a CART random forest regressor, based on DecisionTree.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nRandomForestRegressor = @load RandomForestRegressor pkg=DecisionTree\n\nDo model = RandomForestRegressor() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in RandomForestRegressor(max_depth=...).\n\nDecisionTreeRegressor implements the standard Random Forest algorithm, originally published in Breiman, L. (2001): \"Random Forests.\", Machine Learning, vol. 45, pp. 5–32","category":"section"},{"location":"models/RandomForestRegressor_DecisionTree/#Training-data","page":"RandomForestRegressor","title":"Training data","text":"In MLJ or MLJBase, bind an instance model to data with\n\nmach = machine(model, X, y)\n\nwhere\n\nX: any table of input features (eg, a DataFrame) whose columns each have one of the following element scitypes: Continuous, Count, or <:OrderedFactor; check column scitypes with schema(X)\ny: the target, which can be any AbstractVector whose element scitype is Continuous; check the scitype with scitype(y)\n\nTrain the machine with fit!(mach, rows=...).","category":"section"},{"location":"models/RandomForestRegressor_DecisionTree/#Hyperparameters","page":"RandomForestRegressor","title":"Hyperparameters","text":"max_depth=-1: max depth of the decision tree (-1=any)\nmin_samples_leaf=1: min number of samples each leaf needs to have\nmin_samples_split=2: min number of samples needed for a split\nmin_purity_increase=0: min purity needed for a split\nn_subfeatures=-1: number of features to select at random (0 for all, -1 for square root of number of features)\nn_trees=10: number of trees to train\nsampling_fraction=0.7  fraction of samples to train each tree on\nfeature_importance: method to use for computing feature importances. One of (:impurity, :split)\nrng=Random.GLOBAL_RNG: random number generator or seed","category":"section"},{"location":"models/RandomForestRegressor_DecisionTree/#Operations","page":"RandomForestRegressor","title":"Operations","text":"predict(mach, Xnew): return predictions of the target given new features Xnew having the same scitype as X above.","category":"section"},{"location":"models/RandomForestRegressor_DecisionTree/#Fitted-parameters","page":"RandomForestRegressor","title":"Fitted parameters","text":"The fields of fitted_params(mach) are:\n\nforest: the Ensemble object returned by the core DecisionTree.jl algorithm","category":"section"},{"location":"models/RandomForestRegressor_DecisionTree/#Report","page":"RandomForestRegressor","title":"Report","text":"The fields of report(mach) are:\n\nfeatures: the names of the features encountered in training","category":"section"},{"location":"models/RandomForestRegressor_DecisionTree/#Accessor-functions","page":"RandomForestRegressor","title":"Accessor functions","text":"feature_importances(mach) returns a vector of (feature::Symbol => importance) pairs; the type of importance is determined by the hyperparameter feature_importance (see above)","category":"section"},{"location":"models/RandomForestRegressor_DecisionTree/#Examples","page":"RandomForestRegressor","title":"Examples","text":"using MLJ\nForest = @load RandomForestRegressor pkg=DecisionTree\nforest = Forest(max_depth=4, min_samples_split=3)\n\nX, y = make_regression(100, 2) ## synthetic data\nmach = machine(forest, X, y) |> fit!\n\nXnew, _ = make_regression(3, 2)\nyhat = predict(mach, Xnew) ## new predictions\n\nfitted_params(mach).forest ## raw `Ensemble` object from DecisionTree.jl\nfeature_importances(mach)\n\nSee also DecisionTree.jl and the unwrapped model type MLJDecisionTreeInterface.DecisionTree.RandomForestRegressor.","category":"section"},{"location":"models/MultiTaskElasticNetRegressor_MLJScikitLearnInterface/#MultiTaskElasticNetRegressor_MLJScikitLearnInterface","page":"MultiTaskElasticNetRegressor","title":"MultiTaskElasticNetRegressor","text":"MultiTaskElasticNetRegressor\n\nA model type for constructing a multi-target elastic net regressor, based on MLJScikitLearnInterface.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nMultiTaskElasticNetRegressor = @load MultiTaskElasticNetRegressor pkg=MLJScikitLearnInterface\n\nDo model = MultiTaskElasticNetRegressor() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in MultiTaskElasticNetRegressor(alpha=...).","category":"section"},{"location":"models/MultiTaskElasticNetRegressor_MLJScikitLearnInterface/#Hyper-parameters","page":"MultiTaskElasticNetRegressor","title":"Hyper-parameters","text":"alpha = 1.0\nl1_ratio = 0.5\nfit_intercept = true\ncopy_X = true\nmax_iter = 1000\ntol = 0.0001\nwarm_start = false\nrandom_state = nothing\nselection = cyclic","category":"section"},{"location":"models/XGBoostCount_XGBoost/#XGBoostCount_XGBoost","page":"XGBoostCount","title":"XGBoostCount","text":"XGBoostCount\n\nA model type for constructing a eXtreme Gradient Boosting Count Regressor, based on XGBoost.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nXGBoostCount = @load XGBoostCount pkg=XGBoost\n\nDo model = XGBoostCount() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in XGBoostCount(test=...).\n\nUnivariate discrete regression using xgboost.","category":"section"},{"location":"models/XGBoostCount_XGBoost/#Training-data","page":"XGBoostCount","title":"Training data","text":"In MLJ or MLJBase, bind an instance model to data with\n\nm = machine(model, X, y)\n\nwhere\n\nX: any table of input features, either an AbstractMatrix or Tables.jl-compatible table.\ny: is an AbstractVector continuous target.\n\nTrain using fit!(m, rows=...).","category":"section"},{"location":"models/XGBoostCount_XGBoost/#Hyper-parameters","page":"XGBoostCount","title":"Hyper-parameters","text":"See https://xgboost.readthedocs.io/en/stable/parameter.html.","category":"section"},{"location":"models/HistGradientBoostingRegressor_MLJScikitLearnInterface/#HistGradientBoostingRegressor_MLJScikitLearnInterface","page":"HistGradientBoostingRegressor","title":"HistGradientBoostingRegressor","text":"HistGradientBoostingRegressor\n\nA model type for constructing a gradient boosting ensemble regression, based on MLJScikitLearnInterface.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nHistGradientBoostingRegressor = @load HistGradientBoostingRegressor pkg=MLJScikitLearnInterface\n\nDo model = HistGradientBoostingRegressor() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in HistGradientBoostingRegressor(loss=...).\n\nThis estimator builds an additive model in a forward stage-wise fashion;  it allows for the optimization of arbitrary differentiable loss functions.  In each stage a regression tree is fit on the negative gradient of the  given loss function.\n\nHistGradientBoostingRegressor is a much faster variant of this  algorithm for intermediate datasets (n_samples >= 10_000).","category":"section"},{"location":"models/EvoTreeMLE_EvoTrees/#EvoTreeMLE_EvoTrees","page":"EvoTreeMLE","title":"EvoTreeMLE","text":"EvoTreeMLE(;kwargs...)\n\nA model type for constructing a EvoTreeMLE, based on EvoTrees.jl, and implementing both an internal API the MLJ model interface. EvoTreeMLE performs maximum likelihood estimation. Assumed distribution is specified through loss kwargs. Both Gaussian and Logistic distributions are supported.","category":"section"},{"location":"models/EvoTreeMLE_EvoTrees/#Hyper-parameters","page":"EvoTreeMLE","title":"Hyper-parameters","text":"early_stopping_rounds::Integer: number of consecutive rounds without metric improvement after which fitting in stopped.\n\nloss=:gaussian:         Loss to be be minimized during training. One of:\n\n:gaussian_mle\n:logistic_mle\nnrounds=100:           Number of rounds. It corresponds to the number of trees that will be sequentially stacked. Must be >= 1.\neta=0.1:              Learning rate. Each tree raw predictions are scaled by eta prior to be added to the stack of predictions. Must be > 0.\n\nA lower eta results in slower learning, requiring a higher nrounds but typically improves model performance.  \n\nL2::T=0.0:            L2 regularization factor on aggregate gain. Must be >= 0. Higher L2 can result in a more robust model.\nlambda::T=0.0:        L2 regularization factor on individual gain. Must be >= 0. Higher lambda can result in a more robust model.\ngamma::T=0.0:         Minimum gain imprvement needed to perform a node split. Higher gamma can result in a more robust model. Must be >= 0.\nmax_depth=6:          Maximum depth of a tree. Must be >= 1. A tree of depth 1 is made of a single prediction leaf. A complete tree of depth N contains 2^(N - 1) terminal leaves and 2^(N - 1) - 1 split nodes. Compute cost is proportional to 2^max_depth. Typical optimal values are in the 3 to 9 range.\nmin_weight=8.0:       Minimum weight needed in a node to perform a split. Matches the number of observations by default or the sum of weights as provided by the weights vector. Must be > 0.\nrowsample=1.0:        Proportion of rows that are sampled at each iteration to build the tree. Should be in ]0, 1].\ncolsample=1.0:        Proportion of columns / features that are sampled at each iteration to build the tree. Should be in ]0, 1].\nnbins=64:             Number of bins into which each feature is quantized. Buckets are defined based on quantiles, hence resulting in equal weight bins. Should be between 2 and 255.\nmonotone_constraints=Dict{Int, Int}(): Specify monotonic constraints using a dict where the key is the feature index and the value the applicable constraint (-1=decreasing, 0=none, 1=increasing).  !Experimental feature: note that for MLE regression, constraints may not be enforced systematically.\ntree_type=:binary    Tree structure to be used. One of:\n:binary:       Each node of a tree is grown independently. Tree are built depthwise until max depth is reach or if min weight or gain (see gamma) stops further node splits.\n:oblivious:    A common splitting condition is imposed to all nodes of a given depth.\nrng=123:              Either an integer used as a seed to the random number generator or an actual random number generator (::Random.AbstractRNG).\ndevice=:cpu: Hardware device to use for computations. Can be either :cpu or gpu. Following losses are not GPU supported at the moment: :logistic_mle.","category":"section"},{"location":"models/EvoTreeMLE_EvoTrees/#Internal-API","page":"EvoTreeMLE","title":"Internal API","text":"Do config = EvoTreeMLE() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in EvoTreeMLE(max_depth=...).","category":"section"},{"location":"models/EvoTreeMLE_EvoTrees/#Training-model","page":"EvoTreeMLE","title":"Training model","text":"A model is built using fit_evotree:\n\nmodel = fit_evotree(config; x_train, y_train, kwargs...)","category":"section"},{"location":"models/EvoTreeMLE_EvoTrees/#Inference","page":"EvoTreeMLE","title":"Inference","text":"Predictions are obtained using predict which returns a Matrix of size [nobs, nparams] where the second dimensions refer to μ & σ for Normal/Gaussian and μ & s for Logistic.\n\nEvoTrees.predict(model, X)\n\nAlternatively, models act as a functor, returning predictions when called as a function with features as argument:\n\nmodel(X)","category":"section"},{"location":"models/EvoTreeMLE_EvoTrees/#MLJ","page":"EvoTreeMLE","title":"MLJ","text":"From MLJ, the type can be imported using:\n\nEvoTreeMLE = @load EvoTreeMLE pkg=EvoTrees\n\nDo model = EvoTreeMLE() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in EvoTreeMLE(loss=...).","category":"section"},{"location":"models/EvoTreeMLE_EvoTrees/#Training-data","page":"EvoTreeMLE","title":"Training data","text":"In MLJ or MLJBase, bind an instance model to data with\n\nmach = machine(model, X, y)\n\nwhere\n\nX: any table of input features (eg, a DataFrame) whose columns each have one of the following element scitypes: Continuous, Count, or <:OrderedFactor; check column scitypes with schema(X)\ny: is the target, which can be any AbstractVector whose element scitype is <:Continuous; check the scitype with scitype(y)\n\nTrain the machine using fit!(mach, rows=...).","category":"section"},{"location":"models/EvoTreeMLE_EvoTrees/#Operations","page":"EvoTreeMLE","title":"Operations","text":"predict(mach, Xnew): returns a vector of Gaussian or Logistic distributions (according to provided loss) given features Xnew having the same scitype as X above.\n\nPredictions are probabilistic.\n\nSpecific metrics can also be predicted using:\n\npredict_mean(mach, Xnew)\npredict_mode(mach, Xnew)\npredict_median(mach, Xnew)","category":"section"},{"location":"models/EvoTreeMLE_EvoTrees/#Fitted-parameters","page":"EvoTreeMLE","title":"Fitted parameters","text":"The fields of fitted_params(mach) are:\n\n:fitresult: The GBTree object returned by EvoTrees.jl fitting algorithm.","category":"section"},{"location":"models/EvoTreeMLE_EvoTrees/#Report","page":"EvoTreeMLE","title":"Report","text":"The fields of report(mach) are:\n\n:features: The names of the features encountered in training.","category":"section"},{"location":"models/EvoTreeMLE_EvoTrees/#Examples","page":"EvoTreeMLE","title":"Examples","text":"## Internal API\nusing EvoTrees\nconfig = EvoTreeMLE(max_depth=5, nbins=32, nrounds=100)\nnobs, nfeats = 1_000, 5\nx_train, y_train = randn(nobs, nfeats), rand(nobs)\nmodel = fit_evotree(config; x_train, y_train)\npreds = EvoTrees.predict(model, x_train)\n\n## MLJ Interface\nusing MLJ\nEvoTreeMLE = @load EvoTreeMLE pkg=EvoTrees\nmodel = EvoTreeMLE(max_depth=5, nbins=32, nrounds=100)\nX, y = @load_boston\nmach = machine(model, X, y) |> fit!\npreds = predict(mach, X)\npreds = predict_mean(mach, X)\npreds = predict_mode(mach, X)\npreds = predict_median(mach, X)","category":"section"},{"location":"models/Pipeline_MLJBase/#Pipeline_MLJBase","page":"Pipeline","title":"Pipeline","text":"Pipeline(component1, component2, ... , componentk; options...)\nPipeline(name1=component1, name2=component2, ..., namek=componentk; options...)\ncomponent1 |> component2 |> ... |> componentk\n\nCreate an instance of a composite model type which sequentially composes the specified components in order. This means component1 receives inputs, whose output is passed to component2, and so forth. A \"component\" is either a Model instance, a model type (converted immediately to its default instance) or any callable object. Here the \"output\" of a model is what predict returns if it is Supervised, or what transform returns if it is Unsupervised.\n\nNames for the component fields are automatically generated unless explicitly specified, as in\n\nPipeline(encoder=ContinuousEncoder(drop_last=false),\n         stand=Standardizer())\n\nThe Pipeline constructor accepts keyword options discussed further below.\n\nOrdinary functions (and other callables) may be inserted in the pipeline as shown in the following example:\n\nPipeline(X->coerce(X, :age=>Continuous), OneHotEncoder, ConstantClassifier)","category":"section"},{"location":"models/Pipeline_MLJBase/#Syntactic-sugar","page":"Pipeline","title":"Syntactic sugar","text":"The |> operator is overloaded to construct pipelines out of models, callables, and existing pipelines:\n\nLinearRegressor = @load LinearRegressor pkg=MLJLinearModels add=true\nPCA = @load PCA pkg=MultivariateStats add=true\n\npipe1 = MLJBase.table |> ContinuousEncoder |> Standardizer\npipe2 = PCA |> LinearRegressor\npipe1 |> pipe2\n\nAt most one of the components may be a supervised model, but this model can appear in any position. A pipeline with a Supervised component is itself Supervised and implements the predict operation.  It is otherwise Unsupervised (possibly Static) and implements transform.","category":"section"},{"location":"models/Pipeline_MLJBase/#Special-operations","page":"Pipeline","title":"Special operations","text":"If all the components are invertible unsupervised models (ie, implement inverse_transform) then inverse_transform is implemented for the pipeline. If there are no supervised models, then predict is nevertheless implemented, assuming the last component is a model that implements it (some clustering models). Similarly, calling transform on a supervised pipeline calls transform on the supervised component.","category":"section"},{"location":"models/Pipeline_MLJBase/#Transformers-that-need-a-target-in-training","page":"Pipeline","title":"Transformers that need a target in training","text":"Some transformers that have type Unsupervised (so that the output of transform is propagated in pipelines) may require a target variable for training. An example are so-called target encoders (which transform categorical input features, based on some target observations). Provided they appear before any Supervised component in the pipelines, such models are supported. Of course a target must be provided whenever training such a pipeline, whether or not it contains a Supervised component.","category":"section"},{"location":"models/Pipeline_MLJBase/#Optional-key-word-arguments","page":"Pipeline","title":"Optional key-word arguments","text":"prediction_type  - prediction type of the pipeline; possible values: :deterministic, :probabilistic, :interval (default=:deterministic if not inferable)\noperation - operation applied to the supervised component model, when present; possible values: predict, predict_mean, predict_median, predict_mode (default=predict)\ncache - whether the internal machines created for component models should cache model-specific representations of data (see machine) (default=true)\n\nwarning: Warning\nSet cache=false to guarantee data anonymization.\n\nTo build more complicated non-branching pipelines, refer to the MLJ manual sections on composing models.","category":"section"},{"location":"homogeneous_ensembles/#Homogeneous-Ensembles","page":"Homogeneous Ensembles","title":"Homogeneous Ensembles","text":"Although an ensemble of models sharing a common set of hyperparameters can be defined using the learning network API, MLJ's EnsembleModel model wrapper is preferred, for convenience and best performance. Examples of using EnsembleModel are given in this Data Science Tutorial.\n\nWhen bagging decision trees, further randomness is normally introduced by subsampling features, when training each node of each tree (Ho (1995), Brieman and Cutler (2001)). A bagged ensemble of such trees is known as a Random Forest. You can see an example of using EnsembleModel to build a random forest in this Data Science Tutorial. However, you may also want to use a canned random forest model. Run models(\"RandomForest\") to list such models.","category":"section"},{"location":"homogeneous_ensembles/#MLJEnsembles.EnsembleModel","page":"Homogeneous Ensembles","title":"MLJEnsembles.EnsembleModel","text":"EnsembleModel(model,\n              atomic_weights=Float64[],\n              bagging_fraction=0.8,\n              n=100,\n              rng=GLOBAL_RNG,\n              acceleration=CPU1(),\n              out_of_bag_measure=[])\n\nCreate a model for training an ensemble of n clones of model, with optional bagging. Ensembling is useful if fit!(machine(atom, data...)) does not create identical models on repeated calls (ie, is a stochastic model, such as a decision tree with randomized node selection criteria), or if bagging_fraction is set to a value less than 1.0, or both.\n\nHere the atomic model must support targets with scitype AbstractVector{<:Finite} (single-target classifiers) or AbstractVector{<:Continuous} (single-target regressors).\n\nIf rng is an integer, then MersenneTwister(rng) is the random number generator used for bagging. Otherwise some AbstractRNG object is expected.\n\nThe atomic predictions are optionally weighted according to the vector atomic_weights (to allow for external optimization) except in the case that model is a Deterministic classifier, in which case atomic_weights are ignored.\n\nThe ensemble model is Deterministic or Probabilistic, according to the corresponding supertype of atom. In the case of deterministic classifiers (target_scitype(atom) <: Abstract{<:Finite}), the predictions are majority votes, and for regressors (target_scitype(atom)<: AbstractVector{<:Continuous}) they are ordinary averages.  Probabilistic predictions are obtained by averaging the atomic probability distribution/mass functions; in particular, for regressors, the ensemble prediction on each input pattern has the type MixtureModel{VF,VS,D} from the Distributions.jl package, where D is the type of predicted distribution for atom.\n\nSpecify acceleration=CPUProcesses() for distributed computing, or CPUThreads() for multithreading.\n\nIf a single measure or non-empty vector of measures is specified by out_of_bag_measure, then out-of-bag estimates of performance are written to the training report (call report on the trained machine wrapping the ensemble model).\n\nImportant: If per-observation or class weights w (not to be confused with atomic weights) are specified when constructing a machine for the ensemble model, as in mach = machine(ensemble_model, X, y, w), then w is used by any measures specified in out_of_bag_measure that support them.\n\n\n\n\n\n","category":"function"},{"location":"openml_integration/#OpenML-Integration","page":"OpenML Integration","title":"OpenML Integration","text":"The OpenML platform provides an integration platform for carrying out and comparing machine learning solutions across a broad collection of public datasets and software platforms.\n\nIntegration with OpenML API is presently limited to querying and downloading datasets.\n\nDocumentation is here.","category":"section"},{"location":"models/ECODDetector_OutlierDetectionPython/#ECODDetector_OutlierDetectionPython","page":"ECODDetector","title":"ECODDetector","text":"ECODDetector(n_jobs = 1)\n\nhttps://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.ecod","category":"section"},{"location":"performance_measures/#Performance-Measures","page":"Performance Measures","title":"Performance Measures","text":"","category":"section"},{"location":"performance_measures/#Quick-links","page":"Performance Measures","title":"Quick links","text":"List of aliases of all measures\nMigration guide for changes to measures in MLJBase 1.0","category":"section"},{"location":"performance_measures/#Introduction","page":"Performance Measures","title":"Introduction","text":"In MLJ loss functions, scoring rules, confusion matrices, sensitivities, etc, are collectively referred to as measures. These measures are provided by the package StatisticalMeasures.jl but are immediately available to the MLJ user. Here's a simple example of direct application of the log_loss measures to compute a training loss:\n\nusing MLJ\nX, y = @load_iris\nDecisionTreeClassifier = @load DecisionTreeClassifier pkg=DecisionTree\ntree = DecisionTreeClassifier(max_depth=2)\nmach = machine(tree, X, y) |> fit!\nyhat = predict(mach, X)\nlog_loss(yhat, y)\n\nFor more examples of direct measure usage, see the StatisticalMeasures.jl tutorial.\n\nA list of all measures, ready to use after running using MLJ or using StatisticalMeasures, is here. Alternatively, call measures() (experimental) to generate a dictionary keyed on available measure constructors, with measure metadata as values.","category":"section"},{"location":"performance_measures/#Custom-measures","page":"Performance Measures","title":"Custom measures","text":"Any measure-like object with appropriate calling behavior can be used with MLJ. To quickly build custom measures, we recommend using the package StatisticalMeasuresBase.jl, which provides this tutorial. Note, in particular, that an \"atomic\" measure can be transformed into a multi-target measure using this package.","category":"section"},{"location":"performance_measures/#Uses-of-measures","page":"Performance Measures","title":"Uses of measures","text":"In MLJ, measures are specified:\n\nwhen evaluating model performance using evaluate!/evaluate; see Evaluating Model Performance\nwhen wrapping models using TunedModel - see Tuning Models\nwhen wrapping iterative models using IteratedModel - see Controlling Iterative Models\nwhen generating learning curves using learning_curve - see Learning Curves\n\nand elsewhere.","category":"section"},{"location":"performance_measures/#Using-LossFunctions.jl","page":"Performance Measures","title":"Using LossFunctions.jl","text":"In previous versions of MLJ, measures from LossFunctions.jl were also available. Now measures from that package must be explicitly imported and wrapped, as described here.","category":"section"},{"location":"performance_measures/#Receiver-operator-characteristics","page":"Performance Measures","title":"Receiver operator characteristics","text":"A related performance evaluation tool provided by StatisticalMeasures.jl, and hence by MLJ, is the roc_curve method. It is  documented here.","category":"section"},{"location":"performance_measures/#Reference","page":"Performance Measures","title":"Reference","text":"StatisticalMeasures.measures","category":"section"},{"location":"performance_measures/#Migration-guide-for-changes-to-measures-in-MLJBase-1.0","page":"Performance Measures","title":"Migration guide for changes to measures in MLJBase 1.0","text":"Prior to MLJBase.jl 1.0 (respectivey, MLJ.jl version 0.19.6) measures were defined in MLJBase.jl (a dependency of MLJ.jl) but now they are provided by MLJ.jl dependency StatisticalMeasures. Effects on users are detailed below:","category":"section"},{"location":"performance_measures/#Breaking-behavior-likely-relevant-to-many-users","page":"Performance Measures","title":"Breaking behavior likely relevant to many users","text":"If using MLJBase without MLJ, then, in Julia 1.9 or higher, StatisticalMeasures must be explicitly imported to use measures that were previously part of MLJBase. If using MLJ, then all previous measures are still available, with the exception of those corresponding to LossFunctions.jl (see below).\nAll measures return a single aggregated measurement. In other words, measures previously reporting a measurement per-observation (previously subtyping Unaggregated) no longer do so. To get per-observation measurements, use the new method StatisticalMeasures.measurements(measure, ŷ, y[, weights, class_weights]).\nThe default measure for regression models (used in evaluate/evaluate! when measures is unspecified) is changed from rms to l2=LPLoss(2) (mean sum of squares).\nMeanAbsoluteError has been removed and instead mae is an alias for LPLoss(p=1).\nMeasures that previously skipped NaN values will now (at least by default) propagate  those values. Missing value behavior is unchanged, except some measures that  previously did not support missing now do.\nAliases for measure types have been removed. For example RMSE (alias for RootMeanSquaredError) is gone. Aliases for instances, such as rms and cross_entropy persist. The exception is precision, for which ppv can be used in its place. (This is to avoid conflict with Base.precision, which was previously pirated.)\ninfo(measure) has been decommissioned; query docstrings or access the new measure traits individually instead. These traits are now provided by StatisticalMeasures.jl and not are not exported. For example, to access the orientation of the measure rms, do import StatisticalMeasures as SM; SM.orientation(rms).\nBehavior of the measures() method, to list all measures and associated traits, has changed. It now returns a dictionary instead of a vector of named tuples; measures(predicate) is decommissioned, but measures(needle) is preserved. (This method, owned by StatisticalMeasures.jl, has some other search options, but is experimental.)\nMeasures that were wraps of losses from LossFunctions.jl are no longer exposed by MLJBase or MLJ. To use such a loss, you must explicitly import LossFunctions and wrap the loss appropriately.  See Using losses from LossFunctions.jl for examples.\nSome user-defined measures working in previous versions of MLJBase.jl may not work without modification, as they must conform to the new StatisticalMeasuresBase.jl API. See this tutorial on how define new measures.\nMeasures with a \"feature argument\" X, as in some_measure(ŷ, y, X), are no longer supported. See What is a measure? for allowed signatures in measures.","category":"section"},{"location":"performance_measures/#Packages-implementing-the-MLJ-model-interface","page":"Performance Measures","title":"Packages implementing the MLJ model interface","text":"The migration of measures is not expected to require any changes to the source code in packges providing implementations of the MLJ model interface (MLJModelInterface.jl) such as MLJDecisionTreeInterface.jl and MLJFlux.jl, and this is confirmed by extensive integration tests. However, some current tests will fail, if they use MLJBase measures. The following should generally suffice to adapt such tests:\n\nAdd StatisticalMeasures as test dependency, and add using StatisticalMeasures to your runtests.jl (and/or included submodules).\nIf measures are qualified, as in MLJBase.rms, then the qualification must be removed or changed to StatisticalMeasures.rms, etc.\nBe aware that the default measure used in methods such as evaluate!, when measure is not specified, is changed from rms to l2 for regression models.\nBe aware of that all measures now report a measurement for every observation, and never an aggregate. See second point above.","category":"section"},{"location":"performance_measures/#Breaking-behavior-possibly-relevant-to-some-developers","page":"Performance Measures","title":"Breaking behavior possibly relevant to some developers","text":"The abstract measure types Aggregated, Unaggregated, Measure have been decommissioned. (A measure is now defined purely by its calling behavior.)\nWhat were previously exported as measure types are now only constructors.\ntarget_scitype(measure) is decommissioned. Related is StatisticalMeasures.observation_scitype(measure) which declares an upper bound on the allowed scitype of a single observation.\nprediction_type(measure) is decommissioned. Instead use StatisticalMeasures.kind_of_proxy(measure).\nThe trait reports_each_observation is decommissioned. Related is StatisticalMeasures.can_report_unaggregated; if false the new measurements method simply returns n copies of the aggregated measurement, where n is the number of observations provided, instead of individual observation-dependent measurements.\naggregation(measure) has been decommissioned. Instead use StatisticalMeasures.external_mode_of_aggregation(measure).\ninstances(measure) has been decommissioned; query docstrings for measure aliases, or follow this example: aliases = measures()[RootMeanSquaredError].aliases.\nis_feature_dependent(measure) has been decommissioned. Measures consuming feature data are not longer supported; see above.\ndistribution_type(measure) has been decommissioned.\ndocstring(measure) has been decommissioned.\nBehavior of aggregate has changed.\nThe following traits, previously exported by MLJBase and MLJ, cannot be applied to measures: supports_weights, supports_class_weights, orientation, human_name. Instead use the traits with these names provided by StatisticalMeausures.jl (they will need to be qualified, as in import StatisticalMeasures; StatisticalMeasures.orientation(measure)).","category":"section"},{"location":"models/GMMDetector_OutlierDetectionPython/#GMMDetector_OutlierDetectionPython","page":"GMMDetector","title":"GMMDetector","text":"GMMDetector(n_components=1,\n               covariance_type=\"full\",\n               tol=0.001,\n               reg_covar=1e-06,\n               max_iter=100,\n               n_init=1,\n               init_params=\"kmeans\",\n               weights_init=None,\n               means_init=None,\n               precisions_init=None,\n               random_state=None,\n               warm_start=False)\n\nhttps://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.gmm","category":"section"},{"location":"models/LGBMRegressor_LightGBM/#LGBMRegressor_LightGBM","page":"LGBMRegressor","title":"LGBMRegressor","text":"LGBMRegressor\n\nA model type for constructing a LightGBM regressor, based on LightGBM.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nLGBMRegressor = @load LGBMRegressor pkg=LightGBM\n\nDo model = LGBMRegressor() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in LGBMRegressor(objective=...).\n\nLightGBM, short for light gradient-boosting machine, is a framework for gradient boosting based on decision tree algorithms and used for classification, regression and other machine learning tasks, with a focus on performance and scalability. This model in particular is used for various types of regression tasks.","category":"section"},{"location":"models/LGBMRegressor_LightGBM/#Training-data","page":"LGBMRegressor","title":"Training data","text":"In MLJ or MLJBase, bind an instance model to data with \n\nmach = machine(model, X, y) \n\nHere:\n\nX is any table of input features (eg, a DataFrame) whose columns are of scitype Continuous; check the column scitypes with schema(X); alternatively, X is any AbstractMatrix with Continuous elements; check the scitype with scitype(X).\ny is a vector of targets whose items are of scitype Continuous. Check the scitype with scitype(y).\n\nTrain the machine using fit!(mach, rows=...).","category":"section"},{"location":"models/LGBMRegressor_LightGBM/#Operations","page":"LGBMRegressor","title":"Operations","text":"predict(mach, Xnew): return predictions of the target given new features Xnew, which should have the same scitype as X above.","category":"section"},{"location":"models/LGBMRegressor_LightGBM/#Hyper-parameters","page":"LGBMRegressor","title":"Hyper-parameters","text":"See https://lightgbm.readthedocs.io/en/v3.3.5/Parameters.html.","category":"section"},{"location":"models/LGBMRegressor_LightGBM/#Fitted-parameters","page":"LGBMRegressor","title":"Fitted parameters","text":"The fields of fitted_params(mach) are:\n\nfitresult: Fitted model information, contains a LGBMRegression object, an empty vector, and the regressor with all its parameters","category":"section"},{"location":"models/LGBMRegressor_LightGBM/#Report","page":"LGBMRegressor","title":"Report","text":"The fields of report(mach) are:\n\ntraining_metrics: A dictionary containing all training metrics.\nimportance: A namedtuple containing:\ngain: The total gain of each split used by the model\nsplit: The number of times each feature is used by the model.","category":"section"},{"location":"models/LGBMRegressor_LightGBM/#Examples","page":"LGBMRegressor","title":"Examples","text":"\nusing DataFrames\nusing MLJ\n\n## load the model\nLGBMRegressor = @load LGBMRegressor pkg=LightGBM \n\nX, y = @load_boston ## a table and a vector \nX = DataFrame(X)\ntrain, test = partition(collect(eachindex(y)), 0.70, shuffle=true)\n\nfirst(X, 3)\nlgb = LGBMRegressor() ## initialise a model with default params\nmach = machine(lgb, X[train, :], y[train]) |> fit!\n\npredict(mach, X[test, :])\n\n## access feature importances\nmodel_report = report(mach)\ngain_importance = model_report.importance.gain\nsplit_importance = model_report.importance.split\n\nSee also LightGBM.jl and the unwrapped model type LightGBM.LGBMRegression","category":"section"},{"location":"models/LMDDDetector_OutlierDetectionPython/#LMDDDetector_OutlierDetectionPython","page":"LMDDDetector","title":"LMDDDetector","text":"LMDDDetector(n_iter = 50,\n                dis_measure = \"aad\",\n                random_state = nothing)\n\nhttps://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.lmdd","category":"section"},{"location":"models/EvoTreeClassifier_EvoTrees/#EvoTreeClassifier_EvoTrees","page":"EvoTreeClassifier","title":"EvoTreeClassifier","text":"EvoTreeClassifier(;kwargs...)\n\nA model type for constructing a EvoTreeClassifier, based on EvoTrees.jl, and implementing both an internal API and the MLJ model interface. EvoTreeClassifier is used to perform multi-class classification, using cross-entropy loss.","category":"section"},{"location":"models/EvoTreeClassifier_EvoTrees/#Hyper-parameters","page":"EvoTreeClassifier","title":"Hyper-parameters","text":"early_stopping_rounds::Integer: number of consecutive rounds without metric improvement after which fitting in stopped.\nnrounds=100:           Number of rounds. It corresponds to the number of trees that will be sequentially stacked. Must be >= 1.\neta=0.1:              Learning rate. Each tree raw predictions are scaled by eta prior to be added to the stack of predictions. Must be > 0. A lower eta results in slower learning, requiring a higher nrounds but typically improves model performance.\nL2::T=0.0:            L2 regularization factor on aggregate gain. Must be >= 0. Higher L2 can result in a more robust model.\nlambda::T=0.0:        L2 regularization factor on individual gain. Must be >= 0. Higher lambda can result in a more robust model.\ngamma::T=0.0:         Minimum gain improvement needed to perform a node split. Higher gamma can result in a more robust model. Must be >= 0.\nmax_depth=6:          Maximum depth of a tree. Must be >= 1. A tree of depth 1 is made of a single prediction leaf. A complete tree of depth N contains 2^(N - 1) terminal leaves and 2^(N - 1) - 1 split nodes. Compute cost is proportional to 2^max_depth. Typical optimal values are in the 3 to 9 range.\nmin_weight=1.0:       Minimum weight needed in a node to perform a split. Matches the number of observations by default or the sum of weights as provided by the weights vector. Must be > 0.\nrowsample=1.0:        Proportion of rows that are sampled at each iteration to build the tree. Should be in ]0, 1].\ncolsample=1.0:        Proportion of columns / features that are sampled at each iteration to build the tree. Should be in ]0, 1].\nnbins=64:             Number of bins into which each feature is quantized. Buckets are defined based on quantiles, hence resulting in equal weight bins. Should be between 2 and 255.\ntree_type=:binary    Tree structure to be used. One of:\n:binary:       Each node of a tree is grown independently. Tree are built depthwise until max depth is reach or if min weight or gain (see gamma) stops further node splits.\n:oblivious:    A common splitting condition is imposed to all nodes of a given depth.\nrng=123:              Either an integer used as a seed to the random number generator or an actual random number generator (::Random.AbstractRNG).\ndevice=:cpu: Hardware device to use for computations. Can be either :cpu or :gpu.","category":"section"},{"location":"models/EvoTreeClassifier_EvoTrees/#Internal-API","page":"EvoTreeClassifier","title":"Internal API","text":"Do config = EvoTreeClassifier() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in EvoTreeClassifier(max_depth=...).","category":"section"},{"location":"models/EvoTreeClassifier_EvoTrees/#Training-model","page":"EvoTreeClassifier","title":"Training model","text":"A model is built using fit_evotree:\n\nmodel = fit_evotree(config; x_train, y_train, kwargs...)","category":"section"},{"location":"models/EvoTreeClassifier_EvoTrees/#Inference","page":"EvoTreeClassifier","title":"Inference","text":"Predictions are obtained using predict which returns a Matrix of size [nobs, K] where K is the number of classes:\n\nEvoTrees.predict(model, X)\n\nAlternatively, models act as a functor, returning predictions when called as a function with features as argument:\n\nmodel(X)","category":"section"},{"location":"models/EvoTreeClassifier_EvoTrees/#MLJ","page":"EvoTreeClassifier","title":"MLJ","text":"From MLJ, the type can be imported using:\n\nEvoTreeClassifier = @load EvoTreeClassifier pkg=EvoTrees\n\nDo model = EvoTreeClassifier() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in EvoTreeClassifier(loss=...).","category":"section"},{"location":"models/EvoTreeClassifier_EvoTrees/#Training-data","page":"EvoTreeClassifier","title":"Training data","text":"In MLJ or MLJBase, bind an instance model to data with\n\nmach = machine(model, X, y)\n\nwhere\n\nX: any table of input features (eg, a DataFrame) whose columns each have one of the following element scitypes: Continuous, Count, or <:OrderedFactor; check column scitypes with schema(X)\ny: is the target, which can be any AbstractVector whose element scitype is <:Multiclas or <:OrderedFactor; check the scitype with scitype(y)\n\nTrain the machine using fit!(mach, rows=...).","category":"section"},{"location":"models/EvoTreeClassifier_EvoTrees/#Operations","page":"EvoTreeClassifier","title":"Operations","text":"predict(mach, Xnew): return predictions of the target given features Xnew having the same scitype as X above. Predictions are probabilistic.\npredict_mode(mach, Xnew): returns the mode of each of the prediction above.","category":"section"},{"location":"models/EvoTreeClassifier_EvoTrees/#Fitted-parameters","page":"EvoTreeClassifier","title":"Fitted parameters","text":"The fields of fitted_params(mach) are:\n\n:fitresult: The GBTree object returned by EvoTrees.jl fitting algorithm.","category":"section"},{"location":"models/EvoTreeClassifier_EvoTrees/#Report","page":"EvoTreeClassifier","title":"Report","text":"The fields of report(mach) are:\n\n:features: The names of the features encountered in training.","category":"section"},{"location":"models/EvoTreeClassifier_EvoTrees/#Examples","page":"EvoTreeClassifier","title":"Examples","text":"## Internal API\nusing EvoTrees\nconfig = EvoTreeClassifier(max_depth=5, nbins=32, nrounds=100)\nnobs, nfeats = 1_000, 5\nx_train, y_train = randn(nobs, nfeats), rand(1:3, nobs)\nmodel = fit_evotree(config; x_train, y_train)\npreds = EvoTrees.predict(model, x_train)\n\n## MLJ Interface\nusing MLJ\nEvoTreeClassifier = @load EvoTreeClassifier pkg=EvoTrees\nmodel = EvoTreeClassifier(max_depth=5, nbins=32, nrounds=100)\nX, y = @load_iris\nmach = machine(model, X, y) |> fit!\npreds = predict(mach, X)\npreds = predict_mode(mach, X)\n\nSee also EvoTrees.jl.","category":"section"},{"location":"models/FactorAnalysis_MultivariateStats/#FactorAnalysis_MultivariateStats","page":"FactorAnalysis","title":"FactorAnalysis","text":"FactorAnalysis\n\nA model type for constructing a factor analysis model, based on MultivariateStats.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nFactorAnalysis = @load FactorAnalysis pkg=MultivariateStats\n\nDo model = FactorAnalysis() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in FactorAnalysis(method=...).\n\nFactor analysis is a linear-Gaussian latent variable model that is closely related to probabilistic PCA. In contrast to the probabilistic PCA model, the covariance of conditional distribution of the observed variable given the latent variable is diagonal rather than isotropic.","category":"section"},{"location":"models/FactorAnalysis_MultivariateStats/#Training-data","page":"FactorAnalysis","title":"Training data","text":"In MLJ or MLJBase, bind an instance model to data with\n\nmach = machine(model, X)\n\nHere:\n\nX is any table of input features (eg, a DataFrame) whose columns   are of scitype Continuous; check column scitypes with schema(X).\n\nTrain the machine using fit!(mach, rows=...).","category":"section"},{"location":"models/FactorAnalysis_MultivariateStats/#Hyper-parameters","page":"FactorAnalysis","title":"Hyper-parameters","text":"method::Symbol=:cm: Method to use to solve the problem, one of :ml, :em, :bayes.\nmaxoutdim=0: Controls the the dimension (number of columns) of the output,   outdim. Specifically, outdim = min(n, indim, maxoutdim), where n is the number of   observations and indim the input dimension.\nmaxiter::Int=1000: Maximum number of iterations.\ntol::Real=1e-6: Convergence tolerance.\neta::Real=tol: Variance lower bound.\nmean::Union{Nothing, Real, Vector{Float64}}=nothing: If nothing, centering will be   computed and applied; if set to 0 no centering is applied (data is assumed   pre-centered); if a vector, the centering is done with that vector.","category":"section"},{"location":"models/FactorAnalysis_MultivariateStats/#Operations","page":"FactorAnalysis","title":"Operations","text":"transform(mach, Xnew): Return a lower dimensional projection of the input Xnew, which should have the same scitype as X above.\ninverse_transform(mach, Xsmall): For a dimension-reduced table Xsmall, such as returned by transform, reconstruct a table, having same the number of columns as the original training data X, that transforms to Xsmall. Mathematically, inverse_transform is a right-inverse for the PCA projection map, whose image is orthogonal to the kernel of that map. In particular, if Xsmall = transform(mach, Xnew), then inverse_transform(Xsmall) is only an approximation to Xnew.","category":"section"},{"location":"models/FactorAnalysis_MultivariateStats/#Fitted-parameters","page":"FactorAnalysis","title":"Fitted parameters","text":"The fields of fitted_params(mach) are:\n\nprojection: Returns the projection matrix, which has size (indim, outdim), where indim and outdim are the number of features of the input and ouput respectively. Each column of the projection matrix corresponds to a factor.","category":"section"},{"location":"models/FactorAnalysis_MultivariateStats/#Report","page":"FactorAnalysis","title":"Report","text":"The fields of report(mach) are:\n\nindim: Dimension (number of columns) of the training data and new data to be transformed.\noutdim: Dimension of transformed data (number of factors).\nvariance: The variance of the factors.\ncovariance_matrix: The estimated covariance matrix.\nmean: The mean of the untransformed training data, of length indim.\nloadings: The factor loadings. A matrix of size (indim, outdim) where indim and outdim are as defined above.","category":"section"},{"location":"models/FactorAnalysis_MultivariateStats/#Examples","page":"FactorAnalysis","title":"Examples","text":"using MLJ\n\nFactorAnalysis = @load FactorAnalysis pkg=MultivariateStats\n\nX, y = @load_iris ## a table and a vector\n\nmodel = FactorAnalysis(maxoutdim=2)\nmach = machine(model, X) |> fit!\n\nXproj = transform(mach, X)\n\nSee also KernelPCA, ICA, PPCA, PCA","category":"section"},{"location":"models/SRRegressor_SymbolicRegression/#SRRegressor_SymbolicRegression","page":"SRRegressor","title":"SRRegressor","text":"SRRegressor\n\nA model type for constructing a Symbolic Regression via Evolutionary Search, based on SymbolicRegression.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nSRRegressor = @load SRRegressor pkg=SymbolicRegression\n\nDo model = SRRegressor() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in SRRegressor(binary_operators=...).\n\nSingle-target Symbolic Regression regressor (SRRegressor) searches for symbolic expressions that predict a single target variable from a set of input variables. All data is assumed to be Continuous. The search is performed using an evolutionary algorithm. This algorithm is described in the paper https://arxiv.org/abs/2305.01582.","category":"section"},{"location":"models/SRRegressor_SymbolicRegression/#Training-data","page":"SRRegressor","title":"Training data","text":"In MLJ or MLJBase, bind an instance model to data with\n\nmach = machine(model, X, y)\n\nOR\n\nmach = machine(model, X, y, w)\n\nHere:\n\nX is any table of input features (eg, a DataFrame) whose columns are of scitype Continuous; check column scitypes with schema(X). Variable names in discovered expressions will be taken from the column names of X, if available. Units in columns of X (use DynamicQuantities for units) will trigger dimensional analysis to be used.\ny is the target, which can be any AbstractVector whose element scitype is   Continuous; check the scitype with scitype(y). Units in y (use DynamicQuantities   for units) will trigger dimensional analysis to be used.\nw is the observation weights which can either be nothing (default) or an AbstractVector whoose element scitype is Count or Continuous.\n\nTrain the machine using fit!(mach), inspect the discovered expressions with report(mach), and predict on new data with predict(mach, Xnew). Note that unlike other regressors, symbolic regression stores a list of trained models. The model chosen from this list is defined by the function selection_method keyword argument, which by default balances accuracy and complexity. You can override this at prediction time by passing a named tuple with keys data and idx.","category":"section"},{"location":"models/SRRegressor_SymbolicRegression/#Hyper-parameters","page":"SRRegressor","title":"Hyper-parameters","text":"binary_operators: Vector of binary operators (functions) to use.   Each operator should be defined for two input scalars,   and one output scalar. All operators   need to be defined over the entire real line (excluding infinity - these   are stopped before they are input), or return NaN where not defined.   For speed, define it so it takes two reals   of the same type as input, and outputs the same type. For the SymbolicUtils   simplification backend, you will need to define a generic method of the   operator so it takes arbitrary types.\nunary_operators: Same, but for   unary operators (one input scalar, gives an output scalar).\nconstraints: Array of pairs specifying size constraints   for each operator. The constraints for a binary operator should be a 2-tuple   (e.g., (-1, -1)) and the constraints for a unary operator should be an Int.   A size constraint is a limit to the size of the subtree   in each argument of an operator. e.g., [(^)=>(-1, 3)] means that the   ^ operator can have arbitrary size (-1) in its left argument,   but a maximum size of 3 in its right argument. Default is   no constraints.\nbatching: Whether to evolve based on small mini-batches of data,   rather than the entire dataset.\nbatch_size: What batch size to use if using batching.\nelementwise_loss: What elementwise loss function to use. Can be one of   the following losses, or any other loss of type   SupervisedLoss. You can also pass a function that takes   a scalar target (left argument), and scalar predicted (right   argument), and returns a scalar. This will be averaged   over the predicted data. If weights are supplied, your   function should take a third argument for the weight scalar.   Included losses:       Regression:           - LPDistLoss{P}(),           - L1DistLoss(),           - L2DistLoss() (mean square),           - LogitDistLoss(),           - HuberLoss(d),           - L1EpsilonInsLoss(ϵ),           - L2EpsilonInsLoss(ϵ),           - PeriodicLoss(c),           - QuantileLoss(τ),       Classification:           - ZeroOneLoss(),           - PerceptronLoss(),           - L1HingeLoss(),           - SmoothedL1HingeLoss(γ),           - ModifiedHuberLoss(),           - L2MarginLoss(),           - ExpLoss(),           - SigmoidLoss(),           - DWDMarginLoss(q).\nloss_function: Alternatively, you may redefine the loss used   as any function of tree::Node{T}, dataset::Dataset{T},   and options::Options, so long as you output a non-negative   scalar of type T. This is useful if you want to use a loss   that takes into account derivatives, or correlations across   the dataset. This also means you could use a custom evaluation   for a particular expression. If you are using   batching=true, then your function should   accept a fourth argument idx, which is either nothing   (indicating that the full dataset should be used), or a vector   of indices to use for the batch.   For example,\n  function my_loss(tree, dataset::Dataset{T,L}, options)::L where {T,L}\n      prediction, flag = eval_tree_array(tree, dataset.X, options)\n      if !flag\n          return L(Inf)\n      end\n      return sum((prediction .- dataset.y) .^ 2) / dataset.n\n  end\npopulations: How many populations of equations to use.\npopulation_size: How many equations in each population.\nncycles_per_iteration: How many generations to consider per iteration.\ntournament_selection_n: Number of expressions considered in each tournament.\ntournament_selection_p: The fittest expression in a tournament is to be   selected with probability p, the next fittest with probability p*(1-p),   and so forth.\ntopn: Number of equations to return to the host process, and to   consider for the hall of fame.\ncomplexity_of_operators: What complexity should be assigned to each operator,   and the occurrence of a constant or variable. By default, this is 1   for all operators. Can be a real number as well, in which case   the complexity of an expression will be rounded to the nearest integer.   Input this in the form of, e.g., [(^) => 3, sin => 2].\ncomplexity_of_constants: What complexity should be assigned to use of a constant.   By default, this is 1.\ncomplexity_of_variables: What complexity should be assigned to each variable.   By default, this is 1.\nalpha: The probability of accepting an equation mutation   during regularized evolution is given by exp(-delta_loss/(alpha * T)),   where T goes from 1 to 0. Thus, alpha=infinite is the same as no annealing.\nmaxsize: Maximum size of equations during the search.\nmaxdepth: Maximum depth of equations during the search, by default   this is set equal to the maxsize.\nparsimony: A multiplicative factor for how much complexity is   punished.\ndimensional_constraint_penalty: An additive factor if the dimensional   constraint is violated.\nuse_frequency: Whether to use a parsimony that adapts to the   relative proportion of equations at each complexity; this will   ensure that there are a balanced number of equations considered   for every complexity.\nuse_frequency_in_tournament: Whether to use the adaptive parsimony described   above inside the score, rather than just at the mutation accept/reject stage.\nadaptive_parsimony_scaling: How much to scale the adaptive parsimony term   in the loss. Increase this if the search is spending too much time   optimizing the most complex equations.\nturbo: Whether to use LoopVectorization.@turbo to evaluate expressions.   This can be significantly faster, but is only compatible with certain   operators. Experimental!\nmigration: Whether to migrate equations between processes.\nhof_migration: Whether to migrate equations from the hall of fame   to processes.\nfraction_replaced: What fraction of each population to replace with   migrated equations at the end of each cycle.\nfraction_replaced_hof: What fraction to replace with hall of fame   equations at the end of each cycle.\nshould_simplify: Whether to simplify equations. If you   pass a custom objective, this will be set to false.\nshould_optimize_constants: Whether to use an optimization algorithm   to periodically optimize constants in equations.\noptimizer_nrestarts: How many different random starting positions to consider   for optimization of constants.\noptimizer_algorithm: Select algorithm to use for optimizing constants. Default   is \"BFGS\", but \"NelderMead\" is also supported.\noptimizer_options: General options for the constant optimization. For details   we refer to the documentation on Optim.Options from the Optim.jl package.   Options can be provided here as NamedTuple, e.g. (iterations=16,), as a   Dict, e.g. Dict(:x_tol => 1.0e-32,), or as an Optim.Options instance.\noutput_file: What file to store equations to, as a backup.\nperturbation_factor: When mutating a constant, either   multiply or divide by (1+perturbation_factor)^(rand()+1).\nprobability_negate_constant: Probability of negating a constant in the equation   when mutating it.\nmutation_weights: Relative probabilities of the mutations. The struct   MutationWeights should be passed to these options.   See its documentation on MutationWeights for the different weights.\ncrossover_probability: Probability of performing crossover.\nannealing: Whether to use simulated annealing.\nwarmup_maxsize_by: Whether to slowly increase the max size from 5 up to   maxsize. If nonzero, specifies the fraction through the search   at which the maxsize should be reached.\nverbosity: Whether to print debugging statements or   not.\nprint_precision: How many digits to print when printing   equations. By default, this is 5.\nsave_to_file: Whether to save equations to a file during the search.\nbin_constraints: See constraints. This is the same, but specified for binary   operators only (for example, if you have an operator that is both a binary   and unary operator).\nuna_constraints: Likewise, for unary operators.\nseed: What random seed to use. nothing uses no seed.\nprogress: Whether to use a progress bar output (verbosity will   have no effect).\nearly_stop_condition: Float - whether to stop early if the mean loss gets below this value.   Function - a function taking (loss, complexity) as arguments and returning true or false.\ntimeout_in_seconds: Float64 - the time in seconds after which to exit (as an alternative to the number of iterations).\nmax_evals: Int (or Nothing) - the maximum number of evaluations of expressions to perform.\nskip_mutation_failures: Whether to simply skip over mutations that fail or are rejected, rather than to replace the mutated   expression with the original expression and proceed normally.\nenable_autodiff: Whether to enable automatic differentiation functionality. This is turned off by default.   If turned on, this will be turned off if one of the operators does not have well-defined gradients.\nnested_constraints: Specifies how many times a combination of operators can be nested. For example,   [sin => [cos => 0], cos => [cos => 2]] specifies that cos may never appear within a sin,   but sin can be nested with itself an unlimited number of times. The second term specifies that cos   can be nested up to 2 times within a cos, so that cos(cos(cos(x))) is allowed (as well as any combination   of + or - within it), but cos(cos(cos(cos(x)))) is not allowed. When an operator is not specified,   it is assumed that it can be nested an unlimited number of times. This requires that there is no operator   which is used both in the unary operators and the binary operators (e.g., - could be both subtract, and negation).   For binary operators, both arguments are treated the same way, and the max of each argument is constrained.\ndeterministic: Use a global counter for the birth time, rather than calls to time(). This gives   perfect resolution, and is therefore deterministic. However, it is not thread safe, and must be used   in serial mode.\ndefine_helper_functions: Whether to define helper functions   for constructing and evaluating trees.\nniterations::Int=10: The number of iterations to perform the search.   More iterations will improve the results.\nparallelism=:multithreading: What parallelism mode to use.   The options are :multithreading, :multiprocessing, and :serial.   By default, multithreading will be used. Multithreading uses less memory,   but multiprocessing can handle multi-node compute. If using :multithreading   mode, the number of threads available to julia are used. If using   :multiprocessing, numprocs processes will be created dynamically if   procs is unset. If you have already allocated processes, pass them   to the procs argument and they will be used.   You may also pass a string instead of a symbol, like \"multithreading\".\nnumprocs::Union{Int, Nothing}=nothing:  The number of processes to use,   if you want equation_search to set this up automatically. By default   this will be 4, but can be any number (you should pick a number <=   the number of cores available).\nprocs::Union{Vector{Int}, Nothing}=nothing: If you have set up   a distributed run manually with procs = addprocs() and @everywhere,   pass the procs to this keyword argument.\naddprocs_function::Union{Function, Nothing}=nothing: If using multiprocessing   (parallelism=:multithreading), and are not passing procs manually,   then they will be allocated dynamically using addprocs. However,   you may also pass a custom function to use instead of addprocs.   This function should take a single positional argument,   which is the number of processes to use, as well as the lazy keyword argument.   For example, if set up on a slurm cluster, you could pass   addprocs_function = addprocs_slurm, which will set up slurm processes.\nheap_size_hint_in_bytes::Union{Int,Nothing}=nothing: On Julia 1.9+, you may set the --heap-size-hint   flag on Julia processes, recommending garbage collection once a process   is close to the recommended size. This is important for long-running distributed   jobs where each process has an independent memory, and can help avoid   out-of-memory errors. By default, this is set to Sys.free_memory() / numprocs.\nruntests::Bool=true: Whether to run (quick) tests before starting the   search, to see if there will be any problems during the equation search   related to the host environment.\nloss_type::Type=Nothing: If you would like to use a different type   for the loss than for the data you passed, specify the type here.   Note that if you pass complex data ::Complex{L}, then the loss   type will automatically be set to L.\nselection_method::Function: Function to selection expression from   the Pareto frontier for use in predict.   See SymbolicRegression.MLJInterfaceModule.choose_best for an example.   This function should return a single integer specifying   the index of the expression to use. By default, this maximizes   the score (a pound-for-pound rating) of expressions reaching the threshold   of 1.5x the minimum loss. To override this at prediction time, you can pass   a named tuple with keys data and idx to predict. See the Operations   section for details.\ndimensions_type::AbstractDimensions: The type of dimensions to use when storing   the units of the data. By default this is DynamicQuantities.SymbolicDimensions.","category":"section"},{"location":"models/SRRegressor_SymbolicRegression/#Operations","page":"SRRegressor","title":"Operations","text":"predict(mach, Xnew): Return predictions of the target given features Xnew, which should have same scitype as X above. The expression used for prediction is defined by the selection_method function, which can be seen by viewing report(mach).best_idx.\npredict(mach, (data=Xnew, idx=i)): Return predictions of the target given features Xnew, which should have same scitype as X above. By passing a named tuple with keys data and idx, you are able to specify the equation you wish to evaluate in idx.","category":"section"},{"location":"models/SRRegressor_SymbolicRegression/#Fitted-parameters","page":"SRRegressor","title":"Fitted parameters","text":"The fields of fitted_params(mach) are:\n\nbest_idx::Int: The index of the best expression in the Pareto frontier,  as determined by the selection_method function. Override in predict by passing   a named tuple with keys data and idx.\nequations::Vector{Node{T}}: The expressions discovered by the search, represented in a dominating Pareto frontier (i.e., the best expressions found for each complexity). T is equal to the element type of the passed data.\nequation_strings::Vector{String}: The expressions discovered by the search, represented as strings for easy inspection.","category":"section"},{"location":"models/SRRegressor_SymbolicRegression/#Report","page":"SRRegressor","title":"Report","text":"The fields of report(mach) are:\n\nbest_idx::Int: The index of the best expression in the Pareto frontier,  as determined by the selection_method function. Override in predict by passing  a named tuple with keys data and idx.\nequations::Vector{Node{T}}: The expressions discovered by the search, represented in a dominating Pareto frontier (i.e., the best expressions found for each complexity).\nequation_strings::Vector{String}: The expressions discovered by the search, represented as strings for easy inspection.\ncomplexities::Vector{Int}: The complexity of each expression in the Pareto frontier.\nlosses::Vector{L}: The loss of each expression in the Pareto frontier, according to the loss function specified in the model. The type L is the loss type, which is usually the same as the element type of data passed (i.e., T), but can differ if complex data types are passed.\nscores::Vector{L}: A metric which considers both the complexity and loss of an expression, equal to the change in the log-loss divided by the change in complexity, relative to the previous expression along the Pareto frontier. A larger score aims to indicate an expression is more likely to be the true expression generating the data, but this is very problem-dependent and generally several other factors should be considered.","category":"section"},{"location":"models/SRRegressor_SymbolicRegression/#Examples","page":"SRRegressor","title":"Examples","text":"using MLJ\nSRRegressor = @load SRRegressor pkg=SymbolicRegression\nX, y = @load_boston\nmodel = SRRegressor(binary_operators=[+, -, *], unary_operators=[exp], niterations=100)\nmach = machine(model, X, y)\nfit!(mach)\ny_hat = predict(mach, X)\n## View the equation used:\nr = report(mach)\nprintln(\"Equation used:\", r.equation_strings[r.best_idx])\n\nWith units and variable names:\n\nusing MLJ\nusing DynamicQuantities\nSRegressor = @load SRRegressor pkg=SymbolicRegression\n\nX = (; x1=rand(32) .* us\"km/h\", x2=rand(32) .* us\"km\")\ny = @. X.x2 / X.x1 + 0.5us\"h\"\nmodel = SRRegressor(binary_operators=[+, -, *, /])\nmach = machine(model, X, y)\nfit!(mach)\ny_hat = predict(mach, X)\n## View the equation used:\nr = report(mach)\nprintln(\"Equation used:\", r.equation_strings[r.best_idx])\n\nSee also MultitargetSRRegressor.","category":"section"},{"location":"models/EvoTreeGaussian_EvoTrees/#EvoTreeGaussian_EvoTrees","page":"EvoTreeGaussian","title":"EvoTreeGaussian","text":"EvoTreeGaussian(;kwargs...)\n\nA model type for constructing a EvoTreeGaussian, based on EvoTrees.jl, and implementing both an internal API the MLJ model interface. EvoTreeGaussian is used to perform Gaussian probabilistic regression, fitting μ and σ parameters to maximize likelihood.","category":"section"},{"location":"models/EvoTreeGaussian_EvoTrees/#Hyper-parameters","page":"EvoTreeGaussian","title":"Hyper-parameters","text":"early_stopping_rounds::Integer: number of consecutive rounds without metric improvement after which fitting in stopped.\nnrounds=100:           Number of rounds. It corresponds to the number of trees that will be sequentially stacked. Must be >= 1.\neta=0.1:              Learning rate. Each tree raw predictions are scaled by eta prior to be added to the stack of predictions. Must be > 0. A lower eta results in slower learning, requiring a higher nrounds but typically improves model performance.\nL2::T=0.0:            L2 regularization factor on aggregate gain. Must be >= 0. Higher L2 can result in a more robust model.\nlambda::T=0.0:        L2 regularization factor on individual gain. Must be >= 0. Higher lambda can result in a more robust model.\ngamma::T=0.0:         Minimum gain imprvement needed to perform a node split. Higher gamma can result in a more robust model. Must be >= 0.\nmax_depth=6:          Maximum depth of a tree. Must be >= 1. A tree of depth 1 is made of a single prediction leaf. A complete tree of depth N contains 2^(N - 1) terminal leaves and 2^(N - 1) - 1 split nodes. Compute cost is proportional to 2^max_depth. Typical optimal values are in the 3 to 9 range.\nmin_weight=8.0:       Minimum weight needed in a node to perform a split. Matches the number of observations by default or the sum of weights as provided by the weights vector. Must be > 0.\nrowsample=1.0:        Proportion of rows that are sampled at each iteration to build the tree. Should be in ]0, 1].\ncolsample=1.0:        Proportion of columns / features that are sampled at each iteration to build the tree. Should be in ]0, 1].\nnbins=64:             Number of bins into which each feature is quantized. Buckets are defined based on quantiles, hence resulting in equal weight bins. Should be between 2 and 255.\nmonotone_constraints=Dict{Int, Int}(): Specify monotonic constraints using a dict where the key is the feature index and the value the applicable constraint (-1=decreasing, 0=none, 1=increasing).  !Experimental feature: note that for Gaussian regression, constraints may not be enforce systematically.\ntree_type=:binary    Tree structure to be used. One of:\n:binary:       Each node of a tree is grown independently. Tree are built depthwise until max depth is reach or if min weight or gain (see gamma) stops further node splits.\n:oblivious:    A common splitting condition is imposed to all nodes of a given depth.\nrng=123:              Either an integer used as a seed to the random number generator or an actual random number generator (::Random.AbstractRNG).\ndevice=:cpu: Hardware device to use for computations. Can be either :cpu or gpu.","category":"section"},{"location":"models/EvoTreeGaussian_EvoTrees/#Internal-API","page":"EvoTreeGaussian","title":"Internal API","text":"Do config = EvoTreeGaussian() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in EvoTreeGaussian(max_depth=...).","category":"section"},{"location":"models/EvoTreeGaussian_EvoTrees/#Training-model","page":"EvoTreeGaussian","title":"Training model","text":"A model is built using fit_evotree:\n\nmodel = fit_evotree(config; x_train, y_train, kwargs...)","category":"section"},{"location":"models/EvoTreeGaussian_EvoTrees/#Inference","page":"EvoTreeGaussian","title":"Inference","text":"Predictions are obtained using predict which returns a Matrix of size [nobs, 2] where the second dimensions refer to μ and σ respectively:\n\nEvoTrees.predict(model, X)\n\nAlternatively, models act as a functor, returning predictions when called as a function with features as argument:\n\nmodel(X)","category":"section"},{"location":"models/EvoTreeGaussian_EvoTrees/#MLJ","page":"EvoTreeGaussian","title":"MLJ","text":"From MLJ, the type can be imported using:\n\nEvoTreeGaussian = @load EvoTreeGaussian pkg=EvoTrees\n\nDo model = EvoTreeGaussian() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in EvoTreeGaussian(loss=...).","category":"section"},{"location":"models/EvoTreeGaussian_EvoTrees/#Training-data","page":"EvoTreeGaussian","title":"Training data","text":"In MLJ or MLJBase, bind an instance model to data with\n\nmach = machine(model, X, y)\n\nwhere\n\nX: any table of input features (eg, a DataFrame) whose columns each have one of the following element scitypes: Continuous, Count, or <:OrderedFactor; check column scitypes with schema(X)\ny: is the target, which can be any AbstractVector whose element scitype is <:Continuous; check the scitype with scitype(y)\n\nTrain the machine using fit!(mach, rows=...).","category":"section"},{"location":"models/EvoTreeGaussian_EvoTrees/#Operations","page":"EvoTreeGaussian","title":"Operations","text":"predict(mach, Xnew): returns a vector of Gaussian distributions given features Xnew having the same scitype as X above.\n\nPredictions are probabilistic.\n\nSpecific metrics can also be predicted using:\n\npredict_mean(mach, Xnew)\npredict_mode(mach, Xnew)\npredict_median(mach, Xnew)","category":"section"},{"location":"models/EvoTreeGaussian_EvoTrees/#Fitted-parameters","page":"EvoTreeGaussian","title":"Fitted parameters","text":"The fields of fitted_params(mach) are:\n\n:fitresult: The GBTree object returned by EvoTrees.jl fitting algorithm.","category":"section"},{"location":"models/EvoTreeGaussian_EvoTrees/#Report","page":"EvoTreeGaussian","title":"Report","text":"The fields of report(mach) are:\n\n:features: The names of the features encountered in training.","category":"section"},{"location":"models/EvoTreeGaussian_EvoTrees/#Examples","page":"EvoTreeGaussian","title":"Examples","text":"## Internal API\nusing EvoTrees\nparams = EvoTreeGaussian(max_depth=5, nbins=32, nrounds=100)\nnobs, nfeats = 1_000, 5\nx_train, y_train = randn(nobs, nfeats), rand(nobs)\nmodel = fit_evotree(params; x_train, y_train)\npreds = EvoTrees.predict(model, x_train)\n\n## MLJ Interface\nusing MLJ\nEvoTreeGaussian = @load EvoTreeGaussian pkg=EvoTrees\nmodel = EvoTreeGaussian(max_depth=5, nbins=32, nrounds=100)\nX, y = @load_boston\nmach = machine(model, X, y) |> fit!\npreds = predict(mach, X)\npreds = predict_mean(mach, X)\npreds = predict_mode(mach, X)\npreds = predict_median(mach, X)","category":"section"},{"location":"models/GaussianMixtureImputer_BetaML/#GaussianMixtureImputer_BetaML","page":"GaussianMixtureImputer","title":"GaussianMixtureImputer","text":"mutable struct GaussianMixtureImputer <: MLJModelInterface.Unsupervised\n\nImpute missing values using a probabilistic approach (Gaussian Mixture Models) fitted using the Expectation-Maximisation algorithm, from the Beta Machine Learning Toolkit (BetaML).","category":"section"},{"location":"models/GaussianMixtureImputer_BetaML/#Hyperparameters:","page":"GaussianMixtureImputer","title":"Hyperparameters:","text":"n_classes::Int64: Number of mixtures (latent classes) to consider [def: 3]\ninitial_probmixtures::Vector{Float64}: Initial probabilities of the categorical distribution (n_classes x 1) [default: []]\nmixtures::Union{Type, Vector{<:BetaML.GMM.AbstractMixture}}: An array (of length n_classes) of the mixtures to employ (see the [?GMM](@ref GMM) module in BetaML). Each mixture object can be provided with or without its parameters (e.g. mean and variance for the gaussian ones). Fully qualified mixtures are useful only if theinitialisationstrategyparameter is  set to \"gived\" This parameter can also be given symply in term of a _type. In this case it is automatically extended to a vector of n_classesmixtures of the specified type. Note that mixing of different mixture types is not currently supported and that currently implemented mixtures areSphericalGaussian,DiagonalGaussianandFullGaussian. [def:DiagonalGaussian`]\ntol::Float64: Tolerance to stop the algorithm [default: 10^(-6)]\nminimum_variance::Float64: Minimum variance for the mixtures [default: 0.05]\nminimum_covariance::Float64: Minimum covariance for the mixtures with full covariance matrix [default: 0]. This should be set different than minimum_variance.\ninitialisation_strategy::String: The computation method of the vector of the initial mixtures. One of the following:\n\"grid\": using a grid approach\n\"given\": using the mixture provided in the fully qualified mixtures parameter\n\"kmeans\": use first kmeans (itself initialised with a \"grid\" strategy) to set the initial mixture centers [default]\nNote that currently \"random\" and \"shuffle\" initialisations are not supported in gmm-based algorithms.\nrng::Random.AbstractRNG: A Random Number Generator to be used in stochastic parts of the code [deafult: Random.GLOBAL_RNG]","category":"section"},{"location":"models/GaussianMixtureImputer_BetaML/#Example-:","page":"GaussianMixtureImputer","title":"Example :","text":"julia> using MLJ\n\njulia> X = [1 10.5;1.5 missing; 1.8 8; 1.7 15; 3.2 40; missing missing; 3.3 38; missing -2.3; 5.2 -2.4] |> table ;\n\njulia> modelType   = @load GaussianMixtureImputer  pkg = \"BetaML\" verbosity=0\nBetaML.Imputation.GaussianMixtureImputer\n\njulia> model     = modelType(initialisation_strategy=\"grid\")\nGaussianMixtureImputer(\n  n_classes = 3, \n  initial_probmixtures = Float64[], \n  mixtures = BetaML.GMM.DiagonalGaussian{Float64}[BetaML.GMM.DiagonalGaussian{Float64}(nothing, nothing), BetaML.GMM.DiagonalGaussian{Float64}(nothing, nothing), BetaML.GMM.DiagonalGaussian{Float64}(nothing, nothing)], \n  tol = 1.0e-6, \n  minimum_variance = 0.05, \n  minimum_covariance = 0.0, \n  initialisation_strategy = \"grid\", \n  rng = Random._GLOBAL_RNG())\n\njulia> mach      = machine(model, X);\n\njulia> fit!(mach);\n[ Info: Training machine(GaussianMixtureImputer(n_classes = 3, …), …).\nIter. 1:        Var. of the post  2.0225921341714286      Log-likelihood -42.96100103213314\n\njulia> X_full       = transform(mach) |> MLJ.matrix\n9×2 Matrix{Float64}:\n 1.0      10.5\n 1.5      14.7366\n 1.8       8.0\n 1.7      15.0\n 3.2      40.0\n 2.51842  15.1747\n 3.3      38.0\n 2.47412  -2.3\n 5.2      -2.4","category":"section"},{"location":"models/HuberRegressor_MLJLinearModels/#HuberRegressor_MLJLinearModels","page":"HuberRegressor","title":"HuberRegressor","text":"HuberRegressor\n\nA model type for constructing a huber regressor, based on MLJLinearModels.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nHuberRegressor = @load HuberRegressor pkg=MLJLinearModels\n\nDo model = HuberRegressor() to construct an instance with default hyper-parameters.\n\nThis model coincides with RobustRegressor, with the exception that the robust loss, rho, is fixed to HuberRho(delta), where delta is a new hyperparameter.\n\nDifferent solver options exist, as indicated under \"Hyperparameters\" below. ","category":"section"},{"location":"models/HuberRegressor_MLJLinearModels/#Training-data","page":"HuberRegressor","title":"Training data","text":"In MLJ or MLJBase, bind an instance model to data with\n\nmach = machine(model, X, y)\n\nwhere:\n\nX is any table of input features (eg, a DataFrame) whose columns have Continuous scitype; check column scitypes with schema(X)\ny is the target, which can be any AbstractVector whose element scitype is Continuous; check the scitype with scitype(y)\n\nTrain the machine using fit!(mach, rows=...).","category":"section"},{"location":"models/HuberRegressor_MLJLinearModels/#Hyperparameters","page":"HuberRegressor","title":"Hyperparameters","text":"delta::Real: parameterizes the HuberRho function (radius of the ball within which the loss     is a quadratic loss) Default: 0.5\nlambda::Real: strength of the regularizer if penalty is :l2 or :l1.     Strength of the L2 regularizer if penalty is :en. Default: 1.0\ngamma::Real: strength of the L1 regularizer if penalty is :en. Default: 0.0\npenalty::Union{String, Symbol}: the penalty to use, either :l2, :l1, :en (elastic net) or :none. Default: :l2\nfit_intercept::Bool: whether to fit the intercept or not. Default: true\npenalize_intercept::Bool: whether to penalize the intercept. Default: false\nscale_penalty_with_samples::Bool: whether to scale the penalty with the number of observations. Default: true\nsolver::Union{Nothing, MLJLinearModels.Solver}: some instance of MLJLinearModels.S where S is one of: LBFGS, IWLSCG, Newton, NewtonCG, if penalty = :l2, and ProxGrad otherwise.\nIf solver = nothing (default) then LBFGS() is used, if penalty = :l2, and otherwise ProxGrad(accel=true) (FISTA) is used.\nSolver aliases: FISTA(; kwargs...) = ProxGrad(accel=true, kwargs...), ISTA(; kwargs...) = ProxGrad(accel=false, kwargs...) Default: nothing","category":"section"},{"location":"models/HuberRegressor_MLJLinearModels/#Example","page":"HuberRegressor","title":"Example","text":"using MLJ\nX, y = make_regression()\nmach = fit!(machine(HuberRegressor(), X, y))\npredict(mach, X)\nfitted_params(mach)\n\nSee also RobustRegressor, QuantileRegressor.","category":"section"},{"location":"models/AdaBoostStumpClassifier_DecisionTree/#AdaBoostStumpClassifier_DecisionTree","page":"AdaBoostStumpClassifier","title":"AdaBoostStumpClassifier","text":"AdaBoostStumpClassifier\n\nA model type for constructing a Ada-boosted stump classifier, based on DecisionTree.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nAdaBoostStumpClassifier = @load AdaBoostStumpClassifier pkg=DecisionTree\n\nDo model = AdaBoostStumpClassifier() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in AdaBoostStumpClassifier(n_iter=...).","category":"section"},{"location":"models/AdaBoostStumpClassifier_DecisionTree/#Training-data","page":"AdaBoostStumpClassifier","title":"Training data","text":"In MLJ or MLJBase, bind an instance model to data with\n\nmach = machine(model, X, y)\n\nwhere:\n\nX: any table of input features (eg, a DataFrame) whose columns each have one of the following element scitypes: Continuous, Count, or <:OrderedFactor; check column scitypes with schema(X)\ny: the target, which can be any AbstractVector whose element scitype is <:OrderedFactor or <:Multiclass; check the scitype with scitype(y)\n\nTrain the machine with fit!(mach, rows=...).","category":"section"},{"location":"models/AdaBoostStumpClassifier_DecisionTree/#Hyperparameters","page":"AdaBoostStumpClassifier","title":"Hyperparameters","text":"n_iter=10:   number of iterations of AdaBoost\nfeature_importance: method to use for computing feature importances. One of (:impurity, :split)\nrng=Random.GLOBAL_RNG: random number generator or seed","category":"section"},{"location":"models/AdaBoostStumpClassifier_DecisionTree/#Operations","page":"AdaBoostStumpClassifier","title":"Operations","text":"predict(mach, Xnew): return predictions of the target given features Xnew having the same scitype as X above. Predictions are probabilistic, but uncalibrated.\npredict_mode(mach, Xnew): instead return the mode of each prediction above.","category":"section"},{"location":"models/AdaBoostStumpClassifier_DecisionTree/#Fitted-Parameters","page":"AdaBoostStumpClassifier","title":"Fitted Parameters","text":"The fields of fitted_params(mach) are:\n\nstumps: the Ensemble object returned by the core DecisionTree.jl algorithm.\ncoefficients: the stump coefficients (one per stump)","category":"section"},{"location":"models/AdaBoostStumpClassifier_DecisionTree/#Report","page":"AdaBoostStumpClassifier","title":"Report","text":"The fields of report(mach) are:\n\nfeatures: the names of the features encountered in training","category":"section"},{"location":"models/AdaBoostStumpClassifier_DecisionTree/#Accessor-functions","page":"AdaBoostStumpClassifier","title":"Accessor functions","text":"feature_importances(mach) returns a vector of (feature::Symbol => importance) pairs; the type of importance is determined by the hyperparameter feature_importance (see above)","category":"section"},{"location":"models/AdaBoostStumpClassifier_DecisionTree/#Examples","page":"AdaBoostStumpClassifier","title":"Examples","text":"using MLJ\nBooster = @load AdaBoostStumpClassifier pkg=DecisionTree\nbooster = Booster(n_iter=15)\n\nX, y = @load_iris\nmach = machine(booster, X, y) |> fit!\n\nXnew = (sepal_length = [6.4, 7.2, 7.4],\n        sepal_width = [2.8, 3.0, 2.8],\n        petal_length = [5.6, 5.8, 6.1],\n        petal_width = [2.1, 1.6, 1.9],)\nyhat = predict(mach, Xnew) ## probabilistic predictions\npredict_mode(mach, Xnew)   ## point predictions\npdf.(yhat, \"virginica\")    ## probabilities for the \"verginica\" class\n\nfitted_params(mach).stumps ## raw `Ensemble` object from DecisionTree.jl\nfitted_params(mach).coefs  ## coefficient associated with each stump\nfeature_importances(mach)\n\nSee also DecisionTree.jl and the unwrapped model type MLJDecisionTreeInterface.DecisionTree.AdaBoostStumpClassifier.","category":"section"},{"location":"models/CBLOFDetector_OutlierDetectionPython/#CBLOFDetector_OutlierDetectionPython","page":"CBLOFDetector","title":"CBLOFDetector","text":"CBLOFDetector(n_clusters = 8,\n                 alpha = 0.9,\n                 beta = 5,\n                 use_weights = false,\n                 random_state = nothing,\n                 n_jobs = 1)\n\nhttps://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.cblof","category":"section"},{"location":"models/LassoLarsRegressor_MLJScikitLearnInterface/#LassoLarsRegressor_MLJScikitLearnInterface","page":"LassoLarsRegressor","title":"LassoLarsRegressor","text":"LassoLarsRegressor\n\nA model type for constructing a Lasso model fit with least angle regression (LARS), based on MLJScikitLearnInterface.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nLassoLarsRegressor = @load LassoLarsRegressor pkg=MLJScikitLearnInterface\n\nDo model = LassoLarsRegressor() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in LassoLarsRegressor(alpha=...).","category":"section"},{"location":"models/LassoLarsRegressor_MLJScikitLearnInterface/#Hyper-parameters","page":"LassoLarsRegressor","title":"Hyper-parameters","text":"alpha = 1.0\nfit_intercept = true\nverbose = false\nprecompute = auto\nmax_iter = 500\neps = 2.220446049250313e-16\ncopy_X = true\nfit_path = true\npositive = false","category":"section"},{"location":"models/TSVDTransformer_TSVD/#TSVDTransformer_TSVD","page":"TSVDTransformer","title":"TSVDTransformer","text":"Truncated SVD dimensionality reduction","category":"section"},{"location":"models/COFDetector_OutlierDetectionPython/#COFDetector_OutlierDetectionPython","page":"COFDetector","title":"COFDetector","text":"COFDetector(n_neighbors = 5,\n               method=\"fast\")\n\nhttps://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.cof","category":"section"},{"location":"models/ProbabilisticSVC_LIBSVM/#ProbabilisticSVC_LIBSVM","page":"ProbabilisticSVC","title":"ProbabilisticSVC","text":"ProbabilisticSVC\n\nA model type for constructing a probabilistic C-support vector classifier, based on LIBSVM.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nProbabilisticSVC = @load ProbabilisticSVC pkg=LIBSVM\n\nDo model = ProbabilisticSVC() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in ProbabilisticSVC(kernel=...).\n\nThis model is identical to SVC with the exception that it predicts probabilities, instead of actual class labels. Probabilities are computed using Platt scaling, which will add to the total computation time.\n\nReference for algorithm and core C-library: C.-C. Chang and C.-J. Lin (2011): \"LIBSVM: a library for support vector machines.\" ACM Transactions on Intelligent Systems and Technology, 2(3):27:1–27:27. Updated at https://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf. \n\nPlatt, John (1999): \"Probabilistic Outputs for Support Vector Machines and Comparisons to Regularized Likelihood Methods.\"","category":"section"},{"location":"models/ProbabilisticSVC_LIBSVM/#Training-data","page":"ProbabilisticSVC","title":"Training data","text":"In MLJ or MLJBase, bind an instance model to data with one of:\n\nmach = machine(model, X, y)\nmach = machine(model, X, y, w)\n\nwhere\n\nX: any table of input features (eg, a DataFrame) whose columns each have Continuous element scitype; check column scitypes with schema(X)\ny: is the target, which can be any AbstractVector whose element scitype is <:OrderedFactor or <:Multiclass; check the scitype with scitype(y)\nw: a dictionary of class weights, keyed on levels(y).\n\nTrain the machine using fit!(mach, rows=...).","category":"section"},{"location":"models/ProbabilisticSVC_LIBSVM/#Hyper-parameters","page":"ProbabilisticSVC","title":"Hyper-parameters","text":"kernel=LIBSVM.Kernel.RadialBasis: either an object that can be called, as in kernel(x1, x2), or one of the built-in kernels from the LIBSVM.jl package listed below.  Here x1 and x2 are vectors whose lengths match the number of columns of the training data X (see \"Examples\" below).\nLIBSVM.Kernel.Linear: (x1, x2) -> x1'*x2\nLIBSVM.Kernel.Polynomial: (x1, x2) -> gamma*x1'*x2 + coef0)^degree\nLIBSVM.Kernel.RadialBasis: (x1, x2) -> (exp(-gamma*norm(x1 - x2)^2))\nLIBSVM.Kernel.Sigmoid: (x1, x2) - > tanh(gamma*x1'*x2 + coef0)\nHere gamma, coef0, degree are other hyper-parameters. Serialization of models with user-defined kernels comes with some restrictions. See LIVSVM.jl issue91\ngamma = 0.0: kernel parameter (see above); if gamma==-1.0 then gamma = 1/nfeatures is used in training, where nfeatures is the number of features (columns of X).  If gamma==0.0 then gamma = 1/(var(Tables.matrix(X))*nfeatures) is used. Actual value used appears in the report (see below).\ncoef0 = 0.0: kernel parameter (see above)\ndegree::Int32 = Int32(3): degree in polynomial kernel (see above)\ncost=1.0 (range (0, Inf)): the parameter denoted C in the cited reference; for greater regularization, decrease cost\ncachesize=200.0 cache memory size in MB\ntolerance=0.001: tolerance for the stopping criterion\nshrinking=true: whether to use shrinking heuristics","category":"section"},{"location":"models/ProbabilisticSVC_LIBSVM/#Operations","page":"ProbabilisticSVC","title":"Operations","text":"predict(mach, Xnew): return probabilistic predictions of the target given features Xnew having the same scitype as X above.","category":"section"},{"location":"models/ProbabilisticSVC_LIBSVM/#Fitted-parameters","page":"ProbabilisticSVC","title":"Fitted parameters","text":"The fields of fitted_params(mach) are:\n\nlibsvm_model: the trained model object created by the LIBSVM.jl package\nencoding: class encoding used internally by libsvm_model - a dictionary of class labels keyed on the internal integer representation","category":"section"},{"location":"models/ProbabilisticSVC_LIBSVM/#Report","page":"ProbabilisticSVC","title":"Report","text":"The fields of report(mach) are:\n\ngamma: actual value of the kernel parameter gamma used in training","category":"section"},{"location":"models/ProbabilisticSVC_LIBSVM/#Examples","page":"ProbabilisticSVC","title":"Examples","text":"","category":"section"},{"location":"models/ProbabilisticSVC_LIBSVM/#Using-a-built-in-kernel","page":"ProbabilisticSVC","title":"Using a built-in kernel","text":"using MLJ\nimport LIBSVM\n\nProbabilisticSVC = @load ProbabilisticSVC pkg=LIBSVM      ## model type\nmodel = ProbabilisticSVC(kernel=LIBSVM.Kernel.Polynomial) ## instance\n\nX, y = @load_iris ## table, vector\nmach = machine(model, X, y) |> fit!\n\nXnew = (sepal_length = [6.4, 7.2, 7.4],\n        sepal_width = [2.8, 3.0, 2.8],\n        petal_length = [5.6, 5.8, 6.1],\n        petal_width = [2.1, 1.6, 1.9],)\n\njulia> probs = predict(mach, Xnew)\n3-element UnivariateFiniteVector{Multiclass{3}, String, UInt32, Float64}:\n UnivariateFinite{Multiclass{3}}(setosa=>0.00186, versicolor=>0.003, virginica=>0.995)\n UnivariateFinite{Multiclass{3}}(setosa=>0.000563, versicolor=>0.0554, virginica=>0.944)\n UnivariateFinite{Multiclass{3}}(setosa=>1.4e-6, versicolor=>1.68e-6, virginica=>1.0)\n\n\njulia> labels = mode.(probs)\n3-element CategoricalArrays.CategoricalArray{String,1,UInt32}:\n \"virginica\"\n \"virginica\"\n \"virginica\"","category":"section"},{"location":"models/ProbabilisticSVC_LIBSVM/#User-defined-kernels","page":"ProbabilisticSVC","title":"User-defined kernels","text":"k(x1, x2) = x1'*x2 ## equivalent to `LIBSVM.Kernel.Linear`\nmodel = ProbabilisticSVC(kernel=k)\nmach = machine(model, X, y) |> fit!\n\nprobs = predict(mach, Xnew)","category":"section"},{"location":"models/ProbabilisticSVC_LIBSVM/#Incorporating-class-weights","page":"ProbabilisticSVC","title":"Incorporating class weights","text":"In either scenario above, we can do:\n\nweights = Dict(\"virginica\" => 1, \"versicolor\" => 20, \"setosa\" => 1)\nmach = machine(model, X, y, weights) |> fit!\n\nprobs = predict(mach, Xnew)\n\nSee also the classifiers SVC, NuSVC and LinearSVC, and LIVSVM.jl and the original C implementation documentation.","category":"section"},{"location":"models/LogisticClassifier_MLJScikitLearnInterface/#LogisticClassifier_MLJScikitLearnInterface","page":"LogisticClassifier","title":"LogisticClassifier","text":"LogisticClassifier\n\nA model type for constructing a logistic regression classifier, based on MLJScikitLearnInterface.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nLogisticClassifier = @load LogisticClassifier pkg=MLJScikitLearnInterface\n\nDo model = LogisticClassifier() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in LogisticClassifier(penalty=...).","category":"section"},{"location":"models/LogisticClassifier_MLJScikitLearnInterface/#Hyper-parameters","page":"LogisticClassifier","title":"Hyper-parameters","text":"penalty = l2\ndual = false\ntol = 0.0001\nC = 1.0\nfit_intercept = true\nintercept_scaling = 1.0\nclass_weight = nothing\nrandom_state = nothing\nsolver = lbfgs\nmax_iter = 100\nmulti_class = auto\nverbose = 0\nwarm_start = false\nn_jobs = nothing\nl1_ratio = nothing","category":"section"},{"location":"models/ContrastEncoder_MLJTransforms/#ContrastEncoder_MLJTransforms","page":"ContrastEncoder","title":"ContrastEncoder","text":"ContrastEncoder\n\nA model type for constructing a contrast encoder, based on MLJTransforms.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nContrastEncoder = @load ContrastEncoder pkg=MLJTransforms\n\nDo model = ContrastEncoder() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in ContrastEncoder(features=...).\n\nContrastEncoder implements the following contrast encoding methods for categorical features: dummy, sum, backward/forward difference, and Helmert coding.  More generally, users can specify a custom contrast or hypothesis matrix, and each feature can be encoded using a different method.","category":"section"},{"location":"models/ContrastEncoder_MLJTransforms/#Training-data","page":"ContrastEncoder","title":"Training data","text":"In MLJ (or MLJBase) bind an instance unsupervised model to data with\n\nmach = machine(model, X)\n\nHere:\n\nX is any table of input features (eg, a DataFrame). Features to be transformed must  have element scitype Multiclass or OrderedFactor. Use schema(X) to  check scitypes.\n\nTrain the machine using fit!(mach, rows=...).","category":"section"},{"location":"models/ContrastEncoder_MLJTransforms/#Hyper-parameters","page":"ContrastEncoder","title":"Hyper-parameters","text":"features=[]: A list of names of categorical features given as symbols to exclude or in clude from encoding, according to the value of ignore, or a single symbol (which is treated as a vector with one symbol), or a callable that returns true for features to be included/excluded.\nmode=:dummy: The type of encoding to use. Can be one of :contrast, :dummy, :sum, :backward_diff, :forward_diff, :helmert or :hypothesis. If ignore=false (features to be encoded are listed explictly in features), then this can be a vector of the same length as features to specify a different contrast encoding scheme for each feature\nbuildmatrix=nothing: A function or other callable with signature buildmatrix(colname,k), where colname is the name of the feature levels and k is it's length, and which returns contrast or hypothesis matrix with row/column ordering consistent with the ordering of levels(col). Only relevant if mode is :contrast or :hypothesis.\nignore=true: Whether to exclude or include the features given in features\nordered_factor=false: Whether to encode OrderedFactor or ignore them","category":"section"},{"location":"models/ContrastEncoder_MLJTransforms/#Operations","page":"ContrastEncoder","title":"Operations","text":"transform(mach, Xnew): Apply contrast encoding to selected Multiclass or OrderedFactor features ofXnewspecified by hyper-parameters, and return the new table. Features that are neitherMulticlassnorOrderedFactor` are always left unchanged.","category":"section"},{"location":"models/ContrastEncoder_MLJTransforms/#Fitted-parameters","page":"ContrastEncoder","title":"Fitted parameters","text":"The fields of fitted_params(mach) are:\n\nvector_given_value_given_feature: A dictionary that maps each level for each column in a subset of the categorical features of X into its frequency.","category":"section"},{"location":"models/ContrastEncoder_MLJTransforms/#Report","page":"ContrastEncoder","title":"Report","text":"The fields of report(mach) are:\n\nencoded_features: The subset of the categorical features of X that were encoded","category":"section"},{"location":"models/ContrastEncoder_MLJTransforms/#Examples","page":"ContrastEncoder","title":"Examples","text":"using MLJ\n\n## Define categorical dataset\nX = (\n    name   = categorical([\"Ben\", \"John\", \"Mary\", \"John\"]),\n    height = [1.85, 1.67, 1.5, 1.67],\n    favnum = categorical([7, 5, 10, 1]),\n    age    = [23, 23, 14, 23],\n)\n\n## Check scitype coercions:\nschema(X)\n\nencoder =  ContrastEncoder(\n    features = [:name, :favnum],\n    ignore = false,\n    mode = [:dummy, :helmert],\n)\nmach = fit!(machine(encoder, X))\nXnew = transform(mach, X)\n\njulia > Xnew\n    (name_John = [1.0, 0.0, 0.0, 0.0],\n    name_Mary = [0.0, 1.0, 0.0, 1.0],\n    height = [1.85, 1.67, 1.5, 1.67],\n    favnum_5 = [0.0, 1.0, 0.0, -1.0],\n    favnum_7 = [2.0, -1.0, 0.0, -1.0],\n    favnum_10 = [-1.0, -1.0, 3.0, -1.0],\n    age = [23, 23, 14, 23],)\n\nSee also OneHotEncoder","category":"section"},{"location":"models/ImageClassifier_MLJFlux/#ImageClassifier_MLJFlux","page":"ImageClassifier","title":"ImageClassifier","text":"ImageClassifier\n\nA model type for constructing a image classifier, based on MLJFlux.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nImageClassifier = @load ImageClassifier pkg=MLJFlux\n\nDo model = ImageClassifier() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in ImageClassifier(builder=...).\n\nImageClassifier classifies images using a neural network adapted to the type of images provided (color or gray scale). Predictions are probabilistic. Users provide a recipe for constructing the network, based on properties of the image encountered, by specifying an appropriate builder. See MLJFlux documentation for more on builders.","category":"section"},{"location":"models/ImageClassifier_MLJFlux/#Training-data","page":"ImageClassifier","title":"Training data","text":"In MLJ or MLJBase, bind an instance model to data with\n\nmach = machine(model, X, y)\n\nHere:\n\nX is any AbstractVector of images with ColorImage or GrayImage scitype; check  the scitype with scitype(X) and refer to ScientificTypes.jl documentation on coercing  typical image formats into an appropriate type.\ny is the target, which can be any AbstractVector whose element  scitype is Multiclass; check the scitype with scitype(y).\n\nTrain the machine with fit!(mach, rows=...).","category":"section"},{"location":"models/ImageClassifier_MLJFlux/#Hyper-parameters","page":"ImageClassifier","title":"Hyper-parameters","text":"builder: An MLJFlux builder that constructs the neural network.  The fallback builds a  depth-16 VGG architecture adapted to the image size and number of target classes, with  no batch normalization; see the Metalhead.jl documentation for details. See the example  below for a user-specified builder. A convenience macro @builder is also  available. See also finaliser below.\noptimiser::Optimisers.Adam(): An Optimisers.jl optimiser. The optimiser performs the updating of the weights of the network. To choose a learning rate (the update rate of the optimizer), a good rule of thumb is to start out at 10e-3, and tune using powers of 10 between 1 and 1e-7.\nloss=Flux.crossentropy: The loss function which the network will optimize. Should be a function which can be called in the form loss(yhat, y).  Possible loss functions are listed in the Flux loss function documentation. For a classification task, the most natural loss functions are:\nFlux.crossentropy: Standard multiclass classification loss, also known as the log loss.\nFlux.logitcrossentopy: Mathematically equal to crossentropy, but numerically more stable than finalising the outputs with softmax and then calculating crossentropy. You will need to specify finaliser=identity to remove MLJFlux's default softmax finaliser, and understand that the output of predict is then unnormalized (no longer probabilistic).\nFlux.tversky_loss: Used with imbalanced data to give more weight to false negatives.\nFlux.focal_loss: Used with highly imbalanced data. Weights harder examples more than easier examples.\nCurrently MLJ measures are not supported values of loss.\nepochs::Int=10: The duration of training, in epochs. Typically, one epoch represents one pass through the complete the training dataset.\nbatch_size::int=1: the batch size to be used for training, representing the number of samples per update of the network weights. Typically, batch size is between 8 and\nIncreassing batch size may accelerate training if acceleration=CUDALibs() and a\nGPU is available.\nlambda::Float64=0: The strength of the weight regularization penalty. Can be any value in the range [0, ∞). Note the history reports unpenalized losses.\nalpha::Float64=0: The L2/L1 mix of regularization, in the range [0, 1]. A value of 0 represents L2 regularization, and a value of 1 represents L1 regularization.\nrng::Union{AbstractRNG, Int64}: The random number generator or seed used during training. The default is Random.default_rng().\noptimizer_changes_trigger_retraining::Bool=false: Defines what happens when re-fitting a machine if the associated optimiser has changed. If true, the associated machine will retrain from scratch on fit! call, otherwise it will not.\nacceleration::AbstractResource=CPU1(): Defines on what hardware training is done. For Training on GPU, use CUDALibs().\nfinaliser=Flux.softmax: The final activation function of the neural network (applied after the network defined by builder). Defaults to Flux.softmax.","category":"section"},{"location":"models/ImageClassifier_MLJFlux/#Operations","page":"ImageClassifier","title":"Operations","text":"predict(mach, Xnew): return predictions of the target given new features Xnew, which should have the same scitype as X above. Predictions are probabilistic but uncalibrated.\npredict_mode(mach, Xnew): Return the modes of the probabilistic predictions returned above.","category":"section"},{"location":"models/ImageClassifier_MLJFlux/#Fitted-parameters","page":"ImageClassifier","title":"Fitted parameters","text":"The fields of fitted_params(mach) are:\n\nchain: The trained \"chain\" (Flux.jl model), namely the series of layers,  functions, and activations  which make up the neural network. This includes  the final layer specified by finaliser (eg, softmax).","category":"section"},{"location":"models/ImageClassifier_MLJFlux/#Report","page":"ImageClassifier","title":"Report","text":"The fields of report(mach) are:\n\ntraining_losses: A vector of training losses (penalised if lambda != 0) in  historical order, of length epochs + 1.  The first element is the pre-training loss.","category":"section"},{"location":"models/ImageClassifier_MLJFlux/#Examples","page":"ImageClassifier","title":"Examples","text":"In this example we use MLJFlux and a custom builder to classify the MNIST image dataset.\n\nusing MLJ\nusing Flux\nimport MLJFlux\nimport Optimisers\nimport MLJIteration ## for `skip` control\n\nFirst we want to download the MNIST dataset, and unpack into images and labels:\n\nimport MLDatasets: MNIST\ndata = MNIST(split=:train)\nimages, labels = data.features, data.targets\n\nIn MLJ, integers cannot be used for encoding categorical data, so we must coerce them into the Multiclass scitype:\n\nlabels = coerce(labels, Multiclass);\n\nAbove images is a single array but MLJFlux requires the images to be a vector of individual image arrays:\n\nimages = coerce(images, GrayImage);\nimages[1]\n\nWe start by defining a suitable builder object. This is a recipe for building the neural network. Our builder will work for images of any (constant) size, whether they be color or black and white (ie, single or multi-channel).  The architecture always consists of six alternating convolution and max-pool layers, and a final dense layer; the filter size and the number of channels after each convolution layer is customizable.\n\nimport MLJFlux\n\nstruct MyConvBuilder\n    filter_size::Int\n    channels1::Int\n    channels2::Int\n    channels3::Int\nend\n\nmake2d(x::AbstractArray) = reshape(x, :, size(x)[end])\n\nfunction MLJFlux.build(b::MyConvBuilder, rng, n_in, n_out, n_channels)\n    k, c1, c2, c3 = b.filter_size, b.channels1, b.channels2, b.channels3\n    mod(k, 2) == 1 || error(\"`filter_size` must be odd. \")\n    p = div(k - 1, 2) ## padding to preserve image size\n    init = Flux.glorot_uniform(rng)\n    front = Chain(\n        Conv((k, k), n_channels => c1, pad=(p, p), relu, init=init),\n        MaxPool((2, 2)),\n        Conv((k, k), c1 => c2, pad=(p, p), relu, init=init),\n        MaxPool((2, 2)),\n        Conv((k, k), c2 => c3, pad=(p, p), relu, init=init),\n        MaxPool((2 ,2)),\n        make2d)\n    d = Flux.outputsize(front, (n_in..., n_channels, 1)) |> first\n    return Chain(front, Dense(d, n_out, init=init))\nend\n\nIt is important to note that in our build function, there is no final softmax. This is applied by default in all MLJFlux classifiers (override this using the finaliser hyperparameter).\n\nNow that our builder is defined, we can instantiate the actual MLJFlux model. If you have a GPU, you can substitute in acceleration=CUDALibs() below to speed up training.\n\nImageClassifier = @load ImageClassifier pkg=MLJFlux\nclf = ImageClassifier(builder=MyConvBuilder(3, 16, 32, 32),\n                      batch_size=50,\n                      epochs=10,\n                      rng=123)\n\nYou can add Flux options such as optimiser and loss in the snippet above. Currently, loss must be a flux-compatible loss, and not an MLJ measure.\n\nNext, we can bind the model with the data in a machine, and train using the first 500 images:\n\nmach = machine(clf, images, labels);\nfit!(mach, rows=1:500, verbosity=2);\nreport(mach)\nchain = fitted_params(mach)\nFlux.params(chain)[2]\n\nWe can tack on 20 more epochs by modifying the epochs field, and iteratively fit some more:\n\nclf.epochs = clf.epochs + 20\nfit!(mach, rows=1:500, verbosity=2);\n\nWe can also make predictions and calculate an out-of-sample loss estimate, using any MLJ measure (loss/score):\n\npredicted_labels = predict(mach, rows=501:1000);\ncross_entropy(predicted_labels, labels[501:1000])\n\nThe preceding fit!/predict/evaluate workflow can be alternatively executed as follows:\n\nevaluate!(mach,\n          resampling=Holdout(fraction_train=0.5),\n          measure=cross_entropy,\n          rows=1:1000,\n          verbosity=0)\n\nSee also NeuralNetworkClassifier.","category":"section"},{"location":"models/DeterministicConstantRegressor_MLJModels/#DeterministicConstantRegressor_MLJModels","page":"DeterministicConstantRegressor","title":"DeterministicConstantRegressor","text":"DeterministicConstantRegressor\n\nA model type for constructing a deterministic constant regressor, based on MLJModels.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nDeterministicConstantRegressor = @load DeterministicConstantRegressor pkg=MLJModels\n\nDo model = DeterministicConstantRegressor() to construct an instance with default hyper-parameters. ","category":"section"},{"location":"models/SMOTE_Imbalance/#SMOTE_Imbalance","page":"SMOTE","title":"SMOTE","text":"Initiate a SMOTE model with the given hyper-parameters.\n\nSMOTE\n\nA model type for constructing a smote, based on Imbalance.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nSMOTE = @load SMOTE pkg=Imbalance\n\nDo model = SMOTE() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in SMOTE(k=...).\n\nSMOTE implements the SMOTE algorithm to correct for class imbalance as in N. V. Chawla, K. W. Bowyer, L. O.Hall, W. P. Kegelmeyer, “SMOTE: synthetic minority over-sampling technique,” Journal of artificial intelligence research, 321-357, 2002.","category":"section"},{"location":"models/SMOTE_Imbalance/#Training-data","page":"SMOTE","title":"Training data","text":"In MLJ or MLJBase, wrap the model in a machine by\n\nmach = machine(model)\n\nThere is no need to provide any data here because the model is a static transformer.\n\nLikewise, there is no need to fit!(mach).\n\nFor default values of the hyper-parameters, model can be constructed by\n\nmodel = SMOTE()","category":"section"},{"location":"models/SMOTE_Imbalance/#Hyperparameters","page":"SMOTE","title":"Hyperparameters","text":"k=5: Number of nearest neighbors to consider in the SMOTE algorithm.  Should be within   the range [1, n - 1], where n is the number of observations; otherwise set to the   nearest of these two values.\nratios=1.0: A parameter that controls the amount of oversampling to be done for each class\nCan be a float and in this case each class will be oversampled to the size of the majority class times the float. By default, all classes are oversampled to the size of the majority class\nCan be a dictionary mapping each class label to the float ratio for that class\nrng::Union{AbstractRNG, Integer}=default_rng(): Either an AbstractRNG object or an Integer    seed to be used with Xoshiro if the Julia VERSION supports it. Otherwise, uses MersenneTwister`.","category":"section"},{"location":"models/SMOTE_Imbalance/#Transform-Inputs","page":"SMOTE","title":"Transform Inputs","text":"X: A matrix or table of floats where each row is an observation from the dataset\ny: An abstract vector of labels (e.g., strings) that correspond to the observations in X","category":"section"},{"location":"models/SMOTE_Imbalance/#Transform-Outputs","page":"SMOTE","title":"Transform Outputs","text":"Xover: A matrix or table that includes original data and the new observations    due to oversampling. depending on whether the input X is a matrix or table respectively\nyover: An abstract vector of labels corresponding to Xover","category":"section"},{"location":"models/SMOTE_Imbalance/#Operations","page":"SMOTE","title":"Operations","text":"transform(mach, X, y): resample the data X and y using SMOTE, returning both the new and original observations","category":"section"},{"location":"models/SMOTE_Imbalance/#Example","page":"SMOTE","title":"Example","text":"using MLJ\nimport Imbalance\n\n## set probability of each class\nclass_probs = [0.5, 0.2, 0.3]                         \nnum_rows, num_continuous_feats = 100, 5\n## generate a table and categorical vector accordingly\nX, y = Imbalance.generate_imbalanced_data(num_rows, num_continuous_feats; \n                                class_probs, rng=42)    \n\njulia> Imbalance.checkbalance(y)\n1: ▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 19 (39.6%) \n2: ▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 33 (68.8%) \n0: ▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 48 (100.0%) \n\n## load SMOTE\nSMOTE = @load SMOTE pkg=Imbalance\n\n## wrap the model in a machine\noversampler = SMOTE(k=5, ratios=Dict(0=>1.0, 1=> 0.9, 2=>0.8), rng=42)\nmach = machine(oversampler)\n\n## provide the data to transform (there is nothing to fit)\nXover, yover = transform(mach, X, y)\n\njulia> Imbalance.checkbalance(yover)\n2: ▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 38 (79.2%) \n1: ▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 43 (89.6%) \n0: ▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 48 (100.0%) \n","category":"section"},{"location":"adding_models_for_general_use/#Adding-Models-for-General-Use","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"To write a complete MLJ model interface for new or existing machine learning models, suitable for addition to the MLJ Model Registry, consult the MLJModelInterface.jl documentation.\n\nFor quick-and-dirty user-defined models see Simple User Defined Models.","category":"section"},{"location":"models/MaxnetBinaryClassifier_Maxnet/#MaxnetBinaryClassifier_Maxnet","page":"MaxnetBinaryClassifier","title":"MaxnetBinaryClassifier","text":"MaxnetBinaryClassifier\n\nA model type for constructing a Maxnet, based on Maxnet.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nMaxnetBinaryClassifier = @load MaxnetBinaryClassifier pkg=Maxnet\n\nDo model = MaxnetBinaryClassifier() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in MaxnetBinaryClassifier(features=...).","category":"section"},{"location":"models/MaxnetBinaryClassifier_Maxnet/#Training-data","page":"MaxnetBinaryClassifier","title":"Training data","text":"In MLJ or MLJBase, bind an instance model to data with\n\nmach = machine(model, X, y)\n\nwhere\n\nX: any table of input features (eg, a DataFrame) whose columns each have one of the following element scitypes: Continuous or <:Multiclass. Check scitypes with schema(X).\ny: is the target, which can be any AbstractVector whose element scitype is <:Binary. The first class should refer to background values, and the second class to presence values.","category":"section"},{"location":"models/MaxnetBinaryClassifier_Maxnet/#Hyper-parameters","page":"MaxnetBinaryClassifier","title":"Hyper-parameters","text":"features: Specifies which features classes to use in the model, e.g. \"lqh\" for linear, quadratic and hinge features.    See also Maxnet.maxnet\nregularization_multiplier = 1.0: 'Adjust how tight the model will fit. Increasing this will reduce overfitting.\nregularization_function: A function to compute the regularization of each feature class. Defaults to Maxnet.default_regularization\naddsamplestobackground = true: Controls wether to add presence values to the background.\nn_knots = 50: The number of knots used for Threshold and Hinge features. A higher number gives more flexibility for these features.\nweight_factor = 100.0: A Float64 value to adjust the weight of the background samples.\nlink = Maxnet.CloglogLink(): The link function to use when predicting. See Maxnet.predict\nclamp = false: Clamp values passed to MLJBase.predict to the range the model was trained on.","category":"section"},{"location":"models/MaxnetBinaryClassifier_Maxnet/#Operations","page":"MaxnetBinaryClassifier","title":"Operations","text":"predict(mach, Xnew): return predictions of the target given features Xnew having the same scitype as X above. Predictions are  probabilistic and can be interpreted as the probability of presence.","category":"section"},{"location":"models/MaxnetBinaryClassifier_Maxnet/#Fitted-Parameters","page":"MaxnetBinaryClassifier","title":"Fitted Parameters","text":"The fields of fitted_params(mach) are:\n\nfitresult: A Tuple where the first entry is the Maxnet.MaxnetModel returned by the Maxnet algorithm   and the second the entry is the classes of y","category":"section"},{"location":"models/MaxnetBinaryClassifier_Maxnet/#Report","page":"MaxnetBinaryClassifier","title":"Report","text":"The fields of report(mach) are:\n\nselected_variables: A Vector of Symbols of the variables that were selected.\nselected_features: A Vector of Maxnet.ModelMatrixColumn with the features that were selected.\ncomplexity: the number of selected features in the model.","category":"section"},{"location":"models/MaxnetBinaryClassifier_Maxnet/#Example","page":"MaxnetBinaryClassifier","title":"Example","text":"using MLJBase, Maxnet\np_a, env = Maxnet.bradypus()\ny = coerce(p_a, Binary)\nX = coerce(env, Count => Continuous)\n\nmach = machine(MaxnetBinaryClassifier(features = \"lqp\"), X, y)\nfit!(mach)\nyhat = MLJBase.predict(mach, env)\n","category":"section"},{"location":"models/PartLS_PartitionedLS/#PartLS_PartitionedLS","page":"PartLS","title":"PartLS","text":"PartLS\n\nA model type for fitting a partitioned least squares model to data. Both an MLJ and native interface are provided.","category":"section"},{"location":"models/PartLS_PartitionedLS/#MLJ-Interface","page":"PartLS","title":"MLJ Interface","text":"From MLJ, the type can be imported using\n\nPartLS = @load PartLS pkg=PartitionedLS\n\nConstruct an instance with default hyper-parameters using the syntax model = PartLS(). Provide keyword arguments to override hyper-parameter defaults, as in model = PartLS(P=...).","category":"section"},{"location":"models/PartLS_PartitionedLS/#Training-data","page":"PartLS","title":"Training data","text":"In MLJ or MLJBase, bind an instance model to data with\n\nmach = machine(model, X, y)\n\nwhere\n\nX: any matrix or table with Continuous element scitype.       Check column scitypes of a table X with schema(X).\ny: any vector with Continuous element scitype. Check scitype with scitype(y).\n\nTrain the machine using fit!(mach).","category":"section"},{"location":"models/PartLS_PartitionedLS/#Hyper-parameters","page":"PartLS","title":"Hyper-parameters","text":"Optimizer: the optimization algorithm to use. It can be Opt, Alt or BnB (names exported by PartitionedLS.jl).\nP: the partition matrix. It is a binary matrix where each row corresponds to a partition and each column corresponds to a feature. The element P_{k, i} = 1 if feature i belongs to partition k.\nη: the regularization parameter. It controls the strength of the regularization.\nϵ: the tolerance parameter. It is used to determine when the Alt optimization algorithm has converged. Only used by the Alt algorithm.\nT: the maximum number of iterations. It is used to determine when to stop the Alt optimization algorithm has converged. Only used by the Alt algorithm.\nrng: the random number generator to use.\nIf nothing, the global random number generator rand is used.\nIf an integer, the global number generator rand is used after seeding it with the given integer.\nIf an object of type AbstractRNG, the given random number generator is used.","category":"section"},{"location":"models/PartLS_PartitionedLS/#Operations","page":"PartLS","title":"Operations","text":"predict(mach, Xnew): return the predictions of the model on new data Xnew","category":"section"},{"location":"models/PartLS_PartitionedLS/#Fitted-parameters","page":"PartLS","title":"Fitted parameters","text":"The fields of fitted_params(mach) are:\n\nα: the values of the α variables. For each partition k, it holds the values of the α variables are such that sum_i in P_k alpha_k = 1.\nβ: the values of the β variables. For each partition k, β_k is the coefficient that multiplies the features in the k-th partition.\nt: the intercept term of the model.\nP: the partition matrix. It is a binary matrix where each row corresponds to a partition and each column corresponds to a feature. The element P_{k, i} = 1 if feature i belongs to partition k.","category":"section"},{"location":"models/PartLS_PartitionedLS/#Examples","page":"PartLS","title":"Examples","text":"PartLS = @load PartLS pkg=PartitionedLS\n\nX = [[1. 2. 3.];\n     [3. 3. 4.];\n     [8. 1. 3.];\n     [5. 3. 1.]]\n\ny = [1.;\n     1.;\n     2.;\n     3.]\n\nP = [[1 0];\n     [1 0];\n     [0 1]]\n\n\nmodel = PartLS(P=P)\nmach = machine(model, X, y) |> fit!\n\n## predictions on the training set:\npredict(mach, X)\n","category":"section"},{"location":"models/PartLS_PartitionedLS/#Native-Interface","page":"PartLS","title":"Native Interface","text":"using PartitionedLS\n\nX = [[1. 2. 3.];\n     [3. 3. 4.];\n     [8. 1. 3.];\n     [5. 3. 1.]]\n\ny = [1.;\n     1.;\n     2.;\n     3.]\n\nP = [[1 0];\n     [1 0];\n     [0 1]]\n\n\n## fit using the optimal algorithm\nresult = fit(Opt, X, y, P, η = 0.0)\ny_hat = predict(result.model, X)\n\nFor other fit keyword options, refer to the \"Hyper-parameters\" section for the MLJ interface.","category":"section"},{"location":"models/HDBSCAN_MLJScikitLearnInterface/#HDBSCAN_MLJScikitLearnInterface","page":"HDBSCAN","title":"HDBSCAN","text":"HDBSCAN\n\nA model type for constructing a hdbscan, based on MLJScikitLearnInterface.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nHDBSCAN = @load HDBSCAN pkg=MLJScikitLearnInterface\n\nDo model = HDBSCAN() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in HDBSCAN(min_cluster_size=...).\n\nHierarchical Density-Based Spatial Clustering of Applications with  Noise. Performs DBSCAN over varying epsilon values and  integrates the result to find a clustering that gives the best  stability over epsilon. This allows HDBSCAN to find clusters of  varying densities (unlike DBSCAN), and be more robust to  parameter selection. ","category":"section"},{"location":"models/MultiTaskElasticNetCVRegressor_MLJScikitLearnInterface/#MultiTaskElasticNetCVRegressor_MLJScikitLearnInterface","page":"MultiTaskElasticNetCVRegressor","title":"MultiTaskElasticNetCVRegressor","text":"MultiTaskElasticNetCVRegressor\n\nA model type for constructing a multi-target elastic net regressor with built-in cross-validation, based on MLJScikitLearnInterface.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nMultiTaskElasticNetCVRegressor = @load MultiTaskElasticNetCVRegressor pkg=MLJScikitLearnInterface\n\nDo model = MultiTaskElasticNetCVRegressor() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in MultiTaskElasticNetCVRegressor(l1_ratio=...).","category":"section"},{"location":"models/MultiTaskElasticNetCVRegressor_MLJScikitLearnInterface/#Hyper-parameters","page":"MultiTaskElasticNetCVRegressor","title":"Hyper-parameters","text":"l1_ratio = 0.5\neps = 0.001\nn_alphas = 100\nalphas = nothing\nfit_intercept = true\nmax_iter = 1000\ntol = 0.0001\ncv = 5\ncopy_X = true\nverbose = 0\nn_jobs = nothing\nrandom_state = nothing\nselection = cyclic","category":"section"},{"location":"models/XGBoostRegressor_XGBoost/#XGBoostRegressor_XGBoost","page":"XGBoostRegressor","title":"XGBoostRegressor","text":"XGBoostRegressor\n\nA model type for constructing a eXtreme Gradient Boosting Regressor, based on XGBoost.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nXGBoostRegressor = @load XGBoostRegressor pkg=XGBoost\n\nDo model = XGBoostRegressor() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in XGBoostRegressor(test=...).\n\nUnivariate continuous regression using xgboost.","category":"section"},{"location":"models/XGBoostRegressor_XGBoost/#Training-data","page":"XGBoostRegressor","title":"Training data","text":"In MLJ or MLJBase, bind an instance model to data with\n\nm = machine(model, X, y)\n\nwhere\n\nX: any table of input features whose columns have Continuous element scitype; check  column scitypes with schema(X).\ny: is an AbstractVector target with Continuous elements; check the scitype with scitype(y).\n\nTrain using fit!(m, rows=...).","category":"section"},{"location":"models/XGBoostRegressor_XGBoost/#Hyper-parameters","page":"XGBoostRegressor","title":"Hyper-parameters","text":"See https://xgboost.readthedocs.io/en/stable/parameter.html.","category":"section"},{"location":"models/LinearCountRegressor_GLM/#LinearCountRegressor_GLM","page":"LinearCountRegressor","title":"LinearCountRegressor","text":"LinearCountRegressor\n\nA model type for constructing a linear count regressor, based on GLM.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nLinearCountRegressor = @load LinearCountRegressor pkg=GLM\n\nDo model = LinearCountRegressor() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in LinearCountRegressor(fit_intercept=...).\n\nLinearCountRegressor is a generalized linear model, specialised to the case of a Count target variable (non-negative, unbounded integer) with user-specified link function. Options exist to specify an intercept or offset feature.","category":"section"},{"location":"models/LinearCountRegressor_GLM/#Training-data","page":"LinearCountRegressor","title":"Training data","text":"In MLJ or MLJBase, bind an instance model to data with one of:\n\nmach = machine(model, X, y)\nmach = machine(model, X, y, w)\n\nHere\n\nX: is any table of input features (eg, a DataFrame) whose columns are of scitype Continuous; check the scitype with schema(X)\ny: is the target, which can be any AbstractVector whose element scitype is Count; check the scitype with schema(y)\nw: is a vector of Real per-observation weights\n\nTrain the machine using fit!(mach, rows=...).","category":"section"},{"location":"models/LinearCountRegressor_GLM/#Hyper-parameters","page":"LinearCountRegressor","title":"Hyper-parameters","text":"fit_intercept=true: Whether to calculate the intercept for this model. If set to false,  no intercept will be calculated (e.g. the data is expected to be centered)\ndistribution=Distributions.Poisson(): The distribution which the residuals/errors of the  model should fit.\nlink=GLM.LogLink(): The function which links the linear prediction function to the  probability of a particular outcome or class. This should be one of the following:  GLM.IdentityLink(), GLM.InverseLink(), GLM.InverseSquareLink(), GLM.LogLink(),  GLM.SqrtLink().\noffsetcol=nothing: Name of the column to be used as an offset, if any.  An offset is a  variable which is known to have a coefficient of 1.\nmaxiter::Integer=30: The maximum number of iterations allowed to achieve convergence.\natol::Real=1e-6: Absolute threshold for convergence. Convergence is achieved when the  relative change in deviance is less than `max(rtol*dev, atol). This term exists to avoid  failure when deviance is unchanged except for rounding errors.\nrtol::Real=1e-6: Relative threshold for convergence. Convergence is achieved when the  relative change in deviance is less than `max(rtol*dev, atol). This term exists to avoid  failure when deviance is unchanged except for rounding errors.\nminstepfac::Real=0.001: Minimum step fraction. Must be between 0 and 1. Lower bound for the factor used to update the linear fit.\nreport_keys: Vector of keys for the report. Possible keys are: :deviance, :dof_residual, :stderror, :vcov, :coef_table and :glm_model. By default only :glm_model is excluded.","category":"section"},{"location":"models/LinearCountRegressor_GLM/#Operations","page":"LinearCountRegressor","title":"Operations","text":"predict(mach, Xnew): return predictions of the target given new features Xnew having  the same Scitype as X above. Predictions are probabilistic.\npredict_mean(mach, Xnew): instead return the mean of each prediction above\npredict_median(mach, Xnew): instead return the median of each prediction above.","category":"section"},{"location":"models/LinearCountRegressor_GLM/#Fitted-parameters","page":"LinearCountRegressor","title":"Fitted parameters","text":"The fields of fitted_params(mach) are:\n\nfeatures: The names of the features encountered during model fitting.\ncoef: The linear coefficients determined by the model.\nintercept: The intercept determined by the model.","category":"section"},{"location":"models/LinearCountRegressor_GLM/#Report","page":"LinearCountRegressor","title":"Report","text":"The fields of report(mach) are:\n\ndeviance: Measure of deviance of fitted model with respect to a perfectly fitted model. For a linear model, this is the weighted residual sum of squares\ndof_residual: The degrees of freedom for residuals, when meaningful.\nstderror: The standard errors of the coefficients.\nvcov: The estimated variance-covariance matrix of the coefficient estimates.\ncoef_table: Table which displays coefficients and summarizes their significance and confidence intervals.\nglm_model: The raw fitted model returned by GLM.lm. Note this points to training data. Refer to the GLM.jl documentation for usage.","category":"section"},{"location":"models/LinearCountRegressor_GLM/#Examples","page":"LinearCountRegressor","title":"Examples","text":"using MLJ\nimport MLJ.Distributions.Poisson\n\n## Generate some data whose target y looks Poisson when conditioned on\n## X:\nN = 10_000\nw = [1.0, -2.0, 3.0]\nmu(x) = exp(w'x) ## mean for a log link function\nXmat = rand(N, 3)\nX = MLJ.table(Xmat)\ny = map(1:N) do i\n    x = Xmat[i, :]\n    rand(Poisson(mu(x)))\nend;\n\nCountRegressor = @load LinearCountRegressor pkg=GLM\nmodel = CountRegressor(fit_intercept=false)\nmach = machine(model, X, y)\nfit!(mach)\n\nXnew = MLJ.table(rand(3, 3))\nyhat = predict(mach, Xnew)\nyhat_point = predict_mean(mach, Xnew)\n\n## get coefficients approximating `w`:\njulia> fitted_params(mach).coef\n3-element Vector{Float64}:\n  0.9969008753103842\n -2.0255901752504775\n  3.014407534033522\n\nreport(mach)\n\nSee also LinearRegressor, LinearBinaryClassifier","category":"section"},{"location":"models/ElasticNetCVRegressor_MLJScikitLearnInterface/#ElasticNetCVRegressor_MLJScikitLearnInterface","page":"ElasticNetCVRegressor","title":"ElasticNetCVRegressor","text":"ElasticNetCVRegressor\n\nA model type for constructing a elastic net regression with built-in cross-validation, based on MLJScikitLearnInterface.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nElasticNetCVRegressor = @load ElasticNetCVRegressor pkg=MLJScikitLearnInterface\n\nDo model = ElasticNetCVRegressor() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in ElasticNetCVRegressor(l1_ratio=...).","category":"section"},{"location":"models/ElasticNetCVRegressor_MLJScikitLearnInterface/#Hyper-parameters","page":"ElasticNetCVRegressor","title":"Hyper-parameters","text":"l1_ratio = 0.5\neps = 0.001\nn_alphas = 100\nalphas = nothing\nfit_intercept = true\nprecompute = auto\nmax_iter = 1000\ntol = 0.0001\ncv = 5\ncopy_X = true\nverbose = 0\nn_jobs = nothing\npositive = false\nrandom_state = nothing\nselection = cyclic","category":"section"},{"location":"models/NeuralNetworkRegressor_BetaML/#NeuralNetworkRegressor_BetaML","page":"NeuralNetworkRegressor","title":"NeuralNetworkRegressor","text":"mutable struct NeuralNetworkRegressor <: MLJModelInterface.Deterministic\n\nA simple but flexible Feedforward Neural Network, from the Beta Machine Learning Toolkit (BetaML) for regression of a single dimensional target.","category":"section"},{"location":"models/NeuralNetworkRegressor_BetaML/#Parameters:","page":"NeuralNetworkRegressor","title":"Parameters:","text":"layers: Array of layer objects [def: nothing, i.e. basic network]. See subtypes(BetaML.AbstractLayer) for supported layers\nloss: Loss (cost) function [def: BetaML.squared_cost]. Should always assume y and ŷ as matrices, even if the regression task is 1-D\nwarning: Warning\nIf you change the parameter loss, you need to either provide its derivative on the parameter dloss or use autodiff with dloss=nothing.\ndloss: Derivative of the loss function [def: BetaML.dsquared_cost, i.e. use the derivative of the squared cost]. Use nothing for autodiff.\nepochs: Number of epochs, i.e. passages trough the whole training sample [def: 200]\nbatch_size: Size of each individual batch [def: 16]\nopt_alg: The optimisation algorithm to update the gradient at each batch [def: BetaML.ADAM()]. See subtypes(BetaML.OptimisationAlgorithm) for supported optimizers\nshuffle: Whether to randomly shuffle the data at each iteration (epoch) [def: true]\ndescr: An optional title and/or description for this model\ncb: A call back function to provide information during training [def: fitting_info]\nrng: Random Number Generator (see FIXEDSEED) [deafult: Random.GLOBAL_RNG]","category":"section"},{"location":"models/NeuralNetworkRegressor_BetaML/#Notes:","page":"NeuralNetworkRegressor","title":"Notes:","text":"data must be numerical\nthe label should be be a n-records vector.","category":"section"},{"location":"models/NeuralNetworkRegressor_BetaML/#Example:","page":"NeuralNetworkRegressor","title":"Example:","text":"julia> using MLJ\n\njulia> X, y        = @load_boston;\n\njulia> modelType   = @load NeuralNetworkRegressor pkg = \"BetaML\" verbosity=0\nBetaML.Nn.NeuralNetworkRegressor\n\njulia> layers                      = [BetaML.DenseLayer(12,20,f=BetaML.relu),BetaML.DenseLayer(20,20,f=BetaML.relu),BetaML.DenseLayer(20,1,f=BetaML.relu)];\n\njulia> model       = modelType(layers=layers,opt_alg=BetaML.ADAM());\nNeuralNetworkRegressor(\n  layers = BetaML.Nn.AbstractLayer[BetaML.Nn.DenseLayer([-0.23249759178069676 -0.4125090172711131 … 0.41401934928739 -0.33017881111237535; -0.27912169279319965 0.270551221249931 … 0.19258414323473344 0.1703002982374256; … ; 0.31186742456482447 0.14776438287394805 … 0.3624993442655036 0.1438885872964824; 0.24363744610286758 -0.3221033024934767 … 0.14886090419299408 0.038411663101909355], [-0.42360286004241765, -0.34355377040029594, 0.11510963232946697, 0.29078650404397893, -0.04940236502546075, 0.05142849152316714, -0.177685375947775, 0.3857630523957018, -0.25454667127064756, -0.1726731848206195, 0.29832456225553444, -0.21138505291162835, -0.15763643112604903, -0.08477044513587562, -0.38436681165349196, 0.20538016429104916, -0.25008157754468335, 0.268681800562054, 0.10600581996650865, 0.4262194464325672], BetaML.Utils.relu, BetaML.Utils.drelu), BetaML.Nn.DenseLayer([-0.08534180387478185 0.19659398307677617 … -0.3413633217504578 -0.0484925247381256; 0.0024419192794883915 -0.14614102508129 … -0.21912059923003044 0.2680725396694708; … ; 0.25151545823147886 -0.27532269951606037 … 0.20739970895058063 0.2891938885916349; -0.1699020711688904 -0.1350423717084296 … 0.16947589410758873 0.3629006047373296], [0.2158116357688406, -0.3255582642532289, -0.057314442103850394, 0.29029696770539953, 0.24994080694366455, 0.3624239027782297, -0.30674318230919984, -0.3854738338935017, 0.10809721838554087, 0.16073511121016176, -0.005923262068960489, 0.3157147976348795, -0.10938918304264739, -0.24521229198853187, -0.307167732178712, 0.0808907777008302, -0.014577497150872254, -0.0011287181458157214, 0.07522282588658086, 0.043366500526073104], BetaML.Utils.relu, BetaML.Utils.drelu), BetaML.Nn.DenseLayer([-0.021367697115938555 -0.28326652172347155 … 0.05346175368370165 -0.26037328415871647], [-0.2313659199724562], BetaML.Utils.relu, BetaML.Utils.drelu)], \n  loss = BetaML.Utils.squared_cost, \n  dloss = BetaML.Utils.dsquared_cost, \n  epochs = 100, \n  batch_size = 32, \n  opt_alg = BetaML.Nn.ADAM(BetaML.Nn.var\"#90#93\"(), 1.0, 0.9, 0.999, 1.0e-8, BetaML.Nn.Learnable[], BetaML.Nn.Learnable[]), \n  shuffle = true, \n  descr = \"\", \n  cb = BetaML.Nn.fitting_info, \n  rng = Random._GLOBAL_RNG())\n\njulia> mach        = machine(model, X, y);\n\njulia> fit!(mach);\n\njulia> ŷ    = predict(mach, X);\n\njulia> hcat(y,ŷ)\n506×2 Matrix{Float64}:\n 24.0  30.7726\n 21.6  28.0811\n 34.7  31.3194\n  ⋮    \n 23.9  30.9032\n 22.0  29.49\n 11.9  27.2438","category":"section"},{"location":"learning_mlj/#Learning-MLJ","page":"Learning MLJ","title":"Learning MLJ","text":"The main sources for MLJ tutorials are:\n\nComplete list: structured by topic\nData Science Tutorials: smaller curated collection of tutorials, generally completed in sequence\n\nIf you are an ML practitioner transitioning from another platform, we highly recommend MLJ for Data Scientists in Two Hours.","category":"section"},{"location":"learning_mlj/#Other-resources","page":"Learning MLJ","title":"Other resources","text":"MLJ Cheatsheet (or here)\nGetting help and reporting problems.\nJulia Data Science\nMLJTutorial: material for a 4 hour MLJ workshop\nMLCourse: Teaching material for an introductory machine learning course at EPFL (for an interactive preview see here).","category":"section"},{"location":"models/LassoLarsICRegressor_MLJScikitLearnInterface/#LassoLarsICRegressor_MLJScikitLearnInterface","page":"LassoLarsICRegressor","title":"LassoLarsICRegressor","text":"LassoLarsICRegressor\n\nA model type for constructing a Lasso model with LARS using BIC or AIC for model selection, based on MLJScikitLearnInterface.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nLassoLarsICRegressor = @load LassoLarsICRegressor pkg=MLJScikitLearnInterface\n\nDo model = LassoLarsICRegressor() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in LassoLarsICRegressor(criterion=...).","category":"section"},{"location":"models/LassoLarsICRegressor_MLJScikitLearnInterface/#Hyper-parameters","page":"LassoLarsICRegressor","title":"Hyper-parameters","text":"criterion = aic\nfit_intercept = true\nverbose = false\nprecompute = auto\nmax_iter = 500\neps = 2.220446049250313e-16\ncopy_X = true\npositive = false","category":"section"},{"location":"models/GaussianMixtureRegressor_BetaML/#GaussianMixtureRegressor_BetaML","page":"GaussianMixtureRegressor","title":"GaussianMixtureRegressor","text":"mutable struct GaussianMixtureRegressor <: MLJModelInterface.Deterministic\n\nA non-linear regressor derived from fitting the data on a probabilistic model (Gaussian Mixture Model). Relatively fast but generally not very precise, except for data with a structure matching the chosen underlying mixture.\n\nThis is the single-target version of the model. If you want to predict several labels (y) at once, use the MLJ model MultitargetGaussianMixtureRegressor.","category":"section"},{"location":"models/GaussianMixtureRegressor_BetaML/#Hyperparameters:","page":"GaussianMixtureRegressor","title":"Hyperparameters:","text":"n_classes::Int64: Number of mixtures (latent classes) to consider [def: 3]\ninitial_probmixtures::Vector{Float64}: Initial probabilities of the categorical distribution (n_classes x 1) [default: []]\nmixtures::Union{Type, Vector{<:BetaML.GMM.AbstractMixture}}: An array (of length n_classes) of the mixtures to employ (see the [?GMM](@ref GMM) module). Each mixture object can be provided with or without its parameters (e.g. mean and variance for the gaussian ones). Fully qualified mixtures are useful only if theinitialisationstrategyparameter is  set to \"gived\" This parameter can also be given symply in term of a _type. In this case it is automatically extended to a vector of n_classesmixtures of the specified type. Note that mixing of different mixture types is not currently supported. [def:[DiagonalGaussian() for i in 1:n_classes]`]\ntol::Float64: Tolerance to stop the algorithm [default: 10^(-6)]\nminimum_variance::Float64: Minimum variance for the mixtures [default: 0.05]\nminimum_covariance::Float64: Minimum covariance for the mixtures with full covariance matrix [default: 0]. This should be set different than minimum_variance (see notes).\ninitialisation_strategy::String: The computation method of the vector of the initial mixtures. One of the following:\n\"grid\": using a grid approach\n\"given\": using the mixture provided in the fully qualified mixtures parameter\n\"kmeans\": use first kmeans (itself initialised with a \"grid\" strategy) to set the initial mixture centers [default]\nNote that currently \"random\" and \"shuffle\" initialisations are not supported in gmm-based algorithms.\nmaximum_iterations::Int64: Maximum number of iterations [def: typemax(Int64), i.e. ∞]\nrng::Random.AbstractRNG: Random Number Generator [deafult: Random.GLOBAL_RNG]","category":"section"},{"location":"models/GaussianMixtureRegressor_BetaML/#Example:","page":"GaussianMixtureRegressor","title":"Example:","text":"julia> using MLJ\n\njulia> X, y      = @load_boston;\n\njulia> modelType = @load GaussianMixtureRegressor pkg = \"BetaML\" verbosity=0\nBetaML.GMM.GaussianMixtureRegressor\n\njulia> model     = modelType()\nGaussianMixtureRegressor(\n  n_classes = 3, \n  initial_probmixtures = Float64[], \n  mixtures = BetaML.GMM.DiagonalGaussian{Float64}[BetaML.GMM.DiagonalGaussian{Float64}(nothing, nothing), BetaML.GMM.DiagonalGaussian{Float64}(nothing, nothing), BetaML.GMM.DiagonalGaussian{Float64}(nothing, nothing)], \n  tol = 1.0e-6, \n  minimum_variance = 0.05, \n  minimum_covariance = 0.0, \n  initialisation_strategy = \"kmeans\", \n  maximum_iterations = 9223372036854775807, \n  rng = Random._GLOBAL_RNG())\n\njulia> mach      = machine(model, X, y);\n\njulia> fit!(mach);\n[ Info: Training machine(GaussianMixtureRegressor(n_classes = 3, …), …).\nIter. 1:        Var. of the post  21.74887448784976       Log-likelihood -21687.09917379566\n\njulia> ŷ         = predict(mach, X)\n506-element Vector{Float64}:\n 24.703442835305577\n 24.70344283512716\n  ⋮\n 17.172486989759676\n 17.172486989759644","category":"section"},{"location":"models/MultitargetNeuralNetworkRegressor_MLJFlux/#MultitargetNeuralNetworkRegressor_MLJFlux","page":"MultitargetNeuralNetworkRegressor","title":"MultitargetNeuralNetworkRegressor","text":"MultitargetNeuralNetworkRegressor\n\nA model type for constructing a multitarget neural network regressor, based on MLJFlux.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nMultitargetNeuralNetworkRegressor = @load MultitargetNeuralNetworkRegressor pkg=MLJFlux\n\nDo model = MultitargetNeuralNetworkRegressor() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in MultitargetNeuralNetworkRegressor(builder=...).\n\nMultitargetNeuralNetworkRegressor is for training a data-dependent Flux.jl neural network to predict a multi-valued Continuous target, represented as a table, given a table of Continuous features. Users provide a recipe for constructing the network, based on properties of the data that is encountered, by specifying an appropriate builder. See MLJFlux documentation for more on builders.\n\nIn addition to features with Continuous scientific element type, this model supports categorical features in the input table. If present, such features are embedded into dense vectors by the use of an additional EntityEmbedderLayer layer after the input, as described in Entity Embeddings of Categorical Variables by Cheng Guo, Felix Berkhahn arXiv, 2016.","category":"section"},{"location":"models/MultitargetNeuralNetworkRegressor_MLJFlux/#Training-data","page":"MultitargetNeuralNetworkRegressor","title":"Training data","text":"In MLJ or MLJBase, bind an instance model to data with\n\nmach = machine(model, X, y)\n\nHere:\n\nX provides input features and is either: (i) a Matrix with Continuous element scitype (typically Float32); or (ii) a table of input features (eg, a DataFrame) whose columns have Continuous, Multiclass or OrderedFactor element scitype; check column scitypes with schema(X).  If any Multiclass or OrderedFactor features appear, the constructed network will use an EntityEmbedderLayer layer to transform them into dense vectors. If X is a Matrix, it is assumed that columns correspond to features and rows corresponding to observations.\ny is the target, which can be any table or matrix of output targets whose element scitype is Continuous; check column scitypes with schema(y). If y is a Matrix, it is assumed to have columns corresponding to variables and rows corresponding to observations.","category":"section"},{"location":"models/MultitargetNeuralNetworkRegressor_MLJFlux/#Hyper-parameters","page":"MultitargetNeuralNetworkRegressor","title":"Hyper-parameters","text":"builder=MLJFlux.Linear(σ=Flux.relu): An MLJFlux builder that constructs a neural network. Possible builders include: Linear, Short, and MLP. See MLJFlux documentation for more on builders, and the example below for using the @builder convenience macro.\noptimiser::Optimisers.Adam(): An Optimisers.jl optimiser. The optimiser performs the updating of the weights of the network. To choose a learning rate (the update rate of the optimizer), a good rule of thumb is to start out at 10e-3, and tune using powers of 10 between 1 and 1e-7.\nloss=Flux.mse: The loss function which the network will optimize. Should be a function which can be called in the form loss(yhat, y).  Possible loss functions are listed in the Flux loss function documentation. For a regression task, natural loss functions are:\nFlux.mse\nFlux.mae\nFlux.msle\nFlux.huber_loss\nCurrently MLJ measures are not supported as loss functions here.\nepochs::Int=10: The duration of training, in epochs. Typically, one epoch represents one pass through the complete the training dataset.\nbatch_size::int=1: the batch size to be used for training, representing the number of samples per update of the network weights. Typically, batch size is between 8 and 512. Increassing batch size may accelerate training if acceleration=CUDALibs() and a GPU is available.\nlambda::Float64=0: The strength of the weight regularization penalty. Can be any value in the range [0, ∞). Note the history reports unpenalized losses.\nalpha::Float64=0: The L2/L1 mix of regularization, in the range [0, 1]. A value of 0 represents L2 regularization, and a value of 1 represents L1 regularization.\nrng::Union{AbstractRNG, Int64}: The random number generator or seed used during training. The default is Random.default_rng().\noptimizer_changes_trigger_retraining::Bool=false: Defines what happens when re-fitting a machine if the associated optimiser has changed. If true, the associated machine will retrain from scratch on fit! call, otherwise it will not.\nacceleration::AbstractResource=CPU1(): Defines on what hardware training is done. For Training on GPU, use CUDALibs().\nembedding_dims: a Dict whose keys are names of categorical features, given as symbols, and whose values are numbers representing the desired dimensionality of the entity embeddings of such features: an integer value of 7, say, sets the embedding dimensionality to 7; a float value of 0.5, say, sets the embedding dimensionality to ceil(0.5 * c), where c is the number of feature levels.  Unspecified feature dimensionality defaults to min(c - 1, 10).","category":"section"},{"location":"models/MultitargetNeuralNetworkRegressor_MLJFlux/#Operations","page":"MultitargetNeuralNetworkRegressor","title":"Operations","text":"predict(mach, Xnew): return predictions of the target given new features Xnew having the same scitype as X above. Predictions are deterministic.\ntransform(mach, Xnew): Assuming Xnew has the same schema as X, transform the categorical features of Xnew into dense Continuous vectors using the MLJFlux.EntityEmbedderLayer layer present in the network. Does nothing in case the model was trained on an input X that lacks categorical features.","category":"section"},{"location":"models/MultitargetNeuralNetworkRegressor_MLJFlux/#Fitted-parameters","page":"MultitargetNeuralNetworkRegressor","title":"Fitted parameters","text":"The fields of fitted_params(mach) are:\n\nchain: The trained \"chain\" (Flux.jl model), namely the series of layers,  functions, and activations  which make up the neural network.","category":"section"},{"location":"models/MultitargetNeuralNetworkRegressor_MLJFlux/#Report","page":"MultitargetNeuralNetworkRegressor","title":"Report","text":"The fields of report(mach) are:\n\ntraining_losses: A vector of training losses (penalised if lambda != 0) in  historical order, of length epochs + 1.  The first element is the pre-training loss.","category":"section"},{"location":"models/MultitargetNeuralNetworkRegressor_MLJFlux/#Examples","page":"MultitargetNeuralNetworkRegressor","title":"Examples","text":"In this example we apply a multi-target regression model to synthetic data:\n\nusing MLJ\nimport MLJFlux\nusing Flux\nimport Optimisers\n\nFirst, we generate some synthetic data (needs MLJBase 0.20.16 or higher):\n\nX, y = make_regression(100, 9; n_targets = 2) ## both tables\nschema(y)\nschema(X)\n\nSplitting off a test set:\n\n(X, Xtest), (y, ytest) = partition((X, y), 0.7, multi=true);\n\nNext, we can define a builder, making use of a convenience macro to do so.  In the following @builder call, n_in is a proxy for the number input features and n_out the number of target variables (both known at fit! time), while rng is a proxy for a RNG (which will be passed from the rng field of model defined below).\n\nbuilder = MLJFlux.@builder begin\n    init=Flux.glorot_uniform(rng)\n    Chain(\n        Dense(n_in, 64, relu, init=init),\n        Dense(64, 32, relu, init=init),\n        Dense(32, n_out, init=init),\n    )\nend\n\nInstantiating the regression model:\n\nMultitargetNeuralNetworkRegressor = @load MultitargetNeuralNetworkRegressor\nmodel = MultitargetNeuralNetworkRegressor(builder=builder, rng=123, epochs=20)\n\nWe will arrange for standardization of the the target by wrapping our model in  TransformedTargetModel, and standardization of the features by inserting the wrapped  model in a pipeline:\n\npipe = Standardizer |> TransformedTargetModel(model, transformer=Standardizer)\n\nIf we fit with a high verbosity (>1), we will see the losses during training. We can also see the losses in the output of report(mach)\n\nmach = machine(pipe, X, y)\nfit!(mach, verbosity=2)\n\n## first element initial loss, 2:end per epoch training losses\nreport(mach).transformed_target_model_deterministic.model.training_losses\n\nFor experimenting with learning rate, see the NeuralNetworkRegressor example.\n\npipe.transformed_target_model_deterministic.model.optimiser = Optimisers.Adam(0.0001)\n\nWith the learning rate fixed, we can now compute a CV estimate of the performance (using all data bound to mach) and compare this with performance on the test set:\n\n\n## CV estimate, based on `(X, y)`:\nevaluate!(mach, resampling=CV(nfolds=5), measure=multitarget_l2)\n\n## loss for `(Xtest, test)`:\nfit!(mach) ## trains on all data `(X, y)`\nyhat = predict(mach, Xtest)\nmultitarget_l2(yhat, ytest)\n\nSee also NeuralNetworkRegressor","category":"section"},{"location":"models/ContinuousEncoder_MLJTransforms/#ContinuousEncoder_MLJTransforms","page":"ContinuousEncoder","title":"ContinuousEncoder","text":"ContinuousEncoder\n\nA model type for constructing a continuous encoder, based on MLJTransforms.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nContinuousEncoder = @load ContinuousEncoder pkg=MLJTransforms\n\nDo model = ContinuousEncoder() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in ContinuousEncoder(drop_last=...).\n\nUse this model to arrange all features (features) of a table to have Continuous element scitype, by applying the following protocol to each feature ftr:\n\nIf ftr is already Continuous retain it.\nIf ftr is Multiclass, one-hot encode it.\nIf ftr is OrderedFactor, replace it with coerce(ftr, Continuous) (vector of floating point integers), unless ordered_factors=false is specified, in which case one-hot encode it.\nIf ftr is Count, replace it with coerce(ftr, Continuous).\nIf ftr has some other element scitype, or was not observed in fitting the encoder, drop it from the table.\n\nWarning: This transformer assumes that levels(col) for any Multiclass or OrderedFactor column, col, is the same for training data and new data to be transformed.\n\nTo selectively one-hot-encode categorical features (without dropping features) use OneHotEncoder instead.","category":"section"},{"location":"models/ContinuousEncoder_MLJTransforms/#Training-data","page":"ContinuousEncoder","title":"Training data","text":"In MLJ or MLJBase, bind an instance model to data with\n\nmach = machine(model, X)\n\nwhere\n\nX: any Tables.jl compatible table. features can be of mixed type but only those with element scitype Multiclass or OrderedFactor can be encoded. Check column scitypes with schema(X).\n\nTrain the machine using fit!(mach, rows=...).","category":"section"},{"location":"models/ContinuousEncoder_MLJTransforms/#Hyper-parameters","page":"ContinuousEncoder","title":"Hyper-parameters","text":"drop_last=true: whether to drop the column corresponding to the final class of one-hot encoded features. For example, a three-class feature is spawned into three new features if drop_last=false, but two just features otherwise.\none_hot_ordered_factors=false: whether to one-hot any feature with OrderedFactor element scitype, or to instead coerce it directly to a (single) Continuous feature using the order","category":"section"},{"location":"models/ContinuousEncoder_MLJTransforms/#Fitted-parameters","page":"ContinuousEncoder","title":"Fitted parameters","text":"The fields of fitted_params(mach) are:\n\nfeatures_to_keep: names of features that will not be dropped from the table\none_hot_encoder: the OneHotEncoder model instance for handling the one-hot encoding\none_hot_encoder_fitresult: the fitted parameters of the OneHotEncoder model","category":"section"},{"location":"models/ContinuousEncoder_MLJTransforms/#Report","page":"ContinuousEncoder","title":"Report","text":"features_to_keep: names of input features that will not be dropped from the table\nnew_features: names of all output features","category":"section"},{"location":"models/ContinuousEncoder_MLJTransforms/#Example","page":"ContinuousEncoder","title":"Example","text":"X = (name=categorical([\"Danesh\", \"Lee\", \"Mary\", \"John\"]),\n     grade=categorical([\"A\", \"B\", \"A\", \"C\"], ordered=true),\n     height=[1.85, 1.67, 1.5, 1.67],\n     n_devices=[3, 2, 4, 3],\n     comments=[\"the force\", \"be\", \"with you\", \"too\"])\n\njulia> schema(X)\n┌───────────┬──────────────────┐\n│ names     │ scitypes         │\n├───────────┼──────────────────┤\n│ name      │ Multiclass{4}    │\n│ grade     │ OrderedFactor{3} │\n│ height    │ Continuous       │\n│ n_devices │ Count            │\n│ comments  │ Textual          │\n└───────────┴──────────────────┘\n\nencoder = ContinuousEncoder(drop_last=true)\nmach = fit!(machine(encoder, X))\nW = transform(mach, X)\n\njulia> schema(W)\n┌──────────────┬────────────┐\n│ names        │ scitypes   │\n├──────────────┼────────────┤\n│ name__Danesh │ Continuous │\n│ name__John   │ Continuous │\n│ name__Lee    │ Continuous │\n│ grade        │ Continuous │\n│ height       │ Continuous │\n│ n_devices    │ Continuous │\n└──────────────┴────────────┘\n\njulia> setdiff(schema(X).names, report(mach).features_to_keep) ## dropped features\n1-element Vector{Symbol}:\n :comments\n\n\nSee also OneHotEncoder","category":"section"},{"location":"models/LaplaceRegressor_LaplaceRedux/#LaplaceRegressor_LaplaceRedux","page":"LaplaceRegressor","title":"LaplaceRegressor","text":"LaplaceRegressor\n\nA model type for constructing a laplace regressor, based on LaplaceRedux.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nLaplaceRegressor = @load LaplaceRegressor pkg=LaplaceRedux\n\nDo model = LaplaceRegressor() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in LaplaceRegressor(model=...).\n\nLaplaceRegressor implements the Laplace Redux – Effortless Bayesian Deep Learning, originally published in Daxberger, E., Kristiadi, A., Immer, A., Eschenhagen, R., Bauer, M., Hennig, P. (2021): \"Laplace Redux – Effortless Bayesian Deep Learning.\", NIPS'21: Proceedings of the 35th International Conference on Neural Information Processing Systems*, Article No. 1537, pp. 20089–20103 for regression models.","category":"section"},{"location":"models/LaplaceRegressor_LaplaceRedux/#Training-data","page":"LaplaceRegressor","title":"Training data","text":"In MLJ or MLJBase, given a dataset X,y and a Flux_Chain adapted to the dataset, pass the chain to the model\n\nlaplace_model = LaplaceRegressor(model = Flux_Chain,kwargs...)\n\nthen bind an instance laplace_model to data with\n\nmach = machine(laplace_model, X, y)\n\nwhere\n\nX: any table of input features (eg, a DataFrame) whose columns each have one of the following element scitypes: Continuous, Count, or <:OrderedFactor; check column scitypes with schema(X)\ny: is the target, which can be any AbstractVector whose element scitype is <:Continuous; check the scitype with scitype(y)\n\nTrain the machine using fit!(mach, rows=...).","category":"section"},{"location":"models/LaplaceRegressor_LaplaceRedux/#Hyperparameters-(format:-name-type-default-value-restrictions)","page":"LaplaceRegressor","title":"Hyperparameters (format: name-type-default value-restrictions)","text":"model::Union{Flux.Chain,Nothing} = nothing:                                                     Either nothing or a Flux model provided by the user and compatible with the dataset. In the former case, LaplaceRedux will use a standard MLP with 2 hidden layers with 20 neurons each.\nflux_loss = Flux.Losses.logitcrossentropy :                                                     a Flux loss function\noptimiser = Adam()                                                                              a Flux optimiser\nepochs::Integer = 1000::(_ > 0):                                                                the number of training epochs.\nbatch_size::Integer = 32::(_ > 0):                                                              the batch size.\nsubset_of_weights::Symbol = :all::(_ in (:all, :last_layer, :subnetwork)):                      the subset of weights to use, either :all, :last_layer, or :subnetwork.\nsubnetwork_indices = nothing:                                                                   the indices of the subnetworks.\nhessian_structure::Union{HessianStructure,Symbol,String} = :full::(_ in (:full, :diagonal)):    the structure of the Hessian matrix, either :full or :diagonal.\nbackend::Symbol = :GGN::(_ in (:GGN, :EmpiricalFisher)):                                        the backend to use, either :GGN or :EmpiricalFisher.\nobservational_noise (alias σ)::Float64 = 1.0:                                                   the standard deviation of the prior distribution.\nprior_mean (alias μ₀)::Float64 = 0.0:                                                           the mean of the prior distribution.\nprior_precision_matrix (alias P₀)::Union{AbstractMatrix,UniformScaling,Nothing} = nothing:      the covariance matrix of the prior distribution.\nfit_prior_nsteps::Int = 100::(_ > 0):                                                          the number of steps used to fit the priors.","category":"section"},{"location":"models/LaplaceRegressor_LaplaceRedux/#Operations","page":"LaplaceRegressor","title":"Operations","text":"predict(mach, Xnew): return predictions of the target given features Xnew having the same scitype as X above. Predictions are probabilistic, but uncalibrated.\npredict_mode(mach, Xnew): instead return the mode of each prediction above.","category":"section"},{"location":"models/LaplaceRegressor_LaplaceRedux/#Fitted-parameters","page":"LaplaceRegressor","title":"Fitted parameters","text":"The fields of fitted_params(mach) are:\n\nmean: The mean of the posterior distribution.\nH: The Hessian of the posterior distribution.\nP: The precision matrix of the posterior distribution.\ncov_matrix: The covariance matrix of the posterior distribution.\nn_data: The number of data points.\nn_params: The number of parameters.\nn_out: The number of outputs.\nloss: The loss value of the posterior distribution.","category":"section"},{"location":"models/LaplaceRegressor_LaplaceRedux/#Report","page":"LaplaceRegressor","title":"Report","text":"The fields of report(mach) are:\n\nloss_history: an array containing the total loss per epoch.","category":"section"},{"location":"models/LaplaceRegressor_LaplaceRedux/#Accessor-functions","page":"LaplaceRegressor","title":"Accessor functions","text":"training_losses(mach): return the loss history from report","category":"section"},{"location":"models/LaplaceRegressor_LaplaceRedux/#Examples","page":"LaplaceRegressor","title":"Examples","text":"using MLJ\nusing Flux\nLaplaceRegressor = @load LaplaceRegressor pkg=LaplaceRedux\nmodel = Chain(\n    Dense(4, 10, relu),\n    Dense(10, 10, relu),\n    Dense(10, 1)\n)\nmodel = LaplaceRegressor(model=model)\n\nX, y = make_regression(100, 4; noise=0.5, sparse=0.2, outliers=0.1)\nmach = machine(model, X, y) |> fit!\n\nXnew, _ = make_regression(3, 4; rng=123)\nyhat = predict(mach, Xnew) ## probabilistic predictions\npredict_mode(mach, Xnew)   ## point predictions\ntraining_losses(mach)      ## loss history per epoch\nfitted_params(mach)        ## NamedTuple with the fitted params of Laplace\n\n\nSee also LaplaceRedux.jl.","category":"section"},{"location":"models/BernoulliNBClassifier_MLJScikitLearnInterface/#BernoulliNBClassifier_MLJScikitLearnInterface","page":"BernoulliNBClassifier","title":"BernoulliNBClassifier","text":"BernoulliNBClassifier\n\nA model type for constructing a Bernoulli naive Bayes classifier, based on MLJScikitLearnInterface.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nBernoulliNBClassifier = @load BernoulliNBClassifier pkg=MLJScikitLearnInterface\n\nDo model = BernoulliNBClassifier() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in BernoulliNBClassifier(alpha=...).\n\nBinomial naive bayes classifier. It is suitable for classification with binary features; features will be binarized based on the binarize keyword (unless it's nothing in which case the features are assumed to be binary).","category":"section"},{"location":"models/MultitargetKNNRegressor_NearestNeighborModels/#MultitargetKNNRegressor_NearestNeighborModels","page":"MultitargetKNNRegressor","title":"MultitargetKNNRegressor","text":"MultitargetKNNRegressor\n\nA model type for constructing a multitarget K-nearest neighbor regressor, based on NearestNeighborModels.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nMultitargetKNNRegressor = @load MultitargetKNNRegressor pkg=NearestNeighborModels\n\nDo model = MultitargetKNNRegressor() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in MultitargetKNNRegressor(K=...).\n\nMulti-target K-Nearest Neighbors regressor (MultitargetKNNRegressor) is a variation of  KNNRegressor that assumes the target variable is vector-valued with Continuous components. (Target data must be presented as a table, however.)","category":"section"},{"location":"models/MultitargetKNNRegressor_NearestNeighborModels/#Training-data","page":"MultitargetKNNRegressor","title":"Training data","text":"In MLJ or MLJBase, bind an instance model to data with\n\nmach = machine(model, X, y)\n\nOR\n\nmach = machine(model, X, y, w)\n\nHere:\n\nX is any table of input features (eg, a DataFrame) whose columns are of scitype Continuous; check column scitypes with schema(X).\ny is the target, which can be any table of responses whose element scitype is  Continuous; check column scitypes with schema(y).\nw is the observation weights which can either be nothing(default) or an  AbstractVector whoose element scitype is Count or Continuous. This is different  from weights kernel which is an hyperparameter to the model, see below.\n\nTrain the machine using fit!(mach, rows=...).","category":"section"},{"location":"models/MultitargetKNNRegressor_NearestNeighborModels/#Hyper-parameters","page":"MultitargetKNNRegressor","title":"Hyper-parameters","text":"K::Int=5 : number of neighbors\nalgorithm::Symbol = :kdtree : one of (:kdtree, :brutetree, :balltree)\nmetric::Metric = Euclidean() : any Metric from    Distances.jl for the    distance between points. For algorithm = :kdtree only metrics which are    instances of Distances.UnionMinkowskiMetric are supported.\nleafsize::Int = algorithm == 10 : determines the number of points    at which to stop splitting the tree. This option is ignored and always taken as 0    for algorithm = :brutetree, since brutetree isn't actually a tree.\nreorder::Bool = true : if true then points which are close in    distance are placed close in memory. In this case, a copy of the original data    will be made so that the original data is left unmodified. Setting this to true    can significantly improve performance of the specified algorithm    (except :brutetree). This option is ignored and always taken as false for    algorithm = :brutetree.\nweights::KNNKernel=Uniform() : kernel used in assigning weights to the    k-nearest neighbors for each observation. An instance of one of the types in    list_kernels(). User-defined weighting functions can be passed by wrapping the    function in a UserDefinedKernel kernel (do ?NearestNeighborModels.UserDefinedKernel for more    info). If observation weights w are passed during machine construction then the    weight assigned to each neighbor vote is the product of the kernel generated    weight for that neighbor and the corresponding observation weight.","category":"section"},{"location":"models/MultitargetKNNRegressor_NearestNeighborModels/#Operations","page":"MultitargetKNNRegressor","title":"Operations","text":"predict(mach, Xnew): Return predictions of the target given features Xnew, which should have same scitype as X above.","category":"section"},{"location":"models/MultitargetKNNRegressor_NearestNeighborModels/#Fitted-parameters","page":"MultitargetKNNRegressor","title":"Fitted parameters","text":"The fields of fitted_params(mach) are:\n\ntree: An instance of either KDTree, BruteTree or BallTree depending on the  value of the algorithm hyperparameter (See hyper-parameters section above).  These are data structures that stores the training data with the view of making  quicker nearest neighbor searches on test data points.","category":"section"},{"location":"models/MultitargetKNNRegressor_NearestNeighborModels/#Examples","page":"MultitargetKNNRegressor","title":"Examples","text":"using MLJ\n\n## Create Data\nX, y = make_regression(10, 5, n_targets=2)\n\n## load MultitargetKNNRegressor\nMultitargetKNNRegressor = @load MultitargetKNNRegressor pkg=NearestNeighborModels\n\n## view possible kernels\nNearestNeighborModels.list_kernels()\n\n## MutlitargetKNNRegressor instantiation\nmodel = MultitargetKNNRegressor(weights = NearestNeighborModels.Inverse())\n\n## Wrap model and required data in an MLJ machine and fit.\nmach = machine(model, X, y) |> fit! \n\n## Predict\ny_hat = predict(mach, X)\n\n\nSee also KNNRegressor","category":"section"},{"location":"models/ABODDetector_OutlierDetectionNeighbors/#ABODDetector_OutlierDetectionNeighbors","page":"ABODDetector","title":"ABODDetector","text":"ABODDetector(k = 5,\n             metric = Euclidean(),\n             algorithm = :kdtree,\n             static = :auto,\n             leafsize = 10,\n             reorder = true,\n             parallel = false,\n             enhanced = false)\n\nDetermine outliers based on the angles to its nearest neighbors. This implements the FastABOD variant described in the paper, that is, it uses the variance of angles to its nearest neighbors, not to the whole dataset, see [1]. \n\nNotice: The scores are inverted, to conform to our notion that higher scores describe higher outlierness.","category":"section"},{"location":"models/ABODDetector_OutlierDetectionNeighbors/#Parameters","page":"ABODDetector","title":"Parameters","text":"k::Integer\n\nNumber of neighbors (must be greater than 0).\n\nmetric::Metric\n\nThis is one of the Metric types defined in the Distances.jl package. It is possible to define your own metrics by creating new types that are subtypes of Metric.\n\nalgorithm::Symbol\n\nOne of (:kdtree, :balltree). In a kdtree, points are recursively split into groups using hyper-planes. Therefore a KDTree only works with axis aligned metrics which are: Euclidean, Chebyshev, Minkowski and Cityblock. A brutetree linearly searches all points in a brute force fashion and works with any Metric. A balltree recursively splits points into groups bounded by hyper-spheres and works with any Metric.\n\nstatic::Union{Bool, Symbol}\n\nOne of (true, false, :auto). Whether the input data for fitting and transform should be statically or dynamically allocated. If true, the data is statically allocated. If false, the data is dynamically allocated. If :auto, the data is dynamically allocated if the product of all dimensions except the last is greater than 100.\n\nleafsize::Int\n\nDetermines at what number of points to stop splitting the tree further. There is a trade-off between traversing the tree and having to evaluate the metric function for increasing number of points.\n\nreorder::Bool\n\nWhile building the tree this will put points close in distance close in memory since this helps with cache locality. In this case, a copy of the original data will be made so that the original data is left unmodified. This can have a significant impact on performance and is by default set to true.\n\nparallel::Bool\n\nParallelize score and predict using all threads available. The number of threads can be set with the JULIA_NUM_THREADS environment variable. Note: fit is not parallel.\n\nenhanced::Bool\n\nWhen enhanced=true, it uses the enhanced ABOD (EABOD) adaptation proposed by [2].","category":"section"},{"location":"models/ABODDetector_OutlierDetectionNeighbors/#Examples","page":"ABODDetector","title":"Examples","text":"using OutlierDetection: ABODDetector, fit, transform\ndetector = ABODDetector()\nX = rand(10, 100)\nmodel, result = fit(detector, X; verbosity=0)\ntest_scores = transform(detector, model, X)","category":"section"},{"location":"models/ABODDetector_OutlierDetectionNeighbors/#References","page":"ABODDetector","title":"References","text":"[1] Kriegel, Hans-Peter; S hubert, Matthias; Zimek, Arthur (2008): Angle-based outlier detection in high-dimensional data.\n\n[2] Li, Xiaojie; Lv, Jian Cheng; Cheng, Dongdong (2015): Angle-Based Outlier Detection Algorithm with More Stable Relationships.","category":"section"},{"location":"models/Stack_MLJBase/#Stack_MLJBase","page":"Stack","title":"Stack","text":"Union{Types...}\n\nA Union type is an abstract type which includes all instances of any of its argument types. This means that T <: Union{T,S} and S <: Union{T,S}.\n\nLike other abstract types, it cannot be instantiated, even if all of its arguments are non abstract.","category":"section"},{"location":"models/Stack_MLJBase/#Examples","page":"Stack","title":"Examples","text":"julia> IntOrString = Union{Int,AbstractString}\nUnion{Int64, AbstractString}\n\njulia> 1 isa IntOrString ## instance of Int is included in the union\ntrue\n\njulia> \"Hello!\" isa IntOrString ## String is also included\ntrue\n\njulia> 1.0 isa IntOrString ## Float64 is not included because it is neither Int nor AbstractString\nfalse","category":"section"},{"location":"models/Stack_MLJBase/#Extended-Help","page":"Stack","title":"Extended Help","text":"Unlike most other parametric types, unions are covariant in their parameters. For example, Union{Real, String} is a subtype of Union{Number, AbstractString}.\n\nThe empty union Union{} is the bottom type of Julia.","category":"section"},{"location":"machines/#Machines","page":"Machines","title":"Machines","text":"Recall from Getting Started that a machine binds a model (i.e., a choice of algorithm + hyperparameters) to data (see more at Constructing machines below). A machine is also the object storing learned parameters.  Under the hood, calling fit! on a machine calls either MLJBase.fit or MLJBase.update, depending on the machine's internal state (as recorded in private fields old_model and old_rows). These lower-level fit and update methods, which are not ordinarily called directly by the user, dispatch on the model and a view of the data defined by the optional rows keyword argument of fit! (all rows by default).","category":"section"},{"location":"machines/#Warm-restarts","page":"Machines","title":"Warm restarts","text":"If a model update method has been implemented for the model, calls to fit! will avoid redundant calculations for certain kinds of model mutations. The main use-case is increasing an iteration parameter, such as the number of epochs in a neural network. To test if SomeIterativeModel supports this feature, check iteration_parameter(SomeIterativeModel) is different from nothing.\n\nusing MLJ; color_off() # hide\ntree = (@load DecisionTreeClassifier pkg=DecisionTree verbosity=0)()\nforest = EnsembleModel(model=tree, n=10);\nX, y = @load_iris;\nmach = machine(forest, X, y)\nfit!(mach, verbosity=2);\n\nGenerally, changing a hyperparameter triggers retraining on calls to subsequent fit!:\n\nforest.bagging_fraction = 0.5;\nfit!(mach, verbosity=2);\n\nHowever, for this iterative model, increasing the iteration parameter only adds models to the existing ensemble:\n\nforest.n = 15;\nfit!(mach, verbosity=2);\n\nCall fit! again without making a change and no retraining occurs:\n\nfit!(mach);\n\nHowever, retraining can be forced:\n\nfit!(mach, force=true);\n\nAnd is re-triggered if the view of the data changes:\n\nfit!(mach, rows=1:100);\n\nfit!(mach, rows=1:100);\n\nIf an iterative model exposes its iteration parameter as a hyperparameter, and it implements the warm restart behavior above, then it can be wrapped in a \"control strategy\", like an early stopping criterion. See Controlling Iterative Models for details.","category":"section"},{"location":"machines/#Inspecting-machines","page":"Machines","title":"Inspecting machines","text":"There are two principal methods for inspecting the outcomes of training in MLJ. To obtain a named-tuple describing the learned parameters (in a user-friendly way where possible) use fitted_params(mach). All other training-related outcomes are inspected with report(mach).\n\nX, y = @load_iris\npca = (@load PCA verbosity=0)()\nmach = machine(pca, X)\nfit!(mach)\n\nfitted_params(mach)\nreport(mach)","category":"section"},{"location":"machines/#Training-losses-and-feature-importances","page":"Machines","title":"Training losses and feature importances","text":"Training losses and feature importances, if reported by a model, will be available in the machine's report (see above). However, there are also direct access methods where supported:\n\ntraining_losses(mach::Machine) -> vector_of_losses\n\nHere vector_of_losses will be in historical order (most recent loss last). This kind of access is supported for model = mach.model if supports_training_losses(model) == true.\n\nfeature_importances(mach::Machine) -> vector_of_pairs\n\nHere a vector_of_pairs is a vector of elements of the form feature => importance_value, where feature is a symbol. For example, vector_of_pairs = [:gender => 0.23, :height => 0.7, :weight => 0.1]. If a model does not support feature importances for some model hyperparameters, every importance_value will be zero. This kind of access is supported for model = mach.model if reports_feature_importances(model) == true.\n\nIf a model can report multiple types of feature importances, then there will be a model hyper-parameter controlling the active type.","category":"section"},{"location":"machines/#Constructing-machines","page":"Machines","title":"Constructing machines","text":"A machine is constructed with the syntax machine(model, args...) where the possibilities for args (called training arguments) are summarized in the table below. Here X and y represent inputs and target, respectively, and Xout is the output of a transform call. Machines for supervised models may have additional training arguments, such as a vector of per-observation weights (in which case supports_weights(model) == true).\n\nmodel supertype machine constructor calls operation calls (first compulsory)\nDeterministic <: Supervised machine(model, X, y, extras...) predict(mach, Xnew), transform(mach, Xnew), inverse_transform(mach, Xout)\nProbabilistic <: Supervised machine(model, X, y, extras...) predict(mach, Xnew), predict_mean(mach, Xnew), predict_median(mach, Xnew), predict_mode(mach, Xnew), transform(mach, Xnew), inverse_transform(mach, Xout)\nUnsupervised (except Static) machine(model, X) transform(mach, Xnew), inverse_transform(mach, Xout), predict(mach, Xnew)\nStatic machine(model) transform(mach, Xnews...), inverse_transform(mach, Xout)\n\nAll operations on machines (predict, transform, etc) have exactly one argument (Xnew or Xout above) after mach, the machine instance. An exception is a machine bound to a Static model, which can have any number of arguments after mach. For more on Static transformers (which have no training arguments) see Static transformers.\n\nA machine is reconstructed from a file using the syntax machine(\"my_machine.jlso\"), or machine(\"my_machine.jlso\", args...) if retraining using new data. See Saving machines below.","category":"section"},{"location":"machines/#Lowering-memory-demands","page":"Machines","title":"Lowering memory demands","text":"For large data sets, you may be able to save memory by suppressing data caching that some models perform to increase speed. To do this, specify cache=false, as in\n\nmachine(model, X, y, cache=false)","category":"section"},{"location":"machines/#Constructing-machines-in-learning-networks","page":"Machines","title":"Constructing machines in learning networks","text":"Instead of data X, y, etc, the machine constructor is provided Node or Source objects (\"dynamic data\") when building a learning network. See Learning Networks for more on this advanced feature.","category":"section"},{"location":"machines/#Saving-machines","page":"Machines","title":"Saving machines","text":"Users can save and restore MLJ machines using any external serialization package by suitably preparing their Machine object, and applying a post-processing step to the deserialized object. This is explained under Using an arbitrary serializer below.\n\nHowever, if a user is happy to use Julia's standard library Serialization module, there is a simplified workflow described first.\n\nThe usual serialization provisos apply. For example, when deserializing you need to have all code on which the serialization object depended loaded at the time of deserialization also. If a hyper-parameter happens to be a user-defined function, then that function must be defined at deserialization. And you should only deserialize objects from trusted sources.","category":"section"},{"location":"machines/#Using-Julia's-native-serializer","page":"Machines","title":"Using Julia's native serializer","text":"","category":"section"},{"location":"machines/#Using-an-arbitrary-serializer","page":"Machines","title":"Using an arbitrary serializer","text":"Since machines contain training data, serializing a machine directly is not recommended. Also, the learned parameters of models implemented in a language other than Julia may not have persistent representations, which means serializing them is useless. To address these two issues, users:\n\nCall serializable(mach) on a machine mach they wish to save (to remove data and create persistent learned parameters)\nSerialize the returned object using SomeSerializationPkg\n\nTo restore the original machine (minus training data) they:\n\nDeserialize using SomeSerializationPkg to obtain a new object mach\nCall restore!(mach) to ensure mach can be used to predict or transform new data.","category":"section"},{"location":"machines/#Internals","page":"Machines","title":"Internals","text":"For a supervised machine, the predict method calls a lower-level MLJBase.predict method, dispatched on the underlying model and the fitresult (see below). To see predict in action, as well as its unsupervised cousins transform and inverse_transform, see Getting Started.\n\nExcept for model, a Machine instance has several fields which the user should not directly access; these include:\n\nmodel - the struct containing the hyperparameters to be used in calls to fit!\nfitresult - the learned parameters in a raw form, initially undefined\nargs - a tuple of the data, each element wrapped in a source node; see Learning Networks (in the supervised learning example above, args = (source(X), source(y)))\nreport - outputs of training not encoded in fitresult (eg, feature rankings), initially undefined\nold_model - a deep copy of the model used in the last call to fit!\nold_rows -  a copy of the row indices used in the last call to fit!\ncache\n\nThe interested reader can learn more about machine internals by examining the simplified code excerpt in Internals.","category":"section"},{"location":"machines/#Reference","page":"Machines","title":"Reference","text":"","category":"section"},{"location":"machines/#MLJModelInterface.fitted_params-Tuple{Machine}","page":"Machines","title":"MLJModelInterface.fitted_params","text":"fitted_params(mach)\n\nReturn the learned parameters for a machine mach that has been fit!, for example the coefficients in a linear model.\n\nThis is a named tuple and human-readable if possible.\n\nIf mach is a machine for a composite model, such as a model constructed using the pipeline syntax model1 |> model2 |> ..., then the returned named tuple has the composite type's field names as keys. The corresponding value is the fitted parameters for the machine in the underlying learning network bound to that model. (If multiple machines share the same model, then the value is a vector.)\n\njulia> using MLJ\njulia> @load LogisticClassifier pkg=MLJLinearModels\njulia> X, y = @load_crabs;\njulia> pipe = Standardizer() |> LogisticClassifier();\njulia> mach = machine(pipe, X, y) |> fit!;\n\njulia> fitted_params(mach).logistic_classifier\n(classes = CategoricalArrays.CategoricalValue{String,UInt32}[\"B\", \"O\"],\n coefs = Pair{Symbol,Float64}[:FL => 3.7095037897680405, :RW => 0.1135739140854546, :CL => -1.6036892745322038, :CW => -4.415667573486482, :BD => 3.238476051092471],\n intercept = 0.0883301599726305,)\n\nSee also report\n\n\n\n\n\n","category":"method"},{"location":"machines/#MLJBase.report-Tuple{Machine}","page":"Machines","title":"MLJBase.report","text":"report(mach)\n\nReturn the report for a machine mach that has been fit!, for example the coefficients in a linear model.\n\nThis is a named tuple and human-readable if possible.\n\nIf mach is a machine for a composite model, such as a model constructed using the pipeline syntax model1 |> model2 |> ..., then the returned named tuple has the composite type's field names as keys. The corresponding value is the report for the machine in the underlying learning network bound to that model. (If multiple machines share the same model, then the value is a vector.)\n\njulia> using MLJ\njulia> @load LinearBinaryClassifier pkg=GLM\njulia> X, y = @load_crabs;\njulia> pipe = Standardizer() |> LinearBinaryClassifier();\njulia> mach = machine(pipe, X, y) |> fit!;\n\njulia> report(mach).linear_binary_classifier\n(deviance = 3.8893386087844543e-7,\n dof_residual = 195.0,\n stderror = [18954.83496713119, 6502.845740757159, 48484.240246060406, 34971.131004997274, 20654.82322484894, 2111.1294584763386],\n vcov = [3.592857686311793e8 9.122732393971942e6 … -8.454645589364915e7 5.38856837634321e6; 9.122732393971942e6 4.228700272808351e7 … -4.978433790526467e7 -8.442545425533723e6; … ; -8.454645589364915e7 -4.978433790526467e7 … 4.2662172244975924e8 2.1799125705781363e7; 5.38856837634321e6 -8.442545425533723e6 … 2.1799125705781363e7 4.456867590446599e6],)\n\n\nSee also fitted_params\n\n\n\n\n\n","category":"method"},{"location":"machines/#MLJModelInterface.save","page":"Machines","title":"MLJModelInterface.save","text":"MLJ.save(filename, mach::Machine)\nMLJ.save(io, mach::Machine)\n\nMLJBase.save(filename, mach::Machine)\nMLJBase.save(io, mach::Machine)\n\nSerialize the machine mach to a file with path filename, or to an input/output stream io (at least IOBuffer instances are supported) using the Serialization module.\n\nTo serialise using a different format, see serializable.\n\nMachines are deserialized using the machine constructor as shown in the example below.\n\nnote: Note\nThe implementation of save for machines changed in MLJ 0.18 (MLJBase 0.20). You can only restore a machine saved using older versions of MLJ using an older version.\n\nExample\n\nusing MLJ\nTree = @load DecisionTreeClassifier\nX, y = @load_iris\nmach = fit!(machine(Tree(), X, y))\n\nMLJ.save(\"tree.jls\", mach)\nmach_predict_only = machine(\"tree.jls\")\npredict(mach_predict_only, X)\n\n# using a buffer:\nio = IOBuffer()\nMLJ.save(io, mach)\nseekstart(io)\npredict_only_mach = machine(io)\npredict(predict_only_mach, X)\n\nwarning: Only load files from trusted sources\nMaliciously constructed JLS files, like pickles, and most other general purpose serialization formats, can allow for arbitrary code execution during loading. This means it is possible for someone to use a JLS file that looks like a serialized MLJ machine as a Trojan horse.\n\nSee also serializable, machine.\n\n\n\n\n\nMLJ.save(mach)\nMLJBase.save(mach)\n\nSave the current machine as an artifact at the location associated with default_logger](@ref).\n\n\n\n\n\n","category":"function"},{"location":"machines/#MLJBase.serializable","page":"Machines","title":"MLJBase.serializable","text":"serializable(mach::Machine)\n\nReturns a shallow copy of the machine to make it serializable. In particular, all training data is removed and, if necessary, learned parameters are replaced with persistent representations.\n\nAny general purpose Julia serializer may be applied to the output of serializable (eg, JLSO, BSON, JLD) but you must call restore!(mach) on the deserialised object mach before using it. See the example below.\n\nIf using Julia's standard Serialization library, a shorter workflow is available using the MLJBase.save (or MLJ.save) method.\n\nA machine returned by serializable is characterized by the property mach.state == -1.\n\nExample using JLSO\n\nusing MLJ\nusing JLSO\nTree = @load DecisionTreeClassifier\ntree = Tree()\nX, y = @load_iris\nmach = fit!(machine(tree, X, y))\n\n# This machine can now be serialized\nsmach = serializable(mach)\nJLSO.save(\"machine.jlso\", :machine => smach)\n\n# Deserialize and restore learned parameters to useable form:\nloaded_mach = JLSO.load(\"machine.jlso\")[:machine]\nrestore!(loaded_mach)\n\npredict(loaded_mach, X)\npredict(mach, X)\n\nSee also restore!, MLJBase.save.\n\n\n\n\n\n","category":"function"},{"location":"machines/#MLJBase.restore!","page":"Machines","title":"MLJBase.restore!","text":"restore!(mach::Machine)\n\nRestore the state of a machine that is currently serializable but which may not be otherwise usable. For such a machine, mach, one has mach.state=1. Intended for restoring deserialized machine objects to a useable form.\n\nFor an example see serializable.\n\n\n\n\n\n","category":"function"},{"location":"machines/#MLJBase.machine","page":"Machines","title":"MLJBase.machine","text":"machine(model, args...; cache=true, scitype_check_level=1)\n\nConstruct a Machine object binding a model, storing hyper-parameters of some machine learning algorithm, to some data, args. Calling fit! on a Machine instance mach stores outcomes of applying the algorithm in mach, which can be inspected using fitted_params(mach) (learned paramters) and report(mach) (other outcomes). This in turn enables generalization to new data using operations such as predict or transform:\n\nusing MLJModels\nX, y = make_regression()\n\nPCA = @load PCA pkg=MultivariateStats\nmodel = PCA()\nmach = machine(model, X)\nfit!(mach, rows=1:50)\ntransform(mach, selectrows(X, 51:100)) # or transform(mach, rows=51:100)\n\nDecisionTreeRegressor = @load DecisionTreeRegressor pkg=DecisionTree\nmodel = DecisionTreeRegressor()\nmach = machine(model, X, y)\nfit!(mach, rows=1:50)\npredict(mach, selectrows(X, 51:100)) # or predict(mach, rows=51:100)\n\nSpecify cache=false to prioritize memory management over speed.\n\nWhen building a learning network, Node objects can be substituted for the concrete data but no type or dimension checks are applied.\n\nChecks on the types of training data\n\nA model articulates its data requirements using scientific types, i.e., using the scitype function instead of the typeof function.\n\nIf scitype_check_level > 0 then the scitype of each arg in args is computed, and this is compared with the scitypes expected by the model, unless args contains Unknown scitypes and scitype_check_level < 4, in which case no further action is taken. Whether warnings are issued or errors thrown depends the level. For details, see default_scitype_check_level, a method to inspect or change the default level (1 at startup).\n\nMachines with model placeholders\n\nA symbol can be substituted for a model in machine constructors to act as a placeholder for a model specified at training time. The symbol must be the field name for a struct whose corresponding value is a model, as shown in the following example:\n\nmutable struct MyComposite\n    transformer\n    classifier\nend\n\nmy_composite = MyComposite(Standardizer(), ConstantClassifier)\n\nX, y = make_blobs()\nmach = machine(:classifier, X, y)\nfit!(mach, composite=my_composite)\n\nThe last two lines are equivalent to\n\nmach = machine(ConstantClassifier(), X, y)\nfit!(mach)\n\nDelaying model specification is used when exporting learning networks as new stand-alone model types. See prefit and the MLJ documentation on learning networks.\n\nSee also fit!, default_scitype_check_level, MLJBase.save, serializable.\n\n\n\n\n\n","category":"function"},{"location":"machines/#StatsAPI.fit!","page":"Machines","title":"StatsAPI.fit!","text":"fit!(mach::Machine, rows=nothing, verbosity=1, force=false, composite=nothing)\n\nFit the machine mach. In the case that mach has Node arguments, first train all other machines on which mach depends.\n\nTo attempt to fit a machine without touching any other machine, use fit_only!. For more on options and the the internal logic of fitting see fit_only!\n\n\n\n\n\nfit!(N::Node;\n     rows=nothing,\n     verbosity=1,\n     force=false,\n     acceleration=CPU1())\n\nTrain all machines required to call the node N, in an appropriate order, but parallelizing where possible using specified acceleration mode.  These machines are those returned by machines(N).\n\nSupported modes of acceleration: CPU1(), CPUThreads().\n\n\n\n\n\n","category":"function"},{"location":"machines/#MLJBase.fit_only!","page":"Machines","title":"MLJBase.fit_only!","text":"MLJBase.fit_only!(\n    mach::Machine;\n    rows=nothing,\n    verbosity=1,\n    force=false,\n    composite=nothing,\n)\n\nWithout mutating any other machine on which it may depend, perform one of the following actions to the machine mach, using the data and model bound to it, and restricting the data to rows if specified:\n\nAb initio training. Ignoring any previous learned parameters and cache, compute and store new learned parameters. Increment mach.state.\nTraining update. Making use of previous learned parameters and/or  cache, replace or mutate existing learned parameters. The effect is  the same (or nearly the same) as in ab initio training, but may be  faster or use less memory, assuming the model supports an update  option (implements MLJBase.update). Increment mach.state.\nNo-operation. Leave existing learned parameters untouched. Do not  increment mach.state.\n\nIf the model, model, bound to mach is a symbol, then instead perform the action using the true model given by getproperty(composite, model). See also machine.\n\nTraining action logic\n\nFor the action to be a no-operation, either mach.frozen == true or or none of the following apply:\n\nmach has never been trained (mach.state == 0).\nforce == true.\nThe state of some other machine on which mach depends has changed since the last time mach was trained (ie, the last time mach.state was last incremented).\nThe specified rows have changed since the last retraining and mach.model does not have Static type.\nmach.model is a Model (i.e, not a symbol) and is different from the last model used for training (but has the same type).\nmach.model is a Model but has a type different from the last model used for training.\nmach.model is a symbol and getproperty(composite, mach.model) is different from the last model used for training (but has the same type).\nmach.model is a symbol and getproperty(composite, mach.model) has a different type from the last model used for training.\n\nIn any of the cases (1) - (4), (6), or (8), mach is trained ab initio. If (5) or (7) is true, then a training update is applied.\n\nTo freeze or unfreeze mach, use freeze!(mach) or thaw!(mach).\n\nImplementation details\n\nThe data to which a machine is bound is stored in mach.args. Each element of args is either a Node object, or, in the case that concrete data was bound to the machine, it is concrete data wrapped in a Source node. In all cases, to obtain concrete data for actual training, each argument N is called, as in N() or N(rows=rows), and either MLJBase.fit (ab initio training) or MLJBase.update (training update) is dispatched on mach.model and this data. See the \"Adding models for general use\" section of the MLJ documentation for more on these lower-level training methods.\n\n\n\n\n\n","category":"function"},{"location":"machines/#MLJBase.default_scitype_check_level","page":"Machines","title":"MLJBase.default_scitype_check_level","text":"default_scitype_check_level()\n\nReturn the current global default value for scientific type checking when constructing machines.\n\ndefault_scitype_check_level(i::Integer)\n\nSet the global default value for scientific type checking to i.\n\nThe effect of the scitype_check_level option in calls of the form machine(model, data, scitype_check_level=...) is summarized below:\n\nscitype_check_level Inspect scitypes? If Unknown in scitypes If other scitype mismatch\n0 ×  \n1 (value at startup) ✓  warning\n2 ✓ warning warning\n3 ✓ warning error\n4 ✓ error error\n\nSee also machine\n\n\n\n\n\n","category":"function"},{"location":"models/TfidfTransformer_MLJText/#TfidfTransformer_MLJText","page":"TfidfTransformer","title":"TfidfTransformer","text":"TfidfTransformer\n\nA model type for constructing a TF-IFD transformer, based on MLJText.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nTfidfTransformer = @load TfidfTransformer pkg=MLJText\n\nDo model = TfidfTransformer() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in TfidfTransformer(max_doc_freq=...).\n\nThe transformer converts a collection of documents, tokenized or pre-parsed as bags of words/ngrams, to a matrix of TF-IDF scores. Here \"TF\" means term-frequency while \"IDF\" means inverse document frequency (defined below). The TF-IDF score is the product of the two. This is a common term weighting scheme in information retrieval, that has also found good use in document classification. The goal of using TF-IDF instead of the raw frequencies of occurrence of a token in a given document is to scale down the impact of tokens that occur very frequently in a given corpus and that are hence empirically less informative than features that occur in a small fraction of the training corpus.\n\nIn textbooks and implementations there is variation in the definition of IDF. Here two IDF definitions are available. The default, smoothed option provides the IDF for a term t as log((1 + n)/(1 + df(t))) + 1, where n is the total number of documents and df(t) the number of documents in which t appears. Setting smooth_df = false provides an IDF of log(n/df(t)) + 1.","category":"section"},{"location":"models/TfidfTransformer_MLJText/#Training-data","page":"TfidfTransformer","title":"Training data","text":"In MLJ or MLJBase, bind an instance model to data with\n\nmach = machine(model, X)\n\nHere:\n\nX is any vector whose elements are either tokenized documents or bags of words/ngrams. Specifically, each element is one of the following:\nA vector of abstract strings (tokens), e.g., [\"I\", \"like\", \"Sam\", \".\", \"Sam\", \"is\", \"nice\", \".\"] (scitype AbstractVector{Textual})\nA dictionary of counts, indexed on abstract strings, e.g., Dict(\"I\"=>1, \"Sam\"=>2, \"Sam is\"=>1) (scitype Multiset{Textual}})\nA dictionary of counts, indexed on plain ngrams, e.g., Dict((\"I\",)=>1, (\"Sam\",)=>2, (\"I\", \"Sam\")=>1) (scitype Multiset{<:NTuple{N,Textual} where N}); here a plain ngram is a tuple of abstract strings.\n\nTrain the machine using fit!(mach, rows=...).","category":"section"},{"location":"models/TfidfTransformer_MLJText/#Hyper-parameters","page":"TfidfTransformer","title":"Hyper-parameters","text":"max_doc_freq=1.0: Restricts the vocabulary that the transformer will consider.  Terms that occur in > max_doc_freq documents will not be considered by the transformer. For example, if max_doc_freq is set to 0.9, terms that are in more than 90% of the documents will be removed.\nmin_doc_freq=0.0: Restricts the vocabulary that the transformer will consider.  Terms that occur in < max_doc_freq documents will not be considered by the transformer. A value of 0.01 means that only terms that are at least in 1% of the documents will be included.\nsmooth_idf=true: Control which definition of IDF to use (see above).","category":"section"},{"location":"models/TfidfTransformer_MLJText/#Operations","page":"TfidfTransformer","title":"Operations","text":"transform(mach, Xnew): Based on the vocabulary and IDF learned in training, return the matrix of TF-IDF scores for Xnew, a vector of the same form as X above. The matrix has size (n, p), where n = length(Xnew) and p the size of the vocabulary. Tokens/ngrams not appearing in the learned vocabulary are scored zero.","category":"section"},{"location":"models/TfidfTransformer_MLJText/#Fitted-parameters","page":"TfidfTransformer","title":"Fitted parameters","text":"The fields of fitted_params(mach) are:\n\nvocab: A vector containing the strings used in the transformer's vocabulary.\nidf_vector: The transformer's calculated IDF vector.","category":"section"},{"location":"models/TfidfTransformer_MLJText/#Examples","page":"TfidfTransformer","title":"Examples","text":"TfidfTransformer accepts a variety of inputs. The example below transforms tokenized documents:\n\nusing MLJ\nimport TextAnalysis\n\nTfidfTransformer = @load TfidfTransformer pkg=MLJText\n\ndocs = [\"Hi my name is Sam.\", \"How are you today?\"]\ntfidf_transformer = TfidfTransformer()\n\njulia> tokenized_docs = TextAnalysis.tokenize.(docs)\n2-element Vector{Vector{String}}:\n [\"Hi\", \"my\", \"name\", \"is\", \"Sam\", \".\"]\n [\"How\", \"are\", \"you\", \"today\", \"?\"]\n\nmach = machine(tfidf_transformer, tokenized_docs)\nfit!(mach)\n\nfitted_params(mach)\n\ntfidf_mat = transform(mach, tokenized_docs)\n\nAlternatively, one can provide documents pre-parsed as ngrams counts:\n\nusing MLJ\nimport TextAnalysis\n\ndocs = [\"Hi my name is Sam.\", \"How are you today?\"]\ncorpus = TextAnalysis.Corpus(TextAnalysis.NGramDocument.(docs, 1, 2))\nngram_docs = TextAnalysis.ngrams.(corpus)\n\njulia> ngram_docs[1]\nDict{AbstractString, Int64} with 11 entries:\n  \"is\"      => 1\n  \"my\"      => 1\n  \"name\"    => 1\n  \".\"       => 1\n  \"Hi\"      => 1\n  \"Sam\"     => 1\n  \"my name\" => 1\n  \"Hi my\"   => 1\n  \"name is\" => 1\n  \"Sam .\"   => 1\n  \"is Sam\"  => 1\n\ntfidf_transformer = TfidfTransformer()\nmach = machine(tfidf_transformer, ngram_docs)\nMLJ.fit!(mach)\nfitted_params(mach)\n\ntfidf_mat = transform(mach, ngram_docs)\n\nSee also CountTransformer, BM25Transformer","category":"section"},{"location":"models/AutoEncoder_BetaML/#AutoEncoder_BetaML","page":"AutoEncoder","title":"AutoEncoder","text":"mutable struct AutoEncoder <: MLJModelInterface.Unsupervised\n\nA ready-to use AutoEncoder, from the Beta Machine Learning Toolkit (BetaML) for ecoding and decoding of data using neural networks","category":"section"},{"location":"models/AutoEncoder_BetaML/#Parameters:","page":"AutoEncoder","title":"Parameters:","text":"encoded_size: The number of neurons (i.e. dimensions) of the encoded data. If the value is a float it is consiered a percentual (to be rounded) of the dimensionality of the data [def: 0.33]\nlayers_size: Inner layer dimension (i.e. number of neurons). If the value is a float it is considered a percentual (to be rounded) of the dimensionality of the data [def: nothing that applies a specific heuristic]. Consider that the underlying neural network is trying to predict multiple values at the same times. Normally this requires many more neurons than a scalar prediction. If e_layers or d_layers are specified, this parameter is ignored for the respective part.\ne_layers: The layers (vector of AbstractLayers) responsable of the encoding of the data [def: nothing, i.e. two dense layers with the inner one of layers_size]. See subtypes(BetaML.AbstractLayer) for supported layers\nd_layers: The layers (vector of AbstractLayers) responsable of the decoding of the data [def: nothing, i.e. two dense layers with the inner one of layers_size]. See subtypes(BetaML.AbstractLayer) for supported layers\nloss: Loss (cost) function [def: BetaML.squared_cost]. Should always assume y and ŷ as (n x d) matrices.\nwarning: Warning\nIf you change the parameter loss, you need to either provide its derivative on the parameter dloss or use autodiff with dloss=nothing.\ndloss: Derivative of the loss function [def: BetaML.dsquared_cost if loss==squared_cost, nothing otherwise, i.e. use the derivative of the squared cost or autodiff]\nepochs: Number of epochs, i.e. passages trough the whole training sample [def: 200]\nbatch_size: Size of each individual batch [def: 8]\nopt_alg: The optimisation algorithm to update the gradient at each batch [def: BetaML.ADAM()] See subtypes(BetaML.OptimisationAlgorithm) for supported optimizers\nshuffle: Whether to randomly shuffle the data at each iteration (epoch) [def: true]\ntunemethod: The method - and its parameters - to employ for hyperparameters autotuning. See SuccessiveHalvingSearch for the default method. To implement automatic hyperparameter tuning during the (first) fit! call simply set autotune=true and eventually change the default tunemethod options (including the parameter ranges, the resources to employ and the loss function to adopt).\ndescr: An optional title and/or description for this model\nrng: Random Number Generator (see FIXEDSEED) [deafult: Random.GLOBAL_RNG]","category":"section"},{"location":"models/AutoEncoder_BetaML/#Notes:","page":"AutoEncoder","title":"Notes:","text":"data must be numerical\nuse transform to obtain the encoded data, and inverse_trasnform to decode to the original data","category":"section"},{"location":"models/AutoEncoder_BetaML/#Example:","page":"AutoEncoder","title":"Example:","text":"julia> using MLJ\n\njulia> X, y        = @load_iris;\n\njulia> modelType   = @load AutoEncoder pkg = \"BetaML\" verbosity=0;\n\njulia> model       = modelType(encoded_size=2,layers_size=10);\n\njulia> mach        = machine(model, X)\nuntrained Machine; caches model-specific representations of data\n  model: AutoEncoder(e_layers = nothing, …)\n  args: \n    1:\tSource @334 ⏎ Table{AbstractVector{Continuous}}\n\njulia> fit!(mach,verbosity=2)\n[ Info: Training machine(AutoEncoder(e_layers = nothing, …), …).\n***\n*** Training  for 200 epochs with algorithm BetaML.Nn.ADAM.\nTraining.. \t avg loss on epoch 1 (1): \t 35.48243542158747\nTraining.. \t avg loss on epoch 20 (20): \t 0.07528042222678126\nTraining.. \t avg loss on epoch 40 (40): \t 0.06293071729378613\nTraining.. \t avg loss on epoch 60 (60): \t 0.057035588828991145\nTraining.. \t avg loss on epoch 80 (80): \t 0.056313167754822875\nTraining.. \t avg loss on epoch 100 (100): \t 0.055521461091809436\nTraining the Neural Network...  52%|██████████████████████████████████████                                   |  ETA: 0:00:01Training.. \t avg loss on epoch 120 (120): \t 0.06015206472927942\nTraining.. \t avg loss on epoch 140 (140): \t 0.05536835903285201\nTraining.. \t avg loss on epoch 160 (160): \t 0.05877560142428245\nTraining.. \t avg loss on epoch 180 (180): \t 0.05476302769966953\nTraining.. \t avg loss on epoch 200 (200): \t 0.049240864053557445\nTraining the Neural Network... 100%|█████████████████████████████████████████████████████████████████████████| Time: 0:00:01\nTraining of 200 epoch completed. Final epoch error: 0.049240864053557445.\ntrained Machine; caches model-specific representations of data\n  model: AutoEncoder(e_layers = nothing, …)\n  args: \n    1:\tSource @334 ⏎ Table{AbstractVector{Continuous}}\n\n\njulia> X_latent    = transform(mach, X)\n150×2 Matrix{Float64}:\n 7.01701   -2.77285\n 6.50615   -2.9279\n 6.5233    -2.60754\n ⋮        \n 6.70196  -10.6059\n 6.46369  -11.1117\n 6.20212  -10.1323\n\njulia> X_recovered = inverse_transform(mach,X_latent)\n150×4 Matrix{Float64}:\n 5.04973  3.55838  1.43251  0.242215\n 4.73689  3.19985  1.44085  0.295257\n 4.65128  3.25308  1.30187  0.244354\n ⋮                          \n 6.50077  2.93602  5.3303   1.87647\n 6.38639  2.83864  5.54395  2.04117\n 6.01595  2.67659  5.03669  1.83234\n\njulia> BetaML.relative_mean_error(MLJ.matrix(X),X_recovered)\n0.03387721261716176\n\n","category":"section"},{"location":"models/SVMLinearRegressor_MLJScikitLearnInterface/#SVMLinearRegressor_MLJScikitLearnInterface","page":"SVMLinearRegressor","title":"SVMLinearRegressor","text":"SVMLinearRegressor\n\nA model type for constructing a linear support vector regressor, based on MLJScikitLearnInterface.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nSVMLinearRegressor = @load SVMLinearRegressor pkg=MLJScikitLearnInterface\n\nDo model = SVMLinearRegressor() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in SVMLinearRegressor(epsilon=...).","category":"section"},{"location":"models/SVMLinearRegressor_MLJScikitLearnInterface/#Hyper-parameters","page":"SVMLinearRegressor","title":"Hyper-parameters","text":"epsilon = 0.0\ntol = 0.0001\nC = 1.0\nloss = epsilon_insensitive\nfit_intercept = true\nintercept_scaling = 1.0\ndual = true\nrandom_state = nothing\nmax_iter = 1000","category":"section"},{"location":"models/DecisionTreeRegressor_BetaML/#DecisionTreeRegressor_BetaML","page":"DecisionTreeRegressor","title":"DecisionTreeRegressor","text":"mutable struct DecisionTreeRegressor <: MLJModelInterface.Deterministic\n\nA simple Decision Tree model for regression with support for Missing data, from the Beta Machine Learning Toolkit (BetaML).","category":"section"},{"location":"models/DecisionTreeRegressor_BetaML/#Hyperparameters:","page":"DecisionTreeRegressor","title":"Hyperparameters:","text":"max_depth::Int64: The maximum depth the tree is allowed to reach. When this is reached the node is forced to become a leaf [def: 0, i.e. no limits]\nmin_gain::Float64: The minimum information gain to allow for a node's partition [def: 0]\nmin_records::Int64: The minimum number of records a node must holds to consider for a partition of it [def: 2]\nmax_features::Int64: The maximum number of (random) features to consider at each partitioning [def: 0, i.e. look at all features]\nsplitting_criterion::Function: This is the name of the function to be used to compute the information gain of a specific partition. This is done by measuring the difference betwwen the \"impurity\" of the labels of the parent node with those of the two child nodes, weighted by the respective number of items. [def: variance]. Either variance or a custom function. It can also be an anonymous function.\nrng::Random.AbstractRNG: A Random Number Generator to be used in stochastic parts of the code [deafult: Random.GLOBAL_RNG]","category":"section"},{"location":"models/DecisionTreeRegressor_BetaML/#Example:","page":"DecisionTreeRegressor","title":"Example:","text":"julia> using MLJ\n\njulia> X, y        = @load_boston;\n\njulia> modelType   = @load DecisionTreeRegressor pkg = \"BetaML\" verbosity=0\nBetaML.Trees.DecisionTreeRegressor\n\njulia> model       = modelType()\nDecisionTreeRegressor(\n  max_depth = 0, \n  min_gain = 0.0, \n  min_records = 2, \n  max_features = 0, \n  splitting_criterion = BetaML.Utils.variance, \n  rng = Random._GLOBAL_RNG())\n\njulia> mach        = machine(model, X, y);\n\njulia> fit!(mach);\n[ Info: Training machine(DecisionTreeRegressor(max_depth = 0, …), …).\n\njulia> ŷ           = predict(mach, X);\n\njulia> hcat(y,ŷ)\n506×2 Matrix{Float64}:\n 24.0  26.35\n 21.6  21.6\n 34.7  34.8\n  ⋮    \n 23.9  23.75\n 22.0  22.2\n 11.9  13.2","category":"section"},{"location":"models/LinearSVC_LIBSVM/#LinearSVC_LIBSVM","page":"LinearSVC","title":"LinearSVC","text":"LinearSVC\n\nA model type for constructing a linear support vector classifier, based on LIBSVM.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nLinearSVC = @load LinearSVC pkg=LIBSVM\n\nDo model = LinearSVC() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in LinearSVC(solver=...).\n\nReference for algorithm and core C-library: Rong-En Fan et al (2008): \"LIBLINEAR: A Library for Large Linear Classification.\" Journal of Machine Learning Research 9 1871-1874. Available at https://www.csie.ntu.edu.tw/~cjlin/papers/liblinear.pdf. \n\nThis model type is similar to SVC from the same package with the setting kernel=LIBSVM.Kernel.KERNEL.Linear, but is optimized for the linear case.","category":"section"},{"location":"models/LinearSVC_LIBSVM/#Training-data","page":"LinearSVC","title":"Training data","text":"In MLJ or MLJBase, bind an instance model to data with one of:\n\nmach = machine(model, X, y)\nmach = machine(model, X, y, w)\n\nwhere\n\nX: any table of input features (eg, a DataFrame) whose columns each have Continuous element scitype; check column scitypes with schema(X)\ny: is the target, which can be any AbstractVector whose element scitype is <:OrderedFactor or <:Multiclass; check the scitype with scitype(y)\nw: a dictionary of class weights, keyed on levels(y).\n\nTrain the machine using fit!(mach, rows=...).","category":"section"},{"location":"models/LinearSVC_LIBSVM/#Hyper-parameters","page":"LinearSVC","title":"Hyper-parameters","text":"solver=LIBSVM.Linearsolver.L2R_L2LOSS_SVC_DUAL: linear solver, which must be one of the following from the LIBSVM.jl package:\nLIBSVM.Linearsolver.L2R_LR: L2-regularized logistic regression (primal))\nLIBSVM.Linearsolver.L2R_L2LOSS_SVC_DUAL: L2-regularized L2-loss support vector classification (dual)\nLIBSVM.Linearsolver.L2R_L2LOSS_SVC: L2-regularized L2-loss support vector classification (primal)\nLIBSVM.Linearsolver.L2R_L1LOSS_SVC_DUAL: L2-regularized L1-loss support vector classification (dual)\nLIBSVM.Linearsolver.MCSVM_CS: support vector classification by Crammer and Singer) LIBSVM.Linearsolver.L1R_L2LOSS_SVC: L1-regularized L2-loss support vector classification)\nLIBSVM.Linearsolver.L1R_LR:  L1-regularized logistic regression\nLIBSVM.Linearsolver.L2R_LR_DUAL: L2-regularized logistic regression (dual)\ntolerance::Float64=Inf: tolerance for the stopping criterion;\ncost=1.0 (range (0, Inf)): the parameter denoted C in the cited reference; for greater regularization, decrease cost\nbias= -1.0: if bias >= 0, instance x becomes [x; bias]; if bias < 0, no bias term added (default -1)","category":"section"},{"location":"models/LinearSVC_LIBSVM/#Operations","page":"LinearSVC","title":"Operations","text":"predict(mach, Xnew): return predictions of the target given features Xnew having the same scitype as X above.","category":"section"},{"location":"models/LinearSVC_LIBSVM/#Fitted-parameters","page":"LinearSVC","title":"Fitted parameters","text":"The fields of fitted_params(mach) are:\n\nlibsvm_model: the trained model object created by the LIBSVM.jl package\nencoding: class encoding used internally by libsvm_model - a dictionary of class labels keyed on the internal integer representation","category":"section"},{"location":"models/LinearSVC_LIBSVM/#Examples","page":"LinearSVC","title":"Examples","text":"using MLJ\nimport LIBSVM\n\nLinearSVC = @load LinearSVC pkg=LIBSVM               ## model type\nmodel = LinearSVC(solver=LIBSVM.Linearsolver.L2R_LR) ## instance\n\nX, y = @load_iris ## table, vector\nmach = machine(model, X, y) |> fit!\n\nXnew = (sepal_length = [6.4, 7.2, 7.4],\n        sepal_width = [2.8, 3.0, 2.8],\n        petal_length = [5.6, 5.8, 6.1],\n        petal_width = [2.1, 1.6, 1.9],)\n\njulia> yhat = predict(mach, Xnew)\n3-element CategoricalArrays.CategoricalArray{String,1,UInt32}:\n \"virginica\"\n \"versicolor\"\n \"virginica\"","category":"section"},{"location":"models/LinearSVC_LIBSVM/#Incorporating-class-weights","page":"LinearSVC","title":"Incorporating class weights","text":"weights = Dict(\"virginica\" => 1, \"versicolor\" => 20, \"setosa\" => 1)\nmach = machine(model, X, y, weights) |> fit!\n\njulia> yhat = predict(mach, Xnew)\n3-element CategoricalArrays.CategoricalArray{String,1,UInt32}:\n \"versicolor\"\n \"versicolor\"\n \"versicolor\"\n\nSee also the SVC and NuSVC classifiers, and LIVSVM.jl and the original C implementation documentation.","category":"section"},{"location":"model_browser/#model_browser","page":"Model Browser","title":"Model Browser","text":"Models may appear under multiple categories.\n\nBelow an encoder is any transformer that does not fall under another category, such as \"Missing Value Imputation\" or \"Dimension Reduction\".","category":"section"},{"location":"model_browser/#Categories","page":"Model Browser","title":"Categories","text":"Regression  |  Classification  |  Outlier Detection  |  Iterative Models  |  Ensemble Models  |  Clustering  |  Dimension Reduction  |  Encoders  |  Bayesian Models  |  Neural Networks  |  Class Imbalance  |  Meta Algorithms  |  Static Models  |  Missing Value Imputation  |  Density Estimation  |  Feature Engineering  |  Text Analysis  |  Image Processing","category":"section"},{"location":"model_browser/#Regression","page":"Model Browser","title":"Regression","text":"ARDRegressor (MLJScikitLearnInterface.jl)\nAdaBoostRegressor (MLJScikitLearnInterface.jl)\nBaggingRegressor (MLJScikitLearnInterface.jl)\nBayesianRidgeRegressor (MLJScikitLearnInterface.jl)\nCatBoostRegressor (CatBoost.jl)\nConstantRegressor (MLJModels.jl)\nDecisionTreeRegressor (BetaML.jl)\nDecisionTreeRegressor (DecisionTree.jl/MLJDecisionTreeInterface.jl)\nDeterministicConstantRegressor (MLJModels.jl)\nDummyRegressor (MLJScikitLearnInterface.jl)\nElasticNetCVRegressor (MLJScikitLearnInterface.jl)\nElasticNetRegressor (MLJLinearModels.jl)\nElasticNetRegressor (MLJScikitLearnInterface.jl)\nEpsilonSVR (LIBSVM.jl/MLJLIBSVMInterface.jl)\nEvoLinearRegressor (EvoLinear.jl)\nEvoSplineRegressor (EvoLinear.jl)\nEvoTreeCount (EvoTrees.jl)\nEvoTreeGaussian (EvoTrees.jl)\nEvoTreeMLE (EvoTrees.jl)\nEvoTreeRegressor (EvoTrees.jl)\nExtraTreesRegressor (MLJScikitLearnInterface.jl)\nGaussianMixtureRegressor (BetaML.jl)\nGaussianProcessRegressor (MLJScikitLearnInterface.jl)\nGradientBoostingRegressor (MLJScikitLearnInterface.jl)\nHistGradientBoostingRegressor (MLJScikitLearnInterface.jl)\nHuberRegressor (MLJLinearModels.jl)\nHuberRegressor (MLJScikitLearnInterface.jl)\nKNNRegressor (NearestNeighborModels.jl)\nKNeighborsRegressor (MLJScikitLearnInterface.jl)\nLADRegressor (MLJLinearModels.jl)\nLGBMRegressor (LightGBM.jl)\nLaplaceRegressor (LaplaceRedux.jl)\nLarsCVRegressor (MLJScikitLearnInterface.jl)\nLarsRegressor (MLJScikitLearnInterface.jl)\nLassoCVRegressor (MLJScikitLearnInterface.jl)\nLassoLarsCVRegressor (MLJScikitLearnInterface.jl)\nLassoLarsICRegressor (MLJScikitLearnInterface.jl)\nLassoLarsRegressor (MLJScikitLearnInterface.jl)\nLassoRegressor (MLJLinearModels.jl)\nLassoRegressor (MLJScikitLearnInterface.jl)\nLinearCountRegressor (GLM.jl/MLJGLMInterface.jl)\nLinearRegressor (GLM.jl/MLJGLMInterface.jl)\nLinearRegressor (MLJLinearModels.jl)\nLinearRegressor (MLJScikitLearnInterface.jl)\nLinearRegressor (MultivariateStats.jl/MLJMultivariateStatsInterface.jl)\nMultiTaskElasticNetCVRegressor (MLJScikitLearnInterface.jl)\nMultiTaskElasticNetRegressor (MLJScikitLearnInterface.jl)\nMultiTaskLassoCVRegressor (MLJScikitLearnInterface.jl)\nMultiTaskLassoRegressor (MLJScikitLearnInterface.jl)\nMultitargetGaussianMixtureRegressor (BetaML.jl)\nMultitargetKNNRegressor (NearestNeighborModels.jl)\nMultitargetLinearRegressor (MultivariateStats.jl/MLJMultivariateStatsInterface.jl)\nMultitargetNeuralNetworkRegressor (BetaML.jl)\nMultitargetNeuralNetworkRegressor (MLJFlux.jl)\nMultitargetRidgeRegressor (MultivariateStats.jl/MLJMultivariateStatsInterface.jl)\nMultitargetSRRegressor (SymbolicRegression.jl)\nNeuralNetworkRegressor (BetaML.jl)\nNeuralNetworkRegressor (MLJFlux.jl)\nNuSVR (LIBSVM.jl/MLJLIBSVMInterface.jl)\nOrthogonalMatchingPursuitCVRegressor (MLJScikitLearnInterface.jl)\nOrthogonalMatchingPursuitRegressor (MLJScikitLearnInterface.jl)\nPartLS (PartitionedLS.jl)\nPassiveAggressiveRegressor (MLJScikitLearnInterface.jl)\nQuantileRegressor (MLJLinearModels.jl)\nRANSACRegressor (MLJScikitLearnInterface.jl)\nRandomForestRegressor (BetaML.jl)\nRandomForestRegressor (DecisionTree.jl/MLJDecisionTreeInterface.jl)\nRandomForestRegressor (MLJScikitLearnInterface.jl)\nRidgeRegressor (MLJLinearModels.jl)\nRidgeRegressor (MLJScikitLearnInterface.jl)\nRidgeRegressor (MultivariateStats.jl/MLJMultivariateStatsInterface.jl)\nRobustRegressor (MLJLinearModels.jl)\nSGDRegressor (MLJScikitLearnInterface.jl)\nSRRegressor (SymbolicRegression.jl)\nSVMLinearRegressor (MLJScikitLearnInterface.jl)\nSVMNuRegressor (MLJScikitLearnInterface.jl)\nSVMRegressor (MLJScikitLearnInterface.jl)\nStableForestRegressor (SIRUS.jl)\nStableRulesRegressor (SIRUS.jl)\nTheilSenRegressor (MLJScikitLearnInterface.jl)\nXGBoostCount (XGBoost.jl/MLJXGBoostInterface.jl)\nXGBoostRegressor (XGBoost.jl/MLJXGBoostInterface.jl)","category":"section"},{"location":"model_browser/#Classification","page":"Model Browser","title":"Classification","text":"AdaBoostClassifier (MLJScikitLearnInterface.jl)\nAdaBoostStumpClassifier (DecisionTree.jl/MLJDecisionTreeInterface.jl)\nBaggingClassifier (MLJScikitLearnInterface.jl)\nBalancedBaggingClassifier (MLJBalancing.jl)\nBayesianLDA (MLJScikitLearnInterface.jl)\nBayesianLDA (MultivariateStats.jl/MLJMultivariateStatsInterface.jl)\nBayesianQDA (MLJScikitLearnInterface.jl)\nBayesianSubspaceLDA (MultivariateStats.jl/MLJMultivariateStatsInterface.jl)\nBernoulliNBClassifier (MLJScikitLearnInterface.jl)\nBinaryThresholdPredictor (MLJModels.jl)\nCatBoostClassifier (CatBoost.jl)\nComplementNBClassifier (MLJScikitLearnInterface.jl)\nConstantClassifier (MLJModels.jl)\nDecisionTreeClassifier (BetaML.jl)\nDecisionTreeClassifier (DecisionTree.jl/MLJDecisionTreeInterface.jl)\nDeterministicConstantClassifier (MLJModels.jl)\nDummyClassifier (MLJScikitLearnInterface.jl)\nEvoTreeClassifier (EvoTrees.jl)\nExtraTreesClassifier (MLJScikitLearnInterface.jl)\nGaussianNBClassifier (MLJScikitLearnInterface.jl)\nGaussianNBClassifier (NaiveBayes.jl/MLJNaiveBayesInterface.jl)\nGaussianProcessClassifier (MLJScikitLearnInterface.jl)\nGradientBoostingClassifier (MLJScikitLearnInterface.jl)\nHistGradientBoostingClassifier (MLJScikitLearnInterface.jl)\nImageClassifier (MLJFlux.jl)\nKNNClassifier (NearestNeighborModels.jl)\nKNeighborsClassifier (MLJScikitLearnInterface.jl)\nKernelPerceptronClassifier (BetaML.jl)\nLDA (MultivariateStats.jl/MLJMultivariateStatsInterface.jl)\nLGBMClassifier (LightGBM.jl)\nLaplaceClassifier (LaplaceRedux.jl)\nLinearBinaryClassifier (GLM.jl/MLJGLMInterface.jl)\nLinearSVC (LIBSVM.jl/MLJLIBSVMInterface.jl)\nLogisticCVClassifier (MLJScikitLearnInterface.jl)\nLogisticClassifier (MLJLinearModels.jl)\nLogisticClassifier (MLJScikitLearnInterface.jl)\nMaxnetBinaryClassifier (Maxnet.jl)\nMultinomialClassifier (MLJLinearModels.jl)\nMultinomialNBClassifier (MLJScikitLearnInterface.jl)\nMultinomialNBClassifier (NaiveBayes.jl/MLJNaiveBayesInterface.jl)\nMultitargetKNNClassifier (NearestNeighborModels.jl)\nNeuralNetworkBinaryClassifier (MLJFlux.jl)\nNeuralNetworkClassifier (BetaML.jl)\nNeuralNetworkClassifier (MLJFlux.jl)\nNuSVC (LIBSVM.jl/MLJLIBSVMInterface.jl)\nOneRuleClassifier (OneRule.jl)\nPassiveAggressiveClassifier (MLJScikitLearnInterface.jl)\nPegasosClassifier (BetaML.jl)\nPerceptronClassifier (BetaML.jl)\nPerceptronClassifier (MLJScikitLearnInterface.jl)\nProbabilisticNuSVC (LIBSVM.jl/MLJLIBSVMInterface.jl)\nProbabilisticSGDClassifier (MLJScikitLearnInterface.jl)\nProbabilisticSVC (LIBSVM.jl/MLJLIBSVMInterface.jl)\nRandomForestClassifier (BetaML.jl)\nRandomForestClassifier (DecisionTree.jl/MLJDecisionTreeInterface.jl)\nRandomForestClassifier (MLJScikitLearnInterface.jl)\nRidgeCVClassifier (MLJScikitLearnInterface.jl)\nRidgeCVRegressor (MLJScikitLearnInterface.jl)\nRidgeClassifier (MLJScikitLearnInterface.jl)\nSGDClassifier (MLJScikitLearnInterface.jl)\nSVC (LIBSVM.jl/MLJLIBSVMInterface.jl)\nSVMClassifier (MLJScikitLearnInterface.jl)\nSVMLinearClassifier (MLJScikitLearnInterface.jl)\nSVMNuClassifier (MLJScikitLearnInterface.jl)\nStableForestClassifier (SIRUS.jl)\nStableRulesClassifier (SIRUS.jl)\nSubspaceLDA (MultivariateStats.jl/MLJMultivariateStatsInterface.jl)\nXGBoostClassifier (XGBoost.jl/MLJXGBoostInterface.jl)","category":"section"},{"location":"model_browser/#Outlier-Detection","page":"Model Browser","title":"Outlier Detection","text":"ABODDetector (OutlierDetectionNeighbors.jl)\nABODDetector (OutlierDetectionPython.jl)\nCBLOFDetector (OutlierDetectionPython.jl)\nCDDetector (OutlierDetectionPython.jl)\nCOFDetector (OutlierDetectionNeighbors.jl)\nCOFDetector (OutlierDetectionPython.jl)\nCOPODDetector (OutlierDetectionPython.jl)\nDNNDetector (OutlierDetectionNeighbors.jl)\nECODDetector (OutlierDetectionPython.jl)\nGMMDetector (OutlierDetectionPython.jl)\nHBOSDetector (OutlierDetectionPython.jl)\nIForestDetector (OutlierDetectionPython.jl)\nINNEDetector (OutlierDetectionPython.jl)\nKDEDetector (OutlierDetectionPython.jl)\nKNNDetector (OutlierDetectionNeighbors.jl)\nKNNDetector (OutlierDetectionPython.jl)\nLMDDDetector (OutlierDetectionPython.jl)\nLOCIDetector (OutlierDetectionPython.jl)\nLODADetector (OutlierDetectionPython.jl)\nLOFDetector (OutlierDetectionNeighbors.jl)\nLOFDetector (OutlierDetectionPython.jl)\nMCDDetector (OutlierDetectionPython.jl)\nOCSVMDetector (OutlierDetectionPython.jl)\nOneClassSVM (LIBSVM.jl/MLJLIBSVMInterface.jl)\nPCADetector (OutlierDetectionPython.jl)\nRODDetector (OutlierDetectionPython.jl)\nSODDetector (OutlierDetectionPython.jl)\nSOSDetector (OutlierDetectionPython.jl)\nTransformedTargetModel (MLJBase.jl)","category":"section"},{"location":"model_browser/#Iterative-Models","page":"Model Browser","title":"Iterative Models","text":"CatBoostClassifier (CatBoost.jl)\nCatBoostRegressor (CatBoost.jl)\nEvoSplineRegressor (EvoLinear.jl)\nEvoTreeClassifier (EvoTrees.jl)\nEvoTreeCount (EvoTrees.jl)\nEvoTreeGaussian (EvoTrees.jl)\nEvoTreeMLE (EvoTrees.jl)\nEvoTreeRegressor (EvoTrees.jl)\nExtraTreesClassifier (MLJScikitLearnInterface.jl)\nExtraTreesRegressor (MLJScikitLearnInterface.jl)\nImageClassifier (MLJFlux.jl)\nIteratedModel (MLJIteration.jl)\nLGBMClassifier (LightGBM.jl)\nLGBMRegressor (LightGBM.jl)\nMultitargetNeuralNetworkRegressor (MLJFlux.jl)\nNeuralNetworkClassifier (MLJFlux.jl)\nNeuralNetworkRegressor (MLJFlux.jl)\nPerceptronClassifier (BetaML.jl)\nPerceptronClassifier (MLJScikitLearnInterface.jl)\nRandomForestClassifier (BetaML.jl)\nRandomForestClassifier (DecisionTree.jl/MLJDecisionTreeInterface.jl)\nRandomForestClassifier (MLJScikitLearnInterface.jl)\nRandomForestImputer (BetaML.jl)\nRandomForestRegressor (BetaML.jl)\nRandomForestRegressor (DecisionTree.jl/MLJDecisionTreeInterface.jl)\nRandomForestRegressor (MLJScikitLearnInterface.jl)\nXGBoostClassifier (XGBoost.jl/MLJXGBoostInterface.jl)\nXGBoostCount (XGBoost.jl/MLJXGBoostInterface.jl)\nXGBoostRegressor (XGBoost.jl/MLJXGBoostInterface.jl)","category":"section"},{"location":"model_browser/#Ensemble-Models","page":"Model Browser","title":"Ensemble Models","text":"BaggingClassifier (MLJScikitLearnInterface.jl)\nBaggingRegressor (MLJScikitLearnInterface.jl)\nCatBoostClassifier (CatBoost.jl)\nCatBoostRegressor (CatBoost.jl)\nEnsembleModel (MLJEnsembles.jl)\nEvoSplineRegressor (EvoLinear.jl)\nEvoTreeClassifier (EvoTrees.jl)\nEvoTreeCount (EvoTrees.jl)\nEvoTreeGaussian (EvoTrees.jl)\nEvoTreeMLE (EvoTrees.jl)\nEvoTreeRegressor (EvoTrees.jl)\nLGBMClassifier (LightGBM.jl)\nLGBMRegressor (LightGBM.jl)\nRandomForestClassifier (BetaML.jl)\nRandomForestClassifier (DecisionTree.jl/MLJDecisionTreeInterface.jl)\nRandomForestClassifier (MLJScikitLearnInterface.jl)\nRandomForestImputer (BetaML.jl)\nRandomForestRegressor (BetaML.jl)\nRandomForestRegressor (DecisionTree.jl/MLJDecisionTreeInterface.jl)\nRandomForestRegressor (MLJScikitLearnInterface.jl)\nStack (MLJBase.jl)\nXGBoostClassifier (XGBoost.jl/MLJXGBoostInterface.jl)\nXGBoostCount (XGBoost.jl/MLJXGBoostInterface.jl)\nXGBoostRegressor (XGBoost.jl/MLJXGBoostInterface.jl)","category":"section"},{"location":"model_browser/#Clustering","page":"Model Browser","title":"Clustering","text":"AffinityPropagation (Clustering.jl/MLJClusteringInterface.jl)\nAffinityPropagation (MLJScikitLearnInterface.jl)\nAgglomerativeClustering (MLJScikitLearnInterface.jl)\nBirch (MLJScikitLearnInterface.jl)\nBisectingKMeans (MLJScikitLearnInterface.jl)\nDBSCAN (Clustering.jl/MLJClusteringInterface.jl)\nDBSCAN (MLJScikitLearnInterface.jl)\nFeatureAgglomeration (MLJScikitLearnInterface.jl)\nGaussianMixtureClusterer (BetaML.jl)\nHDBSCAN (MLJScikitLearnInterface.jl)\nHierarchicalClustering (Clustering.jl/MLJClusteringInterface.jl)\nKMeans (Clustering.jl/MLJClusteringInterface.jl)\nKMeans (MLJScikitLearnInterface.jl)\nKMeans (ParallelKMeans.jl)\nKMeansClusterer (BetaML.jl)\nKMedoids (Clustering.jl/MLJClusteringInterface.jl)\nKMedoidsClusterer (BetaML.jl)\nMeanShift (MLJScikitLearnInterface.jl)\nMiniBatchKMeans (MLJScikitLearnInterface.jl)\nOPTICS (MLJScikitLearnInterface.jl)\nSelfOrganizingMap (SelfOrganizingMaps.jl)\nSpectralClustering (MLJScikitLearnInterface.jl)","category":"section"},{"location":"model_browser/#Dimension-Reduction","page":"Model Browser","title":"Dimension Reduction","text":"AutoEncoder (BetaML.jl)\nBayesianLDA (MLJScikitLearnInterface.jl)\nBayesianLDA (MultivariateStats.jl/MLJMultivariateStatsInterface.jl)\nBayesianQDA (MLJScikitLearnInterface.jl)\nBayesianSubspaceLDA (MultivariateStats.jl/MLJMultivariateStatsInterface.jl)\nBirch (MLJScikitLearnInterface.jl)\nBisectingKMeans (MLJScikitLearnInterface.jl)\nFactorAnalysis (MultivariateStats.jl/MLJMultivariateStatsInterface.jl)\nFeatureSelector (FeatureSelection.jl)\nKMeans (Clustering.jl/MLJClusteringInterface.jl)\nKMeans (MLJScikitLearnInterface.jl)\nKMeans (ParallelKMeans.jl)\nKMedoids (Clustering.jl/MLJClusteringInterface.jl)\nKernelPCA (MultivariateStats.jl/MLJMultivariateStatsInterface.jl)\nLDA (MultivariateStats.jl/MLJMultivariateStatsInterface.jl)\nMiniBatchKMeans (MLJScikitLearnInterface.jl)\nPCA (MultivariateStats.jl/MLJMultivariateStatsInterface.jl)\nPPCA (MultivariateStats.jl/MLJMultivariateStatsInterface.jl)\nRecursiveFeatureElimination (FeatureSelection.jl)\nSelfOrganizingMap (SelfOrganizingMaps.jl)\nSubspaceLDA (MultivariateStats.jl/MLJMultivariateStatsInterface.jl)\nTSVDTransformer (TSVD.jl/MLJTSVDInterface.jl)","category":"section"},{"location":"model_browser/#Encoders","page":"Model Browser","title":"Encoders","text":"BM25Transformer (MLJText.jl)\nCardinalityReducer (MLJTransforms.jl)\nContinuousEncoder (MLJTransforms.jl)\nContrastEncoder (MLJTransforms.jl)\nCountTransformer (MLJText.jl)\nEntityEmbedder (MLJFlux.jl)\nFrequencyEncoder (MLJTransforms.jl)\nICA (MultivariateStats.jl/MLJMultivariateStatsInterface.jl)\nMissingnessEncoder (MLJTransforms.jl)\nOneHotEncoder (MLJTransforms.jl)\nOrdinalEncoder (MLJTransforms.jl)\nStandardizer (MLJTransforms.jl)\nTargetEncoder (MLJTransforms.jl)\nTfidfTransformer (MLJText.jl)\nUnivariateBoxCoxTransformer (MLJTransforms.jl)\nUnivariateDiscretizer (MLJTransforms.jl)\nUnivariateStandardizer (unknown.jl/MLJTransforms.jl)\nUnivariateTimeTypeToContinuous (MLJTransforms.jl)","category":"section"},{"location":"model_browser/#Bayesian-Models","page":"Model Browser","title":"Bayesian Models","text":"ARDRegressor (MLJScikitLearnInterface.jl)\nBayesianLDA (MLJScikitLearnInterface.jl)\nBayesianLDA (MultivariateStats.jl/MLJMultivariateStatsInterface.jl)\nBayesianQDA (MLJScikitLearnInterface.jl)\nBayesianRidgeRegressor (MLJScikitLearnInterface.jl)\nBayesianSubspaceLDA (MultivariateStats.jl/MLJMultivariateStatsInterface.jl)\nBernoulliNBClassifier (MLJScikitLearnInterface.jl)\nComplementNBClassifier (MLJScikitLearnInterface.jl)\nGaussianNBClassifier (MLJScikitLearnInterface.jl)\nGaussianNBClassifier (NaiveBayes.jl/MLJNaiveBayesInterface.jl)\nGaussianProcessClassifier (MLJScikitLearnInterface.jl)\nGaussianProcessRegressor (MLJScikitLearnInterface.jl)\nLaplaceClassifier (LaplaceRedux.jl)\nLaplaceRegressor (LaplaceRedux.jl)\nMultinomialNBClassifier (MLJScikitLearnInterface.jl)\nMultinomialNBClassifier (NaiveBayes.jl/MLJNaiveBayesInterface.jl)","category":"section"},{"location":"model_browser/#Neural-Networks","page":"Model Browser","title":"Neural Networks","text":"EntityEmbedder (MLJFlux.jl)\nKernelPerceptronClassifier (BetaML.jl)\nLaplaceClassifier (LaplaceRedux.jl)\nLaplaceRegressor (LaplaceRedux.jl)\nMaxnetBinaryClassifier (Maxnet.jl)\nMultitargetNeuralNetworkRegressor (BetaML.jl)\nMultitargetNeuralNetworkRegressor (MLJFlux.jl)\nNeuralNetworkBinaryClassifier (MLJFlux.jl)\nNeuralNetworkClassifier (BetaML.jl)\nNeuralNetworkClassifier (MLJFlux.jl)\nNeuralNetworkRegressor (BetaML.jl)\nNeuralNetworkRegressor (MLJFlux.jl)\nPerceptronClassifier (BetaML.jl)\nPerceptronClassifier (MLJScikitLearnInterface.jl)","category":"section"},{"location":"model_browser/#Class-Imbalance","page":"Model Browser","title":"Class Imbalance","text":"BalancedBaggingClassifier (MLJBalancing.jl)\nBalancedModel (MLJBalancing.jl)\nBorderlineSMOTE1 (Imbalance.jl)\nClusterUndersampler (Imbalance.jl)\nENNUndersampler (Imbalance.jl)\nROSE (Imbalance.jl)\nRandomOversampler (Imbalance.jl)\nRandomUndersampler (Imbalance.jl)\nRandomWalkOversampler (Imbalance.jl)\nSMOTE (Imbalance.jl)\nSMOTEN (Imbalance.jl)\nSMOTENC (Imbalance.jl)\nTomekUndersampler (Imbalance.jl)","category":"section"},{"location":"model_browser/#Meta-Algorithms","page":"Model Browser","title":"Meta Algorithms","text":"BalancedBaggingClassifier (MLJBalancing.jl)\nBalancedModel (MLJBalancing.jl)\nBinaryThresholdPredictor (MLJModels.jl)\nEnsembleModel (MLJEnsembles.jl)\nEntityEmbedder (MLJFlux.jl)\nIteratedModel (MLJIteration.jl)\nPipeline (MLJBase.jl)\nRecursiveFeatureElimination (FeatureSelection.jl)\nResampler (MLJBase.jl)\nStack (MLJBase.jl)\nTransformedTargetModel (MLJBase.jl)\nTunedModel (MLJTuning.jl)","category":"section"},{"location":"model_browser/#Static-Models","page":"Model Browser","title":"Static Models","text":"AgglomerativeClustering (MLJScikitLearnInterface.jl)\nDBSCAN (Clustering.jl/MLJClusteringInterface.jl)\nDBSCAN (MLJScikitLearnInterface.jl)\nFeatureAgglomeration (MLJScikitLearnInterface.jl)\nHDBSCAN (MLJScikitLearnInterface.jl)\nInteractionTransformer (MLJTransforms.jl)\nOPTICS (MLJScikitLearnInterface.jl)\nSpectralClustering (MLJScikitLearnInterface.jl)","category":"section"},{"location":"model_browser/#Missing-Value-Imputation","page":"Model Browser","title":"Missing Value Imputation","text":"FillImputer (MLJTransforms.jl)\nGaussianMixtureImputer (BetaML.jl)\nGeneralImputer (BetaML.jl)\nRandomForestImputer (BetaML.jl)\nSimpleImputer (BetaML.jl)\nUnivariateFillImputer (MLJTransforms.jl)","category":"section"},{"location":"model_browser/#Density-Estimation","page":"Model Browser","title":"Density Estimation","text":"GaussianMixtureClusterer (BetaML.jl)\nGaussianMixtureImputer (BetaML.jl)\nGaussianMixtureRegressor (BetaML.jl)\nMultitargetGaussianMixtureRegressor (BetaML.jl)","category":"section"},{"location":"model_browser/#Feature-Engineering","page":"Model Browser","title":"Feature Engineering","text":"FeatureAgglomeration (MLJScikitLearnInterface.jl)\nFeatureSelector (FeatureSelection.jl)\nInteractionTransformer (MLJTransforms.jl)\nRecursiveFeatureElimination (FeatureSelection.jl)","category":"section"},{"location":"model_browser/#Text-Analysis","page":"Model Browser","title":"Text Analysis","text":"BM25Transformer (MLJText.jl)\nCountTransformer (MLJText.jl)\nTfidfTransformer (MLJText.jl)","category":"section"},{"location":"model_browser/#Image-Processing","page":"Model Browser","title":"Image Processing","text":"ImageClassifier (MLJFlux.jl)","category":"section"},{"location":"linear_pipelines/#Linear-Pipelines","page":"Linear Pipelines","title":"Linear Pipelines","text":"In MLJ a pipeline is a composite model in which models are chained together in a linear (non-branching) chain. For other arrangements, including custom architectures via learning networks, see Composing Models.\n\nFor purposes of illustration, consider a supervised learning problem with the following toy data:\n\nusing MLJ\nX = (age    = [23, 45, 34, 25, 67],\n     gender = categorical(['m', 'm', 'f', 'm', 'f']));\ny = [67.0, 81.5, 55.6, 90.0, 61.1]\n     nothing # hide\n\nWe would like to train using a K-nearest neighbor model, but the model type KNNRegressor assumes the features are all Continuous. This can be fixed by first:\n\ncoercing the :age feature to have Continuous type by replacing X with coerce(X, :age=>Continuous)\nstandardizing continuous features and one-hot encoding the Multiclass features using the ContinuousEncoder model\n\nHowever, we can avoid separately applying these preprocessing steps (two of which require fit! steps) by combining them with the supervised KKNRegressor model in a new pipeline model, using Julia's |> syntax:\n\nKNNRegressor = @load KNNRegressor pkg=NearestNeighborModels\npipe = (X -> coerce(X, :age=>Continuous)) |> ContinuousEncoder() |> KNNRegressor(K=2)\n\nWe see above that pipe is a model whose hyperparameters are themselves other models or a function. (The names of these hyper-parameters are automatically generated. To specify your own names, use the explicit Pipeline constructor instead.)\n\nThe |> syntax can also be used to extend an existing pipeline or concatenate two existing pipelines. So, we could instead have defined:\n\npipe_transformer = (X -> coerce(X, :age=>Continuous)) |> ContinuousEncoder()\npipe = pipe_transformer |> KNNRegressor(K=2)\n\nA pipeline is just a model like any other. For example, we can evaluate its performance on the data above:\n\nevaluate(pipe, X, y, resampling=CV(nfolds=3), measure=mae)\n\nTo include target transformations in a pipeline, wrap the supervised component using TransformedTargetModel.","category":"section"},{"location":"linear_pipelines/#MLJBase.Pipeline","page":"Linear Pipelines","title":"MLJBase.Pipeline","text":"Pipeline(component1, component2, ... , componentk; options...)\nPipeline(name1=component1, name2=component2, ..., namek=componentk; options...)\ncomponent1 |> component2 |> ... |> componentk\n\nCreate an instance of a composite model type which sequentially composes the specified components in order. This means component1 receives inputs, whose output is passed to component2, and so forth. A \"component\" is either a Model instance, a model type (converted immediately to its default instance) or any callable object. Here the \"output\" of a model is what predict returns if it is Supervised, or what transform returns if it is Unsupervised.\n\nNames for the component fields are automatically generated unless explicitly specified, as in\n\nPipeline(encoder=ContinuousEncoder(drop_last=false),\n         stand=Standardizer())\n\nThe Pipeline constructor accepts keyword options discussed further below.\n\nOrdinary functions (and other callables) may be inserted in the pipeline as shown in the following example:\n\nPipeline(X->coerce(X, :age=>Continuous), OneHotEncoder, ConstantClassifier)\n\nSyntactic sugar\n\nThe |> operator is overloaded to construct pipelines out of models, callables, and existing pipelines:\n\nLinearRegressor = @load LinearRegressor pkg=MLJLinearModels add=true\nPCA = @load PCA pkg=MultivariateStats add=true\n\npipe1 = MLJBase.table |> ContinuousEncoder |> Standardizer\npipe2 = PCA |> LinearRegressor\npipe1 |> pipe2\n\nAt most one of the components may be a supervised model, but this model can appear in any position. A pipeline with a Supervised component is itself Supervised and implements the predict operation.  It is otherwise Unsupervised (possibly Static) and implements transform.\n\nSpecial operations\n\nIf all the components are invertible unsupervised models (ie, implement inverse_transform) then inverse_transform is implemented for the pipeline. If there are no supervised models, then predict is nevertheless implemented, assuming the last component is a model that implements it (some clustering models). Similarly, calling transform on a supervised pipeline calls transform on the supervised component.\n\nTransformers that need a target in training\n\nSome transformers that have type Unsupervised (so that the output of transform is propagated in pipelines) may require a target variable for training. An example are so-called target encoders (which transform categorical input features, based on some target observations). Provided they appear before any Supervised component in the pipelines, such models are supported. Of course a target must be provided whenever training such a pipeline, whether or not it contains a Supervised component.\n\nOptional key-word arguments\n\nprediction_type  - prediction type of the pipeline; possible values: :deterministic, :probabilistic, :interval (default=:deterministic if not inferable)\noperation - operation applied to the supervised component model, when present; possible values: predict, predict_mean, predict_median, predict_mode (default=predict)\ncache - whether the internal machines created for component models should cache model-specific representations of data (see machine) (default=true)\n\nwarning: Warning\nSet cache=false to guarantee data anonymization.\n\nTo build more complicated non-branching pipelines, refer to the MLJ manual sections on composing models.\n\n\n\n\n\n","category":"function"},{"location":"models/HierarchicalClustering_Clustering/#HierarchicalClustering_Clustering","page":"HierarchicalClustering","title":"HierarchicalClustering","text":"HierarchicalClustering\n\nA model type for constructing a hierarchical clusterer, based on Clustering.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nHierarchicalClustering = @load HierarchicalClustering pkg=Clustering\n\nDo model = HierarchicalClustering() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in HierarchicalClustering(linkage=...).\n\nHierarchical Clustering is a clustering algorithm that organizes the data in a dendrogram based on distances between groups of points and computes cluster assignments by cutting the dendrogram at a given height. More information is available at the Clustering.jl documentation. Use predict to get cluster assignments. The dendrogram and the dendrogram cutter are accessed from the machine report (see below).\n\nThis is a static implementation, i.e., it does not generalize to new data instances, and there is no training data. For clusterers that do generalize, see KMeans or KMedoids.\n\nIn MLJ or MLJBase, create a machine with\n\nmach = machine(model)","category":"section"},{"location":"models/HierarchicalClustering_Clustering/#Hyper-parameters","page":"HierarchicalClustering","title":"Hyper-parameters","text":"linkage = :single: linkage method (:single, :average, :complete, :ward, :ward_presquared)\nmetric = SqEuclidean: metric (see Distances.jl for available metrics)\nbranchorder = :r: branchorder (:r, :barjoseph, :optimal)\nh = nothing: height at which the dendrogram is cut\nk = 3: number of clusters.\n\nIf both k and h are specified, it is guaranteed that the number of clusters is not less than k and their height is not above h.","category":"section"},{"location":"models/HierarchicalClustering_Clustering/#Operations","page":"HierarchicalClustering","title":"Operations","text":"predict(mach, X): return cluster label assignments, as an unordered CategoricalVector. Here X is any table of input features (eg, a DataFrame) whose columns are of scitype Continuous; check column scitypes with schema(X).","category":"section"},{"location":"models/HierarchicalClustering_Clustering/#Report","page":"HierarchicalClustering","title":"Report","text":"After calling predict(mach), the fields of report(mach)  are:\n\ndendrogram: the dendrogram that was computed when calling predict.\ncutter: a dendrogram cutter that can be called with a height h or a number of clusters k, to obtain a new assignment of the data points to clusters (see example below).","category":"section"},{"location":"models/HierarchicalClustering_Clustering/#Examples","page":"HierarchicalClustering","title":"Examples","text":"using MLJ\n\nX, labels  = make_moons(400, noise=0.09, rng=1) ## synthetic data with 2 clusters; X\n\nHierarchicalClustering = @load HierarchicalClustering pkg=Clustering\nmodel = HierarchicalClustering(linkage = :complete)\nmach = machine(model)\n\n## compute and output cluster assignments for observations in `X`:\nyhat = predict(mach, X)\n\n## plot dendrogram:\nusing StatsPlots\nplot(report(mach).dendrogram)\n\n## make new predictions by cutting the dendrogram at another height\nreport(mach).cutter(h = 2.5)","category":"section"},{"location":"models/SMOTENC_Imbalance/#SMOTENC_Imbalance","page":"SMOTENC","title":"SMOTENC","text":"Initiate a SMOTENC model with the given hyper-parameters.\n\nSMOTENC\n\nA model type for constructing a smotenc, based on Imbalance.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nSMOTENC = @load SMOTENC pkg=Imbalance\n\nDo model = SMOTENC() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in SMOTENC(k=...).\n\nSMOTENC implements the SMOTENC algorithm to correct for class imbalance as in N. V. Chawla, K. W. Bowyer, L. O.Hall, W. P. Kegelmeyer, “SMOTE: synthetic minority over-sampling technique,” Journal of artificial intelligence research, 321-357, 2002.","category":"section"},{"location":"models/SMOTENC_Imbalance/#Training-data","page":"SMOTENC","title":"Training data","text":"In MLJ or MLJBase, wrap the model in a machine by\n\nmach = machine(model)\n\nThere is no need to provide any data here because the model is a static transformer.\n\nLikewise, there is no need to fit!(mach).\n\nFor default values of the hyper-parameters, model can be constructed by\n\nmodel = SMOTENC()","category":"section"},{"location":"models/SMOTENC_Imbalance/#Hyperparameters","page":"SMOTENC","title":"Hyperparameters","text":"k=5: Number of nearest neighbors to consider in the SMOTENC algorithm.  Should be within   the range [1, n - 1], where n is the number of observations; otherwise set to the   nearest of these two values.\nratios=1.0: A parameter that controls the amount of oversampling to be done for each class\nCan be a float and in this case each class will be oversampled to the size of the majority class times the float. By default, all classes are oversampled to the size of the majority class\nCan be a dictionary mapping each class label to the float ratio for that class\nknn_tree: Decides the tree used in KNN computations. Either \"Brute\" or \"Ball\".   BallTree can be much faster but may lead to inaccurate results.\nrng::Union{AbstractRNG, Integer}=default_rng(): Either an AbstractRNG object or an Integer    seed to be used with Xoshiro if the Julia VERSION supports it. Otherwise, uses MersenneTwister`.","category":"section"},{"location":"models/SMOTENC_Imbalance/#Transform-Inputs","page":"SMOTENC","title":"Transform Inputs","text":"X: A table with element scitypes that subtype Union{Finite, Infinite}.     Elements in nominal columns should subtype Finite (i.e., have scitype OrderedFactor or Multiclass) and    elements in continuous columns should subtype Infinite (i.e., have scitype Count or Continuous).\ny: An abstract vector of labels (e.g., strings) that correspond to the observations in X","category":"section"},{"location":"models/SMOTENC_Imbalance/#Transform-Outputs","page":"SMOTENC","title":"Transform Outputs","text":"Xover: A matrix or table that includes original data and the new observations    due to oversampling. depending on whether the input X is a matrix or table respectively\nyover: An abstract vector of labels corresponding to Xover","category":"section"},{"location":"models/SMOTENC_Imbalance/#Operations","page":"SMOTENC","title":"Operations","text":"transform(mach, X, y): resample the data X and y using SMOTENC, returning both the new and original observations","category":"section"},{"location":"models/SMOTENC_Imbalance/#Example","page":"SMOTENC","title":"Example","text":"using MLJ\nusing ScientificTypes\nimport Imbalance\n\n## set probability of each class\nclass_probs = [0.5, 0.2, 0.3]                         \nnum_rows = 100\nnum_continuous_feats = 3\n## want two categorical features with three and two possible values respectively\nnum_vals_per_category = [3, 2]\n\n## generate a table and categorical vector accordingly\nX, y = Imbalance.generate_imbalanced_data(num_rows, num_continuous_feats; \n                                class_probs, num_vals_per_category, rng=42)                      \njulia> Imbalance.checkbalance(y)\n1: ▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 19 (39.6%) \n2: ▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 33 (68.8%) \n0: ▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 48 (100.0%) \n\njulia> ScientificTypes.schema(X).scitypes\n(Continuous, Continuous, Continuous, Continuous, Continuous)\n## coerce nominal columns to a finite scitype (multiclass or ordered factor)\nX = coerce(X, :Column4=>Multiclass, :Column5=>Multiclass)\n\n## load SMOTE-NC\nSMOTENC = @load SMOTENC pkg=Imbalance\n\n## wrap the model in a machine\noversampler = SMOTENC(k=5, ratios=Dict(0=>1.0, 1=> 0.9, 2=>0.8), rng=42)\nmach = machine(oversampler)\n\n## provide the data to transform (there is nothing to fit)\nXover, yover = transform(mach, X, y)\n\njulia> Imbalance.checkbalance(yover)\n2: ▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 38 (79.2%) \n1: ▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 43 (89.6%) \n0: ▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 48 (100.0%) ","category":"section"},{"location":"models/EvoTreeCount_EvoTrees/#EvoTreeCount_EvoTrees","page":"EvoTreeCount","title":"EvoTreeCount","text":"EvoTreeCount(;kwargs...)\n\nA model type for constructing a EvoTreeCount, based on EvoTrees.jl, and implementing both an internal API the MLJ model interface. EvoTreeCount is used to perform Poisson probabilistic regression on count target.","category":"section"},{"location":"models/EvoTreeCount_EvoTrees/#Hyper-parameters","page":"EvoTreeCount","title":"Hyper-parameters","text":"early_stopping_rounds::Integer: number of consecutive rounds without metric improvement after which fitting in stopped.\nnrounds=100:           Number of rounds. It corresponds to the number of trees that will be sequentially stacked. Must be >= 1.\neta=0.1:              Learning rate. Each tree raw predictions are scaled by eta prior to be added to the stack of predictions. Must be > 0. A lower eta results in slower learning, requiring a higher nrounds but typically improves model performance.\nL2::T=0.0:            L2 regularization factor on aggregate gain. Must be >= 0. Higher L2 can result in a more robust model.\nlambda::T=0.0:        L2 regularization factor on individual gain. Must be >= 0. Higher lambda can result in a more robust model.\ngamma::T=0.0:         Minimum gain imprvement needed to perform a node split. Higher gamma can result in a more robust model.\nmax_depth=6:          Maximum depth of a tree. Must be >= 1. A tree of depth 1 is made of a single prediction leaf. A complete tree of depth N contains 2^(N - 1) terminal leaves and 2^(N - 1) - 1 split nodes. Compute cost is proportional to 2^max_depth. Typical optimal values are in the 3 to 9 range.\nmin_weight=1.0:       Minimum weight needed in a node to perform a split. Matches the number of observations by default or the sum of weights as provided by the weights vector. Must be > 0.\nrowsample=1.0:        Proportion of rows that are sampled at each iteration to build the tree. Should be ]0, 1].\ncolsample=1.0:        Proportion of columns / features that are sampled at each iteration to build the tree. Should be ]0, 1].\nnbins=64:             Number of bins into which each feature is quantized. Buckets are defined based on quantiles, hence resulting in equal weight bins. Should be between 2 and 255.\nmonotone_constraints=Dict{Int, Int}(): Specify monotonic constraints using a dict where the key is the feature index and the value the applicable constraint (-1=decreasing, 0=none, 1=increasing).\ntree_type=:binary    Tree structure to be used. One of:\n:binary:       Each node of a tree is grown independently. Tree are built depthwise until max depth is reach or if min weight or gain (see gamma) stops further node splits.\n:oblivious:    A common splitting condition is imposed to all nodes of a given depth.\nrng=123:              Either an integer used as a seed to the random number generator or an actual random number generator (::Random.AbstractRNG).\ndevice=:cpu: Hardware device to use for computations. Can be either :cpu or :gpu.","category":"section"},{"location":"models/EvoTreeCount_EvoTrees/#Internal-API","page":"EvoTreeCount","title":"Internal API","text":"Do config = EvoTreeCount() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in EvoTreeCount(max_depth=...).","category":"section"},{"location":"models/EvoTreeCount_EvoTrees/#Training-model","page":"EvoTreeCount","title":"Training model","text":"A model is built using fit_evotree:\n\nmodel = fit_evotree(config; x_train, y_train, kwargs...)","category":"section"},{"location":"models/EvoTreeCount_EvoTrees/#Inference","page":"EvoTreeCount","title":"Inference","text":"Predictions are obtained using predict which returns a Vector of length nobs:\n\nEvoTrees.predict(model, X)\n\nAlternatively, models act as a functor, returning predictions when called as a function with features as argument:\n\nmodel(X)","category":"section"},{"location":"models/EvoTreeCount_EvoTrees/#MLJ","page":"EvoTreeCount","title":"MLJ","text":"From MLJ, the type can be imported using:\n\nEvoTreeCount = @load EvoTreeCount pkg=EvoTrees\n\nDo model = EvoTreeCount() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in EvoTreeCount(loss=...).","category":"section"},{"location":"models/EvoTreeCount_EvoTrees/#Training-data","page":"EvoTreeCount","title":"Training data","text":"In MLJ or MLJBase, bind an instance model to data with     mach = machine(model, X, y) where\n\nX: any table of input features (eg, a DataFrame) whose columns each have one of the following element scitypes: Continuous, Count, or <:OrderedFactor; check column scitypes with schema(X)\ny: is the target, which can be any AbstractVector whose element scitype is <:Count; check the scitype with scitype(y)\n\nTrain the machine using fit!(mach, rows=...).","category":"section"},{"location":"models/EvoTreeCount_EvoTrees/#Operations","page":"EvoTreeCount","title":"Operations","text":"predict(mach, Xnew): returns a vector of Poisson distributions given features Xnew having the same scitype as X above. Predictions are probabilistic.\n\nSpecific metrics can also be predicted using:\n\npredict_mean(mach, Xnew)\npredict_mode(mach, Xnew)\npredict_median(mach, Xnew)","category":"section"},{"location":"models/EvoTreeCount_EvoTrees/#Fitted-parameters","page":"EvoTreeCount","title":"Fitted parameters","text":"The fields of fitted_params(mach) are:\n\n:fitresult: The GBTree object returned by EvoTrees.jl fitting algorithm.","category":"section"},{"location":"models/EvoTreeCount_EvoTrees/#Report","page":"EvoTreeCount","title":"Report","text":"The fields of report(mach) are:\n\n:features: The names of the features encountered in training.","category":"section"},{"location":"models/EvoTreeCount_EvoTrees/#Examples","page":"EvoTreeCount","title":"Examples","text":"## Internal API\nusing EvoTrees\nconfig = EvoTreeCount(max_depth=5, nbins=32, nrounds=100)\nnobs, nfeats = 1_000, 5\nx_train, y_train = randn(nobs, nfeats), rand(0:2, nobs)\nmodel = fit_evotree(config; x_train, y_train)\npreds = EvoTrees.predict(model, x_train)\n\nusing MLJ\nEvoTreeCount = @load EvoTreeCount pkg=EvoTrees\nmodel = EvoTreeCount(max_depth=5, nbins=32, nrounds=100)\nnobs, nfeats = 1_000, 5\nX, y = randn(nobs, nfeats), rand(0:2, nobs)\nmach = machine(model, X, y) |> fit!\npreds = predict(mach, X)\npreds = predict_mean(mach, X)\npreds = predict_mode(mach, X)\npreds = predict_median(mach, X)\n\n\nSee also EvoTrees.jl.","category":"section"},{"location":"list_of_supported_models/#model_list","page":"List of Supported Models","title":"List of Supported Models","text":"List all models and associated model traits with models().\n\nFor a list of models organized around function (\"classification\", \"regression\", etc.), see the Model Browser, or this graphical alternative.\n\nThe list below was last updated in September, 2025.\n\nIndications of \"maturity\" in the table below are approximate, surjective, and possibly out-of-date. A decision to use or not use a model in a critical application should be based on a user's independent assessment.\n\nexperimental: indicates the package is fairly new and/or is under active development; you can help by testing these packages and making them more robust,\nlow: indicate a package that has reached a roughly stable form in terms of interface and which is unlikely to contain serious bugs. It may be missing some functionality found in similar packages. It has not benefited from a high level of use\nmedium: indicates the package is fairly mature but may benefit from optimizations and/or extra features; you can help by suggesting either,\nhigh: indicates the package is very mature and functionalities are expected to have been fairly optimiser and tested.\n\nPackage Interface Pkg Models Maturity Note\nBetaML.jl - DecisionTreeClassifier, RandomForestClassifier, NeuralNetworkClassifier, PerceptronClassifier, KernelPerceptronClassifier, PegasosClassifier, DecisionTreeRegressor, RandomForestRegressor, NeuralNetworkRegressor, MultitargetNeuralNetworkRegressor, GaussianMixtureRegressor, MultitargetGaussianMixtureRegressor, KMeansClusterer, KMedoidsClusterer, GaussianMixtureClusterer,  SimpleImputer,  GaussianMixtureImputer, RandomForestImputer, GeneralImputer, AutoEncoder medium \nCatBoost.jl - CatBoostRegressor, CatBoostClassifier high \nClustering.jl MLJClusteringInterface.jl KMeans, KMedoids, AffinityPropagation, DBSCAN, HierarchicalClustering high² \nDecisionTree.jl MLJDecisionTreeInterface.jl DecisionTreeClassifier, DecisionTreeRegressor, AdaBoostStumpClassifier, RandomForestClassifier, RandomForestRegressor high \nEvoTrees.jl - EvoTreeRegressor, EvoTreeClassifier, EvoTreeCount, EvoTreeGaussian, EvoTreeMLE medium tree-based gradient boosting models\nEvoLinear.jl - EvoLinearRegressor medium linear boosting models\nGLM.jl MLJGLMInterface.jl LinearRegressor, LinearBinaryClassifier, LinearCountRegressor medium² \nImbalance.jl - RandomOversampler, RandomWalkOversampler, ROSE, SMOTE, BorderlineSMOTE1, SMOTEN, SMOTENC, RandomUndersampler, ClusterUndersampler,  ENNUndersampler, TomekUndersampler, low \nLIBSVM.jl MLJLIBSVMInterface.jl LinearSVC, SVC, NuSVC, NuSVR, EpsilonSVR, OneClassSVM high also via ScikitLearn.jl\nLightGBM.jl - LGBMClassifier, LGBMRegressor high \nFeatureSelector.jl - FeatureSelector, RecursiveFeatureElimination low \nFlux.jl MLJFlux.jl NeuralNetworkRegressor, NeuralNetworkClassifier, MultitargetNeuralNetworkRegressor, ImageClassifier, EntityEmbedder medium \nMLJBalancing.jl - BalancedBaggingClassifier low \nMLJLinearModels.jl - LinearRegressor, RidgeRegressor, LassoRegressor, ElasticNetRegressor, QuantileRegressor, HuberRegressor, RobustRegressor, LADRegressor, LogisticClassifier, MultinomialClassifier medium \nMLJModels.jl (built-in) - ConstantClassifier, ConstantRegressor, DeterministicConstantClassifier, DeterministicConstantRegressor, BinaryThreshholdPredictor mature \nMLJText.jl - TfidfTransformer, BM25Transformer, CountTransformer low \nMLJTransforms.jl (built-in) - ContinuousEncoder, FillImputer, InteractionTransformer, OneHotEncoder, Standardizer, UnivariateBoxCoxTransformer, UnivariateDiscretizer, UnivariateFillImputer,  UnivariateTimeTypeToContinuous, OrdinalEncoder, FrequencyEncoder, TargetEncoder, ContrastEncoder, CardinalityReducer, MissingnessEncoder medium \nMultivariateStats.jl MLJMultivariateStatsInterface.jl LinearRegressor, MultitargetLinearRegressor, RidgeRegressor, MultitargetRidgeRegressor, PCA, KernelPCA, ICA, LDA, BayesianLDA, SubspaceLDA, BayesianSubspaceLDA, FactorAnalysis, PPCA high \nNaiveBayes.jl MLJNaiveBayesInterface.jl GaussianNBClassifier, MultinomialNBClassifier, HybridNBClassifier low \nNearestNeighborModels.jl - KNNClassifier, KNNRegressor, MultitargetKNNClassifier, MultitargetKNNRegressor high \nOneRule.jl - OneRuleClassifier experimental \nOutlierDetectionNeighbors.jl - ABODDetector, COFDetector, DNNDetector, KNNDetector, LOFDetector medium \nOutlierDetectionNetworks.jl - AEDetector, DSADDetector, ESADDetector medium \nOutlierDetectionPython.jl - ABODDetector, CBLOFDetector, CDDetector, COFDetector, COPODDetector, ECODDetector, GMMDetector, HBOSDetector, IForestDetector, INNEDetector, KDEDetector, KNNDetector, LMDDDetector, LOCIDetector, LODADetector, LOFDetector, MCDDetector, OCSVMDetector, PCADetector, RODDetector, SODDetector, SOSDetector high \nParallelKMeans.jl - KMeans experimental \nPartialLeastSquaresRegressor.jl - PLSRegressor, KPLSRegressor experimental \nPartitionedLS.jl - PartLS low \nScikitLearn.jl MLJScikitLearnInterface.jl ARDRegressor, AdaBoostClassifier, AdaBoostRegressor, AffinityPropagation, AgglomerativeClustering, BaggingClassifier, BaggingRegressor, BayesianLDA, BayesianQDA, BayesianRidgeRegressor, BernoulliNBClassifier, Birch, ComplementNBClassifier, DBSCAN, DummyClassifier, DummyRegressor, ElasticNetCVRegressor, ElasticNetRegressor, ExtraTreesClassifier, ExtraTreesRegressor, FeatureAgglomeration, GaussianNBClassifier, GaussianProcessClassifier, GaussianProcessRegressor, GradientBoostingClassifier, GradientBoostingRegressor, HuberRegressor, KMeans, KNeighborsClassifier, KNeighborsRegressor, LarsCVRegressor, LarsRegressor, LassoCVRegressor, LassoLarsCVRegressor, LassoLarsICRegressor, LassoLarsRegressor, LassoRegressor, LinearRegressor, LogisticCVClassifier, LogisticClassifier, MeanShift, MiniBatchKMeans, MultiTaskElasticNetCVRegressor, MultiTaskElasticNetRegressor, MultiTaskLassoCVRegressor, MultiTaskLassoRegressor, MultinomialNBClassifier, OPTICS, OrthogonalMatchingPursuitCVRegressor, OrthogonalMatchingPursuitRegressor, PassiveAggressiveClassifier, PassiveAggressiveRegressor, PerceptronClassifier, ProbabilisticSGDClassifier, RANSACRegressor, RandomForestClassifier, RandomForestRegressor, RidgeCVClassifier, RidgeCVRegressor, RidgeClassifier, RidgeRegressor, SGDClassifier, SGDRegressor, SVMClassifier, SVMLClassifier, SVMLRegressor, SVMNuClassifier, SVMNuRegressor, SVMRegressor, SpectralClustering, TheilSenRegressor high² \nSIRUS.jl - StableForestClassifier, StableForestRegressor, StableRulesClassifier, StableRulesRegressor low \nSymbolicRegression.jl - MultitargetSRRegressor, SRRegressor experimental \nTSVD.jl MLJTSVDInterface.jl TSVDTransformer high \nXGBoost.jl MLJXGBoostInterface.jl XGBoostRegressor, XGBoostClassifier, XGBoostCount high \n\nNotes \n\n¹Models not in the MLJ registry are not included in integration tests. Consult package documentation to see how to load them. There may be issues loading these models simultaneously with other registered models.\n\n²Some models are missing and assistance is welcome to complete the interface. Post a message on the Julia #mlj Slack channel if you would like to help, thanks!","category":"section"},{"location":"models/UnivariateStandardizer_unknown/#UnivariateStandardizer_unknown","page":"UnivariateStandardizer","title":"UnivariateStandardizer","text":"UnivariateStandardizer()\n\nTransformer type for standardizing (whitening) single variable data.\n\nThis model may be deprecated in the future. Consider using Standardizer, which handles both tabular and univariate data.","category":"section"},{"location":"models/GaussianProcessClassifier_MLJScikitLearnInterface/#GaussianProcessClassifier_MLJScikitLearnInterface","page":"GaussianProcessClassifier","title":"GaussianProcessClassifier","text":"GaussianProcessClassifier\n\nA model type for constructing a Gaussian process classifier, based on MLJScikitLearnInterface.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nGaussianProcessClassifier = @load GaussianProcessClassifier pkg=MLJScikitLearnInterface\n\nDo model = GaussianProcessClassifier() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in GaussianProcessClassifier(kernel=...).","category":"section"},{"location":"models/GaussianProcessClassifier_MLJScikitLearnInterface/#Hyper-parameters","page":"GaussianProcessClassifier","title":"Hyper-parameters","text":"kernel = nothing\noptimizer = fmin_l_bfgs_b\nn_restarts_optimizer = 0\ncopy_X_train = true\nrandom_state = nothing\nmax_iter_predict = 100\nwarm_start = false\nmulti_class = one_vs_rest","category":"section"},{"location":"models/SpectralClustering_MLJScikitLearnInterface/#SpectralClustering_MLJScikitLearnInterface","page":"SpectralClustering","title":"SpectralClustering","text":"SpectralClustering\n\nA model type for constructing a spectral clustering, based on MLJScikitLearnInterface.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nSpectralClustering = @load SpectralClustering pkg=MLJScikitLearnInterface\n\nDo model = SpectralClustering() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in SpectralClustering(n_clusters=...).\n\nApply clustering to a projection of the normalized Laplacian.  In practice spectral clustering is very useful when the structure of the individual clusters is highly non-convex or more generally when a measure of the center and spread of the cluster is not a suitable description of the complete cluster. For instance when clusters are nested circles on the 2D plane.","category":"section"},{"location":"models/BalancedModel_MLJBalancing/#BalancedModel_MLJBalancing","page":"BalancedModel","title":"BalancedModel","text":"BalancedModel(; model=nothing, balancer1=balancer_model1, balancer2=balancer_model2, ...)\nBalancedModel(model;  balancer1=balancer_model1, balancer2=balancer_model2, ...)\n\nGiven a classification model, and one or more balancer models that all implement the MLJModelInterface,     BalancedModel allows constructing a sequential pipeline that wraps an arbitrary number of balancing models     and a classifier together in a sequential pipeline.","category":"section"},{"location":"models/BalancedModel_MLJBalancing/#Operation","page":"BalancedModel","title":"Operation","text":"During training, data is first passed to balancer1 and the result is passed to balancer2 and so on, the result from the final balancer   is then passed to the classifier for training.\nDuring prediction, the balancers have no effect.","category":"section"},{"location":"models/BalancedModel_MLJBalancing/#Arguments","page":"BalancedModel","title":"Arguments","text":"model::Supervised: A classification model that implements the MLJModelInterface.\nbalancer1::Static=...: The first balancer model to pass the data to. This keyword argument can have any name.\nbalancer2::Static=...: The second balancer model to pass the data to. This keyword argument can have any name.\nand so on for an arbitrary number of balancers.","category":"section"},{"location":"models/BalancedModel_MLJBalancing/#Returns","page":"BalancedModel","title":"Returns","text":"An instance of type ProbabilisticBalancedModel or DeterministicBalancedModel, depending on the prediction type of model.","category":"section"},{"location":"models/BalancedModel_MLJBalancing/#Example","page":"BalancedModel","title":"Example","text":"using MLJ\nusing Imbalance\n\n## generate data\nX, y = Imbalance.generate_imbalanced_data(1000, 5; class_probs=[0.2, 0.3, 0.5])\n\n## prepare classification and balancing models\nSMOTENC = @load SMOTENC pkg=Imbalance verbosity=0\nTomekUndersampler = @load TomekUndersampler pkg=Imbalance verbosity=0\nLogisticClassifier = @load LogisticClassifier pkg=MLJLinearModels verbosity=0\n\noversampler = SMOTENC(k=5, ratios=1.0, rng=42)\nundersampler = TomekUndersampler(min_ratios=0.5, rng=42)\nlogistic_model = LogisticClassifier()\n\n## wrap them in a BalancedModel\nbalanced_model = BalancedModel(model=logistic_model, balancer1=oversampler, balancer2=undersampler)\n\n## now this behaves as a unified model that can be trained, validated, fine-tuned, etc.\nmach = machine(balanced_model, X, y)\nfit!(mach)","category":"section"},{"location":"models/ElasticNetRegressor_MLJLinearModels/#ElasticNetRegressor_MLJLinearModels","page":"ElasticNetRegressor","title":"ElasticNetRegressor","text":"ElasticNetRegressor\n\nA model type for constructing a elastic net regressor, based on MLJLinearModels.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nElasticNetRegressor = @load ElasticNetRegressor pkg=MLJLinearModels\n\nDo model = ElasticNetRegressor() to construct an instance with default hyper-parameters.\n\nElastic net is a linear model with objective function\n\n$\n\n|Xθ - y|₂²/2 + n⋅λ|θ|₂²/2 + n⋅γ|θ|₁ $\n\nwhere n is the number of observations.\n\nIf  scale_penalty_with_samples = false the objective function is instead\n\n$\n\n|Xθ - y|₂²/2 + λ|θ|₂²/2 + γ|θ|₁ $\n\n.\n\nDifferent solver options exist, as indicated under \"Hyperparameters\" below. ","category":"section"},{"location":"models/ElasticNetRegressor_MLJLinearModels/#Training-data","page":"ElasticNetRegressor","title":"Training data","text":"In MLJ or MLJBase, bind an instance model to data with\n\nmach = machine(model, X, y)\n\nwhere:\n\nX is any table of input features (eg, a DataFrame) whose columns have Continuous scitype; check column scitypes with schema(X)\ny is the target, which can be any AbstractVector whose element scitype is Continuous; check the scitype with scitype(y)\n\nTrain the machine using fit!(mach, rows=...).","category":"section"},{"location":"models/ElasticNetRegressor_MLJLinearModels/#Hyperparameters","page":"ElasticNetRegressor","title":"Hyperparameters","text":"lambda::Real: strength of the L2 regularization. Default: 1.0\ngamma::Real: strength of the L1 regularization. Default: 0.0\nfit_intercept::Bool: whether to fit the intercept or not. Default: true\npenalize_intercept::Bool: whether to penalize the intercept. Default: false\nscale_penalty_with_samples::Bool: whether to scale the penalty with the number of observations. Default: true\nsolver::Union{Nothing, MLJLinearModels.Solver}: any instance of MLJLinearModels.ProxGrad.\nIf solver=nothing (default) then ProxGrad(accel=true) (FISTA) is used.\nSolver aliases: FISTA(; kwargs...) = ProxGrad(accel=true, kwargs...), ISTA(; kwargs...) = ProxGrad(accel=false, kwargs...).  Default: nothing","category":"section"},{"location":"models/ElasticNetRegressor_MLJLinearModels/#Example","page":"ElasticNetRegressor","title":"Example","text":"using MLJ\nX, y = make_regression()\nmach = fit!(machine(ElasticNetRegressor(), X, y))\npredict(mach, X)\nfitted_params(mach)\n\nSee also LassoRegressor.","category":"section"},{"location":"models/KMeans_Clustering/#KMeans_Clustering","page":"KMeans","title":"KMeans","text":"KMeans\n\nA model type for constructing a K-means clusterer, based on Clustering.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nKMeans = @load KMeans pkg=Clustering\n\nDo model = KMeans() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in KMeans(k=...).\n\nK-means is a classical method for clustering or vector quantization. It produces a fixed number of clusters, each associated with a center (also known as a prototype), and each data point is assigned to a cluster with the nearest center.\n\nFrom a mathematical standpoint, K-means is a coordinate descent algorithm that solves the following optimization problem:\n\n:$\n\n\\text{minimize} \\ \\sum{i=1}^n \\| \\mathbf{x}i - \\boldsymbol{\\mu}{zi} \\|^2 \\ \\text{w.r.t.} \\ (\\boldsymbol{\\mu}, z) :$\n\nHere, boldsymbolmu_k is the center of the k-th cluster, and z_i is an index of the cluster for i-th point mathbfx_i.","category":"section"},{"location":"models/KMeans_Clustering/#Training-data","page":"KMeans","title":"Training data","text":"In MLJ or MLJBase, bind an instance model to data with\n\nmach = machine(model, X)\n\nHere:\n\nX is any table of input features (eg, a DataFrame) whose columns are of scitype Continuous; check column  scitypes with schema(X).\n\nTrain the machine using fit!(mach, rows=...).","category":"section"},{"location":"models/KMeans_Clustering/#Hyper-parameters","page":"KMeans","title":"Hyper-parameters","text":"k=3: The number of centroids to use in clustering.\nmetric::SemiMetric=Distances.SqEuclidean: The metric used to calculate the clustering. Must have type PreMetric from Distances.jl.\ninit = :kmpp: One of the following options to indicate how cluster seeds should be initialized:\n:kmpp: KMeans++\n:kmenc: K-medoids initialization based on centrality\n:rand: random\nan instance of Clustering.SeedingAlgorithm from Clustering.jl\nan integer vector of length k that provides the indices of points to use as initial cluster centers.\nSee documentation of Clustering.jl.","category":"section"},{"location":"models/KMeans_Clustering/#Operations","page":"KMeans","title":"Operations","text":"predict(mach, Xnew): return cluster label assignments, given new  features Xnew having the same Scitype as X above.\ntransform(mach, Xnew): instead return the mean pairwise distances from  new samples to the cluster centers.","category":"section"},{"location":"models/KMeans_Clustering/#Fitted-parameters","page":"KMeans","title":"Fitted parameters","text":"The fields of fitted_params(mach) are:\n\ncenters: The coordinates of the cluster centers.","category":"section"},{"location":"models/KMeans_Clustering/#Report","page":"KMeans","title":"Report","text":"The fields of report(mach) are:\n\nassignments: The cluster assignments of each point in the training data.\ncluster_labels: The labels assigned to each cluster.","category":"section"},{"location":"models/KMeans_Clustering/#Examples","page":"KMeans","title":"Examples","text":"using MLJ\nKMeans = @load KMeans pkg=Clustering\n\ntable = load_iris()\ny, X = unpack(table, ==(:target), rng=123)\nmodel = KMeans(k=3)\nmach = machine(model, X) |> fit!\n\nyhat = predict(mach, X)\n@assert yhat == report(mach).assignments\n\ncompare = zip(yhat, y) |> collect;\ncompare[1:8] ## clusters align with classes\n\ncenter_dists = transform(mach, fitted_params(mach).centers')\n\n@assert center_dists[1][1] == 0.0\n@assert center_dists[2][2] == 0.0\n@assert center_dists[3][3] == 0.0\n\nSee also KMedoids","category":"section"},{"location":"models/PassiveAggressiveClassifier_MLJScikitLearnInterface/#PassiveAggressiveClassifier_MLJScikitLearnInterface","page":"PassiveAggressiveClassifier","title":"PassiveAggressiveClassifier","text":"PassiveAggressiveClassifier\n\nA model type for constructing a passive aggressive classifier, based on MLJScikitLearnInterface.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nPassiveAggressiveClassifier = @load PassiveAggressiveClassifier pkg=MLJScikitLearnInterface\n\nDo model = PassiveAggressiveClassifier() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in PassiveAggressiveClassifier(C=...).","category":"section"},{"location":"models/PassiveAggressiveClassifier_MLJScikitLearnInterface/#Hyper-parameters","page":"PassiveAggressiveClassifier","title":"Hyper-parameters","text":"C = 1.0\nfit_intercept = true\nmax_iter = 100\ntol = 0.001\nearly_stopping = false\nvalidation_fraction = 0.1\nn_iter_no_change = 5\nshuffle = true\nverbose = 0\nloss = hinge\nn_jobs = nothing\nrandom_state = 0\nwarm_start = false\nclass_weight = nothing\naverage = false","category":"section"},{"location":"tuning_models/#Tuning-Models","page":"Tuning Models","title":"Tuning Models","text":"MLJ provides several built-in and third-party options for optimizing a model's hyper-parameters.  The quick-reference table below omits some advanced keyword options.\n\ntuning strategy notes package to import package providing the core algorithm\nGrid(goal=nothing, resolution=10) shuffled by default; goal is upper bound for number of grid points MLJ.jl or MLJTuning.jl MLJTuning.jl\nRandomSearch(rng=GLOBAL_RNG) with customizable priors MLJ.jl or MLJTuning.jl MLJTuning.jl\nLatinHypercube(rng=GLOBAL_RNG) with discrete parameter support MLJ.jl or MLJTuning.jl LatinHypercubeSampling\nMLJTreeParzenTuning() See this example for usage TreeParzen.jl TreeParzen.jl (port to Julia of hyperopt)\nParticleSwarm(n_particles=3, rng=GLOBAL_RNG) Standard Kennedy-Eberhart algorithm, plus discrete parameter support MLJParticleSwarmOptimization.jl MLJParticleSwarmOptimization.jl\nAdaptiveParticleSwarm(n_particles=3, rng=GLOBAL_RNG) Zhan et al. variant with automated swarm coefficient updates, plus discrete parameter support MLJParticleSwarmOptimization.jl MLJParticleSwarmOptimization.jl\nExplicit() For an explicit list of models of varying type MLJ.jl or MLJTuning.jl MLJTuning.jl\n\nBelow we illustrate hyperparameter optimization using the Grid, RandomSearch, LatinHypercube and Explicit tuning strategies.","category":"section"},{"location":"tuning_models/#Overview","page":"Tuning Models","title":"Overview","text":"In MLJ model tuning is implemented as a model wrapper. After wrapping a model in a tuning strategy and binding the wrapped model to data in a machine called mach, calling fit!(mach) instigates a search for optimal model hyperparameters, within a specified range, and then uses all supplied data to train the best model. To predict using that model, one then calls predict(mach, Xnew). In this way, the wrapped model may be viewed as a \"self-tuning\" version of the unwrapped model. That is, wrapping the model simply transforms certain hyper-parameters into learned parameters.\n\nA corollary of the tuning-as-wrapper approach is that the evaluation of the performance of a TunedModel instance using evaluate! implies nested resampling. This approach is inspired by MLR. See also below.\n\nIn MLJ, tuning is an iterative procedure, with an iteration parameter n, the total number of model instances to be evaluated. Accordingly, tuning can be controlled using MLJ's IteratedModel wrapper. After familiarizing oneself with the TunedModel wrapper described below, see Controlling model tuning for more on this advanced feature.\n\nFor a more in-depth overview of tuning in MLJ, or for implementation details, see the MLJTuning documentation. For a complete list of options see the TunedModel doc-string below.","category":"section"},{"location":"tuning_models/#Tuning-a-single-hyperparameter-using-a-grid-search-(regression-example)","page":"Tuning Models","title":"Tuning a single hyperparameter using a grid search (regression example)","text":"using MLJ\nX = MLJ.table(rand(100, 10));\ny = 2X.x1 - X.x2 + 0.05*rand(100);\nTree = @load DecisionTreeRegressor pkg=DecisionTree verbosity=0;\ntree = Tree()\n\nLet's tune min_purity_increase in the model above, using a grid-search. To do so we will use the simplest range object, a one-dimensional range object constructed using the range method:\n\nr = range(tree, :min_purity_increase, lower=0.001, upper=1.0, scale=:log);\nself_tuning_tree = TunedModel(\n    model=tree,\n    resampling=CV(nfolds=3),\n    tuning=Grid(resolution=10),\n    range=r,\n    measure=rms\n);\n\nIncidentally, a grid is generated internally \"over the range\" by calling the iterator method with an appropriate resolution:\n\niterator(r, 5)\n\nNon-numeric hyperparameters are handled a little differently:\n\nselector = FeatureSelector();\nr2 = range(selector, :features, values = [[:x1,], [:x1, :x2]]);\niterator(r2)\n\nUnbounded ranges are also permitted. See the range and iterator docstrings below for details, and the sampler docstring for generating random samples from one-dimensional ranges (used internally by the RandomSearch strategy).\n\nReturning to the wrapped tree model:\n\nmach = machine(self_tuning_tree, X, y);\nfit!(mach, verbosity=0)\n\nWe can inspect the detailed results of the grid search with report(mach) or just retrieve the optimal model, as here:\n\nfitted_params(mach).best_model\n\nFor more detailed information, we can look at report(mach), for example:\n\nentry = report(mach).best_history_entry\n\nPredicting on new input observations using the optimal model, trained on all the data bound to mach:\n\nXnew = MLJ.table(rand(3, 10));\npredict(mach, Xnew)\n\nOr predicting on some subset of the observations bound to mach:\n\ntest = 1:3\npredict(mach, rows=test)\n\nFor tuning using only a subset train of all observation indices, specify rows=train in the above fit! call. In that case, the above predict calls would be based on training the optimal model on all train rows.","category":"section"},{"location":"tuning_models/#A-probabilistic-classifier-example","page":"Tuning Models","title":"A probabilistic classifier example","text":"Tuning a classifier is not essentially different from tuning a regressor. A common gotcha however is to overlook the distinction between supervised models that make point predictions (subtypes of Deterministic) and those that make probabilistic predictions (subtypes of Probabilistic). The DecisionTreeRegressor model in the preceding illustration was deterministic, so this example will consider a probabilistic classifier:\n\ninfo(\"KNNClassifier\").prediction_type\n\nX, y = @load_iris\nKNN = @load KNNClassifier verbosity=0\nknn = KNN()\n\nWe'll tune the hyperparameter K in the model above, using a grid-search once more:\n\nK_range = range(knn, :K, lower=5, upper=20);\n\nSince the model is probabilistic, we can choose either: (i) a probabilistic measure, such as brier_loss; or (ii) use a deterministic measure, such as misclassification_rate (which means predict_mean is called instead of predict under the hood).\n\nCase (i) - probabilistic measure:\n\nself_tuning_knn = TunedModel(\n    model=knn,\n    resampling = CV(nfolds=4, rng=1234),\n    tuning = Grid(resolution=5),\n    range = K_range,\n    measure = BrierLoss()\n);\n\nmach = machine(self_tuning_knn, X, y);\nfit!(mach, verbosity=0);\n\nCase (ii) - deterministic measure:\n\nself_tuning_knn = TunedModel(\n    model=knn,\n    resampling = CV(nfolds=4, rng=1234),\n    tuning = Grid(resolution=5),\n    range = K_range,\n    measure = MisclassificationRate()\n)\n\nmach = machine(self_tuning_knn, X, y);\nfit!(mach, verbosity=0);\n\nLet's inspect the best model and corresponding evaluation of the metric in case (ii):\n\nentry = report(mach).best_history_entry\n\nentry.model.K\n\nRecall that fitting mach also retrains the optimal model on all available data. The following is therefore an optimal model prediction based on all available data:\n\npredict(mach, rows=148:150)","category":"section"},{"location":"tuning_models/#Specifying-a-custom-measure","page":"Tuning Models","title":"Specifying a custom measure","text":"Users may specify a custom loss or scoring function, so long as it complies with the StatisticalMeasuresBase.jl API and implements the appropriate orientation trait (Score() or Loss()) from that package.  For example, we suppose define a \"new\" scoring function custom_accuracy by\n\ncustom_accuracy(yhat, y) = mean(y .== yhat); # yhat - prediction, y - ground truth\n\nIn tuning, scores are maximised, while losses are minimised. So here we declare\n\nimport StatisticalMeasuresBase as SMB\nSMB.orientation(::typeof(custom_accuracy)) = SMB.Score()\n\nFor full details on constructing custom measures, see StatisticalMeasuresBase.jl.\n\nself_tuning_knn = TunedModel(\n    model=knn,\n    resampling = CV(nfolds=4),\n    tuning = Grid(resolution=5),\n    range = K_range,\n    measure = [custom_accuracy, MulticlassFScore()],\n    operation = predict_mode\n);\n\nmach = machine(self_tuning_knn, X, y)\nfit!(mach, verbosity=0)\nentry = report(mach).best_history_entry\n\nentry.model.K","category":"section"},{"location":"tuning_models/#Tuning-multiple-nested-hyperparameters","page":"Tuning Models","title":"Tuning multiple nested hyperparameters","text":"The forest model below has another model, namely a DecisionTreeRegressor, as a hyperparameter:\n\ntree = Tree() # defined above\nforest = EnsembleModel(model=tree)\n\nRanges for nested hyperparameters are specified using dot syntax. In this case, we will specify a goal for the total number of grid points:\n\nr1 = range(forest, :(model.n_subfeatures), lower=1, upper=9);\nr2 = range(forest, :bagging_fraction, lower=0.4, upper=1.0);\nself_tuning_forest = TunedModel(\n    model=forest,\n    tuning=Grid(goal=30),\n    resampling=CV(nfolds=6),\n    range=[r1, r2],\n    measure=rms);\n\nX = MLJ.table(rand(100, 10));\ny = 2X.x1 - X.x2 + 0.05*rand(100);\n\nmach = machine(self_tuning_forest, X, y);\nfit!(mach, verbosity=0);\n\nWe can plot the grid search results:\n\nusing Plots\nplot(mach)\n\n(Image: )\n\nInstead of specifying a goal, we can declare a global resolution, which is overridden for a particular parameter by pairing its range with the resolution desired. In the next example, the default resolution=100 is applied to the r2 field, but a resolution of 3 is applied to the r1 field. Additionally, we ask that the grid points be randomly traversed and the total number of evaluations be limited to 25.\n\ntuning = Grid(resolution=100, shuffle=true, rng=1234)\nself_tuning_forest = TunedModel(\n    model=forest,\n    tuning=tuning,\n    resampling=CV(nfolds=6),\n    range=[(r1, 3), r2],\n    measure=rms,\n    n=25\n);\nfit!(machine(self_tuning_forest, X, y), verbosity=0);\n\nFor more options for a grid search, see Grid below.","category":"section"},{"location":"tuning_models/#Tuning-using-a-random-search","page":"Tuning Models","title":"Tuning using a random search","text":"Let's attempt to tune the same hyperparameters using a RandomSearch tuning strategy. By default, bounded numeric ranges like r1 and r2 are sampled uniformly (before rounding, in the case of the integer range r1). Positive unbounded ranges are sampled using a Gamma distribution by default, and all others using a (truncated) normal distribution.\n\nself_tuning_forest = TunedModel(\n    model=forest,\n    tuning=RandomSearch(),\n    resampling=CV(nfolds=6),\n    range=[r1, r2],\n    measure=rms,\n    n=25\n);\nX = MLJ.table(rand(100, 10));\ny = 2X.x1 - X.x2 + 0.05*rand(100);\nmach = machine(self_tuning_forest, X, y);\nfit!(mach, verbosity=0)\n\nusing Plots\nplot(mach)\n\n(Image: )\n\nThe prior distributions used for sampling each hyperparameter can be customized, as can the global fallbacks. See the RandomSearch doc-string below for details.","category":"section"},{"location":"tuning_models/#Tuning-using-Latin-hypercube-sampling","page":"Tuning Models","title":"Tuning using Latin hypercube sampling","text":"One can also tune the hyperparameters using the LatinHypercube tuning strategy.  This method uses a genetic-based optimization algorithm based on the inverse of the Audze-Eglais function, using the library LatinHypercubeSampling.jl.\n\nWe'll work with the data X, y and ranges r1 and r2 defined above and instantiate a Latin hypercube resampling strategy:\n\nlatin = LatinHypercube(gens=2, popsize=120)\n\nHere gens is the number of generations to run the optimisation for and popsize is the population size in the genetic algorithm. For more on these and other LatinHypercube parameters refer to the LatinHypercubeSampling.jl documentation. Pay attention that gens and popsize are not to be confused with the iteration parameter n in the construction of a corresponding TunedModel instance, which specifies the total number of models to be evaluated, independent of the tuning strategy.\n\nFor this illustration we'll add a third, nominal,  hyper-parameter:\n\nr3 = range(forest, :(model.post_prune), values=[true, false]);\nself_tuning_forest = TunedModel(\n    model=forest,\n    tuning=latin,\n    resampling=CV(nfolds=6),\n    range=[r1, r2, r3],\n    measure=rms,\n    n=25\n);\nmach = machine(self_tuning_forest, X, y);\nfit!(mach, verbosity=0)\n\nusing Plots\nplot(mach)\n\n(Image: )","category":"section"},{"location":"tuning_models/#explicit","page":"Tuning Models","title":"Comparing models of different type and nested cross-validation","text":"Instead of mutating hyperparameters of a fixed model, one can instead optimise over an explicit list of models, whose types are allowed to vary. As with other tuning strategies, evaluating the resulting TunedModel itself implies nested resampling (e.g., nested cross-validation) which we now examine in a bit more detail.\n\ntree = (@load DecisionTreeClassifier pkg=DecisionTree verbosity=0)()\nknn = (@load KNNClassifier pkg=NearestNeighborModels verbosity=0)()\nmodels = [tree, knn]\nnothing # hide\n\nThe following model is equivalent to the best in models by using 3-fold cross-validation:\n\nmulti_model = TunedModel(\n    models=models,\n    resampling=CV(nfolds=3),\n    measure=log_loss,\n    check_measure=false\n)\nnothing # hide\n\nNote that there is no need to specify a tuning strategy or range but we do specify models (plural) instead of model. Evaluating multi_model implies nested cross-validation (each model gets evaluated 2 x 3 times):\n\nX, y = make_blobs()\n\ne = evaluate(multi_model, X, y, resampling=CV(nfolds=2), measure=log_loss, verbosity=6)\n\nNow, for example, we can get the best model for the first fold out of the two folds:\n\ne.report_per_fold[1].best_model\n\nAnd the losses in the outer loop (these still have to be matched to the best performing model):\n\ne.per_fold\n\nIt is also possible to get the results for the nested evaluations. For example, for the first fold of the outer loop and the second model:\n\ne.report_per_fold[2].history[1]","category":"section"},{"location":"tuning_models/#Reference","page":"Tuning Models","title":"Reference","text":"","category":"section"},{"location":"tuning_models/#Base.range","page":"Tuning Models","title":"Base.range","text":"r = range(model, :hyper; values=...)\n\nDefine a one-dimensional NominalRange object for a field hyper of model. Note that r is not directly iterable but iterator(r) is.\n\nA nested hyperparameter is specified using dot notation. For example, :(atom.max_depth) specifies the max_depth hyperparameter of the submodel model.atom.\n\nr = range(model, :hyper; upper=..., lower..., unit=..., origin=...,\n          scale=nothing)\n\nDefine a one-dimensional NumericRange object for a Real property hyper of model. Note that r is not directly iteratable but iterator(r, n)is an iterator of length n. To generate random elements from r, instead apply rand methods to sampler(r). The supported scales are :linear,:log, :logminus, :log10, :log10minus, :log2, or a callable object.\n\nBy default, the behaviour of the constructed object depends on the type of the value of the hyperparameter :hyper at model at the time of construction. To override this behaviour (for instance if model is not available) specify a type in place of model so the behaviour is determined by the value of the specified type.\n\nA nested hyperparameter is specified using dot notation (see above).\n\nkeyword options\n\nIf scale is unspecified, it is set to :linear, :log, :log10minus, or :linear, according to whether the interval (lower, upper) is bounded, right-unbounded, left-unbounded, or doubly unbounded, respectively.  Note upper=Inf and lower=-Inf are allowed.\n\nYou must specify at least two of the keyword arguments upper, lower, unit and origin. The last two parameters are used when fitting some distributions to unbounded NumericRange objects. See Distributions.fit for details.\n\nA range is unbounded if lower=-Inf or upper=Inf, or one of these is left unspecified. In this case, both unit and origin must be specified. In the bounded case, these have the radius and midpoint of [lower, upper] as fallbacks.\n\nIf values is specified, the other keyword arguments are ignored and a NominalRange object is returned (see above).\n\nSee also: iterator, sampler\n\n\n\n\n\n","category":"function"},{"location":"tuning_models/#MLJBase.iterator","page":"Tuning Models","title":"MLJBase.iterator","text":"iterator([rng, ], r::NominalRange, [,n])\niterator([rng, ], r::NumericRange, n)\n\nReturn an iterator (currently a vector) for a ParamRange object r. In the first case iteration is over all values stored in the range (or just the first n, if n is specified). In the second case, the iteration is over approximately n ordered values, generated as follows:\n\nFirst, exactly n values are generated between U and L, with a spacing determined by r.scale (uniform if scale=:linear) where U and L are given by the following table:\nr.lower r.upper L U\nfinite finite r.lower r.upper\n-Inf finite r.upper - 2r.unit r.upper\nfinite Inf r.lower r.lower + 2r.unit\n-Inf Inf r.origin - r.unit r.origin + r.unit\nIf a callable f is provided as scale, then a uniform spacing is always applied in (1) but f is broadcast over the results. (Unlike ordinary scales, this alters the effective range of values generated, instead of just altering the spacing.)\nIf r is a discrete numeric range (r isa NumericRange{<:Integer}) then the values are additionally rounded, with any duplicate values removed. Otherwise all the values are used (and there are exacltly n of them).\nFinally, if a random number generator rng is specified, then the values are returned in random order (sampling without replacement), and otherwise they are returned in numeric order, or in the order provided to the range constructor, in the case of a NominalRange.\n\n\n\n\n\n","category":"function"},{"location":"tuning_models/#Distributions.sampler","page":"Tuning Models","title":"Distributions.sampler","text":"sampler(r::NominalRange, probs::AbstractVector{<:Real})\nsampler(r::NominalRange)\nsampler(r::NumericRange{T}, d)\n\nConstruct an object s which can be used to generate random samples from a ParamRange object r (a one-dimensional range) using one of the following calls:\n\nrand(s)             # for one sample\nrand(s, n)          # for n samples\nrand(rng, s [, n])  # to specify an RNG\n\nThe argument probs can be any probability vector with the same length as r.values. The second sampler method above calls the first with a uniform probs vector.\n\nThe argument d can be either an arbitrary instance of UnivariateDistribution from the Distributions.jl package, or one of a Distributions.jl types for which fit(d, ::NumericRange) is defined. These include: Arcsine, Uniform, Biweight, Cosine, Epanechnikov, SymTriangularDist, Triweight, Normal, Gamma, InverseGaussian, Logistic, LogNormal, Cauchy, Gumbel, Laplace, and Poisson; but see the doc-string for Distributions.fit for an up-to-date list.\n\nIf d is an instance, then sampling is from a truncated form of the supplied distribution d, the truncation bounds being r.lower and r.upper (the attributes r.origin and r.unit attributes are ignored). For discrete numeric ranges (T <: Integer) the samples are rounded.\n\nIf d is a type then a suitably truncated distribution is automatically generated using Distributions.fit(d, r).\n\nImportant. Values are generated with no regard to r.scale, except in the special case r.scale is a callable object f. In that case, f is applied to all values generated by rand as described above (prior to rounding, in the case of discrete numeric ranges).\n\nExamples\n\njulia> r = range(Char, :letter, values=collect(\"abc\"))\njulia> s = sampler(r, [0.1, 0.2, 0.7])\njulia> samples =  rand(s, 1000);\njulia> StatsBase.countmap(samples)\nDict{Char,Int64} with 3 entries:\n  'a' => 107\n  'b' => 205\n  'c' => 688\n\njulia> r = range(Int, :k, lower=2, upper=6) # numeric but discrete\njulia> s = sampler(r, Normal)\njulia> samples = rand(s, 1000);\njulia> UnicodePlots.histogram(samples)\n           ┌                                        ┐\n[2.0, 2.5) ┤▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 119\n[2.5, 3.0) ┤ 0\n[3.0, 3.5) ┤▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 296\n[3.5, 4.0) ┤ 0\n[4.0, 4.5) ┤▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 275\n[4.5, 5.0) ┤ 0\n[5.0, 5.5) ┤▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 221\n[5.5, 6.0) ┤ 0\n[6.0, 6.5) ┤▇▇▇▇▇▇▇▇▇▇▇ 89\n           └                                        ┘\n\n\n\n\n\n","category":"function"},{"location":"tuning_models/#StatsAPI.fit-Union{Tuple{D}, Tuple{Type{D}, NumericRange}} where D<:Distributions.Distribution","page":"Tuning Models","title":"StatsAPI.fit","text":"Distributions.fit(D, r::MLJBase.NumericRange)\n\nFit and return a distribution d of type D to the one-dimensional range r.\n\nOnly types D in the table below are supported.\n\nThe distribution d is constructed in two stages. First, a distributon d0, characterized by the conditions in the second column of the table, is fit to r. Then d0 is truncated between r.lower and r.upper to obtain d.\n\nDistribution type D Characterization of d0\nArcsine, Uniform, Biweight, Cosine, Epanechnikov, SymTriangularDist, Triweight minimum(d) = r.lower, maximum(d) = r.upper\nNormal, Gamma, InverseGaussian, Logistic, LogNormal mean(d) = r.origin, std(d) = r.unit\nCauchy, Gumbel, Laplace, (Normal) Dist.location(d) = r.origin, Dist.scale(d)  = r.unit\nPoisson Dist.mean(d) = r.unit\n\nHere Dist = Distributions.\n\n\n\n\n\n","category":"method"},{"location":"tuning_models/#MLJTuning.TunedModel","page":"Tuning Models","title":"MLJTuning.TunedModel","text":"tuned_model = TunedModel(; model=<model to be mutated>,\n                         tuning=RandomSearch(),\n                         resampling=Holdout(),\n                         range=nothing,\n                         measure=nothing,\n                         n=default_n(tuning, range),\n                         operation=nothing,\n                         other_options...)\n\nConstruct a model wrapper for hyper-parameter optimization of a supervised learner, specifying the tuning strategy and model whose hyper-parameters are to be mutated.\n\ntuned_model = TunedModel(; models=<models to be compared>,\n                         resampling=Holdout(),\n                         measure=nothing,\n                         n=length(models),\n                         operation=nothing,\n                         other_options...)\n\nConstruct a wrapper for multiple models, for selection of an optimal one (equivalent to specifying tuning=Explicit() and range=models above). Elements of the iterator models need not have a common type, but they must all be Deterministic or all be Probabilistic and this is not checked but inferred from the first element generated.\n\nSee below for a complete list of options.\n\nTraining\n\nCalling fit!(mach) on a machine mach=machine(tuned_model, X, y) or mach=machine(tuned_model, X, y, w) will:\n\nInstigate a search, over clones of model, with the hyperparameter mutations specified by range, for a model optimizing the specified measure, using performance evaluations carried out using the specified tuning strategy and resampling strategy. In the case models is explictly listed, the search is instead over the models generated by the iterator models.\nFit an internal machine, based on the optimal model fitted_params(mach).best_model, wrapping the optimal model object in all the provided data X, y(, w). Calling predict(mach, Xnew) then returns predictions on Xnew of this internal machine. The final train can be supressed by setting train_best=false.\n\nSearch space\n\nThe range objects supported depend on the tuning strategy specified. Query the strategy docstring for details. To optimize over an explicit list v of models of the same type, use strategy=Explicit() and specify model=v[1] and range=v.\n\nThe number of models searched is specified by n. If unspecified, then MLJTuning.default_n(tuning, range) is used. When n is increased and fit!(mach) called again, the old search history is re-instated and the search continues where it left off.\n\nMeasures (metrics)\n\nIf more than one measure is specified, then only the first is optimized (unless strategy is multi-objective) but the performance against every measure specified will be computed and reported in report(mach).best_performance and other relevant attributes of the generated report. Options exist to pass per-observation weights or class weights to measures; see below.\n\nImportant. If a custom measure, my_measure is used, and the measure is a score, rather than a loss, be sure to check that MLJ.orientation(my_measure) == :score to ensure maximization of the measure, rather than minimization. Override an incorrect value with MLJ.orientation(::typeof(my_measure)) = :score.\n\nAccessing the fitted parameters and other training (tuning) outcomes\n\nA Plots.jl plot of performance estimates is returned by plot(mach) or heatmap(mach).\n\nOnce a tuning machine mach has bee trained as above, then fitted_params(mach) has these keys/values:\n\nkey value\nbest_model optimal model instance\nbest_fitted_params learned parameters of the optimal model\n\nThe named tuple report(mach) includes these keys/values:\n\nkey value\nbest_model optimal model instance\nbest_history_entry corresponding entry in the history, including performance estimate\nbest_report report generated by fitting the optimal model to all data\nhistory tuning strategy-specific history of all evaluations\n\nplus other key/value pairs specific to the tuning strategy.\n\nEach element of history is a property-accessible object with these properties:\n\nkey value\nmeasure vector of measures (metrics)\nmeasurement vector of measurements, one per measure\nper_fold vector of vectors of unaggregated per-fold measurements\nevaluation full PerformanceEvaluation/CompactPerformaceEvaluation object\n\nComplete list of key-word options\n\nmodel: Supervised model prototype that is cloned and mutated to generate models for evaluation\nmodels: Alternatively, an iterator of MLJ models to be explicitly evaluated. These may have varying types.\ntuning=RandomSearch(): tuning strategy to be applied (eg, Grid()). See the Tuning Models section of the MLJ manual for a complete list of options.\nresampling=Holdout(): resampling strategy (eg, Holdout(), CV()), StratifiedCV()) to be applied in performance evaluations\nmeasure: measure or measures to be applied in performance evaluations; only the first used in optimization (unless the strategy is multi-objective) but all reported to the history\nweights: per-observation weights to be passed the measure(s) in performance evaluations, where supported. Check support with supports_weights(measure).\nclass_weights: class weights to be passed the measure(s) in performance evaluations, where supported. Check support with supports_class_weights(measure).\nrepeats=1: for generating train/test sets multiple times in resampling (\"Monte Carlo\" resampling); see evaluate! for details\noperation/operations - One of predict, predict_mean, predict_mode, predict_median, or predict_joint, or a vector of these of the same length as measure/measures. Automatically inferred if left unspecified.\nrange: range object; tuning strategy documentation describes supported types\nselection_heuristic: the rule determining how the best model is decided. According to the default heuristic, NaiveSelection(), measure (or the first element of measure) is evaluated for each resample and these per-fold measurements are aggregrated. The model with the lowest (resp. highest) aggregate is chosen if the measure is a :loss (resp. a :score).\nn: number of iterations (ie, models to be evaluated); set by tuning strategy if left unspecified\ntrain_best=true: whether to train the optimal model\nacceleration=default_resource(): mode of parallelization for tuning strategies that support this\nacceleration_resampling=CPU1(): mode of parallelization for resampling\ncheck_measure=true: whether to check measure is compatible with the specified model and operation)\ncache=true: whether to cache model-specific representations of user-suplied data; set to false to conserve memory. Speed gains likely limited to the case resampling isa Holdout.\ncompact_history=true: whether to write CompactPerformanceEvaluation](@ref) or regular PerformanceEvaluation objects to the history (accessed via the :evaluation key); the compact form excludes some fields to conserve memory.\nlogger=default_logger(): a logger for externally reporting model performance evaluations, such as an MLJFlow.Logger instance. On startup, default_logger()=nothing; use default_logger(logger) to set a global logger. \n\n\n\n\n\n","category":"function"},{"location":"tuning_models/#MLJTuning.Grid","page":"Tuning Models","title":"MLJTuning.Grid","text":"Grid(goal=nothing, resolution=10, rng=Random.GLOBAL_RNG, shuffle=true)\n\nInstantiate a Cartesian grid-based hyperparameter tuning strategy with a specified number of grid points as goal, or using a specified default resolution in each numeric dimension.\n\nSupported ranges:\n\nA single one-dimensional range or vector of one-dimensioinal ranges can be specified. Specifically, in Grid search, the range field of a TunedModel instance can be:\n\nA single one-dimensional range - ie, ParamRange object - r, or pair of the form (r, res) where res specifies a resolution to override the default resolution.\nAny vector of objects of the above form\n\nTwo elements of a range vector may share the same field attribute, with the effect that their grids are combined, as in Example 3 below.\n\nParamRange objects are constructed using the range method.\n\nExample 1:\n\nrange(model, :hyper1, lower=1, origin=2, unit=1)\n\nExample 2:\n\n[(range(model, :hyper1, lower=1, upper=10), 15),\n  range(model, :hyper2, lower=2, upper=4),\n  range(model, :hyper3, values=[:ball, :tree])]\n\nExample 3:\n\n# a range generating the grid `[1, 2, 10, 20, 30]` for `:hyper1`:\n[range(model, :hyper1, values=[1, 2]),\n (range(model, :hyper1, lower= 10, upper=30), 3)]\n\nNote: All the field values of the ParamRange objects (:hyper1, :hyper2, :hyper3 in the preceding example) must refer to field names a of single model (the model specified during TunedModel construction).\n\nAlgorithm\n\nThis is a standard grid search with the following specifics: In all cases all values of each specified NominalRange are exhausted. If goal is specified, then all resolutions are ignored, and a global resolution is applied to the NumericRange objects that maximizes the number of grid points, subject to the restriction that this not exceed goal. (This assumes no field appears twice in the range vector.) Otherwise the default resolution and any parameter-specific resolutions apply.\n\nIn all cases the models generated are shuffled using rng, unless shuffle=false.\n\nSee also TunedModel, range.\n\n\n\n\n\n","category":"type"},{"location":"tuning_models/#MLJTuning.RandomSearch","page":"Tuning Models","title":"MLJTuning.RandomSearch","text":"RandomSearch(bounded=Distributions.Uniform,\n             positive_unbounded=Distributions.Gamma,\n             other=Distributions.Normal,\n             rng=Random.GLOBAL_RNG)\n\nInstantiate a random search tuning strategy, for searching over Cartesian hyperparameter domains, with customizable priors in each dimension.\n\nSupported ranges\n\nA single one-dimensional range or vector of one-dimensioinal ranges can be specified. If not paired with a prior, then one is fitted, according to fallback distribution types specified by the tuning strategy hyperparameters. Specifically, in RandomSearch, the range field of a TunedModel instance can be:\n\na single one-dimensional range (ParamRange object) r\na pair of the form (r, d), with r as above and where d is:\na probability vector of the same length as r.values (r a NominalRange)\nany Distributions.UnivariateDistribution instance (r a NumericRange)\none of the subtypes of Distributions.UnivariateDistribution listed in the table below, for automatic fitting using Distributions.fit(d, r), a distribution whose support always lies between r.lower and r.upper (r a NumericRange)\nany pair of the form (field, s), where field is the (possibly nested) name of a field of the model to be tuned, and s an arbitrary sampler object for that field. This means only that rand(rng, s) is defined and returns valid values for the field.\nany vector of objects of the above form\n\nA range vector may contain multiple entries for the same model field, as in range = [(:lambda, s1), (:alpha, s), (:lambda, s2)]. In that case the entry used in each iteration is random.\n\ndistribution types for fitting to ranges of this type\nArcsine, Uniform, Biweight, Cosine, Epanechnikov, SymTriangularDist, Triweight bounded\nGamma, InverseGaussian, Poisson positive (bounded or unbounded)\nNormal, Logistic, LogNormal, Cauchy, Gumbel, Laplace any\n\nParamRange objects are constructed using the range method.\n\nExamples\n\nusing Distributions\n\nrange1 = range(model, :hyper1, lower=0, upper=1)\n\nrange2 = [(range(model, :hyper1, lower=1, upper=10), Arcsine),\n          range(model, :hyper2, lower=2, upper=Inf, unit=1, origin=3),\n          (range(model, :hyper2, lower=2, upper=4), Normal(0, 3)),\n          (range(model, :hyper3, values=[:ball, :tree]), [0.3, 0.7])]\n\n# uniform sampling of :(atom.λ) from [0, 1] without defining a NumericRange:\nstruct MySampler end\nBase.rand(rng::Random.AbstractRNG, ::MySampler) = rand(rng)\nrange3 = (:(atom.λ), MySampler())\n\nAlgorithm\n\nIn each iteration, a model is generated for evaluation by mutating the fields of a deep copy of model. The range vector is shuffled and the fields sampled according to the new order (repeated fields being mutated more than once). For a range entry of the form (field, s) the algorithm calls rand(rng, s) and mutates the field field of the model clone to have this value. For an entry of the form (r, d), s is substituted with sampler(r, d). If no d is specified, then sampling is uniform (with replacement) if r is a NominalRange, and is otherwise given by the defaults specified by the tuning strategy parameters bounded, positive_unbounded, and other, depending on the field values of the NumericRange object r.\n\nSee also TunedModel, range, sampler.\n\n\n\n\n\n","category":"type"},{"location":"tuning_models/#MLJTuning.LatinHypercube","page":"Tuning Models","title":"MLJTuning.LatinHypercube","text":"LatinHypercube(gens = 1,\n               popsize = 100,\n               ntour = 2,\n               ptour = 0.8.,\n               interSampleWeight = 1.0,\n               ae_power = 2,\n               periodic_ae = false,\n               rng=Random.GLOBAL_RNG)\n\nInstantiate grid-based hyperparameter tuning strategy using the library LatinHypercubeSampling.jl.\n\nAn optimised Latin Hypercube sampling plan is created using a genetic based optimization algorithm based on the inverse of the Audze-Eglais function.  The optimization is run for nGenerations and creates n models for evaluation, where n is specified by a corresponding TunedModel instance, as in\n\ntuned_model = TunedModel(model=...,\n                         tuning=LatinHypercube(...),\n                         range=...,\n                         measures=...,\n                         n=...)\n\n(See TunedModel for complete options.)\n\nTo use a periodic version of the Audze-Eglais function (to reduce clustering along the boundaries) specify periodic_ae = true.\n\nSupported ranges:\n\nA single one-dimensional range or vector of one-dimensioinal ranges can be specified. Specifically, in LatinHypercubeSampling search, the range field of a TunedModel instance can be:\n\nA single one-dimensional range - ie, ParamRange object - r, constructed\n\nusing the range method.\n\nAny vector of objects of the above form\n\nBoth NumericRanges and NominalRanges are supported, and hyper-parameter values are sampled on a scale specified by the range (eg, r.scale = :log).\n\n\n\n\n\n","category":"type"},{"location":"models/DummyClassifier_MLJScikitLearnInterface/#DummyClassifier_MLJScikitLearnInterface","page":"DummyClassifier","title":"DummyClassifier","text":"DummyClassifier\n\nA model type for constructing a dummy classifier, based on MLJScikitLearnInterface.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nDummyClassifier = @load DummyClassifier pkg=MLJScikitLearnInterface\n\nDo model = DummyClassifier() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in DummyClassifier(strategy=...).\n\nDummyClassifier is a classifier that makes predictions using simple rules.","category":"section"},{"location":"models/StableForestRegressor_SIRUS/#StableForestRegressor_SIRUS","page":"StableForestRegressor","title":"StableForestRegressor","text":"StableForestRegressor\n\nA model type for constructing a stable forest regressor, based on SIRUS.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nStableForestRegressor = @load StableForestRegressor pkg=SIRUS\n\nDo model = StableForestRegressor() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in StableForestRegressor(rng=...).\n\nStableForestRegressor implements the random forest regressor with a stabilized forest structure (Bénard et al., 2021).","category":"section"},{"location":"models/StableForestRegressor_SIRUS/#Training-data","page":"StableForestRegressor","title":"Training data","text":"In MLJ or MLJBase, bind an instance model to data with\n\nmach = machine(model, X, y)\n\nwhere\n\nX: any table of input features (eg, a DataFrame) whose columns each have one of the following element scitypes: Continuous, Count, or <:OrderedFactor; check column scitypes with schema(X)\ny: the target, which can be any AbstractVector whose element scitype is <:OrderedFactor or <:Multiclass; check the scitype with scitype(y)\n\nTrain the machine with fit!(mach, rows=...).","category":"section"},{"location":"models/StableForestRegressor_SIRUS/#Hyperparameters","page":"StableForestRegressor","title":"Hyperparameters","text":"rng::AbstractRNG=default_rng(): Random number generator.   Using a StableRNG from StableRNGs.jl is advised.\npartial_sampling::Float64=0.7:   Ratio of samples to use in each subset of the data.   The default should be fine for most cases.\nn_trees::Int=1000:   The number of trees to use.   It is advisable to use at least thousand trees to for a better rule selection, and   in turn better predictive performance.\nmax_depth::Int=2:   The depth of the tree.   A lower depth decreases model complexity and can therefore improve accuracy when the sample size is small (reduce overfitting).\nq::Int=10: Number of cutpoints to use per feature.   The default value should be fine for most situations.\nmin_data_in_leaf::Int=5: Minimum number of data points per leaf.","category":"section"},{"location":"models/StableForestRegressor_SIRUS/#Fitted-parameters","page":"StableForestRegressor","title":"Fitted parameters","text":"The fields of fitted_params(mach) are:\n\nfitresult: A StableForest object.","category":"section"},{"location":"models/StableForestRegressor_SIRUS/#Operations","page":"StableForestRegressor","title":"Operations","text":"predict(mach, Xnew): Return a vector of predictions for each row of Xnew.","category":"section"},{"location":"models/SVC_LIBSVM/#SVC_LIBSVM","page":"SVC","title":"SVC","text":"SVC\n\nA model type for constructing a C-support vector classifier, based on LIBSVM.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nSVC = @load SVC pkg=LIBSVM\n\nDo model = SVC() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in SVC(kernel=...).\n\nThis model predicts actual class labels. To predict probabilities, use instead ProbabilisticSVC.\n\nReference for algorithm and core C-library: C.-C. Chang and C.-J. Lin (2011): \"LIBSVM: a library for support vector machines.\" ACM Transactions on Intelligent Systems and Technology, 2(3):27:1–27:27. Updated at https://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf. ","category":"section"},{"location":"models/SVC_LIBSVM/#Training-data","page":"SVC","title":"Training data","text":"In MLJ or MLJBase, bind an instance model to data with one of:\n\nmach = machine(model, X, y)\nmach = machine(model, X, y, w)\n\nwhere\n\nX: any table of input features (eg, a DataFrame) whose columns each have Continuous element scitype; check column scitypes with schema(X)\ny: is the target, which can be any AbstractVector whose element scitype is <:OrderedFactor or <:Multiclass; check the scitype with scitype(y)\nw: a dictionary of class weights, keyed on levels(y).\n\nTrain the machine using fit!(mach, rows=...).","category":"section"},{"location":"models/SVC_LIBSVM/#Hyper-parameters","page":"SVC","title":"Hyper-parameters","text":"kernel=LIBSVM.Kernel.RadialBasis: either an object that can be called, as in kernel(x1, x2), or one of the built-in kernels from the LIBSVM.jl package listed below.  Here x1 and x2 are vectors whose lengths match the number of columns of the training data X (see \"Examples\" below).\nLIBSVM.Kernel.Linear: (x1, x2) -> x1'*x2\nLIBSVM.Kernel.Polynomial: (x1, x2) -> gamma*x1'*x2 + coef0)^degree\nLIBSVM.Kernel.RadialBasis: (x1, x2) -> (exp(-gamma*norm(x1 - x2)^2))\nLIBSVM.Kernel.Sigmoid: (x1, x2) - > tanh(gamma*x1'*x2 + coef0)\nHere gamma, coef0, degree are other hyper-parameters. Serialization of models with user-defined kernels comes with some restrictions. See LIVSVM.jl issue91\ngamma = 0.0: kernel parameter (see above); if gamma==-1.0 then gamma = 1/nfeatures is used in training, where nfeatures is the number of features (columns of X).  If gamma==0.0 then gamma = 1/(var(Tables.matrix(X))*nfeatures) is used. Actual value used appears in the report (see below).\ncoef0 = 0.0: kernel parameter (see above)\ndegree::Int32 = Int32(3): degree in polynomial kernel (see above)\ncost=1.0 (range (0, Inf)): the parameter denoted C in the cited reference; for greater regularization, decrease cost\ncachesize=200.0 cache memory size in MB\ntolerance=0.001: tolerance for the stopping criterion\nshrinking=true: whether to use shrinking heuristics","category":"section"},{"location":"models/SVC_LIBSVM/#Operations","page":"SVC","title":"Operations","text":"predict(mach, Xnew): return predictions of the target given features Xnew having the same scitype as X above.","category":"section"},{"location":"models/SVC_LIBSVM/#Fitted-parameters","page":"SVC","title":"Fitted parameters","text":"The fields of fitted_params(mach) are:\n\nlibsvm_model: the trained model object created by the LIBSVM.jl package\nencoding: class encoding used internally by libsvm_model - a dictionary of class labels keyed on the internal integer representation","category":"section"},{"location":"models/SVC_LIBSVM/#Report","page":"SVC","title":"Report","text":"The fields of report(mach) are:\n\ngamma: actual value of the kernel parameter gamma used in training","category":"section"},{"location":"models/SVC_LIBSVM/#Examples","page":"SVC","title":"Examples","text":"","category":"section"},{"location":"models/SVC_LIBSVM/#Using-a-built-in-kernel","page":"SVC","title":"Using a built-in kernel","text":"using MLJ\nimport LIBSVM\n\nSVC = @load SVC pkg=LIBSVM                   ## model type\nmodel = SVC(kernel=LIBSVM.Kernel.Polynomial) ## instance\n\nX, y = @load_iris ## table, vector\nmach = machine(model, X, y) |> fit!\n\nXnew = (sepal_length = [6.4, 7.2, 7.4],\n        sepal_width = [2.8, 3.0, 2.8],\n        petal_length = [5.6, 5.8, 6.1],\n        petal_width = [2.1, 1.6, 1.9],)\n\njulia> yhat = predict(mach, Xnew)\n3-element CategoricalArrays.CategoricalArray{String,1,UInt32}:\n \"virginica\"\n \"virginica\"\n \"virginica\"","category":"section"},{"location":"models/SVC_LIBSVM/#User-defined-kernels","page":"SVC","title":"User-defined kernels","text":"k(x1, x2) = x1'*x2 ## equivalent to `LIBSVM.Kernel.Linear`\nmodel = SVC(kernel=k)\nmach = machine(model, X, y) |> fit!\n\njulia> yhat = predict(mach, Xnew)\n3-element CategoricalArrays.CategoricalArray{String,1,UInt32}:\n \"virginica\"\n \"virginica\"\n \"virginica\"","category":"section"},{"location":"models/SVC_LIBSVM/#Incorporating-class-weights","page":"SVC","title":"Incorporating class weights","text":"In either scenario above, we can do:\n\nweights = Dict(\"virginica\" => 1, \"versicolor\" => 20, \"setosa\" => 1)\nmach = machine(model, X, y, weights) |> fit!\n\njulia> yhat = predict(mach, Xnew)\n3-element CategoricalArrays.CategoricalArray{String,1,UInt32}:\n \"versicolor\"\n \"versicolor\"\n \"versicolor\"\n\nSee also the classifiers ProbabilisticSVC, NuSVC and LinearSVC. And see LIVSVM.jl and the original C implementation documentation.","category":"section"},{"location":"modifying_behavior/#Modifying-Behavior","page":"Modifying Behavior","title":"Modifying Behavior","text":"To modify behavior of MLJ you will need to clone the relevant component package (e.g., MLJBase.jl) - or a fork thereof - and modify your local julia environment to use your local clone in place of the official release. For example, you might proceed something like this:\n\nusing Pkg\nPkg.activate(\"my_MLJ_enf\", shared=true)\nPkg.develop(\"path/to/my/local/MLJBase\")\n\nTo test your local clone, do\n\nPkg.test(\"MLJBase\")\n\nFor more on package management, see here.","category":"section"},{"location":"models/INNEDetector_OutlierDetectionPython/#INNEDetector_OutlierDetectionPython","page":"INNEDetector","title":"INNEDetector","text":"INNEDetector(n_estimators=200,\n                max_samples=\"auto\",\n                random_state=None)\n\nhttps://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.inne","category":"section"},{"location":"models/COFDetector_OutlierDetectionNeighbors/#COFDetector_OutlierDetectionNeighbors","page":"COFDetector","title":"COFDetector","text":"COFDetector(k = 5,\n            metric = Euclidean(),\n            algorithm = :kdtree,\n            leafsize = 10,\n            reorder = true,\n            parallel = false)\n\nLocal outlier density based on chaining distance between graphs of neighbors, as described in [1].","category":"section"},{"location":"models/COFDetector_OutlierDetectionNeighbors/#Parameters","page":"COFDetector","title":"Parameters","text":"k::Integer\n\nNumber of neighbors (must be greater than 0).\n\nmetric::Metric\n\nThis is one of the Metric types defined in the Distances.jl package. It is possible to define your own metrics by creating new types that are subtypes of Metric.\n\nalgorithm::Symbol\n\nOne of (:kdtree, :balltree). In a kdtree, points are recursively split into groups using hyper-planes. Therefore a KDTree only works with axis aligned metrics which are: Euclidean, Chebyshev, Minkowski and Cityblock. A brutetree linearly searches all points in a brute force fashion and works with any Metric. A balltree recursively splits points into groups bounded by hyper-spheres and works with any Metric.\n\nstatic::Union{Bool, Symbol}\n\nOne of (true, false, :auto). Whether the input data for fitting and transform should be statically or dynamically allocated. If true, the data is statically allocated. If false, the data is dynamically allocated. If :auto, the data is dynamically allocated if the product of all dimensions except the last is greater than 100.\n\nleafsize::Int\n\nDetermines at what number of points to stop splitting the tree further. There is a trade-off between traversing the tree and having to evaluate the metric function for increasing number of points.\n\nreorder::Bool\n\nWhile building the tree this will put points close in distance close in memory since this helps with cache locality. In this case, a copy of the original data will be made so that the original data is left unmodified. This can have a significant impact on performance and is by default set to true.\n\nparallel::Bool\n\nParallelize score and predict using all threads available. The number of threads can be set with the JULIA_NUM_THREADS environment variable. Note: fit is not parallel.","category":"section"},{"location":"models/COFDetector_OutlierDetectionNeighbors/#Examples","page":"COFDetector","title":"Examples","text":"using OutlierDetection: COFDetector, fit, transform\ndetector = COFDetector()\nX = rand(10, 100)\nmodel, result = fit(detector, X; verbosity=0)\ntest_scores = transform(detector, model, X)","category":"section"},{"location":"models/COFDetector_OutlierDetectionNeighbors/#References","page":"COFDetector","title":"References","text":"[1] Tang, Jian; Chen, Zhixiang; Fu, Ada Wai-Chee; Cheung, David Wai-Lok (2002): Enhancing Effectiveness of Outlier Detections for Low Density Patterns.","category":"section"},{"location":"models/SMOTEN_Imbalance/#SMOTEN_Imbalance","page":"SMOTEN","title":"SMOTEN","text":"Initiate a SMOTEN model with the given hyper-parameters.\n\nSMOTEN\n\nA model type for constructing a smoten, based on Imbalance.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nSMOTEN = @load SMOTEN pkg=Imbalance\n\nDo model = SMOTEN() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in SMOTEN(k=...).\n\nSMOTEN implements the SMOTEN algorithm to correct for class imbalance as in N. V. Chawla, K. W. Bowyer, L. O.Hall, W. P. Kegelmeyer, “SMOTEN: synthetic minority over-sampling technique,” Journal of artificial intelligence research, 321-357, 2002.","category":"section"},{"location":"models/SMOTEN_Imbalance/#Training-data","page":"SMOTEN","title":"Training data","text":"In MLJ or MLJBase, wrap the model in a machine by\n\nmach = machine(model)\n\nThere is no need to provide any data here because the model is a static transformer.\n\nLikewise, there is no need to fit!(mach).\n\nFor default values of the hyper-parameters, model can be constructed by\n\nmodel = SMOTEN()","category":"section"},{"location":"models/SMOTEN_Imbalance/#Hyperparameters","page":"SMOTEN","title":"Hyperparameters","text":"k=5: Number of nearest neighbors to consider in the SMOTEN algorithm.  Should be within   the range [1, n - 1], where n is the number of observations; otherwise set to the   nearest of these two values.\nratios=1.0: A parameter that controls the amount of oversampling to be done for each class\nCan be a float and in this case each class will be oversampled to the size of the majority class times the float. By default, all classes are oversampled to the size of the majority class\nCan be a dictionary mapping each class label to the float ratio for that class\nrng::Union{AbstractRNG, Integer}=default_rng(): Either an AbstractRNG object or an Integer    seed to be used with Xoshiro if the Julia VERSION supports it. Otherwise, uses MersenneTwister`.","category":"section"},{"location":"models/SMOTEN_Imbalance/#Transform-Inputs","page":"SMOTEN","title":"Transform Inputs","text":"X: A matrix of integers or a table with element scitypes that subtype Finite.     That is, for table inputs each column should have either OrderedFactor or Multiclass as the element scitype.\ny: An abstract vector of labels (e.g., strings) that correspond to the observations in X","category":"section"},{"location":"models/SMOTEN_Imbalance/#Transform-Outputs","page":"SMOTEN","title":"Transform Outputs","text":"Xover: A matrix or table that includes original data and the new observations    due to oversampling. depending on whether the input X is a matrix or table respectively\nyover: An abstract vector of labels corresponding to Xover","category":"section"},{"location":"models/SMOTEN_Imbalance/#Operations","page":"SMOTEN","title":"Operations","text":"transform(mach, X, y): resample the data X and y using SMOTEN, returning both the new and original observations","category":"section"},{"location":"models/SMOTEN_Imbalance/#Example","page":"SMOTEN","title":"Example","text":"using MLJ\nusing ScientificTypes\nimport Imbalance\n\n## set probability of each class\nclass_probs = [0.5, 0.2, 0.3]                         \nnum_rows = 100\nnum_continuous_feats = 0\n## want two categorical features with three and two possible values respectively\nnum_vals_per_category = [3, 2]\n\n## generate a table and categorical vector accordingly\nX, y = Imbalance.generate_imbalanced_data(num_rows, num_continuous_feats; \n                                class_probs, num_vals_per_category, rng=42)                      \njulia> Imbalance.checkbalance(y)\n1: ▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 19 (39.6%) \n2: ▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 33 (68.8%) \n0: ▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 48 (100.0%) \n\njulia> ScientificTypes.schema(X).scitypes\n(Count, Count)\n\n## coerce to a finite scitype (multiclass or ordered factor)\nX = coerce(X, autotype(X, :few_to_finite))\n\n## load SMOTEN\nSMOTEN = @load SMOTEN pkg=Imbalance\n\n## wrap the model in a machine\noversampler = SMOTEN(k=5, ratios=Dict(0=>1.0, 1=> 0.9, 2=>0.8), rng=42)\nmach = machine(oversampler)\n\n## provide the data to transform (there is nothing to fit)\nXover, yover = transform(mach, X, y)\n\njulia> Imbalance.checkbalance(yover)\n2: ▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 38 (79.2%) \n1: ▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 43 (89.6%) \n0: ▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 48 (100.0%) ","category":"section"},{"location":"models/NeuralNetworkClassifier_BetaML/#NeuralNetworkClassifier_BetaML","page":"NeuralNetworkClassifier","title":"NeuralNetworkClassifier","text":"mutable struct NeuralNetworkClassifier <: MLJModelInterface.Probabilistic\n\nA simple but flexible Feedforward Neural Network, from the Beta Machine Learning Toolkit (BetaML) for classification  problems.","category":"section"},{"location":"models/NeuralNetworkClassifier_BetaML/#Parameters:","page":"NeuralNetworkClassifier","title":"Parameters:","text":"layers: Array of layer objects [def: nothing, i.e. basic network]. See subtypes(BetaML.AbstractLayer) for supported layers. The last \"softmax\" layer is automatically added.\nloss: Loss (cost) function [def: BetaML.crossentropy]. Should always assume y and ŷ as matrices.\nwarning: Warning\nIf you change the parameter loss, you need to either provide its derivative on the parameter dloss or use autodiff with dloss=nothing.\ndloss: Derivative of the loss function [def: BetaML.dcrossentropy, i.e. the derivative of the cross-entropy]. Use nothing for autodiff.\nepochs: Number of epochs, i.e. passages trough the whole training sample [def: 200]\nbatch_size: Size of each individual batch [def: 16]\nopt_alg: The optimisation algorithm to update the gradient at each batch [def: BetaML.ADAM()]. See subtypes(BetaML.OptimisationAlgorithm) for supported optimizers\nshuffle: Whether to randomly shuffle the data at each iteration (epoch) [def: true]\ndescr: An optional title and/or description for this model\ncb: A call back function to provide information during training [def: BetaML.fitting_info]\ncategories: The categories to represent as columns. [def: nothing, i.e. unique training values].\nhandle_unknown: How to handle categories not seens in training or not present in the provided categories array? \"error\" (default) rises an error, \"infrequent\" adds a specific column for these categories.\nother_categories_name: Which value during prediction to assign to this \"other\" category (i.e. categories not seen on training or not present in the provided categories array? [def: nothing, i.e. typemax(Int64) for integer vectors and \"other\" for other types]. This setting is active only if handle_unknown=\"infrequent\" and in that case it MUST be specified if Y is neither integer or strings\nrng: Random Number Generator [deafult: Random.GLOBAL_RNG]","category":"section"},{"location":"models/NeuralNetworkClassifier_BetaML/#Notes:","page":"NeuralNetworkClassifier","title":"Notes:","text":"data must be numerical\nthe label should be a n-records by n-dimensions matrix (e.g. a one-hot-encoded data for classification), where the output columns should be interpreted as the probabilities for each categories.","category":"section"},{"location":"models/NeuralNetworkClassifier_BetaML/#Example:","page":"NeuralNetworkClassifier","title":"Example:","text":"julia> using MLJ\n\njulia> X, y        = @load_iris;\n\njulia> modelType   = @load NeuralNetworkClassifier pkg = \"BetaML\" verbosity=0\nBetaML.Nn.NeuralNetworkClassifier\n\njulia> layers      = [BetaML.DenseLayer(4,8,f=BetaML.relu),BetaML.DenseLayer(8,8,f=BetaML.relu),BetaML.DenseLayer(8,3,f=BetaML.relu),BetaML.VectorFunctionLayer(3,f=BetaML.softmax)];\n\njulia> model       = modelType(layers=layers,opt_alg=BetaML.ADAM())\nNeuralNetworkClassifier(\n  layers = BetaML.Nn.AbstractLayer[BetaML.Nn.DenseLayer([-0.376173352338049 0.7029289511758696 -0.5589563304592478 -0.21043274001651874; 0.044758889527899415 0.6687689636685921 0.4584331114653877 0.6820506583840453; … ; -0.26546358457167507 -0.28469736227283804 -0.164225549922154 -0.516785639164486; -0.5146043550684141 -0.0699113265130964 0.14959906603941908 -0.053706860039406834], [0.7003943613125758, -0.23990840466587576, -0.23823126271387746, 0.4018101580410387, 0.2274483050356888, -0.564975060667734, 0.1732063297031089, 0.11880299829896945], BetaML.Utils.relu, BetaML.Utils.drelu), BetaML.Nn.DenseLayer([-0.029467850439546583 0.4074661266592745 … 0.36775675246760053 -0.595524555448422; 0.42455597698371306 -0.2458082732997091 … -0.3324220683462514 0.44439454998610595; … ; -0.2890883863364267 -0.10109249362508033 … -0.0602680568207582 0.18177278845097555; -0.03432587226449335 -0.4301192922760063 … 0.5646018168286626 0.47269177680892693], [0.13777442835428688, 0.5473306726675433, 0.3781939472904011, 0.24021813428130567, -0.0714779477402877, -0.020386373530818958, 0.5465466618404464, -0.40339790713616525], BetaML.Utils.relu, BetaML.Utils.drelu), BetaML.Nn.DenseLayer([0.6565120540082393 0.7139211611842745 … 0.07809812467915389 -0.49346311403373844; -0.4544472987041656 0.6502667641568863 … 0.43634608676548214 0.7213049952968921; 0.41212264783075303 -0.21993289366360613 … 0.25365007887755064 -0.5664469566269569], [-0.6911986792747682, -0.2149343209329364, -0.6347727539063817], BetaML.Utils.relu, BetaML.Utils.drelu), BetaML.Nn.VectorFunctionLayer{0}(fill(NaN), 3, 3, BetaML.Utils.softmax, BetaML.Utils.dsoftmax, nothing)], \n  loss = BetaML.Utils.crossentropy, \n  dloss = BetaML.Utils.dcrossentropy, \n  epochs = 100, \n  batch_size = 32, \n  opt_alg = BetaML.Nn.ADAM(BetaML.Nn.var\"#90#93\"(), 1.0, 0.9, 0.999, 1.0e-8, BetaML.Nn.Learnable[], BetaML.Nn.Learnable[]), \n  shuffle = true, \n  descr = \"\", \n  cb = BetaML.Nn.fitting_info, \n  categories = nothing, \n  handle_unknown = \"error\", \n  other_categories_name = nothing, \n  rng = Random._GLOBAL_RNG())\n\njulia> mach        = machine(model, X, y);\n\njulia> fit!(mach);\n\njulia> classes_est = predict(mach, X)\n150-element CategoricalDistributions.UnivariateFiniteVector{Multiclass{3}, String, UInt8, Float64}:\n UnivariateFinite{Multiclass{3}}(setosa=>0.575, versicolor=>0.213, virginica=>0.213)\n UnivariateFinite{Multiclass{3}}(setosa=>0.573, versicolor=>0.213, virginica=>0.213)\n ⋮\n UnivariateFinite{Multiclass{3}}(setosa=>0.236, versicolor=>0.236, virginica=>0.529)\n UnivariateFinite{Multiclass{3}}(setosa=>0.254, versicolor=>0.254, virginica=>0.492)","category":"section"},{"location":"models/GradientBoostingRegressor_MLJScikitLearnInterface/#GradientBoostingRegressor_MLJScikitLearnInterface","page":"GradientBoostingRegressor","title":"GradientBoostingRegressor","text":"GradientBoostingRegressor\n\nA model type for constructing a gradient boosting ensemble regression, based on MLJScikitLearnInterface.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nGradientBoostingRegressor = @load GradientBoostingRegressor pkg=MLJScikitLearnInterface\n\nDo model = GradientBoostingRegressor() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in GradientBoostingRegressor(loss=...).\n\nThis estimator builds an additive model in a forward stage-wise fashion;  it allows for the optimization of arbitrary differentiable loss functions.  In each stage a regression tree is fit on the negative gradient of the  given loss function.\n\nHistGradientBoostingRegressor is a much faster variant of this  algorithm for intermediate datasets (n_samples >= 10_000).","category":"section"},{"location":"models/BayesianLDA_MultivariateStats/#BayesianLDA_MultivariateStats","page":"BayesianLDA","title":"BayesianLDA","text":"BayesianLDA\n\nA model type for constructing a Bayesian LDA model, based on MultivariateStats.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nBayesianLDA = @load BayesianLDA pkg=MultivariateStats\n\nDo model = BayesianLDA() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in BayesianLDA(method=...).\n\nThe Bayesian multiclass LDA algorithm learns a projection matrix as described in ordinary LDA.  Predicted class posterior probability distributions are derived by applying Bayes' rule with a multivariate Gaussian class-conditional distribution. A prior class distribution can be specified by the user or inferred from training data class frequency.\n\nSee also the package documentation.  For more information about the algorithm, see Li, Zhu and Ogihara (2006): Using Discriminant Analysis for Multi-class Classification: An Experimental Investigation.","category":"section"},{"location":"models/BayesianLDA_MultivariateStats/#Training-data","page":"BayesianLDA","title":"Training data","text":"In MLJ or MLJBase, bind an instance model to data with\n\nmach = machine(model, X, y)\n\nHere:\n\nX is any table of input features (eg, a DataFrame) whose columns are of scitype Continuous; check column scitypes with schema(X).\ny is the target, which can be any AbstractVector whose element scitype is OrderedFactor or Multiclass; check the scitype with scitype(y)\n\nTrain the machine using fit!(mach, rows=...).","category":"section"},{"location":"models/BayesianLDA_MultivariateStats/#Hyper-parameters","page":"BayesianLDA","title":"Hyper-parameters","text":"method::Symbol=:gevd: choice of solver, one of :gevd or :whiten methods.\ncov_w::StatsBase.SimpleCovariance(): An estimator for the within-class covariance (used in computing the within-class scatter matrix, Sw). Any robust estimator from CovarianceEstimation.jl can be used.\ncov_b::StatsBase.SimpleCovariance(): The same as cov_w but for the between-class covariance (used in computing the between-class scatter matrix, Sb).\noutdim::Int=0: The output dimension, i.e., dimension of the transformed space, automatically set to min(indim, nclasses-1) if equal to 0.\nregcoef::Float64=1e-6: The regularization coefficient. A positive value regcoef*eigmax(Sw) where Sw is the within-class scatter matrix, is added to the diagonal of Sw to improve numerical stability. This can be useful if using the standard covariance estimator.\npriors::Union{Nothing, UnivariateFinite{<:Any, <:Any, <:Any, <:Real}, Dict{<:Any, <:Real}} = nothing: For use in prediction with Bayes rule. If priors = nothing then priors are estimated from the class proportions in the training data. Otherwise it requires a Dict or UnivariateFinite object specifying the classes with non-zero probabilities in the training target.","category":"section"},{"location":"models/BayesianLDA_MultivariateStats/#Operations","page":"BayesianLDA","title":"Operations","text":"transform(mach, Xnew): Return a lower dimensional projection of the input Xnew, which should have the same scitype as X above.\npredict(mach, Xnew): Return predictions of the target given features Xnew, which should have the same scitype as X above. Predictions are probabilistic but uncalibrated.\npredict_mode(mach, Xnew): Return the modes of the probabilistic predictions returned above.","category":"section"},{"location":"models/BayesianLDA_MultivariateStats/#Fitted-parameters","page":"BayesianLDA","title":"Fitted parameters","text":"The fields of fitted_params(mach) are:\n\nclasses: The classes seen during model fitting.\nprojection_matrix: The learned projection matrix, of size (indim, outdim), where indim and outdim are the input and output dimensions respectively (See Report section below).\npriors: The class priors for classification. As inferred from training target y, if not user-specified. A UnivariateFinite object with levels consistent with levels(y).","category":"section"},{"location":"models/BayesianLDA_MultivariateStats/#Report","page":"BayesianLDA","title":"Report","text":"The fields of report(mach) are:\n\nindim: The dimension of the input space i.e the number of training features.\noutdim: The dimension of the transformed space the model is projected to.\nmean: The mean of the untransformed training data. A vector of length indim.\nnclasses: The number of classes directly observed in the training data (which can be less than the total number of classes in the class pool).\nclass_means: The class-specific means of the training data. A matrix of size (indim, nclasses) with the ith column being the class-mean of the ith class in classes (See fitted params section above).\nclass_weights: The weights (class counts) of each class. A vector of length nclasses with the ith element being the class weight of the ith class in classes. (See fitted params section above.)\nSb: The between class scatter matrix.\nSw: The within class scatter matrix.","category":"section"},{"location":"models/BayesianLDA_MultivariateStats/#Examples","page":"BayesianLDA","title":"Examples","text":"using MLJ\n\nBayesianLDA = @load BayesianLDA pkg=MultivariateStats\n\nX, y = @load_iris ## a table and a vector\n\nmodel = BayesianLDA()\nmach = machine(model, X, y) |> fit!\n\nXproj = transform(mach, X)\ny_hat = predict(mach, X)\nlabels = predict_mode(mach, X)\n\nSee also LDA, SubspaceLDA, BayesianSubspaceLDA","category":"section"},{"location":"models/BinaryThresholdPredictor_MLJModels/#BinaryThresholdPredictor_MLJModels","page":"BinaryThresholdPredictor","title":"BinaryThresholdPredictor","text":"BinaryThresholdPredictor(model; threshold=0.5)\n\nWrap the Probabilistic model, model, assumed to support binary classification, as a Deterministic model, by applying the specified threshold to the positive class probability. In addition to conventional supervised classifiers, it can also be applied to outlier detection models that predict normalized scores - in the form of appropriate UnivariateFinite distributions - that is, models that subtype AbstractProbabilisticUnsupervisedDetector or AbstractProbabilisticSupervisedDetector.\n\nBy convention the positive class is the second class returned by levels(y), where y is the target.\n\nIf threshold=0.5 then calling predict on the wrapped model is equivalent to calling predict_mode on the atomic model.","category":"section"},{"location":"models/BinaryThresholdPredictor_MLJModels/#Example","page":"BinaryThresholdPredictor","title":"Example","text":"Below is an application to the well-known Pima Indian diabetes dataset, including optimization of the threshold parameter, with a high balanced accuracy the objective. The target class distribution is 500 positives to 268 negatives.\n\nLoading the data:\n\nusing MLJ, Random\nrng = Xoshiro(123)\n\ndiabetes = OpenML.load(43582)\noutcome, X = unpack(diabetes, ==(:Outcome), rng=rng);\ny = coerce(Int.(outcome), OrderedFactor);\n\nChoosing a probabilistic classifier:\n\nEvoTreesClassifier = @load EvoTreesClassifier\nprob_predictor = EvoTreesClassifier()\n\nWrapping in TunedModel to get a deterministic classifier with threshold as a new hyperparameter:\n\npoint_predictor = BinaryThresholdPredictor(prob_predictor, threshold=0.6)\nXnew, _ = make_moons(3, rng=rng)\nmach = machine(point_predictor, X, y) |> fit!\npredict(mach, X)[1:3] ## [0, 0, 0]\n\nEstimating performance:\n\nbalanced = BalancedAccuracy(adjusted=true)\ne = evaluate!(mach, resampling=CV(nfolds=6), measures=[balanced, accuracy])\ne.measurement[1] ## 0.405 ± 0.089\n\nWrapping in tuning strategy to learn threshold that maximizes balanced accuracy:\n\nr = range(point_predictor, :threshold, lower=0.1, upper=0.9)\ntuned_point_predictor = TunedModel(\n    point_predictor,\n    tuning=RandomSearch(rng=rng),\n    resampling=CV(nfolds=6),\n    range = r,\n    measure=balanced,\n    n=30,\n)\nmach2 = machine(tuned_point_predictor, X, y) |> fit!\noptimized_point_predictor = report(mach2).best_model\noptimized_point_predictor.threshold ## 0.260\npredict(mach2, X)[1:3] ## [1, 1, 0]\n\nEstimating the performance of the auto-thresholding model (nested resampling here):\n\ne = evaluate!(mach2, resampling=CV(nfolds=6), measure=[balanced, accuracy])\ne.measurement[1] ## 0.477 ± 0.110","category":"section"},{"location":"models/GaussianMixtureClusterer_BetaML/#GaussianMixtureClusterer_BetaML","page":"GaussianMixtureClusterer","title":"GaussianMixtureClusterer","text":"mutable struct GaussianMixtureClusterer <: MLJModelInterface.Unsupervised\n\nA Expectation-Maximisation clustering algorithm with customisable mixtures, from the Beta Machine Learning Toolkit (BetaML).","category":"section"},{"location":"models/GaussianMixtureClusterer_BetaML/#Hyperparameters:","page":"GaussianMixtureClusterer","title":"Hyperparameters:","text":"n_classes::Int64: Number of mixtures (latent classes) to consider [def: 3]\ninitial_probmixtures::AbstractVector{Float64}: Initial probabilities of the categorical distribution (n_classes x 1) [default: []]\nmixtures::Union{Type, Vector{<:BetaML.GMM.AbstractMixture}}: An array (of length n_classes) of the mixtures to employ (see the ?GMM module). Each mixture object can be provided with or without its parameters (e.g. mean and variance for the gaussian ones). Fully qualified mixtures are useful only if the initialisation_strategy parameter is set to \"gived\". This parameter can also be given symply in term of a type. In this case it is automatically extended to a vector of n_classes mixtures of the specified type. Note that mixing of different mixture types is not currently supported. [def: [DiagonalGaussian() for i in 1:n_classes]]\ntol::Float64: Tolerance to stop the algorithm [default: 10^(-6)]\nminimum_variance::Float64: Minimum variance for the mixtures [default: 0.05]\nminimum_covariance::Float64: Minimum covariance for the mixtures with full covariance matrix [default: 0]. This should be set different than minimum_variance (see notes).\ninitialisation_strategy::String: The computation method of the vector of the initial mixtures. One of the following:\n\"grid\": using a grid approach\n\"given\": using the mixture provided in the fully qualified mixtures parameter\n\"kmeans\": use first kmeans (itself initialised with a \"grid\" strategy) to set the initial mixture centers [default]\nNote that currently \"random\" and \"shuffle\" initialisations are not supported in gmm-based algorithms.\nmaximum_iterations::Int64: Maximum number of iterations [def: typemax(Int64), i.e. ∞]\nrng::Random.AbstractRNG: Random Number Generator [deafult: Random.GLOBAL_RNG]","category":"section"},{"location":"models/GaussianMixtureClusterer_BetaML/#Example:","page":"GaussianMixtureClusterer","title":"Example:","text":"\njulia> using MLJ\n\njulia> X, y        = @load_iris;\n\njulia> modelType   = @load GaussianMixtureClusterer pkg = \"BetaML\" verbosity=0\nBetaML.GMM.GaussianMixtureClusterer\n\njulia> model       = modelType()\nGaussianMixtureClusterer(\n  n_classes = 3, \n  initial_probmixtures = Float64[], \n  mixtures = BetaML.GMM.DiagonalGaussian{Float64}[BetaML.GMM.DiagonalGaussian{Float64}(nothing, nothing), BetaML.GMM.DiagonalGaussian{Float64}(nothing, nothing), BetaML.GMM.DiagonalGaussian{Float64}(nothing, nothing)], \n  tol = 1.0e-6, \n  minimum_variance = 0.05, \n  minimum_covariance = 0.0, \n  initialisation_strategy = \"kmeans\", \n  maximum_iterations = 9223372036854775807, \n  rng = Random._GLOBAL_RNG())\n\njulia> mach        = machine(model, X);\n\njulia> fit!(mach);\n[ Info: Training machine(GaussianMixtureClusterer(n_classes = 3, …), …).\nIter. 1:        Var. of the post  10.800150114964184      Log-likelihood -650.0186451891216\n\njulia> classes_est = predict(mach, X)\n150-element CategoricalDistributions.UnivariateFiniteVector{Multiclass{3}, Int64, UInt32, Float64}:\n UnivariateFinite{Multiclass{3}}(1=>1.0, 2=>4.17e-15, 3=>2.1900000000000003e-31)\n UnivariateFinite{Multiclass{3}}(1=>1.0, 2=>1.25e-13, 3=>5.87e-31)\n UnivariateFinite{Multiclass{3}}(1=>1.0, 2=>4.5e-15, 3=>1.55e-32)\n UnivariateFinite{Multiclass{3}}(1=>1.0, 2=>6.93e-14, 3=>3.37e-31)\n ⋮\n UnivariateFinite{Multiclass{3}}(1=>5.39e-25, 2=>0.0167, 3=>0.983)\n UnivariateFinite{Multiclass{3}}(1=>7.5e-29, 2=>0.000106, 3=>1.0)\n UnivariateFinite{Multiclass{3}}(1=>1.6e-20, 2=>0.594, 3=>0.406)","category":"section"},{"location":"models/RandomForestClassifier_BetaML/#RandomForestClassifier_BetaML","page":"RandomForestClassifier","title":"RandomForestClassifier","text":"mutable struct RandomForestClassifier <: MLJModelInterface.Probabilistic\n\nA simple Random Forest model for classification with support for Missing data, from the Beta Machine Learning Toolkit (BetaML).","category":"section"},{"location":"models/RandomForestClassifier_BetaML/#Hyperparameters:","page":"RandomForestClassifier","title":"Hyperparameters:","text":"n_trees::Int64\nmax_depth::Int64: The maximum depth the tree is allowed to reach. When this is reached the node is forced to become a leaf [def: 0, i.e. no limits]\nmin_gain::Float64: The minimum information gain to allow for a node's partition [def: 0]\nmin_records::Int64: The minimum number of records a node must holds to consider for a partition of it [def: 2]\nmax_features::Int64: The maximum number of (random) features to consider at each partitioning [def: 0, i.e. square root of the data dimensions]\nsplitting_criterion::Function: This is the name of the function to be used to compute the information gain of a specific partition. This is done by measuring the difference betwwen the \"impurity\" of the labels of the parent node with those of the two child nodes, weighted by the respective number of items. [def: gini]. Either gini, entropy or a custom function. It can also be an anonymous function.\nβ::Float64: Parameter that regulate the weights of the scoring of each tree, to be (optionally) used in prediction based on the error of the individual trees computed on the records on which trees have not been trained. Higher values favour \"better\" trees, but too high values will cause overfitting [def: 0, i.e. uniform weigths]\nrng::Random.AbstractRNG: A Random Number Generator to be used in stochastic parts of the code [deafult: Random.GLOBAL_RNG]","category":"section"},{"location":"models/RandomForestClassifier_BetaML/#Example-:","page":"RandomForestClassifier","title":"Example :","text":"julia> using MLJ\n\njulia> X, y        = @load_iris;\n\njulia> modelType   = @load RandomForestClassifier pkg = \"BetaML\" verbosity=0\nBetaML.Trees.RandomForestClassifier\n\njulia> model       = modelType()\nRandomForestClassifier(\n  n_trees = 30, \n  max_depth = 0, \n  min_gain = 0.0, \n  min_records = 2, \n  max_features = 0, \n  splitting_criterion = BetaML.Utils.gini, \n  β = 0.0, \n  rng = Random._GLOBAL_RNG())\n\njulia> mach        = machine(model, X, y);\n\njulia> fit!(mach);\n[ Info: Training machine(RandomForestClassifier(n_trees = 30, …), …).\n\njulia> cat_est    = predict(mach, X)\n150-element CategoricalDistributions.UnivariateFiniteVector{Multiclass{3}, String, UInt32, Float64}:\n UnivariateFinite{Multiclass{3}}(setosa=>1.0, versicolor=>0.0, virginica=>0.0)\n UnivariateFinite{Multiclass{3}}(setosa=>1.0, versicolor=>0.0, virginica=>0.0)\n ⋮\n UnivariateFinite{Multiclass{3}}(setosa=>0.0, versicolor=>0.0, virginica=>1.0)\n UnivariateFinite{Multiclass{3}}(setosa=>0.0, versicolor=>0.0667, virginica=>0.933)","category":"section"},{"location":"models/DecisionTreeClassifier_DecisionTree/#DecisionTreeClassifier_DecisionTree","page":"DecisionTreeClassifier","title":"DecisionTreeClassifier","text":"DecisionTreeClassifier\n\nA model type for constructing a CART decision tree classifier, based on DecisionTree.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nDecisionTreeClassifier = @load DecisionTreeClassifier pkg=DecisionTree\n\nDo model = DecisionTreeClassifier() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in DecisionTreeClassifier(max_depth=...).\n\nDecisionTreeClassifier implements the CART algorithm, originally published in Breiman, Leo; Friedman, J. H.; Olshen, R. A.; Stone, C. J. (1984): \"Classification and regression trees\". Monterey, CA: Wadsworth & Brooks/Cole Advanced Books & Software..","category":"section"},{"location":"models/DecisionTreeClassifier_DecisionTree/#Training-data","page":"DecisionTreeClassifier","title":"Training data","text":"In MLJ or MLJBase, bind an instance model to data with\n\nmach = machine(model, X, y)\n\nwhere\n\nX: any table of input features (eg, a DataFrame) whose columns each have one of the following element scitypes: Continuous, Count, or <:OrderedFactor; check column scitypes with schema(X)\ny: is the target, which can be any AbstractVector whose element scitype is <:OrderedFactor or <:Multiclass; check the scitype with scitype(y)\n\nTrain the machine using fit!(mach, rows=...).","category":"section"},{"location":"models/DecisionTreeClassifier_DecisionTree/#Hyperparameters","page":"DecisionTreeClassifier","title":"Hyperparameters","text":"max_depth=-1:          max depth of the decision tree (-1=any)\nmin_samples_leaf=1:    max number of samples each leaf needs to have\nmin_samples_split=2:   min number of samples needed for a split\nmin_purity_increase=0: min purity needed for a split\nn_subfeatures=0: number of features to select at random (0 for all)\npost_prune=false:      set to true for post-fit pruning\nmerge_purity_threshold=1.0: (post-pruning) merge leaves having                          combined purity >= merge_purity_threshold\ndisplay_depth=5:       max depth to show when displaying the tree\nfeature_importance: method to use for computing feature importances. One of (:impurity, :split)\nrng=Random.GLOBAL_RNG: random number generator or seed","category":"section"},{"location":"models/DecisionTreeClassifier_DecisionTree/#Operations","page":"DecisionTreeClassifier","title":"Operations","text":"predict(mach, Xnew): return predictions of the target given features Xnew having the same scitype as X above. Predictions are probabilistic, but uncalibrated.\npredict_mode(mach, Xnew): instead return the mode of each prediction above.","category":"section"},{"location":"models/DecisionTreeClassifier_DecisionTree/#Fitted-parameters","page":"DecisionTreeClassifier","title":"Fitted parameters","text":"The fields of fitted_params(mach) are:\n\nraw_tree: the raw Node, Leaf or Root object returned by the core DecisionTree.jl algorithm\ntree: a visualizable, wrapped version of raw_tree implementing the AbstractTrees.jl interface; see \"Examples\" below\nencoding: dictionary of target classes keyed on integers used internally by DecisionTree.jl\nfeatures: the names of the features encountered in training, in an order consistent with the output of print_tree (see below)","category":"section"},{"location":"models/DecisionTreeClassifier_DecisionTree/#Report","page":"DecisionTreeClassifier","title":"Report","text":"The fields of report(mach) are:\n\nclasses_seen: list of target classes actually observed in training\nprint_tree: alternative method to print the fitted tree, with single argument the tree depth; interpretation requires internal integer-class encoding (see \"Fitted parameters\" above).\nfeatures: the names of the features encountered in training, in an order consistent with the output of print_tree (see below)","category":"section"},{"location":"models/DecisionTreeClassifier_DecisionTree/#Accessor-functions","page":"DecisionTreeClassifier","title":"Accessor functions","text":"feature_importances(mach) returns a vector of (feature::Symbol => importance) pairs; the type of importance is determined by the hyperparameter feature_importance (see above)","category":"section"},{"location":"models/DecisionTreeClassifier_DecisionTree/#Examples","page":"DecisionTreeClassifier","title":"Examples","text":"using MLJ\nDecisionTreeClassifier = @load DecisionTreeClassifier pkg=DecisionTree\nmodel = DecisionTreeClassifier(max_depth=3, min_samples_split=3)\n\nX, y = @load_iris\nmach = machine(model, X, y) |> fit!\n\nXnew = (sepal_length = [6.4, 7.2, 7.4],\n        sepal_width = [2.8, 3.0, 2.8],\n        petal_length = [5.6, 5.8, 6.1],\n        petal_width = [2.1, 1.6, 1.9],)\nyhat = predict(mach, Xnew) ## probabilistic predictions\npredict_mode(mach, Xnew)   ## point predictions\npdf.(yhat, \"virginica\")    ## probabilities for the \"verginica\" class\n\njulia> tree = fitted_params(mach).tree\npetal_length < 2.45\n├─ setosa (50/50)\n└─ petal_width < 1.75\n   ├─ petal_length < 4.95\n   │  ├─ versicolor (47/48)\n   │  └─ virginica (4/6)\n   └─ petal_length < 4.85\n      ├─ virginica (2/3)\n      └─ virginica (43/43)\n\nusing Plots, TreeRecipe\nplot(tree) ## for a graphical representation of the tree\n\nfeature_importances(mach)\n\nSee also DecisionTree.jl and the unwrapped model type MLJDecisionTreeInterface.DecisionTree.DecisionTreeClassifier.","category":"section"},{"location":"models/DNNDetector_OutlierDetectionNeighbors/#DNNDetector_OutlierDetectionNeighbors","page":"DNNDetector","title":"DNNDetector","text":"DNNDetector(d = 0,\n            metric = Euclidean(),\n            algorithm = :kdtree,\n            leafsize = 10,\n            reorder = true,\n            parallel = false)\n\nAnomaly score based on the number of neighbors in a hypersphere of radius d. Knorr et al. [1] directly converted the resulting outlier scores to labels, thus this implementation does not fully reflect the approach from the paper.","category":"section"},{"location":"models/DNNDetector_OutlierDetectionNeighbors/#Parameters","page":"DNNDetector","title":"Parameters","text":"d::Real\n\nThe hypersphere radius used to calculate the global density of an instance.\n\nmetric::Metric\n\nThis is one of the Metric types defined in the Distances.jl package. It is possible to define your own metrics by creating new types that are subtypes of Metric.\n\nalgorithm::Symbol\n\nOne of (:kdtree, :balltree). In a kdtree, points are recursively split into groups using hyper-planes. Therefore a KDTree only works with axis aligned metrics which are: Euclidean, Chebyshev, Minkowski and Cityblock. A brutetree linearly searches all points in a brute force fashion and works with any Metric. A balltree recursively splits points into groups bounded by hyper-spheres and works with any Metric.\n\nstatic::Union{Bool, Symbol}\n\nOne of (true, false, :auto). Whether the input data for fitting and transform should be statically or dynamically allocated. If true, the data is statically allocated. If false, the data is dynamically allocated. If :auto, the data is dynamically allocated if the product of all dimensions except the last is greater than 100.\n\nleafsize::Int\n\nDetermines at what number of points to stop splitting the tree further. There is a trade-off between traversing the tree and having to evaluate the metric function for increasing number of points.\n\nreorder::Bool\n\nWhile building the tree this will put points close in distance close in memory since this helps with cache locality. In this case, a copy of the original data will be made so that the original data is left unmodified. This can have a significant impact on performance and is by default set to true.\n\nparallel::Bool\n\nParallelize score and predict using all threads available. The number of threads can be set with the JULIA_NUM_THREADS environment variable. Note: fit is not parallel.","category":"section"},{"location":"models/DNNDetector_OutlierDetectionNeighbors/#Examples","page":"DNNDetector","title":"Examples","text":"using OutlierDetection: DNNDetector, fit, transform\ndetector = DNNDetector()\nX = rand(10, 100)\nmodel, result = fit(detector, X; verbosity=0)\ntest_scores = transform(detector, model, X)","category":"section"},{"location":"models/DNNDetector_OutlierDetectionNeighbors/#References","page":"DNNDetector","title":"References","text":"[1] Knorr, Edwin M.; Ng, Raymond T. (1998): Algorithms for Mining Distance-Based Outliers in Large Datasets.","category":"section"},{"location":"models/RidgeRegressor_MultivariateStats/#RidgeRegressor_MultivariateStats","page":"RidgeRegressor","title":"RidgeRegressor","text":"RidgeRegressor\n\nA model type for constructing a ridge regressor, based on MultivariateStats.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nRidgeRegressor = @load RidgeRegressor pkg=MultivariateStats\n\nDo model = RidgeRegressor() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in RidgeRegressor(lambda=...).\n\nRidgeRegressor adds a quadratic penalty term to least squares regression, for regularization. Ridge regression is particularly useful in the case of multicollinearity. Options exist to specify a bias term, and to adjust the strength of the penalty term.","category":"section"},{"location":"models/RidgeRegressor_MultivariateStats/#Training-data","page":"RidgeRegressor","title":"Training data","text":"In MLJ or MLJBase, bind an instance model to data with\n\nmach = machine(model, X, y)\n\nHere:\n\nX is any table of input features (eg, a DataFrame) whose columns are of scitype    Continuous; check column scitypes with schema(X).\ny is the target, which can be any AbstractVector whose element scitype is    Continuous; check the scitype with scitype(y)\n\nTrain the machine using fit!(mach, rows=...).","category":"section"},{"location":"models/RidgeRegressor_MultivariateStats/#Hyper-parameters","page":"RidgeRegressor","title":"Hyper-parameters","text":"lambda=1.0: Is the non-negative parameter for the regularization strength. If lambda    is 0, ridge regression is equivalent to linear least squares regression, and as lambda    approaches infinity, all the linear coefficients approach 0.\nbias=true: Include the bias term if true, otherwise fit without bias term.","category":"section"},{"location":"models/RidgeRegressor_MultivariateStats/#Operations","page":"RidgeRegressor","title":"Operations","text":"predict(mach, Xnew): Return predictions of the target given new features Xnew, which    should have the same scitype as X above.","category":"section"},{"location":"models/RidgeRegressor_MultivariateStats/#Fitted-parameters","page":"RidgeRegressor","title":"Fitted parameters","text":"The fields of fitted_params(mach) are:\n\ncoefficients: The linear coefficients determined by the model.\nintercept: The intercept determined by the model.","category":"section"},{"location":"models/RidgeRegressor_MultivariateStats/#Examples","page":"RidgeRegressor","title":"Examples","text":"using MLJ\n\nRidgeRegressor = @load RidgeRegressor pkg=MultivariateStats\npipe = Standardizer() |> RidgeRegressor(lambda=10)\n\nX, y = @load_boston\n\nmach = machine(pipe, X, y) |> fit!\nyhat = predict(mach, X)\ntraining_error = l1(yhat, y) |> mean\n\nSee also LinearRegressor, MultitargetLinearRegressor, MultitargetRidgeRegressor","category":"section"},{"location":"models/GradientBoostingClassifier_MLJScikitLearnInterface/#GradientBoostingClassifier_MLJScikitLearnInterface","page":"GradientBoostingClassifier","title":"GradientBoostingClassifier","text":"GradientBoostingClassifier\n\nA model type for constructing a gradient boosting classifier, based on MLJScikitLearnInterface.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nGradientBoostingClassifier = @load GradientBoostingClassifier pkg=MLJScikitLearnInterface\n\nDo model = GradientBoostingClassifier() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in GradientBoostingClassifier(loss=...).\n\nThis algorithm builds an additive model in a forward stage-wise fashion;  it allows for the optimization of arbitrary differentiable loss functions.  In each stage n_classes_ regression trees are fit on the negative gradient  of the loss function, e.g. binary or multiclass log loss. Binary  classification is a special case where only a single regression tree is induced.\n\nHistGradientBoostingClassifier is a much faster variant of this  algorithm for intermediate datasets (n_samples >= 10_000).","category":"section"},{"location":"models/SGDClassifier_MLJScikitLearnInterface/#SGDClassifier_MLJScikitLearnInterface","page":"SGDClassifier","title":"SGDClassifier","text":"SGDClassifier\n\nA model type for constructing a sgd classifier, based on MLJScikitLearnInterface.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nSGDClassifier = @load SGDClassifier pkg=MLJScikitLearnInterface\n\nDo model = SGDClassifier() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in SGDClassifier(loss=...).","category":"section"},{"location":"models/SGDClassifier_MLJScikitLearnInterface/#Hyper-parameters","page":"SGDClassifier","title":"Hyper-parameters","text":"loss = hinge\npenalty = l2\nalpha = 0.0001\nl1_ratio = 0.15\nfit_intercept = true\nmax_iter = 1000\ntol = 0.001\nshuffle = true\nverbose = 0\nepsilon = 0.1\nn_jobs = nothing\nrandom_state = nothing\nlearning_rate = optimal\neta0 = 0.0\npower_t = 0.5\nearly_stopping = false\nvalidation_fraction = 0.1\nn_iter_no_change = 5\nclass_weight = nothing\nwarm_start = false\naverage = false","category":"section"},{"location":"models/FeatureSelector_FeatureSelection/#FeatureSelector_FeatureSelection","page":"FeatureSelector","title":"FeatureSelector","text":"FeatureSelector\n\nA model type for constructing a feature selector, based on unknown.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nFeatureSelector = @load FeatureSelector pkg=unknown\n\nDo model = FeatureSelector() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in FeatureSelector(features=...).\n\nUse this model to select features (columns) of a table, usually as part of a model Pipeline.","category":"section"},{"location":"models/FeatureSelector_FeatureSelection/#Training-data","page":"FeatureSelector","title":"Training data","text":"In MLJ or MLJBase, bind an instance model to data with\n\nmach = machine(model, X)\n\nwhere\n\nX: any table of input features, where \"table\" is in the sense of Tables.jl\n\nTrain the machine using fit!(mach, rows=...).","category":"section"},{"location":"models/FeatureSelector_FeatureSelection/#Hyper-parameters","page":"FeatureSelector","title":"Hyper-parameters","text":"features: one of the following, with the behavior indicated:\n[] (empty, the default): filter out all features (columns) which were not encountered in training\nnon-empty vector of feature names (symbols): keep only the specified features (ignore=false) or keep only unspecified features (ignore=true)\nfunction or other callable: keep a feature if the callable returns true on its name. For example, specifying FeatureSelector(features = name -> name in [:x1, :x3], ignore = true) has the same effect as FeatureSelector(features = [:x1, :x3], ignore = true), namely to select all features, with the exception of :x1 and :x3.\nignore: whether to ignore or keep specified features, as explained above","category":"section"},{"location":"models/FeatureSelector_FeatureSelection/#Operations","page":"FeatureSelector","title":"Operations","text":"transform(mach, Xnew): select features from the table Xnew as specified by the model, taking features seen during training into account, if relevant","category":"section"},{"location":"models/FeatureSelector_FeatureSelection/#Fitted-parameters","page":"FeatureSelector","title":"Fitted parameters","text":"The fields of fitted_params(mach) are:\n\nfeatures_to_keep: the features that will be selected","category":"section"},{"location":"models/FeatureSelector_FeatureSelection/#Example","page":"FeatureSelector","title":"Example","text":"using MLJ\n\nX = (ordinal1 = [1, 2, 3],\n     ordinal2 = coerce([\"x\", \"y\", \"x\"], OrderedFactor),\n     ordinal3 = [10.0, 20.0, 30.0],\n     ordinal4 = [-20.0, -30.0, -40.0],\n     nominal = coerce([\"Your father\", \"he\", \"is\"], Multiclass));\n\nselector = FeatureSelector(features=[:ordinal3, ], ignore=true);\n\njulia> transform(fit!(machine(selector, X)), X)\n(ordinal1 = [1, 2, 3],\n ordinal2 = CategoricalValue{Symbol,UInt32}[\"x\", \"y\", \"x\"],\n ordinal4 = [-20.0, -30.0, -40.0],\n nominal = CategoricalValue{String,UInt32}[\"Your father\", \"he\", \"is\"],)\n","category":"section"},{"location":"models/PCA_MultivariateStats/#PCA_MultivariateStats","page":"PCA","title":"PCA","text":"PCA\n\nA model type for constructing a pca, based on MultivariateStats.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nPCA = @load PCA pkg=MultivariateStats\n\nDo model = PCA() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in PCA(maxoutdim=...).\n\nPrincipal component analysis learns a linear projection onto a lower dimensional space while preserving most of the initial variance seen in the training data.","category":"section"},{"location":"models/PCA_MultivariateStats/#Training-data","page":"PCA","title":"Training data","text":"In MLJ or MLJBase, bind an instance model to data with\n\nmach = machine(model, X)\n\nHere:\n\nX is any table of input features (eg, a DataFrame) whose columns are of scitype Continuous; check column scitypes with schema(X).\n\nTrain the machine using fit!(mach, rows=...).","category":"section"},{"location":"models/PCA_MultivariateStats/#Hyper-parameters","page":"PCA","title":"Hyper-parameters","text":"maxoutdim=0: Together with variance_ratio, controls the output dimension outdim chosen by the model. Specifically, suppose that k is the smallest integer such that retaining the k most significant principal components accounts for variance_ratio of the total variance in the training data. Then outdim = min(outdim, maxoutdim). If maxoutdim=0 (default) then the effective maxoutdim is min(n, indim - 1) where n is the number of observations and indim the number of features in the training data.\nvariance_ratio::Float64=0.99: The ratio of variance preserved after the transformation\nmethod=:auto: The method to use to solve the problem. Choices are\n:svd: Support Vector Decomposition of the matrix.\n:cov: Covariance matrix decomposition.\n:auto: Use :cov if the matrices first dimension is smaller than its second dimension and otherwise use :svd\nmean=nothing: if nothing, centering will be computed and applied, if set to 0 no centering (data is assumed pre-centered); if a vector is passed, the centering is done with that vector.","category":"section"},{"location":"models/PCA_MultivariateStats/#Operations","page":"PCA","title":"Operations","text":"transform(mach, Xnew): Return a lower dimensional projection of the input Xnew, which should have the same scitype as X above.\ninverse_transform(mach, Xsmall): For a dimension-reduced table Xsmall, such as returned by transform, reconstruct a table, having same the number of columns as the original training data X, that transforms to Xsmall. Mathematically, inverse_transform is a right-inverse for the PCA projection map, whose image is orthogonal to the kernel of that map. In particular, if Xsmall = transform(mach, Xnew), then inverse_transform(Xsmall) is only an approximation to Xnew.","category":"section"},{"location":"models/PCA_MultivariateStats/#Fitted-parameters","page":"PCA","title":"Fitted parameters","text":"The fields of fitted_params(mach) are:\n\nprojection: Returns the projection matrix, which has size (indim, outdim), where indim and outdim are the number of features of the input and output respectively.","category":"section"},{"location":"models/PCA_MultivariateStats/#Report","page":"PCA","title":"Report","text":"The fields of report(mach) are:\n\nindim: Dimension (number of columns) of the training data and new data to be transformed.\noutdim = min(n, indim, maxoutdim) is the output dimension; here n is the number of observations.\ntprincipalvar: Total variance of the principal components.\ntresidualvar: Total residual variance.\ntvar: Total observation variance (principal + residual variance).\nmean: The mean of the untransformed training data, of length indim.\nprincipalvars: The variance of the principal components. An AbstractVector of length outdim\nloadings: The models loadings, weights for each variable used when calculating principal components. A matrix of size (indim, outdim) where indim and outdim are as defined above.","category":"section"},{"location":"models/PCA_MultivariateStats/#Examples","page":"PCA","title":"Examples","text":"using MLJ\n\nPCA = @load PCA pkg=MultivariateStats\n\nX, y = @load_iris ## a table and a vector\n\nmodel = PCA(maxoutdim=2)\nmach = machine(model, X) |> fit!\n\nXproj = transform(mach, X)\n\nSee also KernelPCA, ICA, FactorAnalysis, PPCA","category":"section"},{"location":"models/ENNUndersampler_Imbalance/#ENNUndersampler_Imbalance","page":"ENNUndersampler","title":"ENNUndersampler","text":"Initiate a ENN undersampling model with the given hyper-parameters.\n\nENNUndersampler\n\nA model type for constructing a enn undersampler, based on Imbalance.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nENNUndersampler = @load ENNUndersampler pkg=Imbalance\n\nDo model = ENNUndersampler() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in ENNUndersampler(k=...).\n\nENNUndersampler undersamples a dataset by removing (\"cleaning\") points that violate a certain condition such as   having a different class compared to the majority of the neighbors as proposed in Dennis L Wilson.    Asymptotic properties of nearest neighbor rules using edited data. IEEE Transactions on Systems, Man,    and Cybernetics, pages 408–421, 1972.","category":"section"},{"location":"models/ENNUndersampler_Imbalance/#Training-data","page":"ENNUndersampler","title":"Training data","text":"In MLJ or MLJBase, wrap the model in a machine by \tmach = machine(model)\n\nThere is no need to provide any data here because the model is a static transformer.\n\nLikewise, there is no need to fit!(mach). \n\nFor default values of the hyper-parameters, model can be constructed by \tmodel = ENNUndersampler()","category":"section"},{"location":"models/ENNUndersampler_Imbalance/#Hyperparameters","page":"ENNUndersampler","title":"Hyperparameters","text":"k::Integer=5: Number of nearest neighbors to consider in the algorithm. Should be within the range 0 < k < n where n is the number of observations in the smallest class.\nkeep_condition::AbstractString=\"mode\": The condition that leads to cleaning a point upon violation. Takes one of \"exists\", \"mode\", \"only mode\" and \"all\"\n\n- `\"exists\"`: the point has at least one neighbor from the same class\n- `\"mode\"`: the class of the point is one of the most frequent classes of the neighbors (there may be many)\n- `\"only mode\"`: the class of the point is the single most frequent class of the neighbors\n- `\"all\"`: the class of the point is the same as all the neighbors\n\nmin_ratios=1.0: A parameter that controls the maximum amount of undersampling to be done for each class. If this algorithm   cleans the data to an extent that this is violated, some of the cleaned points will be revived randomly so that it is satisfied.\nCan be a float and in this case each class will be at most undersampled to the size of the minority class times the float. By default, all classes are undersampled to the size of the minority class\nCan be a dictionary mapping each class label to the float minimum ratio for that class\nforce_min_ratios=false: If true, and this algorithm cleans the data such that the ratios for each class   exceed those specified in min_ratios then further undersampling will be perform so that the final ratios   are equal to min_ratios.\nrng::Union{AbstractRNG, Integer}=default_rng(): Either an AbstractRNG object or an Integer    seed to be used with Xoshiro if the Julia VERSION supports it. Otherwise, uses MersenneTwister`.\ntry_preserve_type::Bool=true: When true, the function will try to not change the type of the input    table (e.g., DataFrame). However, for some tables, this may not succeed, and in this case, the table returned will   be a column table (named-tuple of vectors). This parameter is ignored if the input is a matrix.","category":"section"},{"location":"models/ENNUndersampler_Imbalance/#Transform-Inputs","page":"ENNUndersampler","title":"Transform Inputs","text":"X: A matrix or table of floats where each row is an observation from the dataset\ny: An abstract vector of labels (e.g., strings) that correspond to the observations in X","category":"section"},{"location":"models/ENNUndersampler_Imbalance/#Transform-Outputs","page":"ENNUndersampler","title":"Transform Outputs","text":"X_under: A matrix or table that includes the data after undersampling    depending on whether the input X is a matrix or table respectively\ny_under: An abstract vector of labels corresponding to X_under","category":"section"},{"location":"models/ENNUndersampler_Imbalance/#Operations","page":"ENNUndersampler","title":"Operations","text":"transform(mach, X, y): resample the data X and y using ENNUndersampler, returning the undersampled versions","category":"section"},{"location":"models/ENNUndersampler_Imbalance/#Example","page":"ENNUndersampler","title":"Example","text":"using MLJ\nimport Imbalance\n\n## set probability of each class\nclass_probs = [0.5, 0.2, 0.3]                         \nnum_rows, num_continuous_feats = 100, 5\n## generate a table and categorical vector accordingly\nX, y = Imbalance.generate_imbalanced_data(num_rows, num_continuous_feats; \n                                min_sep=0.01, stds=[3.0 3.0 3.0], class_probs, rng=42)     \n\njulia> Imbalance.checkbalance(y; ref=\"minority\")\n1: ▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 19 (100.0%) \n2: ▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 33 (173.7%) \n0: ▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 48 (252.6%) \n\n## load ENN model type:\nENNUndersampler = @load ENNUndersampler pkg=Imbalance\n\n## underample the majority classes to  sizes relative to the minority class:\nundersampler = ENNUndersampler(min_ratios=0.5, rng=42)\nmach = machine(undersampler)\nX_under, y_under = transform(mach, X, y)\n\njulia> Imbalance.checkbalance(y_under; ref=\"minority\")\n2: ▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 10 (100.0%) \n1: ▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 10 (100.0%) \n0: ▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 24 (240.0%) ","category":"section"},{"location":"acceleration_and_parallelism/#Acceleration-and-Parallelism","page":"Acceleration and Parallelism","title":"Acceleration and Parallelism","text":"","category":"section"},{"location":"acceleration_and_parallelism/#User-facing-interface","page":"Acceleration and Parallelism","title":"User-facing interface","text":"To enable composable, extensible acceleration of core MLJ methods, ComputationalResources.jl is utilized to provide some basic types and functions to make implementing acceleration easy. However, ambitious users or package authors have the option to define their own types to be passed as resources to acceleration, which must be <:ComputationalResources.AbstractResource.\n\nMethods which support some form of acceleration support the acceleration keyword argument, which can be passed a \"resource\" from ComputationalResources. For example, passing acceleration=CPUProcesses() will utilize Distributed's multiprocessing functionality to accelerate the computation, while acceleration=CPUThreads() will use Julia's PARTR threading model to perform acceleration.\n\nThe default computational resource is CPU1(), which is simply serial processing via CPU. The default resource can be changed as in this example: MLJ.default_resource(CPUProcesses()). The argument must always have type <:ComputationalResource.AbstractResource. To inspect the current default, use MLJ.default_resource().\n\nnote: Note\nYou cannot use CPUThreads() with models wrapping python code.","category":"section"},{"location":"models/Standardizer_MLJTransforms/#Standardizer_MLJTransforms","page":"Standardizer","title":"Standardizer","text":"Standardizer\n\nA model type for constructing a standardizer, based on MLJTransforms.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nStandardizer = @load Standardizer pkg=MLJTransforms\n\nDo model = Standardizer() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in Standardizer(features=...).\n\nUse this model to standardize (whiten) a Continuous vector, or relevant columns of a table. The rescalings applied by this transformer to new data are always those learned during the training phase, which are generally different from what would actually standardize the new data.","category":"section"},{"location":"models/Standardizer_MLJTransforms/#Training-data","page":"Standardizer","title":"Training data","text":"In MLJ or MLJBase, bind an instance model to data with\n\nmach = machine(model, X)\n\nwhere\n\nX: any Tables.jl compatible table or any abstract vector with Continuous element scitype (any abstract float vector). Only features in a table with Continuous scitype can be standardized; check column scitypes with schema(X).\n\nTrain the machine using fit!(mach, rows=...).","category":"section"},{"location":"models/Standardizer_MLJTransforms/#Hyper-parameters","page":"Standardizer","title":"Hyper-parameters","text":"features: one of the following, with the behavior indicated below:\n[] (empty, the default): standardize all features (columns) having Continuous element scitype\nnon-empty vector of feature names (symbols): standardize only the Continuous features in the vector (if ignore=false) or Continuous features not named in the vector (ignore=true).\nfunction or other callable: standardize a feature if the callable returns true on its name. For example, Standardizer(features = name -> name in [:x1, :x3], ignore = true, count=true) has the same effect as Standardizer(features = [:x1, :x3], ignore = true, count=true), namely to standardize all Continuous and Count features, with the exception of :x1 and :x3.\nNote this behavior is further modified if the ordered_factor or count flags are set to true; see below\nignore=false: whether to ignore or standardize specified features, as explained above\nordered_factor=false: if true, standardize any OrderedFactor feature wherever a Continuous feature would be standardized, as described above\ncount=false: if true, standardize any Count feature wherever a Continuous feature would be standardized, as described above","category":"section"},{"location":"models/Standardizer_MLJTransforms/#Operations","page":"Standardizer","title":"Operations","text":"transform(mach, Xnew): return Xnew with relevant features standardized according to the rescalings learned during fitting of mach.\ninverse_transform(mach, Z): apply the inverse transformation to Z, so that inverse_transform(mach, transform(mach, Xnew)) is approximately the same as Xnew; unavailable if ordered_factor or count flags were set to true.","category":"section"},{"location":"models/Standardizer_MLJTransforms/#Fitted-parameters","page":"Standardizer","title":"Fitted parameters","text":"The fields of fitted_params(mach) are:\n\nfeatures_fit - the names of features that will be standardized\nmeans - the corresponding untransformed mean values\nstds - the corresponding untransformed standard deviations","category":"section"},{"location":"models/Standardizer_MLJTransforms/#Report","page":"Standardizer","title":"Report","text":"The fields of report(mach) are:\n\nfeatures_fit: the names of features that will be standardized","category":"section"},{"location":"models/Standardizer_MLJTransforms/#Examples","page":"Standardizer","title":"Examples","text":"using MLJ\n\nX = (ordinal1 = [1, 2, 3],\n     ordinal2 = coerce([:x, :y, :x], OrderedFactor),\n     ordinal3 = [10.0, 20.0, 30.0],\n     ordinal4 = [-20.0, -30.0, -40.0],\n     nominal = coerce([\"Your father\", \"he\", \"is\"], Multiclass));\n\njulia> schema(X)\n┌──────────┬──────────────────┐\n│ names    │ scitypes         │\n├──────────┼──────────────────┤\n│ ordinal1 │ Count            │\n│ ordinal2 │ OrderedFactor{2} │\n│ ordinal3 │ Continuous       │\n│ ordinal4 │ Continuous       │\n│ nominal  │ Multiclass{3}    │\n└──────────┴──────────────────┘\n\nstand1 = Standardizer();\n\njulia> transform(fit!(machine(stand1, X)), X)\n(ordinal1 = [1, 2, 3],\n ordinal2 = CategoricalValue{Symbol,UInt32}[:x, :y, :x],\n ordinal3 = [-1.0, 0.0, 1.0],\n ordinal4 = [1.0, 0.0, -1.0],\n nominal = CategoricalValue{String,UInt32}[\"Your father\", \"he\", \"is\"],)\n\nstand2 = Standardizer(features=[:ordinal3, ], ignore=true, count=true);\n\njulia> transform(fit!(machine(stand2, X)), X)\n(ordinal1 = [-1.0, 0.0, 1.0],\n ordinal2 = CategoricalValue{Symbol,UInt32}[:x, :y, :x],\n ordinal3 = [10.0, 20.0, 30.0],\n ordinal4 = [1.0, 0.0, -1.0],\n nominal = CategoricalValue{String,UInt32}[\"Your father\", \"he\", \"is\"],)\n\nSee also OneHotEncoder, ContinuousEncoder.","category":"section"},{"location":"models/MiniBatchKMeans_MLJScikitLearnInterface/#MiniBatchKMeans_MLJScikitLearnInterface","page":"MiniBatchKMeans","title":"MiniBatchKMeans","text":"MiniBatchKMeans\n\nA model type for constructing a Mini-Batch K-Means clustering., based on MLJScikitLearnInterface.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nMiniBatchKMeans = @load MiniBatchKMeans pkg=MLJScikitLearnInterface\n\nDo model = MiniBatchKMeans() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in MiniBatchKMeans(n_clusters=...).","category":"section"},{"location":"models/MiniBatchKMeans_MLJScikitLearnInterface/#Hyper-parameters","page":"MiniBatchKMeans","title":"Hyper-parameters","text":"n_clusters = 8\nmax_iter = 100\nbatch_size = 100\nverbose = 0\ncompute_labels = true\nrandom_state = nothing\ntol = 0.0\nmax_no_improvement = 10\ninit_size = nothing\nn_init = 3\ninit = k-means++\nreassignment_ratio = 0.01","category":"section"},{"location":"models/TomekUndersampler_Imbalance/#TomekUndersampler_Imbalance","page":"TomekUndersampler","title":"TomekUndersampler","text":"Initiate a tomek undersampling model with the given hyper-parameters.\n\nTomekUndersampler\n\nA model type for constructing a tomek undersampler, based on Imbalance.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nTomekUndersampler = @load TomekUndersampler pkg=Imbalance\n\nDo model = TomekUndersampler() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in TomekUndersampler(min_ratios=...).\n\nTomekUndersampler undersamples by removing any point that is part of a tomek link in the data. As defined in,  Ivan Tomek. Two modifications of cnn. IEEE Trans. Systems, Man and Cybernetics, 6:769–772, 1976.","category":"section"},{"location":"models/TomekUndersampler_Imbalance/#Training-data","page":"TomekUndersampler","title":"Training data","text":"In MLJ or MLJBase, wrap the model in a machine by     mach = machine(model)\n\nThere is no need to provide any data here because the model is a static transformer.\n\nLikewise, there is no need to fit!(mach). \n\nFor default values of the hyper-parameters, model can be constructed by     model = TomekUndersampler()","category":"section"},{"location":"models/TomekUndersampler_Imbalance/#Hyperparameters","page":"TomekUndersampler","title":"Hyperparameters","text":"min_ratios=1.0: A parameter that controls the maximum amount of undersampling to be done for each class. If this algorithm   cleans the data to an extent that this is violated, some of the cleaned points will be revived randomly so that it is satisfied.\nCan be a float and in this case each class will be at most undersampled to the size of the minority class times the float. By default, all classes are undersampled to the size of the minority class\nCan be a dictionary mapping each class label to the float minimum ratio for that class\nforce_min_ratios=false: If true, and this algorithm cleans the data such that the ratios for each class   exceed those specified in min_ratios then further undersampling will be perform so that the final ratios   are equal to min_ratios.\nrng::Union{AbstractRNG, Integer}=default_rng(): Either an AbstractRNG object or an Integer    seed to be used with Xoshiro if the Julia VERSION supports it. Otherwise, uses MersenneTwister`.\ntry_preserve_type::Bool=true: When true, the function will try to not change the type of the input    table (e.g., DataFrame). However, for some tables, this may not succeed, and in this case, the table returned will   be a column table (named-tuple of vectors). This parameter is ignored if the input is a matrix.","category":"section"},{"location":"models/TomekUndersampler_Imbalance/#Transform-Inputs","page":"TomekUndersampler","title":"Transform Inputs","text":"X: A matrix or table of floats where each row is an observation from the dataset\ny: An abstract vector of labels (e.g., strings) that correspond to the observations in X","category":"section"},{"location":"models/TomekUndersampler_Imbalance/#Transform-Outputs","page":"TomekUndersampler","title":"Transform Outputs","text":"X_under: A matrix or table that includes the data after undersampling    depending on whether the input X is a matrix or table respectively\ny_under: An abstract vector of labels corresponding to X_under","category":"section"},{"location":"models/TomekUndersampler_Imbalance/#Operations","page":"TomekUndersampler","title":"Operations","text":"transform(mach, X, y): resample the data X and y using TomekUndersampler, returning both the new and original observations","category":"section"},{"location":"models/TomekUndersampler_Imbalance/#Example","page":"TomekUndersampler","title":"Example","text":"using MLJ\nimport Imbalance\n\n## set probability of each class\nclass_probs = [0.5, 0.2, 0.3]                         \nnum_rows, num_continuous_feats = 100, 5\n## generate a table and categorical vector accordingly\nX, y = Imbalance.generate_imbalanced_data(num_rows, num_continuous_feats; \n                                min_sep=0.01, stds=[3.0 3.0 3.0], class_probs, rng=42)   \n\njulia> Imbalance.checkbalance(y; ref=\"minority\")\n1: ▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 19 (100.0%) \n2: ▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 33 (173.7%) \n0: ▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 48 (252.6%) \n\n## load TomekUndersampler model type:\nTomekUndersampler = @load TomekUndersampler pkg=Imbalance\n\n## Underample the majority classes to  sizes relative to the minority class:\ntomek_undersampler = TomekUndersampler(min_ratios=1.0, rng=42)\nmach = machine(tomek_undersampler)\nX_under, y_under = transform(mach, X, y)\n\njulia> Imbalance.checkbalance(y_under; ref=\"minority\")\n1: ▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 19 (100.0%) \n2: ▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 22 (115.8%) \n0: ▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 36 (189.5%)","category":"section"},{"location":"models/InteractionTransformer_MLJTransforms/#InteractionTransformer_MLJTransforms","page":"InteractionTransformer","title":"InteractionTransformer","text":"InteractionTransformer\n\nA model type for constructing a interaction transformer, based on MLJTransforms.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nInteractionTransformer = @load InteractionTransformer pkg=MLJTransforms\n\nDo model = InteractionTransformer() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in InteractionTransformer(order=...).\n\nGenerates all polynomial interaction terms up to the given order for the subset of chosen columns.  Any column that contains elements with scitype <:Infinite is a valid basis to generate interactions.  If features is not specified, all such columns with scitype <:Infinite in the table are used as a basis.\n\nIn MLJ or MLJBase, you can transform features X with the single call\n\ntransform(machine(model), X)\n\nSee also the example below.","category":"section"},{"location":"models/InteractionTransformer_MLJTransforms/#Hyper-parameters","page":"InteractionTransformer","title":"Hyper-parameters","text":"order: Maximum order of interactions to be generated.\nfeatures: Restricts interations generation to those columns","category":"section"},{"location":"models/InteractionTransformer_MLJTransforms/#Operations","page":"InteractionTransformer","title":"Operations","text":"transform(machine(model), X): Generates polynomial interaction terms out of table X using the hyper-parameters specified in model.","category":"section"},{"location":"models/InteractionTransformer_MLJTransforms/#Example","page":"InteractionTransformer","title":"Example","text":"using MLJ\n\nX = (\n    A = [1, 2, 3],\n    B = [4, 5, 6],\n    C = [7, 8, 9],\n    D = [\"x₁\", \"x₂\", \"x₃\"]\n)\nit = InteractionTransformer(order=3)\nmach = machine(it)\n\njulia> transform(mach, X)\n(A = [1, 2, 3],\n B = [4, 5, 6],\n C = [7, 8, 9],\n D = [\"x₁\", \"x₂\", \"x₃\"],\n A_B = [4, 10, 18],\n A_C = [7, 16, 27],\n B_C = [28, 40, 54],\n A_B_C = [28, 80, 162],)\n\nit = InteractionTransformer(order=2, features=[:A, :B])\nmach = machine(it)\n\njulia> transform(mach, X)\n(A = [1, 2, 3],\n B = [4, 5, 6],\n C = [7, 8, 9],\n D = [\"x₁\", \"x₂\", \"x₃\"],\n A_B = [4, 10, 18],)\n","category":"section"},{"location":"models/MultinomialNBClassifier_NaiveBayes/#MultinomialNBClassifier_NaiveBayes","page":"MultinomialNBClassifier","title":"MultinomialNBClassifier","text":"MultinomialNBClassifier\n\nA model type for constructing a multinomial naive Bayes classifier, based on NaiveBayes.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nMultinomialNBClassifier = @load MultinomialNBClassifier pkg=NaiveBayes\n\nDo model = MultinomialNBClassifier() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in MultinomialNBClassifier(alpha=...).\n\nThe multinomial naive Bayes classifier is often applied when input features consist of a counts (scitype Count) and when observations for a fixed target class are generated from a multinomial distribution with fixed probability vector, but whose sample length varies from observation to observation. For example, features might represent word counts in text documents being classified by sentiment.","category":"section"},{"location":"models/MultinomialNBClassifier_NaiveBayes/#Training-data","page":"MultinomialNBClassifier","title":"Training data","text":"In MLJ or MLJBase, bind an instance model to data with\n\nmach = machine(model, X, y)\n\nHere:\n\nX is any table of input features (eg, a DataFrame) whose columns are of scitype Count; check the column scitypes with schema(X).\ny is the target, which can be any AbstractVector whose element scitype is Finite; check the scitype with schema(y).\n\nTrain the machine using fit!(mach, rows=...).","category":"section"},{"location":"models/MultinomialNBClassifier_NaiveBayes/#Hyper-parameters","page":"MultinomialNBClassifier","title":"Hyper-parameters","text":"alpha=1: Lindstone smoothing in estimation of multinomial probability vectors from training histograms (default corresponds to Laplacian smoothing).","category":"section"},{"location":"models/MultinomialNBClassifier_NaiveBayes/#Operations","page":"MultinomialNBClassifier","title":"Operations","text":"predict(mach, Xnew): return predictions of the target given new features Xnew, which should have the same scitype as X above.\npredict_mode(mach, Xnew): Return the mode of above predictions.","category":"section"},{"location":"models/MultinomialNBClassifier_NaiveBayes/#Fitted-parameters","page":"MultinomialNBClassifier","title":"Fitted parameters","text":"The fields of fitted_params(mach) are:\n\nc_counts: A dictionary containing the observed count of each input class.\nx_counts: A dictionary containing the categorical counts of each input class.\nx_totals: The sum of each count (input feature), ungrouped.\nn_obs: The total number of observations in the training data.","category":"section"},{"location":"models/MultinomialNBClassifier_NaiveBayes/#Examples","page":"MultinomialNBClassifier","title":"Examples","text":"using MLJ\nimport TextAnalysis\n\nCountTransformer = @load CountTransformer pkg=MLJText\nMultinomialNBClassifier = @load MultinomialNBClassifier pkg=NaiveBayes\n\ntokenized_docs = TextAnalysis.tokenize.([\n    \"I am very mad. You never listen.\",\n    \"You seem to be having trouble? Can I help you?\",\n    \"Our boss is mad at me. I hope he dies.\",\n    \"His boss wants to help me. She is nice.\",\n    \"Thank you for your help. It is nice working with you.\",\n    \"Never do that again! I am so mad. \",\n])\n\nsentiment = [\n    \"negative\",\n    \"positive\",\n    \"negative\",\n    \"positive\",\n    \"positive\",\n    \"negative\",\n]\n\nmach1 = machine(CountTransformer(), tokenized_docs) |> fit!\n\n## matrix of counts:\nX = transform(mach1, tokenized_docs)\n\n## to ensure scitype(y) <: AbstractVector{<:OrderedFactor}:\ny = coerce(sentiment, OrderedFactor)\n\nclassifier = MultinomialNBClassifier()\nmach2 = machine(classifier, X, y)\nfit!(mach2, rows=1:4)\n\n## probabilistic predictions:\ny_prob = predict(mach2, rows=5:6) ## distributions\npdf.(y_prob, \"positive\") ## probabilities for \"positive\"\nlog_loss(y_prob, y[5:6])\n\n## point predictions:\nyhat = mode.(y_prob) ## or `predict_mode(mach2, rows=5:6)`\n\nSee also GaussianNBClassifier","category":"section"},{"location":"models/OneRuleClassifier_OneRule/#OneRuleClassifier_OneRule","page":"OneRuleClassifier","title":"OneRuleClassifier","text":"OneRuleClassifier\n\nA model type for constructing a one rule classifier, based on OneRule.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nOneRuleClassifier = @load OneRuleClassifier pkg=OneRule\n\nDo model = OneRuleClassifier() to construct an instance with default hyper-parameters. \n\nOneRuleClassifier implements the OneRule method for classification by Robert Holte      (\"Very simple classification rules perform well on most commonly used datasets\"      in: Machine Learning 11.1 (1993), pp. 63-90). \n\nFor more information see:\n\n- Witten, Ian H., Eibe Frank, and Mark A. Hall. \n  Data Mining Practical Machine Learning Tools and Techniques Third Edition. \n  Morgan Kaufmann, 2017, pp. 93-96.\n- [Machine Learning - (One|Simple) Rule](https://datacadamia.com/data_mining/one_rule)\n- [OneRClassifier - One Rule for Classification](http://rasbt.github.io/mlxtend/user_guide/classifier/OneRClassifier/)","category":"section"},{"location":"models/OneRuleClassifier_OneRule/#Training-data","page":"OneRuleClassifier","title":"Training data","text":"In MLJ or MLJBase, bind an instance model to data with     mach = machine(model, X, y) where\n\nX: any table of input features (eg, a DataFrame) whose columns each have one of the following element scitypes: Multiclass, OrderedFactor, or <:Finite; check column scitypes with schema(X)\ny: is the target, which can be any AbstractVector whose element scitype is OrderedFactor or Multiclass; check the scitype with scitype(y)\n\nTrain the machine using fit!(mach, rows=...).","category":"section"},{"location":"models/OneRuleClassifier_OneRule/#Hyper-parameters","page":"OneRuleClassifier","title":"Hyper-parameters","text":"This classifier has no hyper-parameters.","category":"section"},{"location":"models/OneRuleClassifier_OneRule/#Operations","page":"OneRuleClassifier","title":"Operations","text":"predict(mach, Xnew): return (deterministic) predictions of the target given features Xnew having the same scitype as X above.","category":"section"},{"location":"models/OneRuleClassifier_OneRule/#Fitted-parameters","page":"OneRuleClassifier","title":"Fitted parameters","text":"The fields of fitted_params(mach) are:\n\ntree: the tree (a OneTree) returned by the core OneTree.jl algorithm\nall_classes: all classes (i.e. levels) of the target (used also internally to transfer levels-information to predict)","category":"section"},{"location":"models/OneRuleClassifier_OneRule/#Report","page":"OneRuleClassifier","title":"Report","text":"The fields of report(mach) are:\n\ntree: The OneTree created based on the training data\nnrules: The number of rules tree contains\nerror_rate: fraction of wrongly classified instances\nerror_count: number of wrongly classified instances\nclasses_seen: list of target classes actually observed in training\nfeatures: the names of the features encountered in training","category":"section"},{"location":"models/OneRuleClassifier_OneRule/#Examples","page":"OneRuleClassifier","title":"Examples","text":"using MLJ\n\nORClassifier = @load OneRuleClassifier pkg=OneRule\n\norc = ORClassifier()\n\noutlook = [\"sunny\", \"sunny\", \"overcast\", \"rainy\", \"rainy\", \"rainy\", \"overcast\", \"sunny\", \"sunny\", \"rainy\",  \"sunny\", \"overcast\", \"overcast\", \"rainy\"]\ntemperature = [\"hot\", \"hot\", \"hot\", \"mild\", \"cool\", \"cool\", \"cool\", \"mild\", \"cool\", \"mild\", \"mild\", \"mild\", \"hot\", \"mild\"]\nhumidity = [\"high\", \"high\", \"high\", \"high\", \"normal\", \"normal\", \"normal\", \"high\", \"normal\", \"normal\", \"normal\", \"high\", \"normal\", \"high\"]\nwindy = [\"false\", \"true\", \"false\", \"false\", \"false\", \"true\", \"true\", \"false\", \"false\", \"false\", \"true\", \"true\", \"false\", \"true\"]\n\nweather_data = (outlook = outlook, temperature = temperature, humidity = humidity, windy = windy)\nplay_data = [\"no\", \"no\", \"yes\", \"yes\", \"yes\", \"no\", \"yes\", \"no\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"no\"]\n\nweather = coerce(weather_data, Textual => Multiclass)\nplay = coerce(play, Multiclass)\n\nmach = machine(orc, weather, play)\nfit!(mach)\n\nyhat = MLJ.predict(mach, weather)       ## in a real context 'new' `weather` data would be used\none_tree = fitted_params(mach).tree\nreport(mach).error_rate\n\nSee also OneRule.jl.","category":"section"},{"location":"models/ProbabilisticNuSVC_LIBSVM/#ProbabilisticNuSVC_LIBSVM","page":"ProbabilisticNuSVC","title":"ProbabilisticNuSVC","text":"ProbabilisticNuSVC\n\nA model type for constructing a probabilistic ν-support vector classifier, based on LIBSVM.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nProbabilisticNuSVC = @load ProbabilisticNuSVC pkg=LIBSVM\n\nDo model = ProbabilisticNuSVC() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in ProbabilisticNuSVC(kernel=...).\n\nThis model is identical to NuSVC with the exception that it predicts probabilities, instead of actual class labels. Probabilities are computed using Platt scaling, which will add to total computation time.\n\nReference for algorithm and core C-library: C.-C. Chang and C.-J. Lin (2011): \"LIBSVM: a library for support vector machines.\" ACM Transactions on Intelligent Systems and Technology, 2(3):27:1–27:27. Updated at https://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf. \n\nPlatt, John (1999): \"Probabilistic Outputs for Support Vector Machines and Comparisons to Regularized Likelihood Methods.\"","category":"section"},{"location":"models/ProbabilisticNuSVC_LIBSVM/#Training-data","page":"ProbabilisticNuSVC","title":"Training data","text":"In MLJ or MLJBase, bind an instance model to data with:\n\nmach = machine(model, X, y)\n\nwhere\n\nX: any table of input features (eg, a DataFrame) whose columns each have Continuous element scitype; check column scitypes with schema(X)\ny: is the target, which can be any AbstractVector whose element scitype is <:OrderedFactor or <:Multiclass; check the scitype with scitype(y)\n\nTrain the machine using fit!(mach, rows=...).","category":"section"},{"location":"models/ProbabilisticNuSVC_LIBSVM/#Hyper-parameters","page":"ProbabilisticNuSVC","title":"Hyper-parameters","text":"kernel=LIBSVM.Kernel.RadialBasis: either an object that can be called, as in kernel(x1, x2), or one of the built-in kernels from the LIBSVM.jl package listed below.  Here x1 and x2 are vectors whose lengths match the number of columns of the training data X (see \"Examples\" below).\nLIBSVM.Kernel.Linear: (x1, x2) -> x1'*x2\nLIBSVM.Kernel.Polynomial: (x1, x2) -> gamma*x1'*x2 + coef0)^degree\nLIBSVM.Kernel.RadialBasis: (x1, x2) -> (exp(-gamma*norm(x1 - x2)^2))\nLIBSVM.Kernel.Sigmoid: (x1, x2) - > tanh(gamma*x1'*x2 + coef0)\nHere gamma, coef0, degree are other hyper-parameters. Serialization of models with user-defined kernels comes with some restrictions. See LIVSVM.jl issue91\ngamma = 0.0: kernel parameter (see above); if gamma==-1.0 then gamma = 1/nfeatures is used in training, where nfeatures is the number of features (columns of X).  If gamma==0.0 then gamma = 1/(var(Tables.matrix(X))*nfeatures) is used. Actual value used appears in the report (see below).\ncoef0 = 0.0: kernel parameter (see above)\ndegree::Int32 = Int32(3): degree in polynomial kernel (see above)\nnu=0.5 (range (0, 1]): An upper bound on the fraction of margin errors and a lower bound of the fraction of support vectors. Denoted ν in the cited paper. Changing nu changes the thickness of the margin (a neighborhood of the decision surface) and a margin error is said to have occurred if a training observation lies on the wrong side of the surface or within the margin.\ncachesize=200.0 cache memory size in MB\ntolerance=0.001: tolerance for the stopping criterion\nshrinking=true: whether to use shrinking heuristics","category":"section"},{"location":"models/ProbabilisticNuSVC_LIBSVM/#Operations","page":"ProbabilisticNuSVC","title":"Operations","text":"predict(mach, Xnew): return predictions of the target given features Xnew having the same scitype as X above.","category":"section"},{"location":"models/ProbabilisticNuSVC_LIBSVM/#Fitted-parameters","page":"ProbabilisticNuSVC","title":"Fitted parameters","text":"The fields of fitted_params(mach) are:\n\nlibsvm_model: the trained model object created by the LIBSVM.jl package\nencoding: class encoding used internally by libsvm_model - a dictionary of class labels keyed on the internal integer representation","category":"section"},{"location":"models/ProbabilisticNuSVC_LIBSVM/#Report","page":"ProbabilisticNuSVC","title":"Report","text":"The fields of report(mach) are:\n\ngamma: actual value of the kernel parameter gamma used in training","category":"section"},{"location":"models/ProbabilisticNuSVC_LIBSVM/#Examples","page":"ProbabilisticNuSVC","title":"Examples","text":"","category":"section"},{"location":"models/ProbabilisticNuSVC_LIBSVM/#Using-a-built-in-kernel","page":"ProbabilisticNuSVC","title":"Using a built-in kernel","text":"using MLJ\nimport LIBSVM\n\nProbabilisticNuSVC = @load ProbabilisticNuSVC pkg=LIBSVM    ## model type\nmodel = ProbabilisticNuSVC(kernel=LIBSVM.Kernel.Polynomial) ## instance\n\nX, y = @load_iris ## table, vector\nmach = machine(model, X, y) |> fit!\n\nXnew = (sepal_length = [6.4, 7.2, 7.4],\n        sepal_width = [2.8, 3.0, 2.8],\n        petal_length = [5.6, 5.8, 6.1],\n        petal_width = [2.1, 1.6, 1.9],)\n\njulia> probs = predict(mach, Xnew)\n3-element UnivariateFiniteVector{Multiclass{3}, String, UInt32, Float64}:\n UnivariateFinite{Multiclass{3}}(setosa=>0.00313, versicolor=>0.0247, virginica=>0.972)\n UnivariateFinite{Multiclass{3}}(setosa=>0.000598, versicolor=>0.0155, virginica=>0.984)\n UnivariateFinite{Multiclass{3}}(setosa=>2.27e-6, versicolor=>2.73e-6, virginica=>1.0)\n\njulia> yhat = mode.(probs)\n3-element CategoricalArrays.CategoricalArray{String,1,UInt32}:\n \"virginica\"\n \"virginica\"\n \"virginica\"","category":"section"},{"location":"models/ProbabilisticNuSVC_LIBSVM/#User-defined-kernels","page":"ProbabilisticNuSVC","title":"User-defined kernels","text":"k(x1, x2) = x1'*x2 ## equivalent to `LIBSVM.Kernel.Linear`\nmodel = ProbabilisticNuSVC(kernel=k)\nmach = machine(model, X, y) |> fit!\n\nprobs = predict(mach, Xnew)\n\nSee also the classifiers NuSVC, SVC, ProbabilisticSVC and LinearSVC. And see LIVSVM.jl and the original C implementation. documentation.","category":"section"},{"location":"models/RandomForestImputer_BetaML/#RandomForestImputer_BetaML","page":"RandomForestImputer","title":"RandomForestImputer","text":"mutable struct RandomForestImputer <: MLJModelInterface.Unsupervised\n\nImpute missing values using Random Forests, from the Beta Machine Learning Toolkit (BetaML).","category":"section"},{"location":"models/RandomForestImputer_BetaML/#Hyperparameters:","page":"RandomForestImputer","title":"Hyperparameters:","text":"n_trees::Int64: Number of (decision) trees in the forest [def: 30]\nmax_depth::Union{Nothing, Int64}: The maximum depth the tree is allowed to reach. When this is reached the node is forced to become a leaf [def: nothing, i.e. no limits]\nmin_gain::Float64: The minimum information gain to allow for a node's partition [def: 0]\nmin_records::Int64: The minimum number of records a node must holds to consider for a partition of it [def: 2]\nmax_features::Union{Nothing, Int64}: The maximum number of (random) features to consider at each partitioning [def: nothing, i.e. square root of the data dimension]\nforced_categorical_cols::Vector{Int64}: Specify the positions of the integer columns to treat as categorical instead of cardinal. [Default: empty vector (all numerical cols are treated as cardinal by default and the others as categorical)]\nsplitting_criterion::Union{Nothing, Function}: Either gini, entropy or variance. This is the name of the function to be used to compute the information gain of a specific partition. This is done by measuring the difference betwwen the \"impurity\" of the labels of the parent node with those of the two child nodes, weighted by the respective number of items. [def: nothing, i.e. gini for categorical labels (classification task) and variance for numerical labels(regression task)]. It can be an anonymous function.\nrecursive_passages::Int64: Define the times to go trough the various columns to impute their data. Useful when there are data to impute on multiple columns. The order of the first passage is given by the decreasing number of missing values per column, the other passages are random [default: 1].\nrng::Random.AbstractRNG: A Random Number Generator to be used in stochastic parts of the code [deafult: Random.GLOBAL_RNG]","category":"section"},{"location":"models/RandomForestImputer_BetaML/#Example:","page":"RandomForestImputer","title":"Example:","text":"julia> using MLJ\n\njulia> X = [1 10.5;1.5 missing; 1.8 8; 1.7 15; 3.2 40; missing missing; 3.3 38; missing -2.3; 5.2 -2.4] |> table ;\n\njulia> modelType   = @load RandomForestImputer  pkg = \"BetaML\" verbosity=0\nBetaML.Imputation.RandomForestImputer\n\njulia> model     = modelType(n_trees=40)\nRandomForestImputer(\n  n_trees = 40, \n  max_depth = nothing, \n  min_gain = 0.0, \n  min_records = 2, \n  max_features = nothing, \n  forced_categorical_cols = Int64[], \n  splitting_criterion = nothing, \n  recursive_passages = 1, \n  rng = Random._GLOBAL_RNG())\n\njulia> mach      = machine(model, X);\n\njulia> fit!(mach);\n[ Info: Training machine(RandomForestImputer(n_trees = 40, …), …).\n\njulia> X_full       = transform(mach) |> MLJ.matrix\n9×2 Matrix{Float64}:\n 1.0      10.5\n 1.5      10.3909\n 1.8       8.0\n 1.7      15.0\n 3.2      40.0\n 2.88375   8.66125\n 3.3      38.0\n 3.98125  -2.3\n 5.2      -2.4","category":"section"},{"location":"models/BayesianSubspaceLDA_MultivariateStats/#BayesianSubspaceLDA_MultivariateStats","page":"BayesianSubspaceLDA","title":"BayesianSubspaceLDA","text":"BayesianSubspaceLDA\n\nA model type for constructing a Bayesian subspace LDA model, based on MultivariateStats.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nBayesianSubspaceLDA = @load BayesianSubspaceLDA pkg=MultivariateStats\n\nDo model = BayesianSubspaceLDA() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in BayesianSubspaceLDA(normalize=...).\n\nThe Bayesian multiclass subspace linear discriminant analysis algorithm learns a projection matrix as described in SubspaceLDA. The posterior class probability distribution is derived as in BayesianLDA.","category":"section"},{"location":"models/BayesianSubspaceLDA_MultivariateStats/#Training-data","page":"BayesianSubspaceLDA","title":"Training data","text":"In MLJ or MLJBase, bind an instance model to data with\n\nmach = machine(model, X, y)\n\nHere:\n\nX is any table of input features (eg, a DataFrame) whose columns are of scitype Continuous; check column scitypes with schema(X).\ny is the target, which can be any AbstractVector whose element scitype is OrderedFactor or Multiclass; check the scitype with scitype(y).\n\nTrain the machine using fit!(mach, rows=...).","category":"section"},{"location":"models/BayesianSubspaceLDA_MultivariateStats/#Hyper-parameters","page":"BayesianSubspaceLDA","title":"Hyper-parameters","text":"normalize=true: Option to normalize the between class variance for the number of observations in each class, one of true or false.\n\noutdim: the ouput dimension, automatically set to min(indim, nclasses-1) if equal   to 0. If a non-zero outdim is passed, then the actual output dimension used is   min(rank, outdim) where rank is the rank of the within-class covariance matrix.\n\npriors::Union{Nothing, UnivariateFinite{<:Any, <:Any, <:Any, <:Real}, Dict{<:Any, <:Real}} = nothing: For use in prediction with Bayes rule. If priors = nothing then priors are estimated from the class proportions in the training data. Otherwise it requires a Dict or UnivariateFinite object specifying the classes with non-zero probabilities in the training target.","category":"section"},{"location":"models/BayesianSubspaceLDA_MultivariateStats/#Operations","page":"BayesianSubspaceLDA","title":"Operations","text":"transform(mach, Xnew): Return a lower dimensional projection of the input Xnew, which should have the same scitype as X above.\npredict(mach, Xnew): Return predictions of the target given features Xnew, which should have same scitype as X above. Predictions are probabilistic but uncalibrated.\npredict_mode(mach, Xnew): Return the modes of the probabilistic predictions returned above.","category":"section"},{"location":"models/BayesianSubspaceLDA_MultivariateStats/#Fitted-parameters","page":"BayesianSubspaceLDA","title":"Fitted parameters","text":"The fields of fitted_params(mach) are:\n\nclasses: The classes seen during model fitting.\nprojection_matrix: The learned projection matrix, of size (indim, outdim), where indim and outdim are the input and output dimensions respectively (See Report section below).\npriors: The class priors for classification. As inferred from training target y, if not user-specified. A UnivariateFinite object with levels consistent with levels(y).","category":"section"},{"location":"models/BayesianSubspaceLDA_MultivariateStats/#Report","page":"BayesianSubspaceLDA","title":"Report","text":"The fields of report(mach) are:\n\nindim: The dimension of the input space i.e the number of training features.\noutdim: The dimension of the transformed space the model is projected to.\nmean: The overall mean of the training data.\nnclasses: The number of classes directly observed in the training data (which can be less than the total number of classes in the class pool).\n\nclass_means: The class-specific means of the training data. A matrix of size   (indim, nclasses) with the ith column being the class-mean of the ith class in   classes (See fitted params section above).\n\nclass_weights: The weights (class counts) of each class. A vector of length nclasses with the ith element being the class weight of the ith class in classes. (See fitted params section above.)\nexplained_variance_ratio: The ratio of explained variance to total variance. Each dimension corresponds to an eigenvalue.","category":"section"},{"location":"models/BayesianSubspaceLDA_MultivariateStats/#Examples","page":"BayesianSubspaceLDA","title":"Examples","text":"using MLJ\n\nBayesianSubspaceLDA = @load BayesianSubspaceLDA pkg=MultivariateStats\n\nX, y = @load_iris ## a table and a vector\n\nmodel = BayesianSubspaceLDA()\nmach = machine(model, X, y) |> fit!\n\nXproj = transform(mach, X)\ny_hat = predict(mach, X)\nlabels = predict_mode(mach, X)\n\nSee also LDA, BayesianLDA, SubspaceLDA","category":"section"},{"location":"models/DecisionTreeRegressor_DecisionTree/#DecisionTreeRegressor_DecisionTree","page":"DecisionTreeRegressor","title":"DecisionTreeRegressor","text":"DecisionTreeRegressor\n\nA model type for constructing a CART decision tree regressor, based on DecisionTree.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nDecisionTreeRegressor = @load DecisionTreeRegressor pkg=DecisionTree\n\nDo model = DecisionTreeRegressor() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in DecisionTreeRegressor(max_depth=...).\n\nDecisionTreeRegressor implements the CART algorithm, originally published in Breiman, Leo; Friedman, J. H.; Olshen, R. A.; Stone, C. J. (1984): \"Classification and regression trees\". Monterey, CA: Wadsworth & Brooks/Cole Advanced Books & Software..","category":"section"},{"location":"models/DecisionTreeRegressor_DecisionTree/#Training-data","page":"DecisionTreeRegressor","title":"Training data","text":"In MLJ or MLJBase, bind an instance model to data with\n\nmach = machine(model, X, y)\n\nwhere\n\nX: any table of input features (eg, a DataFrame) whose columns each have one of the following element scitypes: Continuous, Count, or <:OrderedFactor; check column scitypes with schema(X)\ny: the target, which can be any AbstractVector whose element scitype is Continuous; check the scitype with scitype(y)\n\nTrain the machine with fit!(mach, rows=...).","category":"section"},{"location":"models/DecisionTreeRegressor_DecisionTree/#Hyperparameters","page":"DecisionTreeRegressor","title":"Hyperparameters","text":"max_depth=-1:          max depth of the decision tree (-1=any)\nmin_samples_leaf=1:    max number of samples each leaf needs to have\nmin_samples_split=2:   min number of samples needed for a split\nmin_purity_increase=0: min purity needed for a split\nn_subfeatures=0: number of features to select at random (0 for all)\npost_prune=false:      set to true for post-fit pruning\nmerge_purity_threshold=1.0: (post-pruning) merge leaves having                          combined purity >= merge_purity_threshold\nfeature_importance: method to use for computing feature importances. One of (:impurity, :split)\nrng=Random.GLOBAL_RNG: random number generator or seed","category":"section"},{"location":"models/DecisionTreeRegressor_DecisionTree/#Operations","page":"DecisionTreeRegressor","title":"Operations","text":"predict(mach, Xnew): return predictions of the target given new features Xnew having the same scitype as X above.","category":"section"},{"location":"models/DecisionTreeRegressor_DecisionTree/#Fitted-parameters","page":"DecisionTreeRegressor","title":"Fitted parameters","text":"The fields of fitted_params(mach) are:\n\ntree: the tree or stump object returned by the core DecisionTree.jl algorithm\nfeatures: the names of the features encountered in training","category":"section"},{"location":"models/DecisionTreeRegressor_DecisionTree/#Report","page":"DecisionTreeRegressor","title":"Report","text":"The fields of report(mach) are:\n\nfeatures: the names of the features encountered in training","category":"section"},{"location":"models/DecisionTreeRegressor_DecisionTree/#Accessor-functions","page":"DecisionTreeRegressor","title":"Accessor functions","text":"feature_importances(mach) returns a vector of (feature::Symbol => importance) pairs; the type of importance is determined by the hyperparameter feature_importance (see above)","category":"section"},{"location":"models/DecisionTreeRegressor_DecisionTree/#Examples","page":"DecisionTreeRegressor","title":"Examples","text":"using MLJ\nDecisionTreeRegressor = @load DecisionTreeRegressor pkg=DecisionTree\nmodel = DecisionTreeRegressor(max_depth=3, min_samples_split=3)\n\nX, y = make_regression(100, 4; rng=123) ## synthetic data\nmach = machine(model, X, y) |> fit!\n\nXnew, _ = make_regression(3, 2; rng=123)\nyhat = predict(mach, Xnew) ## new predictions\n\njulia> fitted_params(mach).tree\nx1 < 0.2758\n├─ x2 < 0.9137\n│  ├─ x1 < -0.9582\n│  │  ├─ 0.9189256882087312 (0/12)\n│  │  └─ -0.23180616021065256 (0/38)\n│  └─ -1.6461153800037722 (0/9)\n└─ x1 < 1.062\n   ├─ x2 < -0.4969\n   │  ├─ -0.9330755147107384 (0/5)\n   │  └─ -2.3287967825015548 (0/17)\n   └─ x2 < 0.4598\n      ├─ -2.931299926506291 (0/11)\n      └─ -4.726518740473489 (0/8)\n\nfeature_importances(mach) ## get feature importances\n\nSee also DecisionTree.jl and the unwrapped model type MLJDecisionTreeInterface.DecisionTree.DecisionTreeRegressor.","category":"section"},{"location":"models/IForestDetector_OutlierDetectionPython/#IForestDetector_OutlierDetectionPython","page":"IForestDetector","title":"IForestDetector","text":"IForestDetector(n_estimators = 100,\n                   max_samples = \"auto\",\n                   max_features = 1.0\n                   bootstrap = false,\n                   random_state = nothing,\n                   verbose = 0,\n                   n_jobs = 1)\n\nhttps://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.iforest","category":"section"},{"location":"models/RODDetector_OutlierDetectionPython/#RODDetector_OutlierDetectionPython","page":"RODDetector","title":"RODDetector","text":"RODDetector(parallel_execution = false)\n\nhttps://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.rod","category":"section"},{"location":"models/FillImputer_MLJTransforms/#FillImputer_MLJTransforms","page":"FillImputer","title":"FillImputer","text":"FillImputer\n\nA model type for constructing a fill imputer, based on MLJTransforms.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nFillImputer = @load FillImputer pkg=MLJTransforms\n\nDo model = FillImputer() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in FillImputer(features=...).\n\nUse this model to impute missing values in tabular data. A fixed \"filler\" value is learned from the training data, one for each column of the table.\n\nFor imputing missing values in a vector, use UnivariateFillImputer instead.","category":"section"},{"location":"models/FillImputer_MLJTransforms/#Training-data","page":"FillImputer","title":"Training data","text":"In MLJ or MLJBase, bind an instance model to data with\n\nmach = machine(model, X)\n\nwhere\n\nX: any table of input features (eg, a DataFrame) whose features each have element scitypes Union{Missing, T}, where T is a subtype of Continuous, Multiclass, OrderedFactor or Count. Check scitypes with schema(X).\n\nTrain the machine using fit!(mach, rows=...).","category":"section"},{"location":"models/FillImputer_MLJTransforms/#Hyper-parameters","page":"FillImputer","title":"Hyper-parameters","text":"features: a vector of names of features (symbols) for which imputation is to be attempted; default is empty, which is interpreted as \"impute all\".\ncontinuous_fill: function or other callable to determine value to be imputed in the case of Continuous (abstract float) data; default is to apply median after skipping missing values\ncount_fill: function or other callable to determine value to be imputed in the case of Count (integer) data; default is to apply rounded median after skipping missing values\nfinite_fill: function or other callable to determine value to be imputed in the case of Multiclass or OrderedFactor data (categorical vectors); default is to apply mode after skipping missing values","category":"section"},{"location":"models/FillImputer_MLJTransforms/#Operations","page":"FillImputer","title":"Operations","text":"transform(mach, Xnew): return Xnew with missing values imputed with the fill values learned when fitting mach","category":"section"},{"location":"models/FillImputer_MLJTransforms/#Fitted-parameters","page":"FillImputer","title":"Fitted parameters","text":"The fields of fitted_params(mach) are:\n\nfeatures_seen_in_fit: the names of features (features) encountered during training\nunivariate_transformer: the univariate model applied to determine   the fillers (it's fields contain the functions defining the filler computations)\nfiller_given_feature: dictionary of filler values, keyed on feature (column) names","category":"section"},{"location":"models/FillImputer_MLJTransforms/#Examples","page":"FillImputer","title":"Examples","text":"using MLJ\nimputer = FillImputer()\n\nX = (a = [1.0, 2.0, missing, 3.0, missing],\n     b = coerce([\"y\", \"n\", \"y\", missing, \"y\"], Multiclass),\n     c = [1, 1, 2, missing, 3])\n\nschema(X)\njulia> schema(X)\n┌───────┬───────────────────────────────┐\n│ names │ scitypes                      │\n├───────┼───────────────────────────────┤\n│ a     │ Union{Missing, Continuous}    │\n│ b     │ Union{Missing, Multiclass{2}} │\n│ c     │ Union{Missing, Count}         │\n└───────┴───────────────────────────────┘\n\nmach = machine(imputer, X)\nfit!(mach)\n\njulia> fitted_params(mach).filler_given_feature\n(filler = 2.0,)\n\njulia> fitted_params(mach).filler_given_feature\nDict{Symbol, Any} with 3 entries:\n  :a => 2.0\n  :b => \"y\"\n  :c => 2\n\njulia> transform(mach, X)\n(a = [1.0, 2.0, 2.0, 3.0, 2.0],\n b = CategoricalValue{String, UInt32}[\"y\", \"n\", \"y\", \"y\", \"y\"],\n c = [1, 1, 2, 2, 3],)\n\nSee also UnivariateFillImputer.","category":"section"},{"location":"models/RandomForestClassifier_MLJScikitLearnInterface/#RandomForestClassifier_MLJScikitLearnInterface","page":"RandomForestClassifier","title":"RandomForestClassifier","text":"RandomForestClassifier\n\nA model type for constructing a random forest classifier, based on MLJScikitLearnInterface.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nRandomForestClassifier = @load RandomForestClassifier pkg=MLJScikitLearnInterface\n\nDo model = RandomForestClassifier() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in RandomForestClassifier(n_estimators=...).\n\nA random forest is a meta estimator that fits a number of  classifying decision trees on various sub-samples of the  dataset and uses averaging to improve the predictive accuracy  and control over-fitting. The sub-sample size is controlled  with the max_samples parameter if bootstrap=True (default),  otherwise the whole dataset is used to build each tree.","category":"section"},{"location":"models/EnsembleModel_MLJEnsembles/#EnsembleModel_MLJEnsembles","page":"EnsembleModel","title":"EnsembleModel","text":"EnsembleModel(model,\n              atomic_weights=Float64[],\n              bagging_fraction=0.8,\n              n=100,\n              rng=GLOBAL_RNG,\n              acceleration=CPU1(),\n              out_of_bag_measure=[])\n\nCreate a model for training an ensemble of n clones of model, with optional bagging. Ensembling is useful if fit!(machine(atom, data...)) does not create identical models on repeated calls (ie, is a stochastic model, such as a decision tree with randomized node selection criteria), or if bagging_fraction is set to a value less than 1.0, or both.\n\nHere the atomic model must support targets with scitype AbstractVector{<:Finite} (single-target classifiers) or AbstractVector{<:Continuous} (single-target regressors).\n\nIf rng is an integer, then MersenneTwister(rng) is the random number generator used for bagging. Otherwise some AbstractRNG object is expected.\n\nThe atomic predictions are optionally weighted according to the vector atomic_weights (to allow for external optimization) except in the case that model is a Deterministic classifier, in which case atomic_weights are ignored.\n\nThe ensemble model is Deterministic or Probabilistic, according to the corresponding supertype of atom. In the case of deterministic classifiers (target_scitype(atom) <: Abstract{<:Finite}), the predictions are majority votes, and for regressors (target_scitype(atom)<: AbstractVector{<:Continuous}) they are ordinary averages.  Probabilistic predictions are obtained by averaging the atomic probability distribution/mass functions; in particular, for regressors, the ensemble prediction on each input pattern has the type MixtureModel{VF,VS,D} from the Distributions.jl package, where D is the type of predicted distribution for atom.\n\nSpecify acceleration=CPUProcesses() for distributed computing, or CPUThreads() for multithreading.\n\nIf a single measure or non-empty vector of measures is specified by out_of_bag_measure, then out-of-bag estimates of performance are written to the training report (call report on the trained machine wrapping the ensemble model).\n\nImportant: If per-observation or class weights w (not to be confused with atomic weights) are specified when constructing a machine for the ensemble model, as in mach = machine(ensemble_model, X, y, w), then w is used by any measures specified in out_of_bag_measure that support them.","category":"section"},{"location":"about_mlj/#About-MLJ","page":"About MLJ","title":"About MLJ","text":"MLJ (Machine Learning in Julia) is a toolbox written in Julia providing a common interface and meta-algorithms for selecting, tuning, evaluating, composing and comparing over 180 machine learning models written in Julia and other languages. In particular MLJ wraps a large number of scikit-learn models.\n\nMLJ is released under the MIT license.","category":"section"},{"location":"about_mlj/#Lightning-tour","page":"About MLJ","title":"Lightning tour","text":"For help learning to use MLJ, see Learning MLJ.\n\nA self-contained notebook and julia script of this demonstration is also available here.\n\nThe first code snippet below creates a new Julia environment MLJ_tour and installs just those packages needed for the tour. See Installation for more on creating a Julia environment for use with MLJ.\n\nJulia installation instructions are here.\n\nusing Pkg\nPkg.activate(\"MLJ_tour\", shared=true)\nPkg.add(\"MLJ\")\nPkg.add(\"MLJIteration\")\nPkg.add(\"EvoTrees\")\n\nIn MLJ a model is just a container for hyper-parameters, and that's all. Here we will apply several kinds of model composition before binding the resulting \"meta-model\" to data in a machine for evaluation using cross-validation.\n\nLoading and instantiating a gradient tree-boosting model:\n\nusing MLJ\nBooster = @load EvoTreeRegressor # loads code defining a model type\nbooster = Booster(max_depth=2)   # specify hyper-parameter at construction\nbooster.nrounds = 50             # or mutate afterwards\n\nThis model is an example of an iterative model. As it stands, the number of iterations nrounds is fixed.","category":"section"},{"location":"about_mlj/#Composition-1:-Wrapping-the-model-to-make-it-\"self-iterating\"","page":"About MLJ","title":"Composition 1: Wrapping the model to make it \"self-iterating\"","text":"Let's create a new model that automatically learns the number of iterations, using the NumberSinceBest(3) criterion, as applied to an out-of-sample l1 loss:\n\nusing MLJIteration\niterated_booster = IteratedModel(model=booster,\n                                 resampling=Holdout(fraction_train=0.8),\n                                 controls=[Step(2), NumberSinceBest(3), NumberLimit(300)],\n                                 measure=l1,\n                                 retrain=true)","category":"section"},{"location":"about_mlj/#Composition-2:-Preprocess-the-input-features","page":"About MLJ","title":"Composition 2: Preprocess the input features","text":"Combining the model with categorical feature encoding:\n\npipe = ContinuousEncoder() |> iterated_booster","category":"section"},{"location":"about_mlj/#Composition-3:-Wrapping-the-model-to-make-it-\"self-tuning\"","page":"About MLJ","title":"Composition 3: Wrapping the model to make it \"self-tuning\"","text":"First, we define a hyper-parameter range for optimization of a (nested) hyper-parameter:\n\nmax_depth_range = range(pipe,\n                        :(deterministic_iterated_model.model.max_depth),\n                        lower = 1,\n                        upper = 10)\n\nNow we can wrap the pipeline model in an optimization strategy to make it \"self-tuning\":\n\nself_tuning_pipe = TunedModel(model=pipe,\n                              tuning=RandomSearch(),\n                              ranges=max_depth_range,\n                              resampling=CV(nfolds=3, rng=456),\n                              measure=l1,\n                              acceleration=CPUThreads(),\n                              n=50)","category":"section"},{"location":"about_mlj/#Binding-to-data-and-evaluating-performance","page":"About MLJ","title":"Binding to data and evaluating performance","text":"Loading a selection of features and labels from the Ames House Price dataset:\n\nX, y = @load_reduced_ames\n\nEvaluating the \"self-tuning\" pipeline model's performance using 5-fold cross-validation (implies multiple layers of nested resampling):\n\njulia> evaluate(self_tuning_pipe, X, y,\n                measures=[l1, l2],\n                resampling=CV(nfolds=5, rng=123),\n                acceleration=CPUThreads(),\n                verbosity=2)\nPerformanceEvaluation object with these fields:\n  measure, measurement, operation, per_fold,\n  per_observation, fitted_params_per_fold,\n  report_per_fold, train_test_pairs\nExtract:\n┌───────────────┬─────────────┬───────────┬───────────────────────────────────────────────┐\n│ measure       │ measurement │ operation │ per_fold                                      │\n├───────────────┼─────────────┼───────────┼───────────────────────────────────────────────┤\n│ LPLoss(p = 1) │ 17200.0     │ predict   │ [16500.0, 17100.0, 16300.0, 17500.0, 18900.0] │\n│ LPLoss(p = 2) │ 6.83e8      │ predict   │ [6.14e8, 6.64e8, 5.98e8, 6.37e8, 9.03e8]      │\n└───────────────┴─────────────┴───────────┴───────────────────────────────────────────────┘","category":"section"},{"location":"about_mlj/#Key-goals","page":"About MLJ","title":"Key goals","text":"Offer a consistent way to use, compose and tune machine learning models in Julia,\nPromote the improvement of the Julia ML/Stats ecosystem by making it easier to use models from a wide range of packages,\nUnlock performance gains by exploiting Julia's support for parallelism, automatic differentiation, GPU, optimization etc.","category":"section"},{"location":"about_mlj/#Key-features","page":"About MLJ","title":"Key features","text":"Data agnostic, train most models on any data X supported by the Tables.jl interface (needs Tables.istable(X) == true).\nExtensive, state-of-the-art, support for model composition (pipelines, stacks and, more generally, learning networks). See more below.\nConvenient syntax to tune and evaluate (composite) models.\nConsistent interface to handle probabilistic predictions.\nExtensible tuning interface, to support a growing number of optimization strategies, and designed to play well with model composition.\nOptions to accelerate model evaluation and tuning with multithreading and/or distributed processing.","category":"section"},{"location":"about_mlj/#Model-composability","page":"About MLJ","title":"Model composability","text":"The generic model composition API's provided by other toolboxes we have surveyed share one or more of the following shortcomings, which do not exist in MLJ:\n\nComposite models do not inherit all the behavior of ordinary models.\nComposition is limited to linear (non-branching) pipelines.\nSupervised components in a linear pipeline can only occur at the end of the pipeline.\nOnly static (unlearned) target transformations/inverse transformations are supported.\nHyper-parameters in homogeneous model ensembles cannot be coupled.\nModel stacking, with out-of-sample predictions for base learners, cannot be implemented (using the generic API alone).\nHyper-parameters and/or learned parameters of component models are not easily inspected or manipulated (by tuning algorithms, for example)\nComposite models cannot implement multiple operations, for example, both a predict and transform method (as in clustering models) or both a transform and inverse_transform method.\n\nSome of these features are demonstrated in this notebook\n\nFor more information see the MLJ design paper or our detailed paper on the composition interface.","category":"section"},{"location":"about_mlj/#Getting-help-and-reporting-problems","page":"About MLJ","title":"Getting help and reporting problems","text":"Users are encouraged to provide feedback on their experience using MLJ and to report issues.\n\nFor a query to have maximum exposure to maintainers and users, start a discussion thread at Julia Discourse Machine Learning and tag your issue \"mlj\". Queries can also be posted as issues, or on the #mlj slack workspace in the Julia Slack channel.\n\nBugs, suggestions, and feature requests can be posted here.\n\nUsers are also welcome to join the #mlj Julia slack channel to ask questions and make suggestions.","category":"section"},{"location":"about_mlj/#Installation","page":"About MLJ","title":"Installation","text":"Initially, it is recommended that MLJ and associated packages be installed in a new environment to avoid package conflicts. You can do this with\n\njulia> using Pkg; Pkg.activate(\"my_MLJ_env\", shared=true)\n\nInstalling MLJ is also done with the package manager:\n\njulia> Pkg.add(\"MLJ\")\n\nOptional: To test your installation, run\n\njulia> Pkg.test(\"MLJ\")\n\nIt is important to note that MLJ is essentially a big wrapper providing unified access to model-providing packages. For this reason, one generally needs to add further packages to your environment to make model-specific code available. This happens automatically when you use MLJ's interactive load command @iload, as in\n\njulia> Tree = @iload DecisionTreeClassifier # load type\njulia> tree = Tree() # instance\n\nwhere you will also be asked to choose a providing package, for more than one provide a DecisionTreeClassifier model. For more on identifying the name of an applicable model, see Model Search. For non-interactive loading of code (e.g., from a module or function) see Loading Model Code.\n\nIt is recommended that you start with models from more mature packages such as DecisionTree.jl, ScikitLearn.jl or XGBoost.jl.\n\nMLJ is supported by several satellite packages (MLJTuning, MLJModelInterface, etc) which the general user is not required to install directly. Developers can learn more about these here.\n\nSee also the alternative installation instructions for Modifying Behavior.","category":"section"},{"location":"about_mlj/#Funding","page":"About MLJ","title":"Funding","text":"MLJ was initially created as a Tools, Practices and Systems project at the Alan Turing Institute in 2019. Current funding is provided by a New Zealand Strategic Science Investment Fund awarded to the University of Auckland.","category":"section"},{"location":"about_mlj/#Citing-MLJ","page":"About MLJ","title":"Citing MLJ","text":"An overview of MLJ design:\n\n(Image: DOI)\n\n@article{Blaom2020,\n  doi = {10.21105/joss.02704},\n  url = {https://doi.org/10.21105/joss.02704},\n  year = {2020},\n  publisher = {The Open Journal},\n  volume = {5},\n  number = {55},\n  pages = {2704},\n  author = {Anthony D. Blaom and Franz Kiraly and Thibaut Lienart and Yiannis Simillides and Diego Arenas and Sebastian J. Vollmer},\n  title = {{MLJ}: A Julia package for composable machine learning},\n  journal = {Journal of Open Source Software}\n}\n\nAn in-depth view of MLJ's model composition design:\n\n(Image: arXiv)\n\n@misc{blaom2020flexible,\n      title={Flexible model composition in machine learning and its implementation in {MLJ}},\n      author={Anthony D. Blaom and Sebastian J. Vollmer},\n      year={2020},\n      eprint={2012.15505},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG}\n}","category":"section"},{"location":"models/PPCA_MultivariateStats/#PPCA_MultivariateStats","page":"PPCA","title":"PPCA","text":"PPCA\n\nA model type for constructing a probabilistic PCA model, based on MultivariateStats.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nPPCA = @load PPCA pkg=MultivariateStats\n\nDo model = PPCA() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in PPCA(maxoutdim=...).\n\nProbabilistic principal component analysis is a dimension-reduction algorithm which represents a constrained form of the Gaussian distribution in which the number of free parameters can be restricted while still allowing the model to capture the dominant correlations in a data set. It is expressed as the maximum likelihood solution of a probabilistic latent variable model. For details, see Bishop (2006): C. M. Pattern Recognition and Machine Learning.","category":"section"},{"location":"models/PPCA_MultivariateStats/#Training-data","page":"PPCA","title":"Training data","text":"In MLJ or MLJBase, bind an instance model to data with\n\nmach = machine(model, X)\n\nHere:\n\nX is any table of input features (eg, a DataFrame) whose columns are of scitype Continuous; check column scitypes with schema(X).\n\nTrain the machine using fit!(mach, rows=...).","category":"section"},{"location":"models/PPCA_MultivariateStats/#Hyper-parameters","page":"PPCA","title":"Hyper-parameters","text":"maxoutdim=0: Controls the the dimension (number of columns) of the output, outdim. Specifically, outdim = min(n, indim, maxoutdim), where n is the number of observations and indim the input dimension.\nmethod::Symbol=:ml: The method to use to solve the problem, one of :ml, :em, :bayes.\nmaxiter::Int=1000: The maximum number of iterations.\ntol::Real=1e-6: The convergence tolerance.\nmean::Union{Nothing, Real, Vector{Float64}}=nothing: If nothing, centering will be computed and applied; if set to 0 no centering is applied (data is assumed pre-centered); if a vector, the centering is done with that vector.","category":"section"},{"location":"models/PPCA_MultivariateStats/#Operations","page":"PPCA","title":"Operations","text":"transform(mach, Xnew): Return a lower dimensional projection of the input Xnew, which should have the same scitype as X above.\ninverse_transform(mach, Xsmall): For a dimension-reduced table Xsmall, such as returned by transform, reconstruct a table, having same the number of columns as the original training data X, that transforms to Xsmall. Mathematically, inverse_transform is a right-inverse for the PCA projection map, whose image is orthogonal to the kernel of that map. In particular, if Xsmall = transform(mach, Xnew), then inverse_transform(Xsmall) is only an approximation to Xnew.","category":"section"},{"location":"models/PPCA_MultivariateStats/#Fitted-parameters","page":"PPCA","title":"Fitted parameters","text":"The fields of fitted_params(mach) are:\n\nprojection: Returns the projection matrix, which has size (indim, outdim), where indim and outdim are the number of features of the input and ouput respectively. Each column of the projection matrix corresponds to a principal component.","category":"section"},{"location":"models/PPCA_MultivariateStats/#Report","page":"PPCA","title":"Report","text":"The fields of report(mach) are:\n\nindim: Dimension (number of columns) of the training data and new data to be transformed.\noutdim: Dimension of transformed data.\ntvat: The variance of the components.\nloadings: The model's loadings matrix. A matrix of size (indim, outdim) where indim and outdim as as defined above.","category":"section"},{"location":"models/PPCA_MultivariateStats/#Examples","page":"PPCA","title":"Examples","text":"using MLJ\n\nPPCA = @load PPCA pkg=MultivariateStats\n\nX, y = @load_iris ## a table and a vector\n\nmodel = PPCA(maxoutdim=2)\nmach = machine(model, X) |> fit!\n\nXproj = transform(mach, X)\n\nSee also KernelPCA, ICA, FactorAnalysis, PCA","category":"section"},{"location":"models/BM25Transformer_MLJText/#BM25Transformer_MLJText","page":"BM25Transformer","title":"BM25Transformer","text":"BM25Transformer\n\nA model type for constructing a b m25 transformer, based on MLJText.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nBM25Transformer = @load BM25Transformer pkg=MLJText\n\nDo model = BM25Transformer() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in BM25Transformer(max_doc_freq=...).\n\nThe transformer converts a collection of documents, tokenized or pre-parsed as bags of words/ngrams, to a matrix of Okapi BM25 document-word statistics. The BM25 scoring function uses both term frequency (TF) and inverse document frequency (IDF, defined below), as in TfidfTransformer, but additionally adjusts for the probability that a user will consider a search result relevant based, on the terms in the search query and those in each document.\n\nIn textbooks and implementations there is variation in the definition of IDF. Here two IDF definitions are available. The default, smoothed option provides the IDF for a term t as log((1 + n)/(1 + df(t))) + 1, where n is the total number of documents and df(t) the number of documents in which t appears. Setting smooth_df = false provides an IDF of log(n/df(t)) + 1.\n\nReferences:\n\nhttp://ethen8181.github.io/machine-learning/search/bm25_intro.html\nhttps://en.wikipedia.org/wiki/Okapi_BM25\nhttps://nlp.stanford.edu/IR-book/html/htmledition/okapi-bm25-a-non-binary-model-1.html","category":"section"},{"location":"models/BM25Transformer_MLJText/#Training-data","page":"BM25Transformer","title":"Training data","text":"In MLJ or MLJBase, bind an instance model to data with\n\nmach = machine(model, X)\n\nHere:\n\nX is any vector whose elements are either tokenized documents or bags of words/ngrams. Specifically, each element is one of the following:\nA vector of abstract strings (tokens), e.g., [\"I\", \"like\", \"Sam\", \".\", \"Sam\", \"is\", \"nice\", \".\"] (scitype AbstractVector{Textual})\nA dictionary of counts, indexed on abstract strings, e.g., Dict(\"I\"=>1, \"Sam\"=>2, \"Sam is\"=>1) (scitype Multiset{Textual}})\nA dictionary of counts, indexed on plain ngrams, e.g., Dict((\"I\",)=>1, (\"Sam\",)=>2, (\"I\", \"Sam\")=>1) (scitype Multiset{<:NTuple{N,Textual} where N}); here a plain ngram is a tuple of abstract strings.\n\nTrain the machine using fit!(mach, rows=...).","category":"section"},{"location":"models/BM25Transformer_MLJText/#Hyper-parameters","page":"BM25Transformer","title":"Hyper-parameters","text":"max_doc_freq=1.0: Restricts the vocabulary that the transformer will consider. Terms that occur in > max_doc_freq documents will not be considered by the transformer. For example, if max_doc_freq is set to 0.9, terms that are in more than 90% of the documents will be removed.\nmin_doc_freq=0.0: Restricts the vocabulary that the transformer will consider. Terms that occur in < max_doc_freq documents will not be considered by the transformer. A value of 0.01 means that only terms that are at least in 1% of the documents will be included.\nκ=2: The term frequency saturation characteristic. Higher values represent slower saturation. What we mean by saturation is the degree to which a term occurring extra times adds to the overall score.\nβ=0.075: Amplifies the particular document length compared to the average length. The bigger β is, the more document length is amplified in terms of the overall score. The default value is 0.75, and the bounds are restricted between 0 and 1.\nsmooth_idf=true: Control which definition of IDF to use (see above).","category":"section"},{"location":"models/BM25Transformer_MLJText/#Operations","page":"BM25Transformer","title":"Operations","text":"transform(mach, Xnew): Based on the vocabulary, IDF, and mean word counts learned in training, return the matrix of BM25 scores for Xnew, a vector of the same form as X above. The matrix has size (n, p), where n = length(Xnew) and p the size of the vocabulary. Tokens/ngrams not appearing in the learned vocabulary are scored zero.","category":"section"},{"location":"models/BM25Transformer_MLJText/#Fitted-parameters","page":"BM25Transformer","title":"Fitted parameters","text":"The fields of fitted_params(mach) are:\n\nvocab: A vector containing the string used in the transformer's vocabulary.\nidf_vector: The transformer's calculated IDF vector.\nmean_words_in_docs: The mean number of words in each document.","category":"section"},{"location":"models/BM25Transformer_MLJText/#Examples","page":"BM25Transformer","title":"Examples","text":"BM25Transformer accepts a variety of inputs. The example below transforms tokenized documents:\n\nusing MLJ\nimport TextAnalysis\n\nBM25Transformer = @load BM25Transformer pkg=MLJText\n\ndocs = [\"Hi my name is Sam.\", \"How are you today?\"]\nbm25_transformer = BM25Transformer()\n\njulia> tokenized_docs = TextAnalysis.tokenize.(docs)\n2-element Vector{Vector{String}}:\n [\"Hi\", \"my\", \"name\", \"is\", \"Sam\", \".\"]\n [\"How\", \"are\", \"you\", \"today\", \"?\"]\n\nmach = machine(bm25_transformer, tokenized_docs)\nfit!(mach)\n\nfitted_params(mach)\n\ntfidf_mat = transform(mach, tokenized_docs)\n\nAlternatively, one can provide documents pre-parsed as ngrams counts:\n\nusing MLJ\nimport TextAnalysis\n\ndocs = [\"Hi my name is Sam.\", \"How are you today?\"]\ncorpus = TextAnalysis.Corpus(TextAnalysis.NGramDocument.(docs, 1, 2))\nngram_docs = TextAnalysis.ngrams.(corpus)\n\njulia> ngram_docs[1]\nDict{AbstractString, Int64} with 11 entries:\n  \"is\"      => 1\n  \"my\"      => 1\n  \"name\"    => 1\n  \".\"       => 1\n  \"Hi\"      => 1\n  \"Sam\"     => 1\n  \"my name\" => 1\n  \"Hi my\"   => 1\n  \"name is\" => 1\n  \"Sam .\"   => 1\n  \"is Sam\"  => 1\n\nbm25_transformer = BM25Transformer()\nmach = machine(bm25_transformer, ngram_docs)\nMLJ.fit!(mach)\nfitted_params(mach)\n\ntfidf_mat = transform(mach, ngram_docs)\n\nSee also TfidfTransformer, CountTransformer","category":"section"},{"location":"models/DeterministicConstantClassifier_MLJModels/#DeterministicConstantClassifier_MLJModels","page":"DeterministicConstantClassifier","title":"DeterministicConstantClassifier","text":"DeterministicConstantClassifier\n\nA model type for constructing a deterministic constant classifier, based on MLJModels.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nDeterministicConstantClassifier = @load DeterministicConstantClassifier pkg=MLJModels\n\nDo model = DeterministicConstantClassifier() to construct an instance with default hyper-parameters. ","category":"section"},{"location":"models/RidgeRegressor_MLJScikitLearnInterface/#RidgeRegressor_MLJScikitLearnInterface","page":"RidgeRegressor","title":"RidgeRegressor","text":"RidgeRegressor\n\nA model type for constructing a ridge regressor, based on MLJScikitLearnInterface.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nRidgeRegressor = @load RidgeRegressor pkg=MLJScikitLearnInterface\n\nDo model = RidgeRegressor() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in RidgeRegressor(alpha=...).","category":"section"},{"location":"models/RidgeRegressor_MLJScikitLearnInterface/#Hyper-parameters","page":"RidgeRegressor","title":"Hyper-parameters","text":"alpha = 1.0\nfit_intercept = true\ncopy_X = true\nmax_iter = 1000\ntol = 0.0001\nsolver = auto\nrandom_state = nothing","category":"section"},{"location":"models/MultiTaskLassoRegressor_MLJScikitLearnInterface/#MultiTaskLassoRegressor_MLJScikitLearnInterface","page":"MultiTaskLassoRegressor","title":"MultiTaskLassoRegressor","text":"MultiTaskLassoRegressor\n\nA model type for constructing a multi-target lasso regressor, based on MLJScikitLearnInterface.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nMultiTaskLassoRegressor = @load MultiTaskLassoRegressor pkg=MLJScikitLearnInterface\n\nDo model = MultiTaskLassoRegressor() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in MultiTaskLassoRegressor(alpha=...).","category":"section"},{"location":"models/MultiTaskLassoRegressor_MLJScikitLearnInterface/#Hyper-parameters","page":"MultiTaskLassoRegressor","title":"Hyper-parameters","text":"alpha = 1.0\nfit_intercept = true\nmax_iter = 1000\ntol = 0.0001\ncopy_X = true\nrandom_state = nothing\nselection = cyclic","category":"section"},{"location":"models/BaggingClassifier_MLJScikitLearnInterface/#BaggingClassifier_MLJScikitLearnInterface","page":"BaggingClassifier","title":"BaggingClassifier","text":"BaggingClassifier\n\nA model type for constructing a bagging ensemble classifier, based on MLJScikitLearnInterface.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nBaggingClassifier = @load BaggingClassifier pkg=MLJScikitLearnInterface\n\nDo model = BaggingClassifier() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in BaggingClassifier(estimator=...).\n\nA Bagging classifier is an ensemble meta-estimator that fits base  classifiers each on random subsets of the original dataset and then  aggregate their individual predictions (either by voting or by  averaging) to form a final prediction. Such a meta-estimator can  typically be used as a way to reduce the variance of a black-box  estimator (e.g., a decision tree), by introducing randomization into  its construction procedure and then making an ensemble out of it.","category":"section"},{"location":"models/GeneralImputer_BetaML/#GeneralImputer_BetaML","page":"GeneralImputer","title":"GeneralImputer","text":"mutable struct GeneralImputer <: MLJModelInterface.Unsupervised\n\nImpute missing values using arbitrary learning models, from the Beta Machine Learning Toolkit (BetaML).\n\nImpute missing values using a vector (one per column) of arbitrary learning models (classifiers/regressors, not necessarily from BetaML) that implement the interface m = Model([options]), train!(m,X,Y) and predict(m,X).","category":"section"},{"location":"models/GeneralImputer_BetaML/#Hyperparameters:","page":"GeneralImputer","title":"Hyperparameters:","text":"cols_to_impute::Union{String, Vector{Int64}}: Columns in the matrix for which to create an imputation model, i.e. to impute. It can be a vector of columns IDs (positions), or the keywords \"auto\" (default) or \"all\". With \"auto\" the model automatically detects the columns with missing data and impute only them. You may manually specify the columns or use \"all\" if you want to create a imputation model for that columns during training even if all training data are non-missing to apply then the training model to further data with possibly missing values.\nestimator::Any: An entimator model (regressor or classifier), with eventually its options (hyper-parameters), to be used to impute the various columns of the matrix. It can also be a cols_to_impute-length vector of different estimators to consider a different estimator for each column (dimension) to impute, for example when some columns are categorical (and will hence require a classifier) and some others are numerical (hence requiring a regressor). [default: nothing, i.e. use BetaML random forests, handling classification and regression jobs automatically].\nmissing_supported::Union{Bool, Vector{Bool}}: Wheter the estimator(s) used to predict the missing data support itself missing data in the training features (X). If not, when the model for a certain dimension is fitted, dimensions with missing data in the same rows of those where imputation is needed are dropped and then only non-missing rows in the other remaining dimensions are considered. It can be a vector of boolean values to specify this property for each individual estimator or a single booleann value to apply to all the estimators [default: false]\nfit_function::Union{Function, Vector{Function}}: The function used by the estimator(s) to fit the model. It should take as fist argument the model itself, as second argument a matrix representing the features, and as third argument a vector representing the labels. This parameter is mandatory for non-BetaML estimators and can be a single value or a vector (one per estimator) in case of different estimator packages used. [default: BetaML.fit!]\npredict_function::Union{Function, Vector{Function}}: The function used by the estimator(s) to predict the labels. It should take as fist argument the model itself and as second argument a matrix representing the features. This parameter is mandatory for non-BetaML estimators and can be a single value or a vector (one per estimator) in case of different estimator packages used. [default: BetaML.predict]\nrecursive_passages::Int64: Define the number of times to go trough the various columns to impute their data. Useful when there are data to impute on multiple columns. The order of the first passage is given by the decreasing number of missing values per column, the other passages are random [default: 1].\nrng::Random.AbstractRNG: A Random Number Generator to be used in stochastic parts of the code [deafult: Random.GLOBAL_RNG]. Note that this influence only the specific GeneralImputer code, the individual estimators may have their own rng (or similar) parameter.","category":"section"},{"location":"models/GeneralImputer_BetaML/#Examples-:","page":"GeneralImputer","title":"Examples :","text":"Using BetaML models:\n\njulia> using MLJ;\njulia> import BetaML ## The library from which to get the individual estimators to be used for each column imputation\njulia> X = [\"a\"         8.2;\n            \"a\"     missing;\n            \"a\"         7.8;\n            \"b\"          21;\n            \"b\"          18;\n            \"c\"        -0.9;\n            missing      20;\n            \"c\"        -1.8;\n            missing    -2.3;\n            \"c\"        -2.4] |> table ;\njulia> modelType = @load GeneralImputer  pkg = \"BetaML\" verbosity=0\nBetaML.Imputation.GeneralImputer\njulia> model     = modelType(estimator=BetaML.DecisionTreeEstimator(),recursive_passages=2);\njulia> mach      = machine(model, X);\njulia> fit!(mach);\n[ Info: Training machine(GeneralImputer(cols_to_impute = auto, …), …).\njulia> X_full       = transform(mach) |> MLJ.matrix\n10×2 Matrix{Any}:\n \"a\"   8.2\n \"a\"   8.0\n \"a\"   7.8\n \"b\"  21\n \"b\"  18\n \"c\"  -0.9\n \"b\"  20\n \"c\"  -1.8\n \"c\"  -2.3\n \"c\"  -2.4\n\nUsing third party packages (in this example DecisionTree):\n\njulia> using MLJ;\njulia> import DecisionTree ## An example of external estimators to be used for each column imputation\njulia> X = [\"a\"         8.2;\n            \"a\"     missing;\n            \"a\"         7.8;\n            \"b\"          21;\n            \"b\"          18;\n            \"c\"        -0.9;\n            missing      20;\n            \"c\"        -1.8;\n            missing    -2.3;\n            \"c\"        -2.4] |> table ;\njulia> modelType   = @load GeneralImputer  pkg = \"BetaML\" verbosity=0\nBetaML.Imputation.GeneralImputer\njulia> model     = modelType(estimator=[DecisionTree.DecisionTreeClassifier(),DecisionTree.DecisionTreeRegressor()], fit_function=DecisionTree.fit!,predict_function=DecisionTree.predict,recursive_passages=2);\njulia> mach      = machine(model, X);\njulia> fit!(mach);\n[ Info: Training machine(GeneralImputer(cols_to_impute = auto, …), …).\njulia> X_full       = transform(mach) |> MLJ.matrix\n10×2 Matrix{Any}:\n \"a\"   8.2\n \"a\"   7.51111\n \"a\"   7.8\n \"b\"  21\n \"b\"  18\n \"c\"  -0.9\n \"b\"  20\n \"c\"  -1.8\n \"c\"  -2.3\n \"c\"  -2.4","category":"section"},{"location":"third_party_packages/#Third-Party-Packages","page":"Third Party Packages","title":"Third Party Packages","text":"A list of third-party packages with integration with MLJ.\n\nLast updated December 2020.\n\nPull requests to update this list are very welcome. Otherwise, you may post an issue requesting this here.","category":"section"},{"location":"third_party_packages/#Packages-providing-models-in-the-MLJ-model-registry","page":"Third Party Packages","title":"Packages providing models in the MLJ model registry","text":"See List of Supported Models","category":"section"},{"location":"third_party_packages/#Providing-unregistered-models:","page":"Third Party Packages","title":"Providing unregistered models:","text":"SossMLJ.jl\nTimeSeriesClassification","category":"section"},{"location":"third_party_packages/#Packages-providing-other-kinds-of-functionality:","page":"Third Party Packages","title":"Packages providing other kinds of functionality:","text":"MLJParticleSwarmOptimization.jl (hyper-parameter optimization strategy)\nTreeParzen.jl (hyper-parameter optimization strategy)\nShapley.jl (feature ranking / interpretation)\nShapML.jl (feature ranking / interpretation)\nFairness.jl (FAIRness metrics)\nOutlierDetection.jl (provides the ProbabilisticDetector wrapper and other outlier detection meta-functionality)\nConformalPrediction.jl (predictive uncertainty quantification through conformal prediction)","category":"section"},{"location":"learning_networks/#Learning-Networks","page":"Learning Networks","title":"Learning Networks","text":"Below is a practical guide to the MLJ implementation of learning networks, which have been described more abstractly in the article:\n\nAnthony D. Blaom and Sebastian J. Voller (2020): Flexible model composition in machine learning and its implementation in MLJ. Preprint, arXiv:2012.15505.\n\nLearning networks, an advanced but powerful MLJ feature, are \"blueprints\" for combining models in flexible ways, beyond ordinary linear pipelines and simple model ensembles. They are simple transformations of your existing workflows which can be \"exported\" to define new, re-usable composite model types (models which typically have other models as hyperparameters).\n\nPipeline models (see Pipeline), and model stacks (see Stack) are both implemented internally as exported learning networks.\n\nnote: Note\nWhile learning networks can be used for complex machine learning workflows, their main purpose is for defining new stand-alone model types, which behave just like any other model type: Instances can be evaluated, tuned, inserted into pipelines, etc.  In serious applications, users are encouraged to export their learning networks, as explained under Exporting a learning network as a new model type below, after testing the network, using a small training dataset.","category":"section"},{"location":"learning_networks/#Learning-networks-by-example","page":"Learning Networks","title":"Learning networks by example","text":"Learning networks are best explained by way of example.","category":"section"},{"location":"learning_networks/#Lazy-computation","page":"Learning Networks","title":"Lazy computation","text":"The core idea of a learning network is delayed or lazy computation. Instead of\n\nX = 4\nY = 3\nZ = 2*X\nW = Y + Z\nW\n\nwe can do\n\nusing MLJ\n\nX = source(4)\nY = source(3)\nZ = 2*X\nW = Y + Z\nW()\n\nIn the first computation X, Y, Z and W are all bound to ordinary data. In the second, they are bound to objects called nodes. The special nodes X and Y constitute \"entry points\" for data, and are called source nodes. As the terminology suggests, we can imagine these objects as part of a \"network\" (a directed acyclic graph) which can aid conceptualization (but is less useful in more complicated examples):\n\n(Image: )","category":"section"},{"location":"learning_networks/#The-origin-of-a-node","page":"Learning Networks","title":"The origin of a node","text":"The source nodes on which a given node depends are called the origins of the node:\n\nos = origins(W)\n\nX in os","category":"section"},{"location":"learning_networks/#Re-using-a-network","page":"Learning Networks","title":"Re-using a network","text":"The advantage of lazy evaluation is that we can change data at a source node to repeat the calculation with new data. One way to do this (discouraged in practice) is to use rebind!:\n\nZ()\n\nrebind!(X, 6) # demonstration only!\nZ()\n\nHowever, if a node has a unique origin, then one instead calls the node on the new data one would like to rebind to that origin:\n\norigins(Z)\n\nZ(6)\n\nZ(4)\n\nThis has the advantage that you don't need to locate the origin and rebind data directly, and the unique-origin restriction turns out to be sufficient for the applications to learning we have in mind.","category":"section"},{"location":"learning_networks/#node_overloading","page":"Learning Networks","title":"Overloading functions for use on nodes","text":"Several built-in function like * and + above are overloaded in MLJBase to work on nodes, as illustrated above. Others that work out-of-the-box include: MLJBase.matrix, MLJBase.table, vcat, hcat, mean, median, mode, first, last, as well as broadcasted versions of log, exp, mean, mode and median. A function like sqrt is not overloaded, so that Q = sqrt(Z) will throw an error. Instead, we do\n\nQ = node(sqrt, Z)\nZ()\n\nQ()\n\nYou can learn more about the node function under More on defining new nodes","category":"section"},{"location":"learning_networks/#A-network-that-learns","page":"Learning Networks","title":"A network that learns","text":"To incorporate learning in a network of nodes MLJ:\n\nAllows binding of machines to nodes instead of data\nGenerates \"operation\" nodes when calling an operation like predict or transform on a machine and node input data. Such nodes point to both a machine (storing learned parameters) and the node from which to fetch data for applying the operation (which, unlike the nodes seen so far, depend on learned parameters to generate output).\n\nFor an example of a learning network that actually learns, we first synthesize some training data X, y, and production data Xnew:\n\nusing MLJ\nX, y = make_blobs(cluster_std=10.0, rng=123)  # `X` is a table, `y` a vector\nXnew, _ = make_blobs(3) # `Xnew` is a table with the same number of columns\nnothing # hide\n\nWe choose a model do some dimension reduction, and another to perform classification:\n\npca = (@load PCA pkg=MultivariateStats verbosity=0)()\ntree = (@load DecisionTreeClassifier pkg=DecisionTree verbosity=0)()\nnothing # hide\n\nTo make our learning lazy, we wrap the training data as source nodes:\n\nXs = source(X)\nys = source(y)\nnothing # hide\n\nAnd, finally, proceed as we would in an ordinary MLJ workflow, with the exception that there is no need to fit! our machines, as training will be carried out lazily later:\n\nmach1 = machine(pca, Xs)\nx = transform(mach1, Xs) # defines a new node because `Xs` is a node\n\nmach2 = machine(tree, x, ys)\nyhat = predict(mach2, x) # defines a new node because `x` is a node\n\nNote that mach1 and mach2 are not themselves nodes. They point to the nodes they need to call to get training data and they are in turn pointed to by other nodes. In fact, an interesting implementation detail is that an \"ordinary\" machine is not actually bound directly to data, but bound to data wrapped in source nodes.\n\nmachine(pca, Xnew).args[1] # `Xnew` is ordinary data\n\nBefore calling a node, we need to fit! the node, to trigger training of all the machines on which it depends:\n\nfit!(yhat)   # can include same keyword options for `fit!(::Machine, ...)`\nyhat()[1:2]  # or `yhat(rows=2)`\n\nThis last represents the prediction on the training data, because that's what resides at our source nodes. However, yhat has the unique origin X (because \"training edges\" in the complete associated directed graph are excluded for this purpose). We can therefore call yhat on our production data to get the corresponding predictions:\n\nyhat(Xnew)\n\nTraining is smart, in the sense that mutating a hyper-parameter of some component model does not force retraining of upstream machines:\n\ntree.max_depth = 1\nfit!(yhat)\nyhat(Xnew)","category":"section"},{"location":"learning_networks/#Multithreaded-training","page":"Learning Networks","title":"Multithreaded training","text":"A more complicated learning network may contain machines that can be trained in parallel. In that case, a call like the following may speed up training:\n\ntree.max_depth = 2\nfit!(yhat, acceleration=CPUThreads())\nnothing # hide\n\nCurrently, only CPU1() (default) and CPUThreads() are supported here.","category":"section"},{"location":"learning_networks/#Exporting-a-learning-network-as-a-new-model-type","page":"Learning Networks","title":"Exporting a learning network as a new model type","text":"Once a learning network has been tested, typically on some small dummy data set, it is ready to be exported as a new, stand-alone, re-usable model type (unattached to any data). We demonstrate the process by way of examples of increasing complexity:\n\nExample A - Mini-pipeline\nMore on replacing models with symbols\nExample B - Multiple operations: transform and inverse transform\nExample C - Blending predictions and exposing internal network state in reports\nExample D - Multiple nodes pointing to the same machine\nExample E - Coupling component model hyper-parameters\nMore on defining new nodes\nExample F - Wrapping a model in a data-dependent tuning strategy","category":"section"},{"location":"learning_networks/#Example-A-Mini-pipeline","page":"Learning Networks","title":"Example A - Mini-pipeline","text":"First we export the simple learning network defined above. (This is for illustration purposes; in practice using the Pipeline syntax model1 |> model2 syntax is more convenient.)","category":"section"},{"location":"learning_networks/#Step-1-Define-a-new-model-struct","page":"Learning Networks","title":"Step 1 - Define a new model struct","text":"We need a type with two fields, one for the preprocessor (pca in the network above) and one for the classifier (tree in the network above).\n\nThe DecisionTreeClassifier type of tree has supertype Probabilistic, because it makes probabilistic predictions, and we assume any other classifier we want to swap out will be the same.\n\nsupertype(typeof(tree))\n\nIn particular, our composite model will also need Probabilistic as supertype. In fact, we must give it the intermediate supertype ProbabilisticNetworkComposite <: Probabilistic, so that we additionally flag it as an exported learning network model type:\n\nmutable struct CompositeA <: ProbabilisticNetworkComposite\n    preprocessor\n    classifier\nend\n\nThe common alternatives are DeterministicNetworkComposite and UnsupervisedNetworkComposite. But all options can be viewed as follows:\n\nusing MLJBase\nNetworkComposite\n\nWe next make our learning network model-generic by substituting each model instance with the corresponding symbol representing a property (field) of the new model struct:\n\nmach1 = machine(:preprocessor, Xs)   # <---- `pca` swapped out for `:preprocessor`\nx = transform(mach1, Xs)\nmach2 = machine(:classifier, x, ys)  # <---- `tree` swapped out for `:classifier`\nyhat = predict(mach2, x)\n\nIncidentally, this network can be used as before except we must provide an instance of CompositeA in our fit! calls, to indicate what actual models the symbols are being substituted with:\n\ncomposite_a = CompositeA(pca, ConstantClassifier())\nfit!(yhat, composite=composite_a)\nyhat(Xnew)\n\nIn this case :preprocessor is being substituted by pca, and :classifier by ConstantClassifier() for training.","category":"section"},{"location":"learning_networks/#Step-2-Wrap-the-learning-network-in-prefit","page":"Learning Networks","title":"Step 2 - Wrap the learning network in prefit","text":"Literally copy and paste the learning network above into the definition of a method called prefit, as shown below (if you have implemented your own MLJ model, you will notice this has the same signature as MLJModelInterface.fit):\n\nimport MLJBase\nfunction MLJBase.prefit(composite::CompositeA, verbosity, X, y)\n\n    # the learning network from above:\n    Xs = source(X)\n    ys = source(y)\n    mach1 = machine(:preprocessor, Xs)\n    x = transform(mach1, Xs)\n    mach2 = machine(:classifier, x, ys)\n    yhat = predict(mach2, x)\n\n    verbosity > 0 && @info \"I'm a noisy fellow!\"\n\n    # return \"learning network interface\":\n    return (; predict=yhat)\nend\n\nThat's it.\n\nGenerally, prefit always returns a learning network interface; see MLJBase.prefit for what this means in general. In this example, the interface dictates that calling predict(mach, Xnew) on a machine mach bound to some instance of CompositeA should internally call yhat(Xnew).\n\nHere's our new composite model type CompositeA in action, combining standardization with KNN classification:\n\nusing MLJ\nX, y = @load_iris\n\nknn = (@load KNNClassifier pkg=NearestNeighborModels verbosity=0)()\ncomposite_a = CompositeA(Standardizer(), knn)\n\nmach = machine(composite_a, X, y) |> fit!\npredict(mach, X)[1:2]\n\nreport(mach).preprocessor\n\nfitted_params(mach).classifier","category":"section"},{"location":"learning_networks/#More-on-replacing-models-with-symbols","page":"Learning Networks","title":"More on replacing models with symbols","text":"Only the first argument model in some expression machine(model, ...) can be replaced with a symbol. These replacements function as hooks for exposing reports and fitted parameters of component models in the report and fitted parameters of the composite model, but these replacements are not absolutely necessary. For example, instead of the line mach1 = machine(:preprocessor, Xs) in the prefit definition, we can do mach1 = machine(composite.preprocessor, Xs). However, report and fittted_params will not include items for the :preprocessor component model in that case.\n\nIf a component model is not explicitly bound to data in a machine (for example, because it is first wrapped in TunedModel) then there are ways to explicitly expose associated fitted parameters or report items. See Example F below.","category":"section"},{"location":"learning_networks/#Example-B-Multiple-operations:-transform-and-inverse-transform","page":"Learning Networks","title":"Example B - Multiple operations: transform and inverse transform","text":"Here's a second mini-pipeline example composing two transformers which both implement inverse transform. We show how to implement an inverse_transform for the composite model too.","category":"section"},{"location":"learning_networks/#Step-1-Define-a-new-model-struct-2","page":"Learning Networks","title":"Step 1 - Define a new model struct","text":"using MLJ\nimport MLJBase\n\nmutable struct CompositeB <: DeterministicNetworkComposite\n    transformer1\n    transformer2\nend","category":"section"},{"location":"learning_networks/#Step-2-Wrap-the-learning-network-in-prefit-2","page":"Learning Networks","title":"Step 2 - Wrap the learning network in prefit","text":"function MLJBase.prefit(composite::CompositeB, verbosity, X)\n    Xs = source(X)\n\n    mach1 = machine(:transformer1, Xs)\n    X1 = transform(mach1, Xs)\n    mach2 = machine(:transformer2, X1)\n    X2 = transform(mach2, X1)\n\n    W1 = inverse_transform(mach2, Xs)\n    W2 = inverse_transform(mach1, W1)\n\n    # the learning network interface:\n    return (; transform=X2, inverse_transform=W2)\nend\n\nHere's a demonstration:\n\nX = rand(100)\n\ncomposite_b = CompositeB(UnivariateBoxCoxTransformer(), Standardizer())\nmach = machine(composite_b, X) |> fit!\nW =  transform(mach, X)\n@assert inverse_transform(mach, W) ≈ X","category":"section"},{"location":"learning_networks/#Example-C-Blending-predictions-and-exposing-internal-network-state-in-reports","page":"Learning Networks","title":"Example C - Blending predictions and exposing internal network state in reports","text":"The code below defines a new composite model type CompositeC that predicts by taking the weighted average of two regressors, and additionally exposes, in the model's report, a measure of disagreement between the two models at time of training. In addition to the two regressors, the new model has two other fields:\n\nmix, controlling the weighting\nacceleration, for the mode of acceleration for training the model (e.g., CPUThreads()).","category":"section"},{"location":"learning_networks/#Step-1-Define-a-new-model-struct-3","page":"Learning Networks","title":"Step 1 - Define a new model struct","text":"using MLJ\nimport MLJBase\n\nmutable struct CompositeC <: DeterministicNetworkComposite\n    regressor1\n    regressor2\n    mix::Float64\n    acceleration\nend","category":"section"},{"location":"learning_networks/#Step-2-Wrap-the-learning-network-in-prefit-3","page":"Learning Networks","title":"Step 2 - Wrap the learning network in prefit","text":"function MLJBase.prefit(composite::CompositeC, verbosity, X, y)\n\n    Xs = source(X)\n    ys = source(y)\n\n    mach1 = machine(:regressor1, Xs, ys)\n    mach2 = machine(:regressor2, Xs, ys)\n\n    yhat1 = predict(mach1, Xs)\n    yhat2 = predict(mach2, Xs)\n\n    # node to return disagreement between the regressor predictions:\n    disagreement = node((y1, y2) -> l2(y1, y2) |> mean, yhat1, yhat2)\n\n    # get the weighted average the predictions of the regressors:\n    λ = composite.mix\n    yhat = (1 - λ)*yhat1 + λ*yhat2\n\n    # the learning network interface:\n    return (\n        predict = yhat,\n        report= (; training_disagreement=disagreement),\n        acceleration = composite.acceleration,\n    )\n\nend\n\nHere's a demonstration:\n\nX, y = make_regression() # a table and a vector\n\nknn = (@load KNNRegressor pkg=NearestNeighborModels verbosity=0)()\ntree =  (@load DecisionTreeRegressor pkg=DecisionTree verbosity=0)()\ncomposite_c = CompositeC(knn, tree, 0.2, CPUThreads())\nmach = machine(composite_c, X, y) |> fit!\nXnew, _ = make_regression(3)\npredict(mach, Xnew)\n\nreport(mach)","category":"section"},{"location":"learning_networks/#Example-D-Multiple-nodes-pointing-to-the-same-machine","page":"Learning Networks","title":"Example D - Multiple nodes pointing to the same machine","text":"When incorporating learned target transformations (such as a standardization) in supervised learning, it is desirable to apply the inverse transformation to predictions, to return them to the original scale. This means re-using learned parameters from an earlier part of your workflow. This poses no problem here, as the next example demonstrates.\n\nThe model type CompositeD defined below applies a preprocessing transformation to input data X (e.g., standardization), learns a transformation for the target y (e.g., an optimal Box-Cox transformation), predicts new target values using a regressor (e.g., Ridge regression), and then inverse-transforms those predictions to restore them to the original scale. (This represents a model we could alternatively build using the TransformedTargetModel wrapper and a Pipeline.)","category":"section"},{"location":"learning_networks/#Step-1-Define-a-new-model-struct-4","page":"Learning Networks","title":"Step 1 - Define a new model struct","text":"using MLJ\nimport MLJBase\n\nmutable struct CompositeD <: DeterministicNetworkComposite\n    preprocessor\n    target_transformer\n    regressor\n    acceleration\nend","category":"section"},{"location":"learning_networks/#Step-2-Wrap-the-learning-network-in-prefit-4","page":"Learning Networks","title":"Step 2 - Wrap the learning network in prefit","text":"Notice that both of the nodes z and yhat in the wrapped learning network point to the same machine (learned parameters) mach2.\n\nfunction MLJBase.prefit(composite::CompositeD, verbosity, X, y)\n\n    Xs = source(X)\n    ys = source(y)\n\n    mach1 = machine(:preprocessor, Xs)\n    W = transform(mach1, Xs)\n\n    mach2 = machine(:target_transformer, ys)\n    z = transform(mach2, ys)\n\n    mach3 =machine(:regressor, W, z)\n    zhat = predict(mach3, W)\n\n    yhat = inverse_transform(mach2, zhat)\n\n    # the learning network interface:\n    return (\n        predict = yhat,\n        acceleration = composite.acceleration,\n    )\n\nend\n\nThe flow of information in the wrapped learning network is visualized below.\n\n(Image: )\n\nHere's an application of our new composite to the Boston dataset:\n\nX, y = @load_boston\n\nstand = Standardizer()\nbox = UnivariateBoxCoxTransformer()\nridge = (@load RidgeRegressor pkg=MultivariateStats verbosity=0)(lambda=92)\ncomposite_d = CompositeD(stand, box, ridge, CPU1())\nevaluate(composite_d, X, y, resampling=CV(nfolds=5), measure=l2, verbosity=0)","category":"section"},{"location":"learning_networks/#Example-E-Coupling-component-model-hyper-parameters","page":"Learning Networks","title":"Example E - Coupling component model hyper-parameters","text":"The composite model in this example combines a clustering model used to reduce the dimension of the feature space (KMeans or KMedoids from Clustering.jl) with ridge regression, but has the following \"coupling\" of the hyperparameters: The amount of ridge regularization depends on the number of specified clusters k, with less regularization for a greater number of clusters. It includes a user-specified coupling coefficient c, and exposes the solver hyper-parameter of the ridge regressor. (Neither the clusterer nor ridge regressor are themselves hyperparameters of the composite.)","category":"section"},{"location":"learning_networks/#Step-1-Define-a-new-model-struct-5","page":"Learning Networks","title":"Step 1 - Define a new model struct","text":"using MLJ\nimport MLJBase\n\nmutable struct CompositeE <: DeterministicNetworkComposite\n    clusterer     # `:kmeans` or `:kmedoids`\n    k::Int        # number of clusters\n    solver        # a ridge regression parameter we want to expose\n    c::Float64    # a \"coupling\" coefficient\nend","category":"section"},{"location":"learning_networks/#Step-2-Wrap-the-learning-network-in-prefit-5","page":"Learning Networks","title":"Step 2 - Wrap the learning network in prefit","text":"RidgeRegressor = @load RidgeRegressor pkg=MLJLinearModels verbosity=0\nKMeans   = @load KMeans pkg=Clustering verbosity=0\nKMedoids = @load KMedoids pkg=Clustering verbosity=0\n\nfunction MLJBase.prefit(composite::CompositeE, verbosity, X, y)\n\n    Xs = source(X)\n    ys = source(y)\n\n    k = composite.k\n    solver = composite.solver\n    c = composite.c\n\n    clusterer = composite.clusterer == :kmeans ? KMeans(; k) : KMedoids(; k)\n    mach1 = machine(clusterer, Xs)\n    Xsmall = transform(mach1, Xs)\n\n    # the coupling - ridge regularization depends on the number of\n    # clusters `k` and the coupling coefficient `c`:\n    lambda = exp(-c/k)\n\n    ridge = RidgeRegressor(; lambda, solver)\n    mach2 = machine(ridge, Xsmall, ys)\n    yhat = predict(mach2, Xsmall)\n\n    return (predict=yhat,)\nend\n\nHere's an application to the Boston dataset in which we optimize the coupling coefficient (see Tuning Models for more on hyper-parameter optimization):\n\nX, y = @load_boston # a table and a vector\n\ncomposite_e = CompositeE(:kmeans, 3, nothing, 0.5)\nr = range(composite_e, :c, lower = -2, upper=2, scale=x->10^x)\ntuned_composite_e = TunedModel(\n    composite_e,\n    range=r,\n    tuning=RandomSearch(rng=123),\n    measure=l2,\n    resampling=CV(nfolds=6),\n    n=100,\n)\nmach = machine(tuned_composite_e, X, y) |> fit!\nreport(mach).best_model","category":"section"},{"location":"learning_networks/#More-on-defining-new-nodes","page":"Learning Networks","title":"More on defining new nodes","text":"Overloading ordinary functions for nodes has already been discussed above. Here's another example:\n\ndivide(x, y) = x/y\n\nX = source(2)\nY = source(3)\n\nZ = node(divide, X, Y)\nnothing # hide\n\nThis means Z() returns divide(X(), Y()), which is divide(2, 3) in this case:\n\nZ()\n\nWe cannot call Z with arguments (e.g., Z(2)) because it does not have a unique origin.\n\nIn all the node examples so far, the first argument of node is a function, and all other arguments are nodes - one node for each argument of the function. A node constructed in this way is called a static node. A dynamic node, which directly depends on the outcome of a training event, is constructed by giving a machine as the second argument, to be passed as the first argument of the function in a node call. For example, we can do\n\nXs = source(rand(4))\nmach = machine(Standardizer(), Xs)\nN = node(transform, mach, Xs) |> fit!\nnothing # hide\n\nThen N has the following calling properties:\n\nN() returns transform(mach, Xs())\nN(Xnew) returns transform(mach, Xs(Xnew)); here Xs(Xnew) is just Xnew because Xs is just a source node.)\n\nN()\n\nN(rand(2))\n\nIn fact, this is precisely how the transform method is internally overloaded to work, when called with a node argument (to return a node instead of data). That is, internally there exists code that amounts to the definition\n\ntransform(mach, X::AbstractNode) = node(transform, mach, X)\n\nHere AbstractNode is the common super-type of Node and Source.\n\nIt sometimes useful to create dynamic nodes with no node arguments, as in\n\nXs = source(rand(10))\nmach = machine(Standardizer(), Xs)\nN = node(fitted_params, mach) |> fit!\nN()\n\nStatic nodes can have also have zero node arguments. These may be viewed as \"constant\" nodes:\n\nN = Node(()-> 42)\nN()\n\nExample F below demonstrates the use of static and dynamic nodes. For more details, see the node docstring.\n\nThere is also an experimental macro @node. If Z is an AbstractNode (Z = source(16), say) then instead of\n\nQ = node(sqrt, Z)\n\none can do\n\nQ = @node sqrt(Z)\n\n(so that Q() == 4). Here's a more complicated application of @node to row-shuffle a table:\n\nusing MLJ, Random\nX = (x1 = [1, 2, 3, 4, 5],\n     x2 = [:one, :two, :three, :four, :five])\nrows(X) = 1:nrows(X)\n\nXs = source(X)\nrs = @node rows(Xs)\nW = @node selectrows(Xs, @node shuffle(rs))\n\nW()\n\nImportant. An argument not in global scope is assumed by @node to be a node or source.","category":"section"},{"location":"learning_networks/#Example-F-Wrapping-a-model-in-a-data-dependent-tuning-strategy","page":"Learning Networks","title":"Example F - Wrapping a model in a data-dependent tuning strategy","text":"When the regularization parameter of a Lasso model is optimized, one commonly searches over a parameter range depending on properties of the training data. Indeed, Lasso (and, more generally, elastic net) implementations commonly provide a method to carry out this data-dependent optimization automatically, using cross-validation. The following example shows how to transform the LassoRegressor model type from MLJLinearModels.jl into a self-tuning model type LassoCVRegressor using the commonly implemented data-dependent tuning strategy. A new dimensionless hyperparameter epsilon controls the lower bound on the parameter range.","category":"section"},{"location":"learning_networks/#Step-1-Define-a-new-model-struct-6","page":"Learning Networks","title":"Step 1 - Define a new model struct","text":"using MLJ\nimport MLJBase\n\nmutable struct LassoCVRegressor <: DeterministicNetworkComposite\n    lasso              # the atomic lasso model (`lasso.lambda` is ignored)\n    epsilon::Float64   # controls lower bound of `lasso.lambda` in tuning\n    resampling         # resampling strategy for optimization of `lambda`\nend\n\n# keyword constructor for convenience:\nLassoRegressor = @load LassoRegressor pkg=MLJLinearModels verbosity=0\nLassoCVRegressor(;\n    lasso=LassoRegressor(),\n    epsilon=0.001,\n    resampling=CV(nfolds=6),\n) = LassoCVRegressor(\n    lasso,\n    epsilon,\n    resampling,\n)\nnothing # hide","category":"section"},{"location":"learning_networks/#Step-2-Wrap-the-learning-network-in-prefit-6","page":"Learning Networks","title":"Step 2 - Wrap the learning network in prefit","text":"In this case, there is no model -> :symbol replacement that makes sense here, because the model is getting wrapped by TunedModel before being bound to nodes in a machine. However, we can expose the the learned lasso coefs and intercept using fitted parameter nodes; and expose the optimal lambda, and range searched, using report nodes (as previously demonstrated in Example C).\n\nfunction MLJBase.prefit(composite::LassoCVRegressor, verbosity, X, y)\n\n    λ_max = maximum(abs.(MLJ.matrix(X)'y))\n\n    Xs = source(X)\n    ys = source(y)\n\n    r = range(\n        composite.lasso,\n        :lambda,\n        lower=composite.epsilon*λ_max,\n        upper=λ_max,\n        scale=:log10,\n    )\n\n    lambda_range = node(()->r)  # a \"constant\" report node\n\n    tuned_lasso = TunedModel(\n        composite.lasso,\n        tuning=Grid(shuffle=false),\n        range = r,\n        measure = l2,\n        resampling=composite.resampling,\n    )\n    mach = machine(tuned_lasso, Xs, ys)\n\n    R = node(report, mach)                                 # `R()` returns `report(mach)`\n    lambda = node(r -> r.best_model.lambda, R)             # a report node\n\n    F = node(fitted_params, mach)             # `F()` returns `fitted_params(mach)`\n    coefs = node(f->f.best_fitted_params.coefs, F)         # a fitted params node\n    intercept = node(f->f.best_fitted_params.intercept, F) # a fitted params node\n\n    yhat = predict(mach, Xs)\n\n    return (\n        predict=yhat,\n        fitted_params=(; coefs, intercept),\n        report=(; lambda, lambda_range),\n   )\n\nend\n\nHere's a demonstration:\n\nX, _ = make_regression(1000, 3, rng=123)\ny = X.x2 - X.x2 + 0.005*X.x3 + 0.05*rand(1000)\nlasso_cv = LassoCVRegressor(epsilon=1e-5)\nmach = machine(lasso_cv, X, y) |> fit!\nreport(mach)\n\nfitted_params(mach)","category":"section"},{"location":"learning_networks/#The-learning-network-API","page":"Learning Networks","title":"The learning network API","text":"Two new julia types are part of learning networks: Source and Node, which share a common abstract supertype AbstractNode.\n\nFormally, a learning network defines two labeled directed acyclic graphs (DAG's) whose nodes are Node or Source objects, and whose labels are Machine objects. We obtain the first DAG from directed edges of the form N1 - N2 whenever N1 is an argument of N2 (see below). Only this DAG is relevant when calling a node, as discussed in the examples above and below. To form the second DAG (relevant when calling or calling fit! on a node) one adds edges for which N1 is training argument of the machine which labels N1. We call the second, larger DAG, the completed learning network (but note only edges of the smaller network are explicitly drawn in diagrams, for simplicity).","category":"section"},{"location":"learning_networks/#Source-nodes","page":"Learning Networks","title":"Source nodes","text":"Only source nodes can reference concrete data. A Source object has a single field, data.","category":"section"},{"location":"learning_networks/#Nodes","page":"Learning Networks","title":"Nodes","text":"See more on fitting nodes at fit! and fit_only!.","category":"section"},{"location":"learning_networks/#MLJBase.Source","page":"Learning Networks","title":"MLJBase.Source","text":"Source\n\nType for a learning network source node. Constructed using source, as in source() or source(rand(2,3)).\n\nSee also source, Node.\n\n\n\n\n\n","category":"type"},{"location":"learning_networks/#MLJBase.source-Tuple{Any}","page":"Learning Networks","title":"MLJBase.source","text":"Xs = source(X=nothing)\n\nDefine, a learning network Source object, wrapping some input data X, which can be nothing for purposes of exporting the network as stand-alone model. For training and testing the unexported network, appropriate vectors, tables, or other data containers are expected.\n\nThe calling behaviour of a Source object is this:\n\nXs() = X\nXs(rows=r) = selectrows(X, r)  # eg, X[r,:] for a DataFrame\nXs(Xnew) = Xnew\n\nSee also: MLJBase.prefit, sources, origins, node.\n\n\n\n\n\n","category":"method"},{"location":"learning_networks/#MLJBase.rebind!","page":"Learning Networks","title":"MLJBase.rebind!","text":"rebind!(s, X)\n\nAttach new data X to an existing source node s. Not a public method.\n\n\n\n\n\n","category":"function"},{"location":"learning_networks/#MLJBase.sources","page":"Learning Networks","title":"MLJBase.sources","text":"sources(N::AbstractNode)\n\nA vector of all sources referenced by calls N() and fit!(N). These are the sources of the ancestor graph of N when including training edges.\n\nNot to be confused with origins(N), in which training edges are excluded.\n\nSee also: origins, source.\n\n\n\n\n\n","category":"function"},{"location":"learning_networks/#MLJBase.origins","page":"Learning Networks","title":"MLJBase.origins","text":"origins(N)\n\nReturn a list of all origins of a node N accessed by a call N(). These are the source nodes of ancestor graph of N if edges corresponding to training arguments are excluded. A Node object cannot be called on new data unless it has a unique origin.\n\nNot to be confused with sources(N) which refers to the same graph but without the training edge deletions.\n\nSee also: node, source.\n\n\n\n\n\n","category":"function"},{"location":"learning_networks/#MLJBase.Node","page":"Learning Networks","title":"MLJBase.Node","text":"Node{T<:Union{Machine,Nothing}}\n\nType for nodes in a learning network that are not Source nodes.\n\nThe key components of a Node are:\n\nAn operation, which will either be static (a fixed function) or dynamic (such as predict or transform).\nA Machine object, on which to dispatch the operation (nothing if the operation is static). The training arguments of the machine are generally other nodes, including Source nodes.\nUpstream connections to other nodes, called its arguments, possibly including Source nodes, one for each data argument of the operation (typically there's just one).\n\nWhen a node N is called, as in N(), it applies the operation on the machine (if there is one) together with the outcome of calls to its node arguments, to compute the return value. For details on a node's calling behavior, see node.\n\nSee also node, Source, origins, sources, fit!.\n\n\n\n\n\n","category":"type"},{"location":"learning_networks/#MLJBase.node","page":"Learning Networks","title":"MLJBase.node","text":"J = node(f, mach::Machine, args...)\n\nDefines a dynamic Node object J wrapping a dynamic operation f (predict, predict_mean, transform, etc), a nodal machine mach and arguments args. Its calling behaviour, which depends on the outcome of training mach (and, implicitly, on training outcomes affecting its arguments) is this:\n\nJ() = f(mach, args[1](), args[2](), ..., args[n]())\nJ(rows=r) = f(mach, args[1](rows=r), args[2](rows=r), ..., args[n](rows=r))\nJ(X) = f(mach, args[1](X), args[2](X), ..., args[n](X))\n\nGenerally n=1 or n=2 in this latter case.\n\npredict(mach, X::AbsractNode, y::AbstractNode)\npredict_mean(mach, X::AbstractNode, y::AbstractNode)\npredict_median(mach, X::AbstractNode, y::AbstractNode)\npredict_mode(mach, X::AbstractNode, y::AbstractNode)\ntransform(mach, X::AbstractNode)\ninverse_transform(mach, X::AbstractNode)\n\nShortcuts for J = node(predict, mach, X, y), etc.\n\nCalling a node is a recursive operation which terminates in the call to a source node (or nodes). Calling nodes on new data X fails unless the number of such nodes is one.\n\nSee also: Node, @node, source, origins.\n\n\n\n\n\n","category":"function"},{"location":"learning_networks/#MLJBase.@node","page":"Learning Networks","title":"MLJBase.@node","text":"@node f(...)\n\nConstruct a new node that applies the function f to some combination of nodes, sources and other arguments.\n\nImportant. An argument not in global scope is assumed to be a node  or source.\n\nExamples\n\njulia> X = source(π)\njulia> W = @node sin(X)\njulia> W()\n0\n\njulia> X = source(1:10)\njulia> Y = @node selectrows(X, 3:4)\njulia> Y()\n3:4\n\njulia> Y([\"one\", \"two\", \"three\", \"four\"])\n2-element Array{Symbol,1}:\n \"three\"\n \"four\"\n\njulia> X1 = source(4)\njulia> X2 = source(5)\njulia> add(a, b, c) = a + b + c\njulia> N = @node add(X1, 1, X2)\njulia> N()\n10\n\n\nSee also node\n\n\n\n\n\n","category":"macro"},{"location":"learning_networks/#MLJBase.prefit","page":"Learning Networks","title":"MLJBase.prefit","text":"MLJBase.prefit(model, verbosity, data...)\n\nReturns a learning network interface (see below) for a learning network with source nodes that wrap data.\n\nA user overloads MLJBase.prefit when exporting a learning network as a new stand-alone model type, of which model above will be an instance. See the MLJ reference manual for details.\n\nA learning network interface is a named tuple declaring certain interface points in  a learning network, to be used when \"exporting\" the network as a new stand-alone model  type. Examples are\n\n (predict=yhat,)\n (transform=Xsmall, acceleration=CPUThreads())\n (predict=yhat, transform=W, report=(loss=loss_node,))\n\nHere yhat, Xsmall, W and loss_node are nodes in the network.\n\nThe keys of the learning network interface always one of the following:\n\nThe name of an operation, such as :predict, :predict_mode, :transform, :inverse_transform. See \"Operation keys\" below.\n:report, for exposing results of calling a node with no arguments in the composite model report. See \"Including report nodes\" below.\n:fitted_params, for exposing results of calling a node with no arguments as fitted parameters of the composite model. See \"Including fitted parameter nodes\" below.\n:acceleration, for articulating acceleration mode for training the network, e.g., CPUThreads(). Corresponding value must be an AbstractResource. If not included, CPU1() is used.\n\nOperation keys\n\nIf the key is an operation, then the value must be a node n in the network with a  unique origin (length(origins(n)) === 1). The intention of a declaration such as  predict=yhat is that the exported model type implements predict, which, when  applied to new data Xnew, should return yhat(Xnew).\n\nIncluding report nodes\n\nIf the key is :report, then the corresponding value must be a named tuple\n\n (k1=n1, k2=n2, ...)\n\nwhose values are all nodes. For each k=n pair, the key k will appear as a key in  the composite model report, with a corresponding value of deepcopy(n()), called  immediatately after training or updating the network.  For examples, refer to the  \"Learning Networks\" section of the MLJ manual.\n\nIncluding fitted parameter nodes\n\nIf the key is :fitted_params, then the behaviour is as for report nodes but results  are exposed as fitted parameters of the composite model instead of the report.\n\n\n\n\n\n","category":"function"},{"location":"models/MultitargetKNNClassifier_NearestNeighborModels/#MultitargetKNNClassifier_NearestNeighborModels","page":"MultitargetKNNClassifier","title":"MultitargetKNNClassifier","text":"MultitargetKNNClassifier\n\nA model type for constructing a multitarget K-nearest neighbor classifier, based on NearestNeighborModels.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nMultitargetKNNClassifier = @load MultitargetKNNClassifier pkg=NearestNeighborModels\n\nDo model = MultitargetKNNClassifier() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in MultitargetKNNClassifier(K=...).\n\nMulti-target K-Nearest Neighbors Classifier (MultitargetKNNClassifier) is a variation of  KNNClassifier that assumes the target variable is vector-valued with Multiclass or OrderedFactor components. (Target data must be presented as a table, however.)","category":"section"},{"location":"models/MultitargetKNNClassifier_NearestNeighborModels/#Training-data","page":"MultitargetKNNClassifier","title":"Training data","text":"In MLJ or MLJBase, bind an instance model to data with\n\nmach = machine(model, X, y)\n\nOR\n\nmach = machine(model, X, y, w)\n\nHere:\n\nX is any table of input features (eg, a DataFrame) whose columns are of scitype Continuous; check column scitypes with schema(X).\nyis the target, which can be any table of responses whose element scitype is either<:Finite(<:Multiclassor<:OrderedFactorwill do); check the columns scitypes withschema(y).  Each column ofy` is assumed to belong to a common categorical pool.\nw is the observation weights which can either be nothing(default) or an  AbstractVector whose element scitype is Count or Continuous. This is different  from weights kernel which is a model hyperparameter, see below.\n\nTrain the machine using fit!(mach, rows=...).","category":"section"},{"location":"models/MultitargetKNNClassifier_NearestNeighborModels/#Hyper-parameters","page":"MultitargetKNNClassifier","title":"Hyper-parameters","text":"K::Int=5 : number of neighbors\nalgorithm::Symbol = :kdtree : one of (:kdtree, :brutetree, :balltree)\nmetric::Metric = Euclidean() : any Metric from    Distances.jl for the    distance between points. For algorithm = :kdtree only metrics which are    instances of Distances.UnionMinkowskiMetric are supported.\nleafsize::Int = algorithm == 10 : determines the number of points    at which to stop splitting the tree. This option is ignored and always taken as 0    for algorithm = :brutetree, since brutetree isn't actually a tree.\nreorder::Bool = true : if true then points which are close in    distance are placed close in memory. In this case, a copy of the original data    will be made so that the original data is left unmodified. Setting this to true    can significantly improve performance of the specified algorithm    (except :brutetree). This option is ignored and always taken as false for    algorithm = :brutetree.\nweights::KNNKernel=Uniform() : kernel used in assigning weights to the    k-nearest neighbors for each observation. An instance of one of the types in    list_kernels(). User-defined weighting functions can be passed by wrapping the    function in a UserDefinedKernel kernel (do ?NearestNeighborModels.UserDefinedKernel for more    info). If observation weights w are passed during machine construction then the    weight assigned to each neighbor vote is the product of the kernel generated    weight for that neighbor and the corresponding observation weight.\noutput_type::Type{<:MultiUnivariateFinite}=DictTable : One of    (ColumnTable, DictTable). The type of table type to use for predictions.   Setting to ColumnTable might improve performance for narrow tables while setting to    DictTable improves performance for wide tables.","category":"section"},{"location":"models/MultitargetKNNClassifier_NearestNeighborModels/#Operations","page":"MultitargetKNNClassifier","title":"Operations","text":"predict(mach, Xnew): Return predictions of the target given features Xnew, which should have same scitype as X above. Predictions are either a ColumnTable or  DictTable of UnivariateFiniteVector columns depending on the value set for the  output_type parameter discussed above. The probabilistic predictions are uncalibrated.\npredict_mode(mach, Xnew): Return the modes of each column of the table of probabilistic  predictions returned above.","category":"section"},{"location":"models/MultitargetKNNClassifier_NearestNeighborModels/#Fitted-parameters","page":"MultitargetKNNClassifier","title":"Fitted parameters","text":"The fields of fitted_params(mach) are:\n\ntree: An instance of either KDTree, BruteTree or BallTree depending on the  value of the algorithm hyperparameter (See hyper-parameters section above).  These are data structures that stores the training data with the view of making  quicker nearest neighbor searches on test data points.","category":"section"},{"location":"models/MultitargetKNNClassifier_NearestNeighborModels/#Examples","page":"MultitargetKNNClassifier","title":"Examples","text":"using MLJ, StableRNGs\n\n## set rng for reproducibility\nrng = StableRNG(10)\n\n## Dataset generation\nn, p = 10, 3\nX = table(randn(rng, n, p)) ## feature table\nfruit, color = categorical([\"apple\", \"orange\"]), categorical([\"blue\", \"green\"])\ny = [(fruit = rand(rng, fruit), color = rand(rng, color)) for _ in 1:n] ## target_table\n## Each column in y has a common categorical pool as expected\nselectcols(y, :fruit) ## categorical array\nselectcols(y, :color) ## categorical array\n\n## Load MultitargetKNNClassifier\nMultitargetKNNClassifier = @load MultitargetKNNClassifier pkg=NearestNeighborModels\n\n## view possible kernels\nNearestNeighborModels.list_kernels()\n\n## MultitargetKNNClassifier instantiation\nmodel = MultitargetKNNClassifier(K=3, weights = NearestNeighborModels.Inverse())\n\n## wrap model and required data in an MLJ machine and fit\nmach = machine(model, X, y) |> fit!\n\n## predict\ny_hat = predict(mach, X)\nlabels = predict_mode(mach, X)\n\n\nSee also KNNClassifier","category":"section"},{"location":"models/TargetEncoder_MLJTransforms/#TargetEncoder_MLJTransforms","page":"TargetEncoder","title":"TargetEncoder","text":"TargetEncoder\n\nA model type for constructing a target encoder, based on MLJTransforms.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nTargetEncoder = @load TargetEncoder pkg=MLJTransforms\n\nDo model = TargetEncoder() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in TargetEncoder(features=...).\n\nTargetEncoder implements target encoding as defined in [1] to encode categorical variables     into continuous ones using statistics from the target variable.","category":"section"},{"location":"models/TargetEncoder_MLJTransforms/#Training-data","page":"TargetEncoder","title":"Training data","text":"In MLJ (or MLJBase) bind an instance model to data with\n\nmach = machine(model, X, y)\n\nHere:\n\nX is any table of input features (eg, a DataFrame). Features to be transformed must  have element scitype Multiclass or OrderedFactor. Use schema(X) to  check scitypes.\ny is the target, which can be any AbstractVector whose element scitype is Continuous or Count for regression problems and Multiclass or OrderedFactor for classification problems; check the scitype with schema(y)\n\nTrain the machine using fit!(mach, rows=...).","category":"section"},{"location":"models/TargetEncoder_MLJTransforms/#Hyper-parameters","page":"TargetEncoder","title":"Hyper-parameters","text":"features=[]: A list of names of categorical features given as symbols to exclude or in clude from encoding, according to the value of ignore, or a single symbol (which is treated as a vector with one symbol), or a callable that returns true for features to be included/excluded.\nignore=true: Whether to exclude or include the features given in features\nordered_factor=false: Whether to encode OrderedFactor or ignore them\nλ: Shrinkage hyperparameter used to mix between posterior and prior statistics as described in [1]\nm: An integer hyperparameter to compute shrinkage as described in [1]. If m=:auto then m will be computed using empirical Bayes estimation as described in [1]","category":"section"},{"location":"models/TargetEncoder_MLJTransforms/#Operations","page":"TargetEncoder","title":"Operations","text":"transform(mach, Xnew): Apply target encoding to selected Multiclass or OrderedFactor features of Xnew specified by hyper-parameters, and return the new table.  Features that are neither Multiclass nor OrderedFactor are always left unchanged.","category":"section"},{"location":"models/TargetEncoder_MLJTransforms/#Fitted-parameters","page":"TargetEncoder","title":"Fitted parameters","text":"The fields of fitted_params(mach) are:\n\ntask: Whether the task is Classification or Regression\ny_statistic_given_feat_level: A dictionary with the necessary statistics to encode each categorical feature. It maps each level in each categorical feature to a statistic computed over the target.","category":"section"},{"location":"models/TargetEncoder_MLJTransforms/#Report","page":"TargetEncoder","title":"Report","text":"The fields of report(mach) are:\n\nencoded_features: The subset of the categorical features of X that were encoded","category":"section"},{"location":"models/TargetEncoder_MLJTransforms/#Examples","page":"TargetEncoder","title":"Examples","text":"using MLJ\n\n## Define categorical features\nA = [\"g\", \"b\", \"g\", \"r\", \"r\",]\nB = [1.0, 2.0, 3.0, 4.0, 5.0,]\nC = [\"f\", \"f\", \"f\", \"m\", \"f\",]\nD = [true, false, true, false, true,]\nE = [1, 2, 3, 4, 5,]\n\n## Define the target variable\ny = [\"c1\", \"c2\", \"c3\", \"c1\", \"c2\",]\n\n## Combine into a named tuple\nX = (A = A, B = B, C = C, D = D, E = E)\n\n## Coerce A, C, D to multiclass and B to continuous and E to ordinal\nX = coerce(X,\n:A => Multiclass,\n:B => Continuous,\n:C => Multiclass,\n:D => Multiclass,\n:E => OrderedFactor,\n)\ny = coerce(y, Multiclass)\n\nencoder = TargetEncoder(ordered_factor = false, lambda = 1.0, m = 0,)\nmach = fit!(machine(encoder, X, y))\nXnew = transform(mach, X)\n\njulia > schema(Xnew)\n┌───────┬──────────────────┬─────────────────────────────────┐\n│ names │ scitypes         │ types                           │\n├───────┼──────────────────┼─────────────────────────────────┤\n│ A_1   │ Continuous       │ Float64                         │\n│ A_2   │ Continuous       │ Float64                         │\n│ A_3   │ Continuous       │ Float64                         │\n│ B     │ Continuous       │ Float64                         │\n│ C_1   │ Continuous       │ Float64                         │\n│ C_2   │ Continuous       │ Float64                         │\n│ C_3   │ Continuous       │ Float64                         │\n│ D_1   │ Continuous       │ Float64                         │\n│ D_2   │ Continuous       │ Float64                         │\n│ D_3   │ Continuous       │ Float64                         │\n│ E     │ OrderedFactor{5} │ CategoricalValue{Int64, UInt32} │\n└───────┴──────────────────┴─────────────────────────────────┘","category":"section"},{"location":"models/TargetEncoder_MLJTransforms/#Reference","page":"TargetEncoder","title":"Reference","text":"[1] Micci-Barreca, Daniele.     “A preprocessing scheme for high-cardinality categorical attributes in classification and prediction problems”     SIGKDD Explor. Newsl. 3, 1 (July 2001), 27–32.\n\nSee also OneHotEncoder","category":"section"},{"location":"models/AdaBoostRegressor_MLJScikitLearnInterface/#AdaBoostRegressor_MLJScikitLearnInterface","page":"AdaBoostRegressor","title":"AdaBoostRegressor","text":"AdaBoostRegressor\n\nA model type for constructing a AdaBoost ensemble regression, based on MLJScikitLearnInterface.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nAdaBoostRegressor = @load AdaBoostRegressor pkg=MLJScikitLearnInterface\n\nDo model = AdaBoostRegressor() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in AdaBoostRegressor(estimator=...).\n\nAn AdaBoost regressor is a meta-estimator that begins by fitting  a regressor on the original dataset and then fits additional  copies of the regressor on the same dataset but where the weights  of instances are adjusted according to the error of the current  prediction. As such, subsequent regressors focus more on difficult  cases.\n\nThis class implements the algorithm known as AdaBoost.R2.","category":"section"},{"location":"models/KMeans_MLJScikitLearnInterface/#KMeans_MLJScikitLearnInterface","page":"KMeans","title":"KMeans","text":"KMeans\n\nA model type for constructing a k means, based on MLJScikitLearnInterface.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nKMeans = @load KMeans pkg=MLJScikitLearnInterface\n\nDo model = KMeans() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in KMeans(n_clusters=...).\n\nK-Means algorithm: find K centroids corresponding to K clusters in the data.","category":"section"},{"location":"models/OrthogonalMatchingPursuitRegressor_MLJScikitLearnInterface/#OrthogonalMatchingPursuitRegressor_MLJScikitLearnInterface","page":"OrthogonalMatchingPursuitRegressor","title":"OrthogonalMatchingPursuitRegressor","text":"OrthogonalMatchingPursuitRegressor\n\nA model type for constructing a orthogonal matching pursuit regressor, based on MLJScikitLearnInterface.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nOrthogonalMatchingPursuitRegressor = @load OrthogonalMatchingPursuitRegressor pkg=MLJScikitLearnInterface\n\nDo model = OrthogonalMatchingPursuitRegressor() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in OrthogonalMatchingPursuitRegressor(n_nonzero_coefs=...).","category":"section"},{"location":"models/OrthogonalMatchingPursuitRegressor_MLJScikitLearnInterface/#Hyper-parameters","page":"OrthogonalMatchingPursuitRegressor","title":"Hyper-parameters","text":"n_nonzero_coefs = nothing\ntol = nothing\nfit_intercept = true\nprecompute = auto","category":"section"},{"location":"learning_curves/#Learning-Curves","page":"Learning Curves","title":"Learning Curves","text":"A learning curve in MLJ is a plot of some performance estimate, as a function of some model hyperparameter. This can be useful when tuning a single model hyperparameter, or when deciding how many iterations are required for some iterative model. The learning_curve method does not actually generate a plot but generates the data needed to do so.\n\nTo generate learning curves you can bind data to a model by instantiating a machine. You can choose to supply all available data, as performance estimates are computed using a resampling strategy, defaulting to Holdout(fraction_train=0.7).\n\nusing MLJ\nX, y = @load_boston;\n\natom = (@load RidgeRegressor pkg=MLJLinearModels)()\nensemble = EnsembleModel(model=atom, n=1000)\nmach = machine(ensemble, X, y)\n\nr_lambda = range(ensemble, :(model.lambda), lower=1e-1, upper=100, scale=:log10)\ncurve = MLJ.learning_curve(mach;\n                           range=r_lambda,\n                           resampling=CV(nfolds=3),\n                           measure=l1)\n\nusing Plots\nplot(curve.parameter_values,\n     curve.measurements,\n     xlab=curve.parameter_name,\n     xscale=curve.parameter_scale,\n     ylab = \"CV estimate of RMS error\")\n\n(Image: )\n\nIf the range hyperparameter is the number of iterations in some iterative model, learning_curve will not restart the training from scratch for each new value, unless a non-holdout resampling strategy is specified (and provided the model implements an appropriate update method). To obtain multiple curves (that are distinct) you will need to pass the name of the model random number generator, rng_name, and specify the random number generators to be used using rngs=... (an integer automatically generates the number specified):\n\natom.lambda = 7.3\nr_n = range(ensemble, :n, lower=1, upper=50)\ncurves = MLJ.learning_curve(mach;\n                            range=r_n,\n                            measure=l1,\n                            verbosity=0,\n                            rng_name=:rng,\n                            rngs=4)\n\nplot(curves.parameter_values,\n     curves.measurements,\n     xlab=curves.parameter_name,\n     ylab=\"Holdout estimate of RMS error\")\n\n(Image: )","category":"section"},{"location":"learning_curves/#API-reference","page":"Learning Curves","title":"API reference","text":"","category":"section"},{"location":"learning_curves/#MLJTuning.learning_curve","page":"Learning Curves","title":"MLJTuning.learning_curve","text":"curve = learning_curve(mach; resolution=30,\n                             resampling=Holdout(),\n                             repeats=1,\n                             measure=default_measure(machine.model),\n                             rows=nothing,\n                             weights=nothing,\n                             operation=nothing,\n                             range=nothing,\n                             acceleration=default_resource(),\n                             acceleration_grid=CPU1(),\n                             rngs=nothing,\n                             rng_name=nothing)\n\nGiven a supervised machine mach, returns a named tuple of objects suitable for generating a plot of performance estimates, as a function of the single hyperparameter specified in range. The tuple curve has the following keys: :parameter_name, :parameter_scale, :parameter_values, :measurements.\n\nTo generate multiple curves for a model with a random number generator (RNG) as a hyperparameter, specify the name, rng_name, of the (possibly nested) RNG field, and a vector rngs of RNG's, one for each curve. Alternatively, set rngs to the number of curves desired, in which case RNG's are automatically generated. The individual curve computations can be distributed across multiple processes using acceleration=CPUProcesses() or acceleration=CPUThreads(). See the second example below for a demonstration.\n\nX, y = @load_boston;\natom = @load RidgeRegressor pkg=MultivariateStats\nensemble = EnsembleModel(atom=atom, n=1000)\nmach = machine(ensemble, X, y)\nr_lambda = range(ensemble, :(atom.lambda), lower=10, upper=500, scale=:log10)\ncurve = learning_curve(mach; range=r_lambda, resampling=CV(), measure=mav)\nusing Plots\nplot(curve.parameter_values,\n     curve.measurements,\n     xlab=curve.parameter_name,\n     xscale=curve.parameter_scale,\n     ylab = \"CV estimate of RMS error\")\n\nIf using a Holdout() resampling strategy (with no shuffling) and if the specified hyperparameter is the number of iterations in some iterative model (and that model has an appropriately overloaded MLJModelInterface.update method) then training is not restarted from scratch for each increment of the parameter, ie the model is trained progressively.\n\natom.lambda=200\nr_n = range(ensemble, :n, lower=1, upper=250)\ncurves = learning_curve(mach; range=r_n, verbosity=0, rng_name=:rng, rngs=3)\nplot!(curves.parameter_values,\n     curves.measurements,\n     xlab=curves.parameter_name,\n     ylab=\"Holdout estimate of RMS error\")\n\n\n\nlearning_curve(model::Supervised, X, y; kwargs...)\nlearning_curve(model::Supervised, X, y, w; kwargs...)\n\nPlot a learning curve (or curves) directly, without first constructing a machine.\n\nSummary of key-word options\n\nresolution - number of points generated from range (number model evaluations); default is 30\nacceleration - parallelization option for passing to evaluate!; an instance of CPU1, CPUProcesses or CPUThreads from the ComputationalResources.jl; default is default_resource()\nacceleration_grid - parallelization option for distributing each performancde evaluation\nrngs - for specifying random number generator(s) to be passed to the model (see above)\nrng_name - name of the model hyper-parameter representing a random number generator (see above); possibly nested\n\nOther key-word options are documented at TunedModel.\n\n\n\n\n\n","category":"function"},{"location":"models/EvoLinearRegressor_EvoLinear/#EvoLinearRegressor_EvoLinear","page":"EvoLinearRegressor","title":"EvoLinearRegressor","text":"EvoLinearRegressor(; kwargs...)\n\nA model type for constructing a EvoLinearRegressor, based on EvoLinear.jl, and implementing both an internal API and the MLJ model interface.","category":"section"},{"location":"models/EvoLinearRegressor_EvoLinear/#Keyword-arguments","page":"EvoLinearRegressor","title":"Keyword arguments","text":"loss=:mse: loss function to be minimised.    Can be one of:\n:mse\n:logistic\n:poisson\n:gamma\n:tweedie\nnrounds=10: maximum number of training rounds.\neta=1: Learning rate. Typically in the range [1e-2, 1].\nL1=0: Regularization penalty applied by shrinking to 0 weight update if update is < L1. No penalty if update > L1. Results in sparse feature selection. Typically in the [0, 1] range on normalized features.\nL2=0: Regularization penalty applied to the squared of the weight update value. Restricts large parameter values. Typically in the [0, 1] range on normalized features.\nrng=123: random seed. Not used at the moment.\nupdater=:all: training method. Only :all is supported at the moment. Gradients for each feature are computed simultaneously, then bias is updated based on all features update.\ndevice=:cpu: Only :cpu is supported at the moment.","category":"section"},{"location":"models/EvoLinearRegressor_EvoLinear/#Internal-API","page":"EvoLinearRegressor","title":"Internal API","text":"Do config = EvoLinearRegressor() to construct an hyper-parameter struct with default hyper-parameters. Provide keyword arguments as listed above to override defaults, for example:\n\nEvoLinearRegressor(loss=:logistic, L1=1e-3, L2=1e-2, nrounds=100)","category":"section"},{"location":"models/EvoLinearRegressor_EvoLinear/#Training-model","page":"EvoLinearRegressor","title":"Training model","text":"A model is built using fit:\n\nconfig = EvoLinearRegressor()\nm = fit(config; x, y, w)","category":"section"},{"location":"models/EvoLinearRegressor_EvoLinear/#Inference","page":"EvoLinearRegressor","title":"Inference","text":"Fitted results is an EvoLinearModel which acts as a prediction function when passed a features matrix as argument.  \n\npreds = m(x)","category":"section"},{"location":"models/EvoLinearRegressor_EvoLinear/#MLJ-Interface","page":"EvoLinearRegressor","title":"MLJ Interface","text":"From MLJ, the type can be imported using:\n\nEvoLinearRegressor = @load EvoLinearRegressor pkg=EvoLinear\n\nDo model = EvoLinearRegressor() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in EvoLinearRegressor(loss=...).","category":"section"},{"location":"models/EvoLinearRegressor_EvoLinear/#Training-model-2","page":"EvoLinearRegressor","title":"Training model","text":"In MLJ or MLJBase, bind an instance model to data with mach = machine(model, X, y) where: \n\nX: any table of input features (eg, a DataFrame) whose columns each have one of the following element scitypes: Continuous, Count, or <:OrderedFactor; check column scitypes with schema(X)\ny: is the target, which can be any AbstractVector whose element scitype is <:Continuous; check the scitype with scitype(y)\n\nTrain the machine using fit!(mach, rows=...).","category":"section"},{"location":"models/EvoLinearRegressor_EvoLinear/#Operations","page":"EvoLinearRegressor","title":"Operations","text":"predict(mach, Xnew): return predictions of the target given\n\nfeatures Xnew having the same scitype as X above. Predictions   are deterministic.","category":"section"},{"location":"models/EvoLinearRegressor_EvoLinear/#Fitted-parameters","page":"EvoLinearRegressor","title":"Fitted parameters","text":"The fields of fitted_params(mach) are:\n\n:fitresult: the EvoLinearModel object returned by EvoLnear.jl fitting algorithm.","category":"section"},{"location":"models/EvoLinearRegressor_EvoLinear/#Report","page":"EvoLinearRegressor","title":"Report","text":"The fields of report(mach) are:\n\n:coef: Vector of coefficients (βs) associated to each of the features.\n:bias: Value of the bias.\n:names: Names of each of the features.","category":"section"},{"location":"models/KernelPerceptronClassifier_BetaML/#KernelPerceptronClassifier_BetaML","page":"KernelPerceptronClassifier","title":"KernelPerceptronClassifier","text":"mutable struct KernelPerceptronClassifier <: MLJModelInterface.Probabilistic\n\nThe kernel perceptron algorithm using one-vs-one for multiclass, from the Beta Machine Learning Toolkit (BetaML).","category":"section"},{"location":"models/KernelPerceptronClassifier_BetaML/#Hyperparameters:","page":"KernelPerceptronClassifier","title":"Hyperparameters:","text":"kernel::Function: Kernel function to employ. See ?radial_kernel or ?polynomial_kernel (once loaded the BetaML package) for details or check ?BetaML.Utils to verify if other kernels are defined (you can alsways define your own kernel) [def: radial_kernel]\nepochs::Int64: Maximum number of epochs, i.e. passages trough the whole training sample [def: 100]\ninitial_errors::Union{Nothing, Vector{Vector{Int64}}}: Initial distribution of the number of errors errors [def: nothing, i.e. zeros]. If provided, this should be a nModels-lenght vector of nRecords integer values vectors , where nModels is computed as (n_classes  * (n_classes - 1)) / 2\nshuffle::Bool: Whether to randomly shuffle the data at each iteration (epoch) [def: true]\nrng::Random.AbstractRNG: A Random Number Generator to be used in stochastic parts of the code [deafult: Random.GLOBAL_RNG]","category":"section"},{"location":"models/KernelPerceptronClassifier_BetaML/#Example:","page":"KernelPerceptronClassifier","title":"Example:","text":"julia> using MLJ\n\njulia> X, y        = @load_iris;\n\njulia> modelType   = @load KernelPerceptronClassifier pkg = \"BetaML\"\n[ Info: For silent loading, specify `verbosity=0`. \nimport BetaML ✔\nBetaML.Perceptron.KernelPerceptronClassifier\n\njulia> model       = modelType()\nKernelPerceptronClassifier(\n  kernel = BetaML.Utils.radial_kernel, \n  epochs = 100, \n  initial_errors = nothing, \n  shuffle = true, \n  rng = Random._GLOBAL_RNG())\n\njulia> mach        = machine(model, X, y);\n\njulia> fit!(mach);\n\njulia> est_classes = predict(mach, X)\n150-element CategoricalDistributions.UnivariateFiniteVector{Multiclass{3}, String, UInt8, Float64}:\n UnivariateFinite{Multiclass{3}}(setosa=>0.665, versicolor=>0.245, virginica=>0.09)\n UnivariateFinite{Multiclass{3}}(setosa=>0.665, versicolor=>0.245, virginica=>0.09)\n ⋮\n UnivariateFinite{Multiclass{3}}(setosa=>0.09, versicolor=>0.245, virginica=>0.665)\n UnivariateFinite{Multiclass{3}}(setosa=>0.09, versicolor=>0.665, virginica=>0.245)","category":"section"},{"location":"model_search/#model_search","page":"Model Search","title":"Model Search","text":"In addition to perusing the Model Browser, one can programatically search MLJ's Model Registry, without actually loading all the packages providing model code. This allows you to efficiently find all models solving a given machine learning task. The task itself is specified with the help of the matching method, and the search executed with the models methods, as detailed below.\n\nFor commonly encountered problems with model search, see also Preparing Data.\n\nA table of all models is also given at List of Supported Models.","category":"section"},{"location":"model_search/#Model-metadata","page":"Model Search","title":"Model metadata","text":"Terminology. In this section the word \"model\" refers to a metadata entry in the model registry, as opposed to an actual model struct that such an entry represents. One can obtain such an entry with the info command:\n\ninfo(\"PCA\")\n\nSo a \"model\" in the present context is just a named tuple containing metadata, and not an actual model type or instance. If two models with the same name occur in different packages, the package name must be specified, as in info(\"LinearRegressor\", pkg=\"GLM\").\n\nModel document strings can be retreived, without importing the defining code, using the doc function:\n\ndoc(\"DecisionTreeClassifier\", pkg=\"DecisionTree\")","category":"section"},{"location":"model_search/#General-model-queries","page":"Model Search","title":"General model queries","text":"We list all models (named tuples) using models(), and list the models for which code is  already loaded with localmodels():\n\nlocalmodels()\nlocalmodels()[2]\n\nOne can search for models containing specified strings or regular expressions in their docstring attributes, as in\n\nmodels(\"forest\")\n\nor by specifying a filter (Bool-valued function):\n\nfilter(model) = model.is_supervised &&\n                model.input_scitype >: MLJ.Table(Continuous) &&\n                model.target_scitype >: AbstractVector{<:Multiclass{3}} &&\n                model.prediction_type == :deterministic\nmodels(filter)\n\nMultiple test arguments may be passed to models, which are applied conjunctively.","category":"section"},{"location":"model_search/#Matching-models-to-data","page":"Model Search","title":"Matching models to data","text":"Common searches are streamlined with the help of the matching command, defined as follows:\n\nmatching(model, X, y) == true exactly when model is supervised  and admits inputs and targets with the scientific types of X and  y, respectively\nmatching(model, X) == true exactly when model is unsupervised  and admits inputs with the scientific types of X.\n\nSo, to search for all supervised probabilistic models handling input X and target y, one can define the testing function task by\n\ntask(model) = matching(model, X, y) && model.prediction_type == :probabilistic\n\nAnd execute the search with\n\nmodels(task)\n\nAlso defined are Bool-valued callable objects matching(model), matching(X, y) and matching(X), with obvious behavior. For example, matching(X, y)(model) = matching(model, X, y).\n\nSo, to search for all models compatible with input X and target y, for example, one executes\n\nmodels(matching(X, y))\n\nwhile the preceding search can also be written\n\nmodels() do model\n    matching(model, X, y) &&\n    model.prediction_type == :probabilistic\nend","category":"section"},{"location":"model_search/#API","page":"Model Search","title":"API","text":"","category":"section"},{"location":"model_search/#MLJModels.models","page":"Model Search","title":"MLJModels.models","text":"models(; wrappers=false)\n\nList all models in the MLJ registry. Here and below model means the registry metadata entry for a genuine model type (a proxy for types whose defining code may not be loaded). To include wrappers and other composite models, such as TunedModel and Stack, specify wrappers=true.\n\nmodels(filters...; wrappers=false)\n\nList all models m for which filter(m) is true, for each filter in filters.\n\nmodels(matching(X, y); wrappers=false)\n\nList all supervised models compatible with training data X, y.\n\nmodels(matching(X); wrappers=false)\n\nList all unsupervised models compatible with training data X.\n\nExample\n\nIf\n\ntask(model) = model.is_supervised && model.is_probabilistic\n\nthen models(task) lists all supervised models making probabilistic predictions.\n\nSee also: localmodels.\n\n\n\n\n\nmodels(needle::Union{AbstractString,Regex}; wrappers=false)\n\nList all models whole name or docstring matches a given needle.\n\n\n\n\n\n","category":"function"},{"location":"model_search/#MLJModels.localmodels","page":"Model Search","title":"MLJModels.localmodels","text":"localmodels(; modl=Main, wrappers=false)\nlocalmodels(filters...; modl=Main, wrappers=false)\nlocalmodels(needle::Union{AbstractString,Regex}; modl=Main, wrappers=false)\n\nList all models currently available to the user from the module modl without importing a package, and which additional pass through the specified filters. Here a filter is a Bool-valued function on models.\n\nUse load_path to get the path to some model returned, as in these examples:\n\nms = localmodels()\nmodel = ms[1]\nload_path(model)\n\nSee also models, load_path.\n\n\n\n\n\n","category":"function"},{"location":"models/HistGradientBoostingClassifier_MLJScikitLearnInterface/#HistGradientBoostingClassifier_MLJScikitLearnInterface","page":"HistGradientBoostingClassifier","title":"HistGradientBoostingClassifier","text":"HistGradientBoostingClassifier\n\nA model type for constructing a hist gradient boosting classifier, based on MLJScikitLearnInterface.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nHistGradientBoostingClassifier = @load HistGradientBoostingClassifier pkg=MLJScikitLearnInterface\n\nDo model = HistGradientBoostingClassifier() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in HistGradientBoostingClassifier(loss=...).\n\nThis algorithm builds an additive model in a forward stage-wise fashion;  it allows for the optimization of arbitrary differentiable loss functions.  In each stage n_classes_ regression trees are fit on the negative gradient  of the loss function, e.g. binary or multiclass log loss. Binary  classification is a special case where only a single regression tree is induced.\n\nHistGradientBoostingClassifier is a much faster variant of this  algorithm for intermediate datasets (n_samples >= 10_000).","category":"section"},{"location":"models/LinearBinaryClassifier_GLM/#LinearBinaryClassifier_GLM","page":"LinearBinaryClassifier","title":"LinearBinaryClassifier","text":"LinearBinaryClassifier\n\nA model type for constructing a linear binary classifier, based on GLM.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nLinearBinaryClassifier = @load LinearBinaryClassifier pkg=GLM\n\nDo model = LinearBinaryClassifier() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in LinearBinaryClassifier(fit_intercept=...).\n\nLinearBinaryClassifier is a generalized linear model, specialised to the case of a binary target variable, with a user-specified link function. Options exist to specify an intercept or offset feature.","category":"section"},{"location":"models/LinearBinaryClassifier_GLM/#Training-data","page":"LinearBinaryClassifier","title":"Training data","text":"In MLJ or MLJBase, bind an instance model to data with one of:\n\nmach = machine(model, X, y)\nmach = machine(model, X, y, w)\n\nHere\n\nX: is any table of input features (eg, a DataFrame) whose columns are of scitype Continuous; check the scitype with schema(X)\ny: is the target, which can be any AbstractVector whose element scitype is <:OrderedFactor(2) or <:Multiclass(2); check the scitype with schema(y)\nw: is a vector of Real per-observation weights\n\nTrain the machine using fit!(mach, rows=...).","category":"section"},{"location":"models/LinearBinaryClassifier_GLM/#Hyper-parameters","page":"LinearBinaryClassifier","title":"Hyper-parameters","text":"fit_intercept=true: Whether to calculate the intercept for this model.  If set to false,  no intercept will be calculated (e.g. the data is expected to be centered)\nlink=GLM.LogitLink: The function which links the linear prediction function to the  probability of a particular outcome or class. This must have type GLM.Link01. Options  include GLM.LogitLink(), GLM.ProbitLink(), CloglogLink(),CauchitLink()`.\noffsetcol=nothing: Name of the column to be used as an offset, if any.  An offset is a  variable which is known to have a coefficient of 1.\nmaxiter::Integer=30: The maximum number of iterations allowed to achieve convergence.\natol::Real=1e-6: Absolute threshold for convergence. Convergence is achieved when the  relative change in deviance is less than `max(rtol*dev, atol). This term exists to avoid  failure when deviance is unchanged except for rounding errors.\nrtol::Real=1e-6: Relative threshold for convergence. Convergence is achieved when the  relative change in deviance is less than `max(rtol*dev, atol). This term exists to avoid  failure when deviance is unchanged except for rounding errors.\nminstepfac::Real=0.001: Minimum step fraction. Must be between 0 and 1. Lower bound for the factor used to update the linear fit.\nreport_keys: Vector of keys for the report. Possible keys are: :deviance, :dof_residual, :stderror, :vcov, :coef_table and :glm_model. By default only :glm_model is excluded.","category":"section"},{"location":"models/LinearBinaryClassifier_GLM/#Operations","page":"LinearBinaryClassifier","title":"Operations","text":"predict(mach, Xnew): Return predictions of the target given features Xnew having the same scitype as X above. Predictions are probabilistic.\npredict_mode(mach, Xnew): Return the modes of the probabilistic predictions returned  above.","category":"section"},{"location":"models/LinearBinaryClassifier_GLM/#Fitted-parameters","page":"LinearBinaryClassifier","title":"Fitted parameters","text":"The fields of fitted_params(mach) are:\n\nfeatures: The names of the features used during model fitting.\ncoef: The linear coefficients determined by the model.\nintercept: The intercept determined by the model.","category":"section"},{"location":"models/LinearBinaryClassifier_GLM/#Report","page":"LinearBinaryClassifier","title":"Report","text":"The fields of report(mach) are:\n\ndeviance: Measure of deviance of fitted model with respect to a perfectly fitted model. For a linear model, this is the weighted residual sum of squares\ndof_residual: The degrees of freedom for residuals, when meaningful.\nstderror: The standard errors of the coefficients.\nvcov: The estimated variance-covariance matrix of the coefficient estimates.\ncoef_table: Table which displays coefficients and summarizes their significance and confidence intervals.\nglm_model: The raw fitted model returned by GLM.lm. Note this points to training data. Refer to the GLM.jl documentation for usage.","category":"section"},{"location":"models/LinearBinaryClassifier_GLM/#Examples","page":"LinearBinaryClassifier","title":"Examples","text":"using MLJ\nimport GLM ## namespace must be available\n\nLinearBinaryClassifier = @load LinearBinaryClassifier pkg=GLM\nclf = LinearBinaryClassifier(fit_intercept=false, link=GLM.ProbitLink())\n\nX, y = @load_crabs\n\nmach = machine(clf, X, y) |> fit!\n\nXnew = (;FL = [8.1, 24.8, 7.2],\n        RW = [5.1, 25.7, 6.4],\n        CL = [15.9, 46.7, 14.3],\n        CW = [18.7, 59.7, 12.2],\n        BD = [6.2, 23.6, 8.4],)\n\nyhat = predict(mach, Xnew) ## probabilistic predictions\npdf(yhat, levels(y)) ## probability matrix\np_B = pdf.(yhat, \"B\")\nclass_labels = predict_mode(mach, Xnew)\n\nfitted_params(mach).features\nfitted_params(mach).coef\nfitted_params(mach).intercept\n\nreport(mach)\n\nSee also LinearRegressor, LinearCountRegressor","category":"section"},{"location":"models/SOSDetector_OutlierDetectionPython/#SOSDetector_OutlierDetectionPython","page":"SOSDetector","title":"SOSDetector","text":"SOSDetector(perplexity = 4.5,\n               metric = \"minkowski\",\n               eps = 1e-5)\n\nhttps://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.sos","category":"section"},{"location":"models/BayesianQDA_MLJScikitLearnInterface/#BayesianQDA_MLJScikitLearnInterface","page":"BayesianQDA","title":"BayesianQDA","text":"BayesianQDA\n\nA model type for constructing a Bayesian quadratic discriminant analysis, based on MLJScikitLearnInterface.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nBayesianQDA = @load BayesianQDA pkg=MLJScikitLearnInterface\n\nDo model = BayesianQDA() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in BayesianQDA(priors=...).","category":"section"},{"location":"models/BayesianQDA_MLJScikitLearnInterface/#Hyper-parameters","page":"BayesianQDA","title":"Hyper-parameters","text":"priors = nothing\nreg_param = 0.0\nstore_covariance = false\ntol = 0.0001","category":"section"},{"location":"models/XGBoostClassifier_XGBoost/#XGBoostClassifier_XGBoost","page":"XGBoostClassifier","title":"XGBoostClassifier","text":"XGBoostClassifier\n\nA model type for constructing a eXtreme Gradient Boosting Classifier, based on XGBoost.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nXGBoostClassifier = @load XGBoostClassifier pkg=XGBoost\n\nDo model = XGBoostClassifier() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in XGBoostClassifier(test=...).\n\nUnivariate classification using xgboost.","category":"section"},{"location":"models/XGBoostClassifier_XGBoost/#Training-data","page":"XGBoostClassifier","title":"Training data","text":"In MLJ or MLJBase, bind an instance model to data with\n\nm = machine(model, X, y)\n\nwhere\n\nX: any table of input features, either an AbstractMatrix or Tables.jl-compatible table.\ny: is an AbstractVector Finite target.\n\nTrain using fit!(m, rows=...).","category":"section"},{"location":"models/XGBoostClassifier_XGBoost/#Hyper-parameters","page":"XGBoostClassifier","title":"Hyper-parameters","text":"See https://xgboost.readthedocs.io/en/stable/parameter.html.","category":"section"},{"location":"models/LODADetector_OutlierDetectionPython/#LODADetector_OutlierDetectionPython","page":"LODADetector","title":"LODADetector","text":"LODADetector(n_bins = 10,\n                n_random_cuts = 100)\n\nhttps://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.loda","category":"section"},{"location":"models/RandomOversampler_Imbalance/#RandomOversampler_Imbalance","page":"RandomOversampler","title":"RandomOversampler","text":"Initiate a random oversampling model with the given hyper-parameters.\n\nRandomOversampler\n\nA model type for constructing a random oversampler, based on Imbalance.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nRandomOversampler = @load RandomOversampler pkg=Imbalance\n\nDo model = RandomOversampler() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in RandomOversampler(ratios=...).\n\nRandomOversampler implements naive oversampling by repeating existing observations with replacement.","category":"section"},{"location":"models/RandomOversampler_Imbalance/#Training-data","page":"RandomOversampler","title":"Training data","text":"In MLJ or MLJBase, wrap the model in a machine by     mach = machine(model)\n\nThere is no need to provide any data here because the model is a static transformer.\n\nLikewise, there is no need to fit!(mach). \n\nFor default values of the hyper-parameters, model can be constructed by     model = RandomOverSampler()","category":"section"},{"location":"models/RandomOversampler_Imbalance/#Hyperparameters","page":"RandomOversampler","title":"Hyperparameters","text":"ratios=1.0: A parameter that controls the amount of oversampling to be done for each class\nCan be a float and in this case each class will be oversampled to the size of the majority class times the float. By default, all classes are oversampled to the size of the majority class\nCan be a dictionary mapping each class label to the float ratio for that class\nrng::Union{AbstractRNG, Integer}=default_rng(): Either an AbstractRNG object or an Integer    seed to be used with Xoshiro if the Julia VERSION supports it. Otherwise, uses MersenneTwister`.","category":"section"},{"location":"models/RandomOversampler_Imbalance/#Transform-Inputs","page":"RandomOversampler","title":"Transform Inputs","text":"X: A matrix of real numbers or a table with element scitypes that subtype Union{Finite, Infinite}.     Elements in nominal columns should subtype Finite (i.e., have scitype OrderedFactor or Multiclass) and    elements in continuous columns should subtype Infinite (i.e., have scitype Count or Continuous).\ny: An abstract vector of labels (e.g., strings) that correspond to the observations in X","category":"section"},{"location":"models/RandomOversampler_Imbalance/#Transform-Outputs","page":"RandomOversampler","title":"Transform Outputs","text":"Xover: A matrix or table that includes original data and the new observations    due to oversampling. depending on whether the input X is a matrix or table respectively\nyover: An abstract vector of labels corresponding to Xover","category":"section"},{"location":"models/RandomOversampler_Imbalance/#Operations","page":"RandomOversampler","title":"Operations","text":"transform(mach, X, y): resample the data X and y using RandomOversampler, returning both the new and original observations","category":"section"},{"location":"models/RandomOversampler_Imbalance/#Example","page":"RandomOversampler","title":"Example","text":"using MLJ\nimport Imbalance\n\n## set probability of each class\nclass_probs = [0.5, 0.2, 0.3]                         \nnum_rows, num_continuous_feats = 100, 5\n## generate a table and categorical vector accordingly\nX, y = Imbalance.generate_imbalanced_data(num_rows, num_continuous_feats; \n                                class_probs, rng=42)    \n\njulia> Imbalance.checkbalance(y)\n1: ▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 19 (39.6%) \n2: ▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 33 (68.8%) \n0: ▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 48 (100.0%) \n\n## load RandomOversampler\nRandomOversampler = @load RandomOversampler pkg=Imbalance\n\n## wrap the model in a machine\noversampler = RandomOversampler(ratios=Dict(0=>1.0, 1=> 0.9, 2=>0.8), rng=42)\nmach = machine(oversampler)\n\n## provide the data to transform (there is nothing to fit)\nXover, yover = transform(mach, X, y)\n\njulia> Imbalance.checkbalance(yover)\n2: ▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 38 (79.2%) \n1: ▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 43 (89.6%) \n0: ▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 48 (100.0%) ","category":"section"},{"location":"models/DummyRegressor_MLJScikitLearnInterface/#DummyRegressor_MLJScikitLearnInterface","page":"DummyRegressor","title":"DummyRegressor","text":"DummyRegressor\n\nA model type for constructing a dummy regressor, based on MLJScikitLearnInterface.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nDummyRegressor = @load DummyRegressor pkg=MLJScikitLearnInterface\n\nDo model = DummyRegressor() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in DummyRegressor(strategy=...).\n\nDummyRegressor is a regressor that makes predictions using simple rules.","category":"section"},{"location":"models/PegasosClassifier_BetaML/#PegasosClassifier_BetaML","page":"PegasosClassifier","title":"PegasosClassifier","text":"mutable struct PegasosClassifier <: MLJModelInterface.Probabilistic\n\nThe gradient-based linear \"pegasos\" classifier using one-vs-all for multiclass, from the Beta Machine Learning Toolkit (BetaML).","category":"section"},{"location":"models/PegasosClassifier_BetaML/#Hyperparameters:","page":"PegasosClassifier","title":"Hyperparameters:","text":"initial_coefficients::Union{Nothing, Matrix{Float64}}: N-classes by D-dimensions matrix of initial linear coefficients [def: nothing, i.e. zeros]\ninitial_constant::Union{Nothing, Vector{Float64}}: N-classes vector of initial contant terms [def: nothing, i.e. zeros]\nlearning_rate::Function: Learning rate [def: (epoch -> 1/sqrt(epoch))]\nlearning_rate_multiplicative::Float64: Multiplicative term of the learning rate [def: 0.5]\nepochs::Int64: Maximum number of epochs, i.e. passages trough the whole training sample [def: 1000]\nshuffle::Bool: Whether to randomly shuffle the data at each iteration (epoch) [def: true]\nforce_origin::Bool: Whether to force the parameter associated with the constant term to remain zero [def: false]\nreturn_mean_hyperplane::Bool: Whether to return the average hyperplane coefficients instead of the final ones  [def: false]\nrng::Random.AbstractRNG: A Random Number Generator to be used in stochastic parts of the code [deafult: Random.GLOBAL_RNG]","category":"section"},{"location":"models/PegasosClassifier_BetaML/#Example:","page":"PegasosClassifier","title":"Example:","text":"julia> using MLJ\n\njulia> X, y        = @load_iris;\n\njulia> modelType   = @load PegasosClassifier pkg = \"BetaML\" verbosity=0\nBetaML.Perceptron.PegasosClassifier\n\njulia> model       = modelType()\nPegasosClassifier(\n  initial_coefficients = nothing, \n  initial_constant = nothing, \n  learning_rate = BetaML.Perceptron.var\"#71#73\"(), \n  learning_rate_multiplicative = 0.5, \n  epochs = 1000, \n  shuffle = true, \n  force_origin = false, \n  return_mean_hyperplane = false, \n  rng = Random._GLOBAL_RNG())\n\njulia> mach        = machine(model, X, y);\n\njulia> fit!(mach);\n\njulia> est_classes = predict(mach, X)\n150-element CategoricalDistributions.UnivariateFiniteVector{Multiclass{3}, String, UInt8, Float64}:\n UnivariateFinite{Multiclass{3}}(setosa=>0.817, versicolor=>0.153, virginica=>0.0301)\n UnivariateFinite{Multiclass{3}}(setosa=>0.791, versicolor=>0.177, virginica=>0.0318)\n ⋮\n UnivariateFinite{Multiclass{3}}(setosa=>0.254, versicolor=>0.5, virginica=>0.246)\n UnivariateFinite{Multiclass{3}}(setosa=>0.283, versicolor=>0.51, virginica=>0.207)","category":"section"},{"location":"models/TheilSenRegressor_MLJScikitLearnInterface/#TheilSenRegressor_MLJScikitLearnInterface","page":"TheilSenRegressor","title":"TheilSenRegressor","text":"TheilSenRegressor\n\nA model type for constructing a Theil-Sen regressor, based on MLJScikitLearnInterface.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nTheilSenRegressor = @load TheilSenRegressor pkg=MLJScikitLearnInterface\n\nDo model = TheilSenRegressor() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in TheilSenRegressor(fit_intercept=...).","category":"section"},{"location":"models/TheilSenRegressor_MLJScikitLearnInterface/#Hyper-parameters","page":"TheilSenRegressor","title":"Hyper-parameters","text":"fit_intercept = true\ncopy_X = true\nmax_subpopulation = 10000\nn_subsamples = nothing\nmax_iter = 300\ntol = 0.001\nrandom_state = nothing\nn_jobs = nothing\nverbose = false","category":"section"},{"location":"models/MultiTaskLassoCVRegressor_MLJScikitLearnInterface/#MultiTaskLassoCVRegressor_MLJScikitLearnInterface","page":"MultiTaskLassoCVRegressor","title":"MultiTaskLassoCVRegressor","text":"MultiTaskLassoCVRegressor\n\nA model type for constructing a multi-target lasso regressor with built-in cross-validation, based on MLJScikitLearnInterface.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nMultiTaskLassoCVRegressor = @load MultiTaskLassoCVRegressor pkg=MLJScikitLearnInterface\n\nDo model = MultiTaskLassoCVRegressor() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in MultiTaskLassoCVRegressor(eps=...).","category":"section"},{"location":"models/MultiTaskLassoCVRegressor_MLJScikitLearnInterface/#Hyper-parameters","page":"MultiTaskLassoCVRegressor","title":"Hyper-parameters","text":"eps = 0.001\nn_alphas = 100\nalphas = nothing\nfit_intercept = true\nmax_iter = 300\ntol = 0.0001\ncopy_X = true\ncv = 5\nverbose = false\nn_jobs = 1\nrandom_state = nothing\nselection = cyclic","category":"section"},{"location":"evaluating_model_performance/#Evaluating-Model-Performance","page":"Evaluating Model Performance","title":"Evaluating Model Performance","text":"MLJ allows quick evaluation of a supervised model's performance against a battery of selected losses or scores, using the evaluate or evaluate! methods. For more on available performance measures, see Performance Measures.\n\nIn addition to hold-out and cross-validation, the user can specify an explicit list of train/test pairs of row indices for resampling, or define new resampling strategies.\n\nFor simultaneously evaluating multiple models, see \"Comparing models of different type and nested cross-validation\".\n\nFor externally logging the outcomes of performance evaluation experiments, see Logging Workflows","category":"section"},{"location":"evaluating_model_performance/#Evaluating-against-a-single-measure","page":"Evaluating Model Performance","title":"Evaluating against a single measure","text":"using MLJ\nX = (a=rand(12), b=rand(12), c=rand(12));\ny = X.a + 2X.b + 0.05*rand(12);\nmodel = (@load RidgeRegressor pkg=MultivariateStats verbosity=0)()\ncv = CV(nfolds=3)\nevaluate(model, X, y, resampling=cv, measure=l2, verbosity=0)\n\nAlternatively, instead of applying evaluate to a model + data, one may call evaluate! on an existing machine wrapping the model in data:\n\nmach = machine(model, X, y)\nevaluate!(mach, resampling=cv, measure=l2, verbosity=0)\n\n(The latter call is a mutating call as the learned parameters stored in the machine potentially change. )","category":"section"},{"location":"evaluating_model_performance/#Multiple-measures","page":"Evaluating Model Performance","title":"Multiple measures","text":"Multiple measures are specified as a vector:\n\nevaluate!(\n    mach,\n    resampling=cv,\n    measures=[l1, rms, rmslp1],\n    verbosity=0,\n)\n\nCustom measures can also be provided.","category":"section"},{"location":"evaluating_model_performance/#Specifying-weights","page":"Evaluating Model Performance","title":"Specifying weights","text":"Per-observation weights can be passed to measures. If a measure does not support weights, the weights are ignored:\n\nholdout = Holdout(fraction_train=0.8)\nweights = [1, 1, 2, 1, 1, 2, 3, 1, 1, 2, 3, 1];\nevaluate!(\n    mach,\n    resampling=CV(nfolds=3),\n    measure=[l2, rsquared],\n    weights=weights,\n)\n\nIn classification problems, use class_weights=... to specify a class weight dictionary.","category":"section"},{"location":"evaluating_model_performance/#User-specified-train/test-sets","page":"Evaluating Model Performance","title":"User-specified train/test sets","text":"Users can either provide an explicit list of train/test pairs of row indices for resampling, as in this example:\n\nfold1 = 1:6; fold2 = 7:12;\nevaluate!(\n    mach,\n    resampling = [(fold1, fold2), (fold2, fold1)],\n    measures=[l1, l2],\n    verbosity=0,\n)\n\nOr the user can define their own re-usable ResamplingStrategy objects; see Custom resampling strategies below.","category":"section"},{"location":"evaluating_model_performance/#Built-in-resampling-strategies","page":"Evaluating Model Performance","title":"Built-in resampling strategies","text":"","category":"section"},{"location":"evaluating_model_performance/#Custom-resampling-strategies","page":"Evaluating Model Performance","title":"Custom resampling strategies","text":"To define a new resampling strategy, make relevant parameters of your strategy the fields of a new type MyResamplingStrategy <: MLJ.ResamplingStrategy, and implement one of the following methods:\n\nMLJ.train_test_pairs(my_strategy::MyResamplingStrategy, rows)\nMLJ.train_test_pairs(my_strategy::MyResamplingStrategy, rows, y)\nMLJ.train_test_pairs(my_strategy::MyResamplingStrategy, rows, X, y)\n\nEach method takes a vector of indices rows and returns a vector [(t1, e1), (t2, e2), ... (tk, ek)] of train/test pairs of row indices selected from rows. Here X, y are the input and target data (ignored in simple strategies, such as Holdout and CV).\n\nHere is the code for the Holdout strategy as an example:\n\nstruct Holdout <: ResamplingStrategy\n    fraction_train::Float64\n    shuffle::Bool\n    rng::Union{Int,AbstractRNG}\n\n    function Holdout(fraction_train, shuffle, rng)\n        0 < fraction_train < 1 ||\n            error(\"`fraction_train` must be between 0 and 1.\")\n        return new(fraction_train, shuffle, rng)\n    end\nend\n\n# Keyword Constructor\nfunction Holdout(; fraction_train::Float64=0.7, shuffle=nothing, rng=nothing)\n    if rng isa Integer\n        rng = MersenneTwister(rng)\n    end\n    if shuffle === nothing\n        shuffle = ifelse(rng===nothing, false, true)\n    end\n    if rng === nothing\n        rng = Random.GLOBAL_RNG\n    end\n    return Holdout(fraction_train, shuffle, rng)\nend\n\nfunction train_test_pairs(holdout::Holdout, rows)\n    train, test = partition(rows, holdout.fraction_train,\n                          shuffle=holdout.shuffle, rng=holdout.rng)\n    return [(train, test),]\nend","category":"section"},{"location":"evaluating_model_performance/#Reference","page":"Evaluating Model Performance","title":"Reference","text":"","category":"section"},{"location":"evaluating_model_performance/#MLJBase.Holdout","page":"Evaluating Model Performance","title":"MLJBase.Holdout","text":"holdout = Holdout(; fraction_train=0.7, shuffle=nothing, rng=nothing)\n\nInstantiate a Holdout resampling strategy, for use in evaluate!, evaluate and in tuning.\n\ntrain_test_pairs(holdout, rows)\n\nReturns the pair [(train, test)], where train and test are vectors such that rows=vcat(train, test) and length(train)/length(rows) is approximatey equal to fraction_train`.\n\nPre-shuffling of rows is controlled by rng and shuffle. If rng is an integer, then the Holdout keyword constructor resets it to MersenneTwister(rng). Otherwise some AbstractRNG object is expected.\n\nIf rng is left unspecified, rng is reset to Random.GLOBAL_RNG, in which case rows are only pre-shuffled if shuffle=true is specified.\n\n\n\n\n\n","category":"type"},{"location":"evaluating_model_performance/#MLJBase.CV","page":"Evaluating Model Performance","title":"MLJBase.CV","text":"cv = CV(; nfolds=6,  shuffle=nothing, rng=nothing)\n\nCross-validation resampling strategy, for use in evaluate!, evaluate and tuning.\n\ntrain_test_pairs(cv, rows)\n\nReturns an nfolds-length iterator of (train, test) pairs of vectors (row indices), where each train and test is a sub-vector of rows. The test vectors are mutually exclusive and exhaust rows. Each train vector is the complement of the corresponding test vector. With no row pre-shuffling, the order of rows is preserved, in the sense that rows coincides precisely with the concatenation of the test vectors, in the order they are generated. The first r test vectors have length n + 1, where n, r = divrem(length(rows), nfolds), and the remaining test vectors have length n.\n\nPre-shuffling of rows is controlled by rng and shuffle. If rng is an integer, then the CV keyword constructor resets it to MersenneTwister(rng). Otherwise some AbstractRNG object is expected.\n\nIf rng is left unspecified, rng is reset to Random.GLOBAL_RNG, in which case rows are only pre-shuffled if shuffle=true is explicitly specified.\n\n\n\n\n\n","category":"type"},{"location":"evaluating_model_performance/#MLJBase.StratifiedCV","page":"Evaluating Model Performance","title":"MLJBase.StratifiedCV","text":"stratified_cv = StratifiedCV(; nfolds=6,\n                               shuffle=false,\n                               rng=Random.GLOBAL_RNG)\n\nStratified cross-validation resampling strategy, for use in evaluate!, evaluate and in tuning. Applies only to classification problems (OrderedFactor or Multiclass targets).\n\ntrain_test_pairs(stratified_cv, rows, y)\n\nReturns an nfolds-length iterator of (train, test) pairs of vectors (row indices) where each train and test is a sub-vector of rows. The test vectors are mutually exclusive and exhaust rows. Each train vector is the complement of the corresponding test vector.\n\nUnlike regular cross-validation, the distribution of the levels of the target y corresponding to each train and test is constrained, as far as possible, to replicate that of y[rows] as a whole.\n\nThe stratified train_test_pairs algorithm is invariant to label renaming. For example, if you run replace!(y, 'a' => 'b', 'b' => 'a') and then re-run train_test_pairs, the returned (train, test) pairs will be the same.\n\nPre-shuffling of rows is controlled by rng and shuffle. If rng is an integer, then the StratifedCV keywod constructor resets it to MersenneTwister(rng). Otherwise some AbstractRNG object is expected.\n\nIf rng is left unspecified, rng is reset to Random.GLOBAL_RNG, in which case rows are only pre-shuffled if shuffle=true is explicitly specified.\n\n\n\n\n\n","category":"type"},{"location":"evaluating_model_performance/#MLJBase.TimeSeriesCV","page":"Evaluating Model Performance","title":"MLJBase.TimeSeriesCV","text":"tscv = TimeSeriesCV(; nfolds=4)\n\nCross-validation resampling strategy, for use in evaluate!, evaluate and tuning, when observations are chronological and not expected to be independent.\n\ntrain_test_pairs(tscv, rows)\n\nReturns an nfolds-length iterator of (train, test) pairs of vectors (row indices), where each train and test is a sub-vector of rows. The rows are partitioned sequentially into nfolds + 1 approximately equal length partitions, where the first partition is the first train set, and the second partition is the first test set. The second train set consists of the first two partitions, and the second test set consists of the third partition, and so on for each fold.\n\nThe first partition (which is the first train set) has length n + r, where n, r = divrem(length(rows), nfolds + 1), and the remaining partitions (all of the test folds) have length n.\n\nExamples\n\njulia> MLJBase.train_test_pairs(TimeSeriesCV(nfolds=3), 1:10)\n3-element Vector{Tuple{UnitRange{Int64}, UnitRange{Int64}}}:\n (1:4, 5:6)\n (1:6, 7:8)\n (1:8, 9:10)\n\njulia> model = (@load RidgeRegressor pkg=MultivariateStats verbosity=0)();\n\njulia> data = @load_sunspots;\n\njulia> X = (lag1 = data.sunspot_number[2:end-1],\n            lag2 = data.sunspot_number[1:end-2]);\n\njulia> y = data.sunspot_number[3:end];\n\njulia> tscv = TimeSeriesCV(nfolds=3);\n\njulia> evaluate(model, X, y, resampling=tscv, measure=rmse, verbosity=0)\n┌───────────────────────────┬───────────────┬────────────────────┐\n│ _.measure                 │ _.measurement │ _.per_fold         │\n├───────────────────────────┼───────────────┼────────────────────┤\n│ RootMeanSquaredError @753 │ 21.7          │ [25.4, 16.3, 22.4] │\n└───────────────────────────┴───────────────┴────────────────────┘\n_.per_observation = [missing]\n_.fitted_params_per_fold = [ … ]\n_.report_per_fold = [ … ]\n_.train_test_rows = [ … ]\n\n\n\n\n\n","category":"type"},{"location":"evaluating_model_performance/#MLJBase.InSample","page":"Evaluating Model Performance","title":"MLJBase.InSample","text":"in_sample = InSample()\n\nInstantiate an InSample resampling strategy, for use in evaluate!, evaluate and in tuning. In this strategy the train and test sets are the same, and consist of all observations specified by the rows keyword argument. If rows is not specified, all supplied rows are used.\n\nExample\n\nusing MLJBase, MLJModels\n\nX, y = make_blobs()  # a table and a vector\nmodel = ConstantClassifier()\ntrain, test = partition(eachindex(y), 0.7)  # train:test = 70:30\n\nCompute in-sample (training) loss:\n\nevaluate(model, X, y, resampling=InSample(), rows=train, measure=brier_loss)\n\nCompute the out-of-sample loss:\n\nevaluate(model, X, y, resampling=[(train, test),], measure=brier_loss)\n\nOr equivalently:\n\nevaluate(model, X, y, resampling=Holdout(fraction_train=0.7), measure=brier_loss)\n\n\n\n\n\n","category":"type"},{"location":"evaluating_model_performance/#MLJBase.evaluate!","page":"Evaluating Model Performance","title":"MLJBase.evaluate!","text":"evaluate!(mach; resampling=CV(), measure=nothing, options...)\n\nEstimate the performance of a machine mach wrapping a supervised model in data, using the specified resampling strategy (defaulting to 6-fold cross-validation) and measure, which can be a single measure or vector. Returns a PerformanceEvaluation object.\n\nAvailable resampling strategies are CV, Holdout, InSample, StratifiedCV and TimeSeriesCV. If resampling is not an instance of one of these, then a vector of tuples of the form (train_rows, test_rows) is expected. For example, setting\n\nresampling = [(1:100, 101:200),\n              (101:200, 1:100)]\n\ngives two-fold cross-validation using the first 200 rows of data.\n\nAny measure conforming to the StatisticalMeasuresBase.jl API can be provided, assuming it can consume multiple observations.\n\nAlthough evaluate! is mutating, mach.model and mach.args are not mutated.\n\nAdditional keyword options\n\nrows - vector of observation indices from which both train and test folds are constructed (default is all observations)\noperation/operations=nothing - One of predict, predict_mean, predict_mode, predict_median, or predict_joint, or a vector of these of the same length as measure/measures. Automatically inferred if left unspecified. For example, predict_mode will be used for a Multiclass target, if model is a probabilistic predictor, but measure is expects literal (point) target predictions. Operations actually applied can be inspected from the operation field of the object returned.\nweights - per-sample Real weights for measures that support them (not to be confused with weights used in training, such as the w in mach = machine(model, X, y, w)).\nclass_weights - dictionary of Real per-class weights for use with measures that support these, in classification problems (not to be confused with weights used in training, such as the w in mach = machine(model, X, y, w)).\nrepeats::Int=1: set to a higher value for repeated (Monte Carlo) resampling. For example, if repeats = 10, then resampling = CV(nfolds=5, shuffle=true), generates a total of 50 (train, test) pairs for evaluation and subsequent aggregation.\nacceleration=CPU1(): acceleration/parallelization option; can be any instance of CPU1, (single-threaded computation), CPUThreads (multi-threaded computation) or CPUProcesses (multi-process computation); default is default_resource(). These types are owned by ComputationalResources.jl.\nforce=false: set to true to force cold-restart of each training event\nverbosity::Int=1 logging level; can be negative\ncheck_measure=true: whether to screen measures for possible incompatibility with the model. Will not catch all incompatibilities.\nper_observation=true: whether to calculate estimates for individual observations; if false the per_observation field of the returned object is populated with missings. Setting to false may reduce compute time and allocations.\nlogger=default_logger() - a logger object for forwarding results to a machine learning tracking platform; see default_logger for details.\ncompact=false - if true, the returned evaluation object excludes these fields: fitted_params_per_fold, report_per_fold, train_test_rows.\n\nSee also evaluate, PerformanceEvaluation, CompactPerformanceEvaluation.\n\n\n\n\n\n","category":"function"},{"location":"evaluating_model_performance/#MLJModelInterface.evaluate","page":"Evaluating Model Performance","title":"MLJModelInterface.evaluate","text":"some meta-models may choose to implement the evaluate operations\n\n\n\n\n\n","category":"function"},{"location":"evaluating_model_performance/#MLJBase.PerformanceEvaluation","page":"Evaluating Model Performance","title":"MLJBase.PerformanceEvaluation","text":"PerformanceEvaluation <: AbstractPerformanceEvaluation\n\nType of object returned by evaluate (for models plus data) or evaluate! (for machines). Such objects encode estimates of the performance (generalization error) of a supervised model or outlier detection model, and store other information ancillary to the computation.\n\nIf evaluate or evaluate! is called with the compact=true option, then a CompactPerformanceEvaluation object is returned instead.\n\nWhen evaluate/evaluate! is called, a number of train/test pairs (\"folds\") of row indices are generated, according to the options provided, which are discussed in the evaluate! doc-string. Rows correspond to observations. The generated train/test pairs are recorded in the train_test_rows field of the PerformanceEvaluation struct, and the corresponding estimates, aggregated over all train/test pairs, are recorded in measurement, a vector with one entry for each measure (metric) recorded in measure.\n\nWhen displayed, a PerformanceEvaluation object includes a value under the heading 1.96*SE, derived from the standard error of the per_fold entries. This value is suitable for constructing a formal 95% confidence interval for the given measurement. Such intervals should be interpreted with caution. See, for example, Bates et al.  (2021).\n\nFields\n\nThese fields are part of the public API of the PerformanceEvaluation struct.\n\nmodel: model used to create the performance evaluation. In the case a   tuning model, this is the best model found.\nmeasure: vector of measures (metrics) used to evaluate performance\nmeasurement: vector of measurements - one for each element of measure - aggregating the performance measurements over all train/test pairs (folds). The aggregation method applied for a given measure m is StatisticalMeasuresBase.external_aggregation_mode(m) (commonly Mean() or Sum())\noperation (e.g., predict_mode): the operations applied for each measure to generate predictions to be evaluated. Possibilities are: predict, predict_mean, predict_mode, predict_median, or predict_joint.\nper_fold: a vector of vectors of individual test fold evaluations (one vector per measure). Useful for obtaining a rough estimate of the variance of the performance estimate.\nper_observation: a vector of vectors of vectors containing individual per-observation measurements: for an evaluation e, e.per_observation[m][f][i] is the measurement for the ith observation in the fth test fold, evaluated using the mth measure.  Useful for some forms of hyper-parameter optimization. Note that an aggregregated measurement for some measure measure is repeated across all observations in a fold if StatisticalMeasures.can_report_unaggregated(measure) == true. If e has been computed with the per_observation=false option, then e_per_observation is a vector of missings.\nfitted_params_per_fold: a vector containing fitted params(mach) for each machine mach trained during resampling - one machine per train/test pair. Use this to extract the learned parameters for each individual training event.\nreport_per_fold: a vector containing report(mach) for each machine mach training in resampling - one machine per train/test pair.\ntrain_test_rows: a vector of tuples, each of the form (train, test), where train and test are vectors of row (observation) indices for training and evaluation respectively.\nresampling: the user-specified resampling strategy to generate the train/test pairs (or literal train/test pairs if that was directly specified).\nrepeats: the number of times the resampling strategy was repeated.\n\nSee also CompactPerformanceEvaluation.\n\n\n\n\n\n","category":"type"},{"location":"evaluating_model_performance/#MLJBase.CompactPerformanceEvaluation","page":"Evaluating Model Performance","title":"MLJBase.CompactPerformanceEvaluation","text":"CompactPerformanceEvaluation <: AbstractPerformanceEvaluation\n\nType of object returned by evaluate (for models plus data) or evaluate! (for machines) when called with the option compact = true. Such objects have the same structure as the PerformanceEvaluation objects returned by default, except that the following fields are omitted to save memory: fitted_params_per_fold, report_per_fold, train_test_rows.\n\nFor more on the remaining fields, see PerformanceEvaluation.\n\n\n\n\n\n","category":"type"},{"location":"evaluating_model_performance/#MLJBase.default_logger","page":"Evaluating Model Performance","title":"MLJBase.default_logger","text":"default_logger()\n\nReturn the current value of the default logger for use with supported machine learning tracking platforms, such as MLflow.\n\nThe default logger is used in calls to evaluate! and evaluate, and in the constructors TunedModel and IteratedModel, unless the logger keyword is explicitly specified.\n\nnote: Note\nPrior to MLJ v0.20.7 (and MLJBase 1.5) the default logger was always nothing.\n\nWhen MLJBase is first loaded, the default logger is nothing.\n\n\n\n\n\ndefault_logger(logger)\n\nReset the default logger.\n\nExample\n\nSuppose an MLflow tracking service is running on a local server at http://127.0.0.1:500. Then in every evaluate call in which logger is not specified, the peformance evaluation is automatically logged to the service, as here:\n\nusing MLJ\nlogger = MLJFlow.Logger(\"http://127.0.0.1:5000/api\")\ndefault_logger(logger)\n\nX, y = make_moons()\nmodel = ConstantClassifier()\nevaluate(model, X, y, measures=[log_loss, accuracy)])\n\n\n\n\n\n","category":"function"},{"location":"models/IteratedModel_MLJIteration/#IteratedModel_MLJIteration","page":"IteratedModel","title":"IteratedModel","text":"IteratedModel(model;\n    controls=MLJIteration.DEFAULT_CONTROLS,\n    resampling=Holdout(),\n    measure=nothing,\n    retrain=false,\n    advanced_options...,\n)\n\nWrap the specified supervised model in the specified iteration controls. Here model should support iteration, which is true if (iteration_parameter(model) is different from nothing.\n\nAvailable controls: Step(), Info(), Warn(), Error(), Callback(), WithLossDo(), WithTrainingLossesDo(), WithNumberDo(), Data(), Disjunction(), GL(), InvalidValue(), Never(), NotANumber(), NumberLimit(), NumberSinceBest(), PQ(), Patience(), Threshold(), TimeLimit(), Warmup(), WithIterationsDo(), WithEvaluationDo(), WithFittedParamsDo(), WithReportDo(), WithMachineDo(), WithModelDo(), CycleLearningRate() and Save().\n\nimportant: Important\nTo make out-of-sample losses available to the controls, the wrapped model is only trained on part of the data, as iteration proceeds. The user may want to force retraining on all data after controlled iteration has finished by specifying retrain=true. See also \"Training\", and the retrain option, under \"Extended help\" below.","category":"section"},{"location":"models/IteratedModel_MLJIteration/#Extended-help","page":"IteratedModel","title":"Extended help","text":"","category":"section"},{"location":"models/IteratedModel_MLJIteration/#Options","page":"IteratedModel","title":"Options","text":"controls=Any[IterationControl.Step(1), EarlyStopping.Patience(5), EarlyStopping.GL(2.0), EarlyStopping.TimeLimit(Dates.Millisecond(108000)), EarlyStopping.InvalidValue()]: Controls are summarized at https://JuliaAI.github.io/MLJ.jl/dev/getting_started/ but query individual doc-strings for details and advanced options. For creating your own controls, refer to the documentation just cited.\nresampling=Holdout(fraction_train=0.7): The default resampling holds back 30% of data for computing an out-of-sample estimate of performance (the \"loss\") for loss-based controls such as WithLossDo. Specify resampling=nothing if all data is to be used for controlled iteration, with each out-of-sample loss replaced by the most recent training loss, assuming this is made available by the model (supports_training_losses(model) == true). If the model does not report a training loss, you can use resampling=InSample() instead. Otherwise, resampling must have type Holdout or be a vector with one element of the form (train_indices, test_indices).\nmeasure=nothing: StatisticalMeasures.jl compatible measure for estimating model performance (the \"loss\", but the orientation is immaterial - i.e., this could be a score). Inferred by default. Ignored if resampling=nothing.\nretrain=false: If retrain=true or resampling=nothing, iterated_model behaves exactly like the original model but with the iteration parameter automatically selected (\"learned\"). That is, the model is retrained on all available data, using the same number of iterations, once controlled iteration has stopped. This is typically desired if wrapping the iterated model further, or when inserting in a pipeline or other composite model. If retrain=false (default) and resampling isa Holdout, then iterated_model behaves like the original model trained on a subset of the provided data.\nweights=nothing: per-observation weights to be passed to measure where supported; if unspecified, these are understood to be uniform.\nclass_weights=nothing: class-weights to be passed to measure where supported; if unspecified, these are understood to be uniform.\noperation=nothing: Operation, such as predict or predict_mode, for computing target values, or proxy target values, for consumption by measure; automatically inferred by default.\ncheck_measure=true: Specify false to override checks on measure for compatibility with the training data.\niteration_parameter=nothing: A symbol, such as :epochs, naming the iteration parameter of model; inferred by default. Note that the actual value of the iteration parameter in the supplied model is ignored; only the value of an internal clone is mutated during training the wrapped model.\ncache=true: Whether or not model-specific representations of data are cached in between iteration parameter increments; specify cache=false to prioritize memory over speed.","category":"section"},{"location":"models/IteratedModel_MLJIteration/#Training","page":"IteratedModel","title":"Training","text":"Training an instance iterated_model of IteratedModel on some data (by binding to a machine and calling fit!, for example) performs the following actions:\n\nAssuming resampling !== nothing, the data is split into train and test sets, according to the specified resampling strategy.\nA clone of the wrapped model, model is bound to the train data in an internal machine, train_mach. If resampling === nothing, all data is used instead. This machine is the object to which controls are applied. For example, Callback(fitted_params |> print) will print the value of fitted_params(train_mach).\nThe iteration parameter of the clone is set to 0.\nThe specified controls are repeatedly applied to train_mach in sequence, until one of the controls triggers a stop. Loss-based controls (eg, Patience(), GL(), Threshold(0.001)) use an out-of-sample loss, obtained by applying measure to predictions and the test target values. (Specifically, these predictions are those returned by operation(train_mach).)  If resampling === nothing then the most recent training loss is used instead. Some controls require both out-of-sample and training losses (eg, PQ()).\nOnce a stop has been triggered, a clone of model is bound to all data in a machine called mach_production below, unless retrain == false (true by default) or resampling === nothing, in which case mach_production coincides with train_mach.","category":"section"},{"location":"models/IteratedModel_MLJIteration/#Prediction","page":"IteratedModel","title":"Prediction","text":"Calling predict(mach, Xnew) in the example above returns predict(mach_production, Xnew). Similar similar statements hold for predict_mean, predict_mode, predict_median.","category":"section"},{"location":"models/IteratedModel_MLJIteration/#Controls-that-mutate-parameters","page":"IteratedModel","title":"Controls that mutate parameters","text":"A control is permitted to mutate the fields (hyper-parameters) of train_mach.model (the clone of model). For example, to mutate a learning rate one might use the control\n\nCallback(mach -> mach.model.eta = 1.05*mach.model.eta)\n\nHowever, unless model supports warm restarts with respect to changes in that parameter, this will trigger retraining of train_mach from scratch, with a different training outcome, which is not recommended.","category":"section"},{"location":"models/IteratedModel_MLJIteration/#Warm-restarts","page":"IteratedModel","title":"Warm restarts","text":"In the following example, the second fit! call will not restart training of the internal train_mach, assuming model supports warm restarts:\n\niterated_model = IteratedModel(\n    model,\n    controls = [Step(1), NumberLimit(100)],\n)\nmach = machine(iterated_model, X, y)\nfit!(mach) ## train for 100 iterations\niterated_model.controls = [Step(1), NumberLimit(50)],\nfit!(mach) ## train for an *extra* 50 iterations\n\nMore generally, if iterated_model is mutated and fit!(mach) is called again, then a warm restart is attempted if the only parameters to change are model or controls or both.\n\nSpecifically, train_mach.model is mutated to match the current value of iterated_model.model and the iteration parameter of the latter is updated to the last value used in the preceding fit!(mach) call. Then repeated application of the (updated) controls begin anew.","category":"section"},{"location":"common_mlj_workflows/#Common-MLJ-Workflows","page":"Common MLJ Workflows","title":"Common MLJ Workflows","text":"This demo assumes you have certain packages in your active package environment. To activate a new environment, \"MyNewEnv\", with just these packages, do this in a new REPL session:\n\nusing Pkg\nPkg.activate(\"MyNewEnv\")\nPkg.add([\"MLJ\", \"RDatasets\", \"DataFrames\", \"MLJDecisionTreeInterface\",\n    \"MLJMultivariateStatsInterface\", \"NearestNeighborModels\", \"MLJGLMInterface\",\n    \"Plots\"])\n\nThe following starts MLJ and shows the current version of MLJ (you can also use Pkg.status()):\n\nusing MLJ\nMLJ_VERSION","category":"section"},{"location":"common_mlj_workflows/#Data-ingestion","page":"Common MLJ Workflows","title":"Data ingestion","text":"import RDatasets\nchanning = RDatasets.dataset(\"boot\", \"channing\")\n\nfirst(channing, 4) |> pretty\n\nInspecting metadata, including column scientific types:\n\nschema(channing)\n\nHorizontally splitting data and shuffling rows.\n\nHere y is the :Exit column and X a table with everything else:\n\ny, X = unpack(channing, ==(:Exit), rng=123)\nnothing # hide\n\nHere y is the :Exit column and X everything else except :Time:\n\ny, X = unpack(channing,\n              ==(:Exit),\n              !=(:Time);\n              rng=123);\nscitype(y)\n\nschema(X)\n\nFixing wrong scientific types in X:\n\nX = coerce(X, :Exit=>Continuous, :Entry=>Continuous, :Cens=>Multiclass);\nschema(X)\n\nLoading a built-in supervised dataset:\n\ntable = load_iris();\nschema(table)\n\nLoading a built-in data set already split into X and y:\n\nX, y = @load_iris;\nselectrows(X, 1:4) # selectrows works whenever `Tables.istable(X)==true`.\n\ny[1:4]\n\nSplitting data vertically after row shuffling:\n\nchanning_train, channing_test = partition(channing, 0.6, rng=123);\nnothing # hide\n\nOr, if already horizontally split:\n\n(Xtrain, Xtest), (ytrain, ytest) = partition((X, y), 0.6, multi=true, rng=123)","category":"section"},{"location":"common_mlj_workflows/#Model-Search","page":"Common MLJ Workflows","title":"Model Search","text":"Reference:   Model Search\n\nSearching for a supervised model:\n\nX, y = @load_boston\nms = models(matching(X, y))\n\nms[6]\n\nmodels(\"Tree\")\n\nA more refined search:\n\nmodels() do model\n    matching(model, X, y) &&\n    model.prediction_type == :deterministic &&\n    model.is_pure_julia\nend;\nnothing # hide\n\nSearching for an unsupervised model:\n\nmodels(matching(X))\n\nGetting the metadata entry for a given model type:\n\ninfo(\"PCA\")\ninfo(\"RidgeRegressor\", pkg=\"MultivariateStats\") # a model type in multiple packages\n\nExtracting the model document string (output omitted):\n\ndoc(\"DecisionTreeClassifier\", pkg=\"DecisionTree\")\nnothing # hide","category":"section"},{"location":"common_mlj_workflows/#Instantiating-a-model","page":"Common MLJ Workflows","title":"Instantiating a model","text":"Reference:   Getting Started, Loading Model Code\n\nAssumes MLJDecisionTreeClassifier is in your environment. Otherwise, try interactive loading with @iload:\n\nTree = @load DecisionTreeClassifier pkg=DecisionTree\ntree = Tree(min_samples_split=5, max_depth=4)\n\nor\n\ntree = (@load DecisionTreeClassifier)()\ntree.min_samples_split = 5\ntree.max_depth = 4","category":"section"},{"location":"common_mlj_workflows/#Evaluating-a-model","page":"Common MLJ Workflows","title":"Evaluating a model","text":"Reference:   Evaluating Model Performance\n\nX, y = @load_boston  # a table and a vector\nKNN = @load KNNRegressor\nknn = KNN()\nevaluate(knn, X, y,\n         resampling=CV(nfolds=5),\n         measure=[RootMeanSquaredError(), LPLoss(1)])\n\nNote RootMeanSquaredError() has alias rms and LPLoss(1) has aliases l1, mae.\n\nDo measures() to list all losses and scores and their aliases, or refer to the StatisticalMeasures.jl docs.","category":"section"},{"location":"common_mlj_workflows/#Basic-fit/evaluate/predict-by-hand","page":"Common MLJ Workflows","title":"Basic fit/evaluate/predict by hand","text":"Reference:   Getting Started, Machines, Evaluating Model Performance, Performance Measures\n\ncrabs = load_crabs() |> DataFrames.DataFrame\nschema(crabs)\n\ny, X = unpack(crabs, ==(:sp), !in([:index, :sex]); rng=123)\n\nTree = @load DecisionTreeClassifier pkg=DecisionTree\ntree = Tree(max_depth=2) # hide\n\nBind the model and data together in a machine, which will additionally, store the learned parameters (fitresults) when fit:\n\nmach = machine(tree, X, y)\n\nSplit row indices into training and evaluation rows:\n\ntrain, test = partition(eachindex(y), 0.7); # 70:30 split\n\nFit on the train data set and evaluate on the test data set:\n\nfit!(mach, rows=train)\nyhat = predict(mach, X[test,:])\nLogLoss(tol=1e-4)(yhat, y[test])\n\nNote LogLoss() has aliases log_loss and cross_entropy.\n\nPredict on the new data set:\n\nXnew = (FL = rand(3), RW = rand(3), CL = rand(3), CW = rand(3), BD = rand(3))\npredict(mach, Xnew)      # a vector of distributions\n\npredict_mode(mach, Xnew) # a vector of point-predictions","category":"section"},{"location":"common_mlj_workflows/#More-performance-evaluation-examples","page":"Common MLJ Workflows","title":"More performance evaluation examples","text":"Evaluating model + data directly:\n\nevaluate(tree, X, y,\n         resampling=Holdout(fraction_train=0.7, shuffle=true, rng=1234),\n         measure=[LogLoss(), Accuracy()])\n\nIf a machine is already defined, as above:\n\nevaluate!(mach,\n          resampling=Holdout(fraction_train=0.7, shuffle=true, rng=1234),\n          measure=[LogLoss(), Accuracy()])\n\nUsing cross-validation:\n\nevaluate!(mach, resampling=CV(nfolds=5, shuffle=true, rng=1234),\n          measure=[LogLoss(), Accuracy()])\n\nWith user-specified train/test pairs of row indices:\n\nf1, f2, f3 = 1:13, 14:26, 27:36\npairs = [(f1, vcat(f2, f3)), (f2, vcat(f3, f1)), (f3, vcat(f1, f2))];\nevaluate!(mach,\n          resampling=pairs,\n          measure=[LogLoss(), Accuracy()])\n\nChanging a hyperparameter and re-evaluating:\n\ntree.max_depth = 3\nevaluate!(mach,\n          resampling=CV(nfolds=5, shuffle=true, rng=1234),\n          measure=[LogLoss(), Accuracy()])","category":"section"},{"location":"common_mlj_workflows/#Inspecting-training-results","page":"Common MLJ Workflows","title":"Inspecting training results","text":"Fit an ordinary least square model to some synthetic data:\n\nx1 = rand(100)\nx2 = rand(100)\n\nX = (x1=x1, x2=x2)\ny = x1 - 2x2 + 0.1*rand(100);\n\nOLS = @load LinearRegressor pkg=GLM\nols = OLS()\nmach =  machine(ols, X, y) |> fit!\n\nGet a named tuple representing the learned parameters, human-readable if appropriate:\n\nfitted_params(mach)\n\nGet other training-related information:\n\nreport(mach)","category":"section"},{"location":"common_mlj_workflows/#Basic-fit/transform-for-unsupervised-models","page":"Common MLJ Workflows","title":"Basic fit/transform for unsupervised models","text":"Load data:\n\nX, y = @load_iris  # a table and a vector\ntrain, test = partition(eachindex(y), 0.97, shuffle=true, rng=123)\n\nInstantiate and fit the model/machine:\n\nPCA = @load PCA\npca = PCA(maxoutdim=2)\nmach = machine(pca, X)\nfit!(mach, rows=train)\n\nTransform selected data bound to the machine:\n\ntransform(mach, rows=test);\n\nTransform new data:\n\nXnew = (sepal_length=rand(3), sepal_width=rand(3),\n        petal_length=rand(3), petal_width=rand(3));\ntransform(mach, Xnew)","category":"section"},{"location":"common_mlj_workflows/#Inverting-learned-transformations","page":"Common MLJ Workflows","title":"Inverting learned transformations","text":"y = rand(100);\nstand = Standardizer()\nmach = machine(stand, y)\nfit!(mach)\nz = transform(mach, y);\n@assert inverse_transform(mach, z) ≈ y # true","category":"section"},{"location":"common_mlj_workflows/#Nested-hyperparameter-tuning","page":"Common MLJ Workflows","title":"Nested hyperparameter tuning","text":"Reference:   Tuning Models\n\nDefine a model with nested hyperparameters:\n\nTree = @load DecisionTreeClassifier pkg=DecisionTree\ntree = Tree()\nforest = EnsembleModel(model=tree, n=300)\n\nDefine ranges for hyperparameters to be tuned:\n\nr1 = range(forest, :bagging_fraction, lower=0.5, upper=1.0, scale=:log10)\n\nr2 = range(forest, :(model.n_subfeatures), lower=1, upper=4) # nested\n\nWrap the model in a tuning strategy:\n\ntuned_forest = TunedModel(model=forest,\n                          tuning=Grid(resolution=12),\n                          resampling=CV(nfolds=6),\n                          ranges=[r1, r2],\n                          measure=BrierLoss())\n\nBound the wrapped model to data:\n\nmach = machine(tuned_forest, X, y)\n\nFitting the resultant machine optimizes the hyperparameters specified in range, using the specified tuning and resampling strategies and performance measure (possibly a vector of measures), and retrains on all data bound to the machine:\n\nfit!(mach)\n\nInspecting the optimal model:\n\nF = fitted_params(mach)\n\nF.best_model\n\nInspecting details of tuning procedure:\n\nr = report(mach);\nkeys(r)\n\nr.history[[1,end]]\n\nVisualizing these results:\n\nusing Plots\nplot(mach)\n\n(Image: )\n\nPredicting on new data using the optimized model trained on all data:\n\npredict(mach, Xnew)","category":"section"},{"location":"common_mlj_workflows/#Constructing-linear-pipelines","page":"Common MLJ Workflows","title":"Constructing linear pipelines","text":"Reference:   Linear Pipelines\n\nConstructing a linear (unbranching) pipeline with a learned target transformation/inverse transformation:\n\nX, y = @load_reduced_ames\nKNN = @load KNNRegressor\nknn_with_target = TransformedTargetModel(model=KNN(K=3), transformer=Standardizer())\n\npipe = (X -> coerce(X, :age=>Continuous)) |> OneHotEncoder() |> knn_with_target\n\nEvaluating the pipeline (just as you would any other model):\n\npipe.one_hot_encoder.drop_last = true # mutate a nested hyper-parameter\nevaluate(pipe, X, y, resampling=Holdout(), measure=RootMeanSquaredError(), verbosity=2)\n\nInspecting the learned parameters in a pipeline:\n\nmach = machine(pipe, X, y) |> fit!\nF = fitted_params(mach)\nF.transformed_target_model_deterministic.model\n\nConstructing a linear (unbranching) pipeline with a static (unlearned) target transformation/inverse transformation:\n\nTree = @load DecisionTreeRegressor pkg=DecisionTree verbosity=0\ntree_with_target = TransformedTargetModel(model=Tree(),\n                                          transformer=y -> log.(y),\n                                          inverse = z -> exp.(z))\npipe2 = (X -> coerce(X, :age=>Continuous)) |> OneHotEncoder() |> tree_with_target\nnothing # hide","category":"section"},{"location":"common_mlj_workflows/#Creating-a-homogeneous-ensemble-of-models","page":"Common MLJ Workflows","title":"Creating a homogeneous ensemble of models","text":"Reference: Homogeneous Ensembles\n\nX, y = @load_iris\nTree = @load DecisionTreeClassifier pkg=DecisionTree\ntree = Tree()\nforest = EnsembleModel(model=tree, bagging_fraction=0.8, n=300)\nmach = machine(forest, X, y)\nevaluate!(mach, measure=LogLoss())","category":"section"},{"location":"common_mlj_workflows/#Performance-curves","page":"Common MLJ Workflows","title":"Performance curves","text":"Generate a plot of performance, as a function of some hyperparameter (building on the preceding example)\n\nSingle performance curve:\n\nr = range(forest, :n, lower=1, upper=1000, scale=:log10)\ncurve = learning_curve(mach,\n                       range=r,\n                       resampling=Holdout(),\n                       resolution=50,\n                       measure=LogLoss(),\n                       verbosity=0)\n\nusing Plots\nplot(curve.parameter_values, curve.measurements,\n     xlab=curve.parameter_name, xscale=curve.parameter_scale)\n\n(Image: )\n\nMultiple curves:\n\ncurve = learning_curve(mach,\n                       range=r,\n                       resampling=Holdout(),\n                       measure=LogLoss(),\n                       resolution=50,\n                       rng_name=:rng,\n                       rngs=4,\n                       verbosity=0)\n\nplot(curve.parameter_values, curve.measurements,\n     xlab=curve.parameter_name, xscale=curve.parameter_scale)\n\n(Image: )","category":"section"},{"location":"models/LinearRegressor_GLM/#LinearRegressor_GLM","page":"LinearRegressor","title":"LinearRegressor","text":"LinearRegressor\n\nA model type for constructing a linear regressor, based on GLM.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nLinearRegressor = @load LinearRegressor pkg=GLM\n\nDo model = LinearRegressor() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in LinearRegressor(fit_intercept=...).\n\nLinearRegressor assumes the target is a continuous variable whose conditional distribution is normal with constant variance, and whose expected value is a linear combination of the features (identity link function). Options exist to specify an intercept or offset feature.","category":"section"},{"location":"models/LinearRegressor_GLM/#Training-data","page":"LinearRegressor","title":"Training data","text":"In MLJ or MLJBase, bind an instance model to data with one of:\n\nmach = machine(model, X, y)\nmach = machine(model, X, y, w)\n\nHere\n\nX: is any table of input features (eg, a DataFrame) whose columns are of scitype Continuous; check the scitype with schema(X)\ny: is the target, which can be any AbstractVector whose element scitype is Continuous; check the scitype with scitype(y)\nw: is a vector of Real per-observation weights","category":"section"},{"location":"models/LinearRegressor_GLM/#Hyper-parameters","page":"LinearRegressor","title":"Hyper-parameters","text":"fit_intercept=true: Whether to calculate the intercept for this model.  If set to false, no intercept will be calculated (e.g. the data is expected  to be centered)\ndropcollinear=false: Whether to drop features in the training data to ensure linear independence.  If true , only the first of each set of linearly-dependent features is used. The coefficient for redundant linearly dependent features is 0.0 and all associated statistics are set to NaN.\noffsetcol=nothing: Name of the column to be used as an offset, if any.  An offset is a variable which is known to have a coefficient of 1.\nreport_keys: Vector of keys for the report. Possible keys are: :deviance, :dof_residual, :stderror, :vcov, :coef_table and :glm_model. By default only :glm_model is excluded.\n\nTrain the machine using fit!(mach, rows=...).","category":"section"},{"location":"models/LinearRegressor_GLM/#Operations","page":"LinearRegressor","title":"Operations","text":"predict(mach, Xnew): return predictions of the target given new  features Xnew having the same Scitype as X above. Predictions are  probabilistic.\npredict_mean(mach, Xnew): instead return the mean of  each prediction above\npredict_median(mach, Xnew): instead return the median of  each prediction above.","category":"section"},{"location":"models/LinearRegressor_GLM/#Fitted-parameters","page":"LinearRegressor","title":"Fitted parameters","text":"The fields of fitted_params(mach) are:\n\nfeatures: The names of the features encountered during model fitting.\ncoef: The linear coefficients determined by the model.\nintercept: The intercept determined by the model.","category":"section"},{"location":"models/LinearRegressor_GLM/#Report","page":"LinearRegressor","title":"Report","text":"When all keys are enabled in report_keys, the following fields are available in report(mach):\n\ndeviance: Measure of deviance of fitted model with respect to a perfectly fitted model. For a linear model, this is the weighted residual sum of squares\ndof_residual: The degrees of freedom for residuals, when meaningful.\nstderror: The standard errors of the coefficients.\nvcov: The estimated variance-covariance matrix of the coefficient estimates.\ncoef_table: Table which displays coefficients and summarizes their significance and confidence intervals.\nglm_model: The raw fitted model returned by GLM.lm. Note this points to training data. Refer to the GLM.jl documentation for usage.","category":"section"},{"location":"models/LinearRegressor_GLM/#Examples","page":"LinearRegressor","title":"Examples","text":"using MLJ\nLinearRegressor = @load LinearRegressor pkg=GLM\nglm = LinearRegressor()\n\nX, y = make_regression(100, 2) ## synthetic data\nmach = machine(glm, X, y) |> fit!\n\nXnew, _ = make_regression(3, 2)\nyhat = predict(mach, Xnew) ## new predictions\nyhat_point = predict_mean(mach, Xnew) ## new predictions\n\nfitted_params(mach).features\nfitted_params(mach).coef ## x1, x2, intercept\nfitted_params(mach).intercept\n\nreport(mach)\n\nSee also LinearCountRegressor, LinearBinaryClassifier","category":"section"},{"location":"models/SelfOrganizingMap_SelfOrganizingMaps/#SelfOrganizingMap_SelfOrganizingMaps","page":"SelfOrganizingMap","title":"SelfOrganizingMap","text":"SelfOrganizingMap\n\nA model type for constructing a self organizing map, based on SelfOrganizingMaps.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nSelfOrganizingMap = @load SelfOrganizingMap pkg=SelfOrganizingMaps\n\nDo model = SelfOrganizingMap() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in SelfOrganizingMap(k=...).\n\nSelfOrganizingMaps implements Kohonen's Self Organizing Map, Proceedings of the IEEE; Kohonen, T.; (1990):\"The self-organizing map\"","category":"section"},{"location":"models/SelfOrganizingMap_SelfOrganizingMaps/#Training-data","page":"SelfOrganizingMap","title":"Training data","text":"In MLJ or MLJBase, bind an instance model to data with     mach = machine(model, X) where\n\nX: an AbstractMatrix or Table of input features whose columns are of scitype Continuous.\n\nTrain the machine with fit!(mach, rows=...).","category":"section"},{"location":"models/SelfOrganizingMap_SelfOrganizingMaps/#Hyper-parameters","page":"SelfOrganizingMap","title":"Hyper-parameters","text":"k=10: Number of nodes along once side of SOM grid. There are k² total nodes.\nη=0.5: Learning rate. Scales adjust made to winning node and its neighbors during each round of training.\nσ²=0.05: The (squared) neighbor radius. Used to determine scale for neighbor node adjustments.\ngrid_type=:rectangular  Node grid geometry. One of (:rectangular, :hexagonal, :spherical).\nη_decay=:exponential Learning rate schedule function. One of (:exponential, :asymptotic)\nσ_decay=:exponential Neighbor radius schedule function. One of (:exponential, :asymptotic, :none)\nneighbor_function=:gaussian Kernel function used to make adjustment to neighbor weights. Scale is set by σ². One of (:gaussian, :mexican_hat).\nmatching_distance=euclidean Distance function from Distances.jl used to determine winning node.\nNepochs=1 Number of times to repeat training on the shuffled dataset.","category":"section"},{"location":"models/SelfOrganizingMap_SelfOrganizingMaps/#Operations","page":"SelfOrganizingMap","title":"Operations","text":"transform(mach, Xnew): returns the coordinates of the winning SOM node for each instance of Xnew. For SOM of gridtype :rectangular and :hexagonal, these are cartesian coordinates. For gridtype :spherical, these are the latitude and longitude in radians.","category":"section"},{"location":"models/SelfOrganizingMap_SelfOrganizingMaps/#Fitted-parameters","page":"SelfOrganizingMap","title":"Fitted parameters","text":"The fields of fitted_params(mach) are:\n\ncoords: The coordinates of each of the SOM nodes (points in the domain of the map) with shape (k², 2)\nweights: Array of weight vectors for the SOM nodes (corresponding points in the map's range) of shape (k², input dimension)","category":"section"},{"location":"models/SelfOrganizingMap_SelfOrganizingMaps/#Report","page":"SelfOrganizingMap","title":"Report","text":"The fields of report(mach) are:\n\nclasses: the index of the winning node for each instance of the training data X interpreted as a class label","category":"section"},{"location":"models/SelfOrganizingMap_SelfOrganizingMaps/#Examples","page":"SelfOrganizingMap","title":"Examples","text":"using MLJ\nsom = @load SelfOrganizingMap pkg=SelfOrganizingMaps\nmodel = som()\nX, y = make_regression(50, 3) ## synthetic data\nmach = machine(model, X) |> fit!\nX̃ = transform(mach, X)\n\nrpt = report(mach)\nclasses = rpt.classes","category":"section"},{"location":"models/OneHotEncoder_MLJTransforms/#OneHotEncoder_MLJTransforms","page":"OneHotEncoder","title":"OneHotEncoder","text":"OneHotEncoder\n\nA model type for constructing a one-hot encoder, based on MLJTransforms.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nOneHotEncoder = @load OneHotEncoder pkg=MLJTransforms\n\nDo model = OneHotEncoder() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in OneHotEncoder(features=...).\n\nUse this model to one-hot encode the Multiclass and OrderedFactor features (columns) of some table, leaving other columns unchanged.\n\nNew data to be transformed may lack features present in the fit data, but no new features can be present.\n\nWarning: This transformer assumes that levels(col) for any Multiclass or OrderedFactor column, col, is the same for training data and new data to be transformed.\n\nTo ensure all features are transformed into Continuous features, or dropped, use ContinuousEncoder instead.","category":"section"},{"location":"models/OneHotEncoder_MLJTransforms/#Training-data","page":"OneHotEncoder","title":"Training data","text":"In MLJ or MLJBase, bind an instance model to data with\n\nmach = machine(model, X)\n\nwhere\n\nX: any Tables.jl compatible table. Columns can be of mixed type but only those with element scitype Multiclass or OrderedFactor can be encoded. Check column scitypes with schema(X).\n\nTrain the machine using fit!(mach, rows=...).","category":"section"},{"location":"models/OneHotEncoder_MLJTransforms/#Hyper-parameters","page":"OneHotEncoder","title":"Hyper-parameters","text":"features: a vector of symbols (feature names). If empty (default) then all Multiclass and OrderedFactor features are encoded. Otherwise, encoding is further restricted to the specified features (ignore=false) or the unspecified features (ignore=true). This default behavior can be modified by the ordered_factor flag.\nordered_factor=false: when true, OrderedFactor features are universally excluded\ndrop_last=true: whether to drop the column corresponding to the final class of encoded features. For example, a three-class feature is spawned into three new features if drop_last=false, but just two features otherwise.","category":"section"},{"location":"models/OneHotEncoder_MLJTransforms/#Fitted-parameters","page":"OneHotEncoder","title":"Fitted parameters","text":"The fields of fitted_params(mach) are:\n\nall_features: names of all features encountered in training\nfitted_levels_given_feature: dictionary of the levels associated with each feature encoded, keyed on the feature name\nref_name_pairs_given_feature: dictionary of pairs r => ftr (such as 0x00000001 => :grad__A) where r is a CategoricalArrays.jl reference integer representing a level, and ftr the corresponding new feature name; the dictionary is keyed on the names of features that are encoded","category":"section"},{"location":"models/OneHotEncoder_MLJTransforms/#Report","page":"OneHotEncoder","title":"Report","text":"The fields of report(mach) are:\n\nfeatures_to_be_encoded: names of input features to be encoded\nnew_features: names of all output features","category":"section"},{"location":"models/OneHotEncoder_MLJTransforms/#Example","page":"OneHotEncoder","title":"Example","text":"using MLJ\n\nX = (name=categorical([\"Danesh\", \"Lee\", \"Mary\", \"John\"]),\n     grade=categorical([\"A\", \"B\", \"A\", \"C\"], ordered=true),\n     height=[1.85, 1.67, 1.5, 1.67],\n     n_devices=[3, 2, 4, 3])\n\njulia> schema(X)\n┌───────────┬──────────────────┐\n│ names     │ scitypes         │\n├───────────┼──────────────────┤\n│ name      │ Multiclass{4}    │\n│ grade     │ OrderedFactor{3} │\n│ height    │ Continuous       │\n│ n_devices │ Count            │\n└───────────┴──────────────────┘\n\nhot = OneHotEncoder(drop_last=true)\nmach = fit!(machine(hot, X))\nW = transform(mach, X)\n\njulia> schema(W)\n┌──────────────┬────────────┐\n│ names        │ scitypes   │\n├──────────────┼────────────┤\n│ name__Danesh │ Continuous │\n│ name__John   │ Continuous │\n│ name__Lee    │ Continuous │\n│ grade__A     │ Continuous │\n│ grade__B     │ Continuous │\n│ height       │ Continuous │\n│ n_devices    │ Count      │\n└──────────────┴────────────┘\n\nSee also ContinuousEncoder.","category":"section"},{"location":"models/MultinomialClassifier_MLJLinearModels/#MultinomialClassifier_MLJLinearModels","page":"MultinomialClassifier","title":"MultinomialClassifier","text":"MultinomialClassifier\n\nA model type for constructing a multinomial classifier, based on MLJLinearModels.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nMultinomialClassifier = @load MultinomialClassifier pkg=MLJLinearModels\n\nDo model = MultinomialClassifier() to construct an instance with default hyper-parameters.\n\nThis model coincides with LogisticClassifier, except certain optimizations possible in the special binary case will not be applied. Its hyperparameters are identical.","category":"section"},{"location":"models/MultinomialClassifier_MLJLinearModels/#Training-data","page":"MultinomialClassifier","title":"Training data","text":"In MLJ or MLJBase, bind an instance model to data with\n\nmach = machine(model, X, y)\n\nwhere:\n\nX is any table of input features (eg, a DataFrame) whose columns have Continuous scitype; check column scitypes with schema(X)\ny is the target, which can be any AbstractVector whose element scitype is <:OrderedFactor or <:Multiclass; check the scitype with scitype(y)\n\nTrain the machine using fit!(mach, rows=...).","category":"section"},{"location":"models/MultinomialClassifier_MLJLinearModels/#Hyperparameters","page":"MultinomialClassifier","title":"Hyperparameters","text":"lambda::Real: strength of the regularizer if penalty is :l2 or :l1.     Strength of the L2 regularizer if penalty is :en. Default: eps()\ngamma::Real: strength of the L1 regularizer if penalty is :en. Default: 0.0\npenalty::Union{String, Symbol}: the penalty to use, either :l2, :l1, :en (elastic net) or :none. Default: :l2\nfit_intercept::Bool: whether to fit the intercept or not. Default: true\npenalize_intercept::Bool: whether to penalize the intercept. Default: false\nscale_penalty_with_samples::Bool: whether to scale the penalty with the number of samples. Default: true\nsolver::Union{Nothing, MLJLinearModels.Solver}: some instance of MLJLinearModels.S where S is one of: LBFGS, NewtonCG, ProxGrad; but subject to the following restrictions:\nIf penalty = :l2, ProxGrad is disallowed. Otherwise, ProxGrad is the only option.\nUnless scitype(y) <: Finite{2} (binary target) Newton is disallowed.\nIf solver = nothing (default) then ProxGrad(accel=true) (FISTA) is used, unless gamma = 0, in which case LBFGS() is used.\nSolver aliases: FISTA(; kwargs...) = ProxGrad(accel=true, kwargs...), ISTA(; kwargs...) = ProxGrad(accel=false, kwargs...) Default: nothing","category":"section"},{"location":"models/MultinomialClassifier_MLJLinearModels/#Example","page":"MultinomialClassifier","title":"Example","text":"using MLJ\nX, y = make_blobs(centers = 3)\nmach = fit!(machine(MultinomialClassifier(), X, y))\npredict(mach, X)\nfitted_params(mach)\n\nSee also LogisticClassifier.","category":"section"},{"location":"models/MultitargetSRRegressor_SymbolicRegression/#MultitargetSRRegressor_SymbolicRegression","page":"MultitargetSRRegressor","title":"MultitargetSRRegressor","text":"MultitargetSRRegressor\n\nA model type for constructing a Multi-Target Symbolic Regression via Evolutionary Search, based on SymbolicRegression.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nMultitargetSRRegressor = @load MultitargetSRRegressor pkg=SymbolicRegression\n\nDo model = MultitargetSRRegressor() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in MultitargetSRRegressor(binary_operators=...).\n\nMulti-target Symbolic Regression regressor (MultitargetSRRegressor) conducts several searches for expressions that predict each target variable from a set of input variables. All data is assumed to be Continuous. The search is performed using an evolutionary algorithm. This algorithm is described in the paper https://arxiv.org/abs/2305.01582.","category":"section"},{"location":"models/MultitargetSRRegressor_SymbolicRegression/#Training-data","page":"MultitargetSRRegressor","title":"Training data","text":"In MLJ or MLJBase, bind an instance model to data with\n\nmach = machine(model, X, y)\n\nOR\n\nmach = machine(model, X, y, w)\n\nHere:\n\nX is any table of input features (eg, a DataFrame) whose columns are of scitype\n\nContinuous; check column scitypes with schema(X). Variable names in discovered expressions will be taken from the column names of X, if available. Units in columns of X (use DynamicQuantities for units) will trigger dimensional analysis to be used.\n\ny is the target, which can be any table of target variables whose element scitype is Continuous; check the scitype with schema(y). Units in columns of y (use DynamicQuantities for units) will trigger dimensional analysis to be used.\nw is the observation weights which can either be nothing (default) or an AbstractVector whoose element scitype is Count or Continuous. The same weights are used for all targets.\n\nTrain the machine using fit!(mach), inspect the discovered expressions with report(mach), and predict on new data with predict(mach, Xnew). Note that unlike other regressors, symbolic regression stores a list of lists of trained models. The models chosen from each of these lists is defined by the function selection_method keyword argument, which by default balances accuracy and complexity. You can override this at prediction time by passing a named tuple with keys data and idx.","category":"section"},{"location":"models/MultitargetSRRegressor_SymbolicRegression/#Hyper-parameters","page":"MultitargetSRRegressor","title":"Hyper-parameters","text":"binary_operators: Vector of binary operators (functions) to use.   Each operator should be defined for two input scalars,   and one output scalar. All operators   need to be defined over the entire real line (excluding infinity - these   are stopped before they are input), or return NaN where not defined.   For speed, define it so it takes two reals   of the same type as input, and outputs the same type. For the SymbolicUtils   simplification backend, you will need to define a generic method of the   operator so it takes arbitrary types.\nunary_operators: Same, but for   unary operators (one input scalar, gives an output scalar).\nconstraints: Array of pairs specifying size constraints   for each operator. The constraints for a binary operator should be a 2-tuple   (e.g., (-1, -1)) and the constraints for a unary operator should be an Int.   A size constraint is a limit to the size of the subtree   in each argument of an operator. e.g., [(^)=>(-1, 3)] means that the   ^ operator can have arbitrary size (-1) in its left argument,   but a maximum size of 3 in its right argument. Default is   no constraints.\nbatching: Whether to evolve based on small mini-batches of data,   rather than the entire dataset.\nbatch_size: What batch size to use if using batching.\nelementwise_loss: What elementwise loss function to use. Can be one of   the following losses, or any other loss of type   SupervisedLoss. You can also pass a function that takes   a scalar target (left argument), and scalar predicted (right   argument), and returns a scalar. This will be averaged   over the predicted data. If weights are supplied, your   function should take a third argument for the weight scalar.   Included losses:       Regression:           - LPDistLoss{P}(),           - L1DistLoss(),           - L2DistLoss() (mean square),           - LogitDistLoss(),           - HuberLoss(d),           - L1EpsilonInsLoss(ϵ),           - L2EpsilonInsLoss(ϵ),           - PeriodicLoss(c),           - QuantileLoss(τ),       Classification:           - ZeroOneLoss(),           - PerceptronLoss(),           - L1HingeLoss(),           - SmoothedL1HingeLoss(γ),           - ModifiedHuberLoss(),           - L2MarginLoss(),           - ExpLoss(),           - SigmoidLoss(),           - DWDMarginLoss(q).\nloss_function: Alternatively, you may redefine the loss used   as any function of tree::Node{T}, dataset::Dataset{T},   and options::Options, so long as you output a non-negative   scalar of type T. This is useful if you want to use a loss   that takes into account derivatives, or correlations across   the dataset. This also means you could use a custom evaluation   for a particular expression. If you are using   batching=true, then your function should   accept a fourth argument idx, which is either nothing   (indicating that the full dataset should be used), or a vector   of indices to use for the batch.   For example,\n  function my_loss(tree, dataset::Dataset{T,L}, options)::L where {T,L}\n      prediction, flag = eval_tree_array(tree, dataset.X, options)\n      if !flag\n          return L(Inf)\n      end\n      return sum((prediction .- dataset.y) .^ 2) / dataset.n\n  end\npopulations: How many populations of equations to use.\npopulation_size: How many equations in each population.\nncycles_per_iteration: How many generations to consider per iteration.\ntournament_selection_n: Number of expressions considered in each tournament.\ntournament_selection_p: The fittest expression in a tournament is to be   selected with probability p, the next fittest with probability p*(1-p),   and so forth.\ntopn: Number of equations to return to the host process, and to   consider for the hall of fame.\ncomplexity_of_operators: What complexity should be assigned to each operator,   and the occurrence of a constant or variable. By default, this is 1   for all operators. Can be a real number as well, in which case   the complexity of an expression will be rounded to the nearest integer.   Input this in the form of, e.g., [(^) => 3, sin => 2].\ncomplexity_of_constants: What complexity should be assigned to use of a constant.   By default, this is 1.\ncomplexity_of_variables: What complexity should be assigned to each variable.   By default, this is 1.\nalpha: The probability of accepting an equation mutation   during regularized evolution is given by exp(-delta_loss/(alpha * T)),   where T goes from 1 to 0. Thus, alpha=infinite is the same as no annealing.\nmaxsize: Maximum size of equations during the search.\nmaxdepth: Maximum depth of equations during the search, by default   this is set equal to the maxsize.\nparsimony: A multiplicative factor for how much complexity is   punished.\ndimensional_constraint_penalty: An additive factor if the dimensional   constraint is violated.\nuse_frequency: Whether to use a parsimony that adapts to the   relative proportion of equations at each complexity; this will   ensure that there are a balanced number of equations considered   for every complexity.\nuse_frequency_in_tournament: Whether to use the adaptive parsimony described   above inside the score, rather than just at the mutation accept/reject stage.\nadaptive_parsimony_scaling: How much to scale the adaptive parsimony term   in the loss. Increase this if the search is spending too much time   optimizing the most complex equations.\nturbo: Whether to use LoopVectorization.@turbo to evaluate expressions.   This can be significantly faster, but is only compatible with certain   operators. Experimental!\nmigration: Whether to migrate equations between processes.\nhof_migration: Whether to migrate equations from the hall of fame   to processes.\nfraction_replaced: What fraction of each population to replace with   migrated equations at the end of each cycle.\nfraction_replaced_hof: What fraction to replace with hall of fame   equations at the end of each cycle.\nshould_simplify: Whether to simplify equations. If you   pass a custom objective, this will be set to false.\nshould_optimize_constants: Whether to use an optimization algorithm   to periodically optimize constants in equations.\noptimizer_nrestarts: How many different random starting positions to consider   for optimization of constants.\noptimizer_algorithm: Select algorithm to use for optimizing constants. Default   is \"BFGS\", but \"NelderMead\" is also supported.\noptimizer_options: General options for the constant optimization. For details   we refer to the documentation on Optim.Options from the Optim.jl package.   Options can be provided here as NamedTuple, e.g. (iterations=16,), as a   Dict, e.g. Dict(:x_tol => 1.0e-32,), or as an Optim.Options instance.\noutput_file: What file to store equations to, as a backup.\nperturbation_factor: When mutating a constant, either   multiply or divide by (1+perturbation_factor)^(rand()+1).\nprobability_negate_constant: Probability of negating a constant in the equation   when mutating it.\nmutation_weights: Relative probabilities of the mutations. The struct   MutationWeights should be passed to these options.   See its documentation on MutationWeights for the different weights.\ncrossover_probability: Probability of performing crossover.\nannealing: Whether to use simulated annealing.\nwarmup_maxsize_by: Whether to slowly increase the max size from 5 up to   maxsize. If nonzero, specifies the fraction through the search   at which the maxsize should be reached.\nverbosity: Whether to print debugging statements or   not.\nprint_precision: How many digits to print when printing   equations. By default, this is 5.\nsave_to_file: Whether to save equations to a file during the search.\nbin_constraints: See constraints. This is the same, but specified for binary   operators only (for example, if you have an operator that is both a binary   and unary operator).\nuna_constraints: Likewise, for unary operators.\nseed: What random seed to use. nothing uses no seed.\nprogress: Whether to use a progress bar output (verbosity will   have no effect).\nearly_stop_condition: Float - whether to stop early if the mean loss gets below this value.   Function - a function taking (loss, complexity) as arguments and returning true or false.\ntimeout_in_seconds: Float64 - the time in seconds after which to exit (as an alternative to the number of iterations).\nmax_evals: Int (or Nothing) - the maximum number of evaluations of expressions to perform.\nskip_mutation_failures: Whether to simply skip over mutations that fail or are rejected, rather than to replace the mutated   expression with the original expression and proceed normally.\nenable_autodiff: Whether to enable automatic differentiation functionality. This is turned off by default.   If turned on, this will be turned off if one of the operators does not have well-defined gradients.\nnested_constraints: Specifies how many times a combination of operators can be nested. For example,   [sin => [cos => 0], cos => [cos => 2]] specifies that cos may never appear within a sin,   but sin can be nested with itself an unlimited number of times. The second term specifies that cos   can be nested up to 2 times within a cos, so that cos(cos(cos(x))) is allowed (as well as any combination   of + or - within it), but cos(cos(cos(cos(x)))) is not allowed. When an operator is not specified,   it is assumed that it can be nested an unlimited number of times. This requires that there is no operator   which is used both in the unary operators and the binary operators (e.g., - could be both subtract, and negation).   For binary operators, both arguments are treated the same way, and the max of each argument is constrained.\ndeterministic: Use a global counter for the birth time, rather than calls to time(). This gives   perfect resolution, and is therefore deterministic. However, it is not thread safe, and must be used   in serial mode.\ndefine_helper_functions: Whether to define helper functions   for constructing and evaluating trees.\nniterations::Int=10: The number of iterations to perform the search.   More iterations will improve the results.\nparallelism=:multithreading: What parallelism mode to use.   The options are :multithreading, :multiprocessing, and :serial.   By default, multithreading will be used. Multithreading uses less memory,   but multiprocessing can handle multi-node compute. If using :multithreading   mode, the number of threads available to julia are used. If using   :multiprocessing, numprocs processes will be created dynamically if   procs is unset. If you have already allocated processes, pass them   to the procs argument and they will be used.   You may also pass a string instead of a symbol, like \"multithreading\".\nnumprocs::Union{Int, Nothing}=nothing:  The number of processes to use,   if you want equation_search to set this up automatically. By default   this will be 4, but can be any number (you should pick a number <=   the number of cores available).\nprocs::Union{Vector{Int}, Nothing}=nothing: If you have set up   a distributed run manually with procs = addprocs() and @everywhere,   pass the procs to this keyword argument.\naddprocs_function::Union{Function, Nothing}=nothing: If using multiprocessing   (parallelism=:multithreading), and are not passing procs manually,   then they will be allocated dynamically using addprocs. However,   you may also pass a custom function to use instead of addprocs.   This function should take a single positional argument,   which is the number of processes to use, as well as the lazy keyword argument.   For example, if set up on a slurm cluster, you could pass   addprocs_function = addprocs_slurm, which will set up slurm processes.\nheap_size_hint_in_bytes::Union{Int,Nothing}=nothing: On Julia 1.9+, you may set the --heap-size-hint   flag on Julia processes, recommending garbage collection once a process   is close to the recommended size. This is important for long-running distributed   jobs where each process has an independent memory, and can help avoid   out-of-memory errors. By default, this is set to Sys.free_memory() / numprocs.\nruntests::Bool=true: Whether to run (quick) tests before starting the   search, to see if there will be any problems during the equation search   related to the host environment.\nloss_type::Type=Nothing: If you would like to use a different type   for the loss than for the data you passed, specify the type here.   Note that if you pass complex data ::Complex{L}, then the loss   type will automatically be set to L.\nselection_method::Function: Function to selection expression from   the Pareto frontier for use in predict.   See SymbolicRegression.MLJInterfaceModule.choose_best for an example.   This function should return a single integer specifying   the index of the expression to use. By default, this maximizes   the score (a pound-for-pound rating) of expressions reaching the threshold   of 1.5x the minimum loss. To override this at prediction time, you can pass   a named tuple with keys data and idx to predict. See the Operations   section for details.\ndimensions_type::AbstractDimensions: The type of dimensions to use when storing   the units of the data. By default this is DynamicQuantities.SymbolicDimensions.","category":"section"},{"location":"models/MultitargetSRRegressor_SymbolicRegression/#Operations","page":"MultitargetSRRegressor","title":"Operations","text":"predict(mach, Xnew): Return predictions of the target given features Xnew, which should have same scitype as X above. The expression used for prediction is defined by the selection_method function, which can be seen by viewing report(mach).best_idx.\npredict(mach, (data=Xnew, idx=i)): Return predictions of the target given features Xnew, which should have same scitype as X above. By passing a named tuple with keys data and idx, you are able to specify the equation you wish to evaluate in idx.","category":"section"},{"location":"models/MultitargetSRRegressor_SymbolicRegression/#Fitted-parameters","page":"MultitargetSRRegressor","title":"Fitted parameters","text":"The fields of fitted_params(mach) are:\n\nbest_idx::Vector{Int}: The index of the best expression in each Pareto frontier, as determined by the selection_method function. Override in predict by passing a named tuple with keys data and idx.\nequations::Vector{Vector{Node{T}}}: The expressions discovered by the search, represented in a dominating Pareto frontier (i.e., the best expressions found for each complexity). The outer vector is indexed by target variable, and the inner vector is ordered by increasing complexity. T is equal to the element type of the passed data.\nequation_strings::Vector{Vector{String}}: The expressions discovered by the search, represented as strings for easy inspection.","category":"section"},{"location":"models/MultitargetSRRegressor_SymbolicRegression/#Report","page":"MultitargetSRRegressor","title":"Report","text":"The fields of report(mach) are:\n\nbest_idx::Vector{Int}: The index of the best expression in each Pareto frontier,  as determined by the selection_method function. Override in predict by passing  a named tuple with keys data and idx.\nequations::Vector{Vector{Node{T}}}: The expressions discovered by the search, represented in a dominating Pareto frontier (i.e., the best expressions found for each complexity). The outer vector is indexed by target variable, and the inner vector is ordered by increasing complexity.\nequation_strings::Vector{Vector{String}}: The expressions discovered by the search, represented as strings for easy inspection.\ncomplexities::Vector{Vector{Int}}: The complexity of each expression in each Pareto frontier.\nlosses::Vector{Vector{L}}: The loss of each expression in each Pareto frontier, according to the loss function specified in the model. The type L is the loss type, which is usually the same as the element type of data passed (i.e., T), but can differ if complex data types are passed.\nscores::Vector{Vector{L}}: A metric which considers both the complexity and loss of an expression, equal to the change in the log-loss divided by the change in complexity, relative to the previous expression along the Pareto frontier. A larger score aims to indicate an expression is more likely to be the true expression generating the data, but this is very problem-dependent and generally several other factors should be considered.","category":"section"},{"location":"models/MultitargetSRRegressor_SymbolicRegression/#Examples","page":"MultitargetSRRegressor","title":"Examples","text":"using MLJ\nMultitargetSRRegressor = @load MultitargetSRRegressor pkg=SymbolicRegression\nX = (a=rand(100), b=rand(100), c=rand(100))\nY = (y1=(@. cos(X.c) * 2.1 - 0.9), y2=(@. X.a * X.b + X.c))\nmodel = MultitargetSRRegressor(binary_operators=[+, -, *], unary_operators=[exp], niterations=100)\nmach = machine(model, X, Y)\nfit!(mach)\ny_hat = predict(mach, X)\n## View the equations used:\nr = report(mach)\nfor (output_index, (eq, i)) in enumerate(zip(r.equation_strings, r.best_idx))\n    println(\"Equation used for \", output_index, \": \", eq[i])\nend\n\nSee also SRRegressor.","category":"section"},{"location":"models/PerceptronClassifier_MLJScikitLearnInterface/#PerceptronClassifier_MLJScikitLearnInterface","page":"PerceptronClassifier","title":"PerceptronClassifier","text":"PerceptronClassifier\n\nA model type for constructing a perceptron classifier, based on MLJScikitLearnInterface.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nPerceptronClassifier = @load PerceptronClassifier pkg=MLJScikitLearnInterface\n\nDo model = PerceptronClassifier() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in PerceptronClassifier(penalty=...).","category":"section"},{"location":"models/PerceptronClassifier_MLJScikitLearnInterface/#Hyper-parameters","page":"PerceptronClassifier","title":"Hyper-parameters","text":"penalty = nothing\nalpha = 0.0001\nfit_intercept = true\nmax_iter = 1000\ntol = 0.001\nshuffle = true\nverbose = 0\neta0 = 1.0\nn_jobs = nothing\nrandom_state = 0\nearly_stopping = false\nvalidation_fraction = 0.1\nn_iter_no_change = 5\nclass_weight = nothing\nwarm_start = false","category":"section"},{"location":"models/KNeighborsRegressor_MLJScikitLearnInterface/#KNeighborsRegressor_MLJScikitLearnInterface","page":"KNeighborsRegressor","title":"KNeighborsRegressor","text":"KNeighborsRegressor\n\nA model type for constructing a K-nearest neighbors regressor, based on MLJScikitLearnInterface.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nKNeighborsRegressor = @load KNeighborsRegressor pkg=MLJScikitLearnInterface\n\nDo model = KNeighborsRegressor() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in KNeighborsRegressor(n_neighbors=...).","category":"section"},{"location":"models/KNeighborsRegressor_MLJScikitLearnInterface/#Hyper-parameters","page":"KNeighborsRegressor","title":"Hyper-parameters","text":"n_neighbors = 5\nweights = uniform\nalgorithm = auto\nleaf_size = 30\np = 2\nmetric = minkowski\nmetric_params = nothing\nn_jobs = nothing","category":"section"},{"location":"models/NeuralNetworkRegressor_MLJFlux/#NeuralNetworkRegressor_MLJFlux","page":"NeuralNetworkRegressor","title":"NeuralNetworkRegressor","text":"NeuralNetworkRegressor\n\nA model type for constructing a neural network regressor, based on MLJFlux.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nNeuralNetworkRegressor = @load NeuralNetworkRegressor pkg=MLJFlux\n\nDo model = NeuralNetworkRegressor() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in NeuralNetworkRegressor(builder=...).\n\nNeuralNetworkRegressor is for training a data-dependent Flux.jl neural network to predict a Continuous target, given a table of Continuous features. Users provide a recipe for constructing the network, based on properties of the data that is encountered, by specifying an appropriate builder. See MLJFlux documentation for more on builders.\n\nIn addition to features with Continuous scientific element type, this model supports categorical features in the input table. If present, such features are embedded into dense vectors by the use of an additional EntityEmbedderLayer layer after the input, as described in Entity Embeddings of Categorical Variables by Cheng Guo, Felix Berkhahn arXiv, 2016.","category":"section"},{"location":"models/NeuralNetworkRegressor_MLJFlux/#Training-data","page":"NeuralNetworkRegressor","title":"Training data","text":"In MLJ or MLJBase, bind an instance model to data with\n\nmach = machine(model, X, y)\n\nHere:\n\nX provides input features and is either: (i) a Matrix with Continuous element scitype (typically Float32); or (ii) a table of input features (eg, a DataFrame) whose columns have Continuous, Multiclass or OrderedFactor element scitype; check column scitypes with schema(X).  If any Multiclass or OrderedFactor features appear, the constructed network will use an EntityEmbedderLayer layer to transform them into dense vectors. If X is a Matrix, it is assumed that columns correspond to features and rows corresponding to observations.\ny is the target, which can be any AbstractVector whose element scitype is Continuous; check the scitype with scitype(y)\n\nTrain the machine with fit!(mach, rows=...).","category":"section"},{"location":"models/NeuralNetworkRegressor_MLJFlux/#Hyper-parameters","page":"NeuralNetworkRegressor","title":"Hyper-parameters","text":"builder=MLJFlux.Linear(σ=Flux.relu): An MLJFlux builder that constructs a neural  network. Possible builders include: MLJFlux.Linear, MLJFlux.Short, and  MLJFlux.MLP. See MLJFlux documentation for more on builders, and the example below  for using the @builder convenience macro.\noptimiser::Optimisers.Adam(): An Optimisers.jl optimiser. The optimiser performs the updating of the weights of the network. To choose a learning rate (the update rate of the optimizer), a good rule of thumb is to start out at 10e-3, and tune using powers of 10 between 1 and 1e-7.\nloss=Flux.mse: The loss function which the network will optimize. Should be a function which can be called in the form loss(yhat, y).  Possible loss functions are listed in the Flux loss function documentation. For a regression task, natural loss functions are:\nFlux.mse\nFlux.mae\nFlux.msle\nFlux.huber_loss\nCurrently MLJ measures are not supported as loss functions here.\nepochs::Int=10: The duration of training, in epochs. Typically, one epoch represents one pass through the complete the training dataset.\nbatch_size::int=1: the batch size to be used for training, representing the number of samples per update of the network weights. Typically, batch size is between 8 and 512. Increasing batch size may accelerate training if acceleration=CUDALibs() and a GPU is available.\nlambda::Float64=0: The strength of the weight regularization penalty. Can be any value in the range [0, ∞). Note the history reports unpenalized losses.\nalpha::Float64=0: The L2/L1 mix of regularization, in the range [0, 1]. A value of 0 represents L2 regularization, and a value of 1 represents L1 regularization.\nrng::Union{AbstractRNG, Int64}: The random number generator or seed used during training. The default is Random.default_rng().\noptimizer_changes_trigger_retraining::Bool=false: Defines what happens when re-fitting a machine if the associated optimiser has changed. If true, the associated machine will retrain from scratch on fit! call, otherwise it will not.\nacceleration::AbstractResource=CPU1(): Defines on what hardware training is done. For Training on GPU, use CUDALibs().\nembedding_dims: a Dict whose keys are names of categorical features, given as symbols, and whose values are numbers representing the desired dimensionality of the entity embeddings of such features: an integer value of 7, say, sets the embedding dimensionality to 7; a float value of 0.5, say, sets the embedding dimensionality to ceil(0.5 * c), where c is the number of feature levels.  Unspecified feature dimensionality defaults to min(c - 1, 10).","category":"section"},{"location":"models/NeuralNetworkRegressor_MLJFlux/#Operations","page":"NeuralNetworkRegressor","title":"Operations","text":"predict(mach, Xnew): return predictions of the target given new features Xnew, which should have the same scitype as X above.\ntransform(mach, Xnew): Assuming Xnew has the same schema as X, transform the categorical features of Xnew into dense Continuous vectors using the MLJFlux.EntityEmbedderLayer layer present in the network. Does nothing in case the model was trained on an input X that lacks categorical features.","category":"section"},{"location":"models/NeuralNetworkRegressor_MLJFlux/#Fitted-parameters","page":"NeuralNetworkRegressor","title":"Fitted parameters","text":"The fields of fitted_params(mach) are:\n\nchain: The trained \"chain\" (Flux.jl model), namely the series of layers, functions,  and activations which make up the neural network.","category":"section"},{"location":"models/NeuralNetworkRegressor_MLJFlux/#Report","page":"NeuralNetworkRegressor","title":"Report","text":"The fields of report(mach) are:\n\ntraining_losses: A vector of training losses (penalized if lambda != 0) in  historical order, of length epochs + 1.  The first element is the pre-training loss.","category":"section"},{"location":"models/NeuralNetworkRegressor_MLJFlux/#Examples","page":"NeuralNetworkRegressor","title":"Examples","text":"In this example we build a regression model for the Boston house price dataset.\n\nusing MLJ\nimport MLJFlux\nusing Flux\nimport Optimisers\n\nFirst, we load in the data: The :MEDV column becomes the target vector y, and all remaining columns go into a table X, with the exception of :CHAS:\n\ndata = OpenML.load(531); ## Loads from https://www.openml.org/d/531\ny, X = unpack(data, ==(:MEDV), !=(:CHAS); rng=123);\n\nscitype(y)\nschema(X)\n\nSince MLJFlux models do not handle ordered factors, we'll treat :RAD as Continuous:\n\nX = coerce(X, :RAD=>Continuous)\n\nSplitting off a test set:\n\n(X, Xtest), (y, ytest) = partition((X, y), 0.7, multi=true);\n\nNext, we can define a builder, making use of a convenience macro to do so.  In the following @builder call, n_in is a proxy for the number input features (which will be known at fit! time) and rng is a proxy for a RNG (which will be passed from the rng field of model defined below). We also have the parameter n_out which is the number of output features. As we are doing single target regression, the value passed will always be 1, but the builder we define will also work for MultitargetNeuralNetworkRegressor.\n\nbuilder = MLJFlux.@builder begin\n    init=Flux.glorot_uniform(rng)\n    Chain(\n        Dense(n_in, 64, relu, init=init),\n        Dense(64, 32, relu, init=init),\n        Dense(32, n_out, init=init),\n    )\nend\n\nInstantiating a model:\n\nNeuralNetworkRegressor = @load NeuralNetworkRegressor pkg=MLJFlux\nmodel = NeuralNetworkRegressor(\n    builder=builder,\n    rng=123,\n    epochs=20\n)\n\nWe arrange for standardization of the the target by wrapping our model in TransformedTargetModel, and standardization of the features by inserting the wrapped model in a pipeline:\n\npipe = Standardizer |> TransformedTargetModel(model, transformer=Standardizer)\n\nIf we fit with a high verbosity (>1), we will see the losses during training. We can also see the losses in the output of report(mach).\n\nmach = machine(pipe, X, y)\nfit!(mach, verbosity=2)\n\n## first element initial loss, 2:end per epoch training losses\nreport(mach).transformed_target_model_deterministic.model.training_losses","category":"section"},{"location":"models/NeuralNetworkRegressor_MLJFlux/#Experimenting-with-learning-rate","page":"NeuralNetworkRegressor","title":"Experimenting with learning rate","text":"We can visually compare how the learning rate affects the predictions:\n\nusing Plots\n\nrates = rates = [5e-5, 1e-4, 0.005, 0.001, 0.05]\nplt=plot()\n\nforeach(rates) do η\n  pipe.transformed_target_model_deterministic.model.optimiser = Optimisers.Adam(η)\n  fit!(mach, force=true, verbosity=0)\n  losses =\n      report(mach).transformed_target_model_deterministic.model.training_losses[3:end]\n  plot!(1:length(losses), losses, label=η)\nend\n\nplt\n\npipe.transformed_target_model_deterministic.model.optimiser.eta = Optimisers.Adam(0.0001)\n\nWith the learning rate fixed, we compute a CV estimate of the performance (using all data bound to mach) and compare this with performance on the test set:\n\n## CV estimate, based on `(X, y)`:\nevaluate!(mach, resampling=CV(nfolds=5), measure=l2)\n\n## loss for `(Xtest, test)`:\nfit!(mach) ## train on `(X, y)`\nyhat = predict(mach, Xtest)\nl2(yhat, ytest)\n\nThese losses, for the pipeline model, refer to the target on the original, unstandardized, scale.\n\nFor implementing stopping criterion and other iteration controls, refer to examples linked from the MLJFlux documentation.\n\nSee also MultitargetNeuralNetworkRegressor","category":"section"},{"location":"models/PassiveAggressiveRegressor_MLJScikitLearnInterface/#PassiveAggressiveRegressor_MLJScikitLearnInterface","page":"PassiveAggressiveRegressor","title":"PassiveAggressiveRegressor","text":"PassiveAggressiveRegressor\n\nA model type for constructing a passive aggressive regressor, based on MLJScikitLearnInterface.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nPassiveAggressiveRegressor = @load PassiveAggressiveRegressor pkg=MLJScikitLearnInterface\n\nDo model = PassiveAggressiveRegressor() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in PassiveAggressiveRegressor(C=...).","category":"section"},{"location":"models/PassiveAggressiveRegressor_MLJScikitLearnInterface/#Hyper-parameters","page":"PassiveAggressiveRegressor","title":"Hyper-parameters","text":"C = 1.0\nfit_intercept = true\nmax_iter = 1000\ntol = 0.0001\nearly_stopping = false\nvalidation_fraction = 0.1\nn_iter_no_change = 5\nshuffle = true\nverbose = 0\nloss = epsilon_insensitive\nepsilon = 0.1\nrandom_state = nothing\nwarm_start = false\naverage = false","category":"section"},{"location":"models/LOCIDetector_OutlierDetectionPython/#LOCIDetector_OutlierDetectionPython","page":"LOCIDetector","title":"LOCIDetector","text":"LOCIDetector(alpha = 0.5,\n                k = 3)\n\nhttps://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.loci","category":"section"},{"location":"api/#Index-of-Methods","page":"Index of Methods","title":"Index of Methods","text":"","category":"section"},{"location":"models/OCSVMDetector_OutlierDetectionPython/#OCSVMDetector_OutlierDetectionPython","page":"OCSVMDetector","title":"OCSVMDetector","text":"OCSVMDetector(kernel = \"rbf\",\n                 degree = 3,\n                 gamma = \"auto\",\n                 coef0 = 0.0,\n                 tol = 0.001,\n                 nu = 0.5,\n                 shrinking = true,\n                 cache_size = 200,\n                 verbose = false,\n                 max_iter = -1)\n\nhttps://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.ocsvm","category":"section"},{"location":"models/ExtraTreesRegressor_MLJScikitLearnInterface/#ExtraTreesRegressor_MLJScikitLearnInterface","page":"ExtraTreesRegressor","title":"ExtraTreesRegressor","text":"ExtraTreesRegressor\n\nA model type for constructing a extra trees regressor, based on MLJScikitLearnInterface.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nExtraTreesRegressor = @load ExtraTreesRegressor pkg=MLJScikitLearnInterface\n\nDo model = ExtraTreesRegressor() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in ExtraTreesRegressor(n_estimators=...).\n\nExtra trees regressor, fits a number of randomized decision trees on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.","category":"section"},{"location":"models/LOFDetector_OutlierDetectionPython/#LOFDetector_OutlierDetectionPython","page":"LOFDetector","title":"LOFDetector","text":"LOFDetector(n_neighbors = 5,\n               algorithm = \"auto\",\n               leaf_size = 30,\n               metric = \"minkowski\",\n               p = 2,\n               metric_params = nothing,\n               n_jobs = 1,\n               novelty = true)\n\nhttps://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.lof","category":"section"},{"location":"models/PerceptronClassifier_BetaML/#PerceptronClassifier_BetaML","page":"PerceptronClassifier","title":"PerceptronClassifier","text":"mutable struct PerceptronClassifier <: MLJModelInterface.Probabilistic\n\nThe classical perceptron algorithm using one-vs-all for multiclass, from the Beta Machine Learning Toolkit (BetaML).","category":"section"},{"location":"models/PerceptronClassifier_BetaML/#Hyperparameters:","page":"PerceptronClassifier","title":"Hyperparameters:","text":"initial_coefficients::Union{Nothing, Matrix{Float64}}: N-classes by D-dimensions matrix of initial linear coefficients [def: nothing, i.e. zeros]\ninitial_constant::Union{Nothing, Vector{Float64}}: N-classes vector of initial contant terms [def: nothing, i.e. zeros]\nepochs::Int64: Maximum number of epochs, i.e. passages trough the whole training sample [def: 1000]\nshuffle::Bool: Whether to randomly shuffle the data at each iteration (epoch) [def: true]\nforce_origin::Bool: Whether to force the parameter associated with the constant term to remain zero [def: false]\nreturn_mean_hyperplane::Bool: Whether to return the average hyperplane coefficients instead of the final ones  [def: false]\nrng::Random.AbstractRNG: A Random Number Generator to be used in stochastic parts of the code [deafult: Random.GLOBAL_RNG]","category":"section"},{"location":"models/PerceptronClassifier_BetaML/#Example:","page":"PerceptronClassifier","title":"Example:","text":"julia> using MLJ\n\njulia> X, y        = @load_iris;\n\njulia> modelType   = @load PerceptronClassifier pkg = \"BetaML\"\n[ Info: For silent loading, specify `verbosity=0`. \nimport BetaML ✔\nBetaML.Perceptron.PerceptronClassifier\n\njulia> model       = modelType()\nPerceptronClassifier(\n  initial_coefficients = nothing, \n  initial_constant = nothing, \n  epochs = 1000, \n  shuffle = true, \n  force_origin = false, \n  return_mean_hyperplane = false, \n  rng = Random._GLOBAL_RNG())\n\njulia> mach        = machine(model, X, y);\n\njulia> fit!(mach);\n[ Info: Training machine(PerceptronClassifier(initial_coefficients = nothing, …), …).\n*** Avg. error after epoch 2 : 0.0 (all elements of the set has been correctly classified)\njulia> est_classes = predict(mach, X)\n150-element CategoricalDistributions.UnivariateFiniteVector{Multiclass{3}, String, UInt8, Float64}:\n UnivariateFinite{Multiclass{3}}(setosa=>1.0, versicolor=>2.53e-34, virginica=>0.0)\n UnivariateFinite{Multiclass{3}}(setosa=>1.0, versicolor=>1.27e-18, virginica=>1.86e-310)\n ⋮\n UnivariateFinite{Multiclass{3}}(setosa=>2.77e-57, versicolor=>1.1099999999999999e-82, virginica=>1.0)\n UnivariateFinite{Multiclass{3}}(setosa=>3.09e-22, versicolor=>4.03e-25, virginica=>1.0)","category":"section"},{"location":"models/ABODDetector_OutlierDetectionPython/#ABODDetector_OutlierDetectionPython","page":"ABODDetector","title":"ABODDetector","text":"ABODDetector(n_neighbors = 5,\n                method = \"fast\")\n\nhttps://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.abod","category":"section"},{"location":"models/TransformedTargetModel_MLJBase/#TransformedTargetModel_MLJBase","page":"TransformedTargetModel","title":"TransformedTargetModel","text":"TransformedTargetModel(model; transformer=nothing, inverse=nothing, cache=true)\n\nWrap the supervised or semi-supervised model in a transformation of the target variable.\n\nHere transformer one of the following:\n\nThe Unsupervised model that is to transform the training target. By default (inverse=nothing) the parameters learned by this transformer are also used to inverse-transform the predictions of model, which means transformer must implement the inverse_transform method. If this is not the case, specify inverse=identity to suppress inversion.\nA callable object for transforming the target, such as y -> log.(y). In this case a callable inverse, such as z -> exp.(z), should be specified.\n\nSpecify cache=false to prioritize memory over speed, or to guarantee data anonymity.\n\nSpecify inverse=identity if model is a probabilistic predictor, as inverse-transforming sample spaces is not supported. Alternatively, replace model with a deterministic model, such as Pipeline(model, y -> mode.(y)).","category":"section"},{"location":"models/TransformedTargetModel_MLJBase/#Examples","page":"TransformedTargetModel","title":"Examples","text":"A model that normalizes the target before applying ridge regression, with predictions returned on the original scale:\n\n@load RidgeRegressor pkg=MLJLinearModels\nmodel = RidgeRegressor()\ntmodel = TransformedTargetModel(model, transformer=Standardizer())\n\nA model that applies a static log transformation to the data, again returning predictions to the original scale:\n\ntmodel2 = TransformedTargetModel(model, transformer=y->log.(y), inverse=z->exp.(y))","category":"section"},{"location":"preparing_data/#Preparing-Data","page":"Preparing Data","title":"Preparing Data","text":"","category":"section"},{"location":"preparing_data/#Splitting-data","page":"Preparing Data","title":"Splitting data","text":"MLJ has two tools for splitting data. To split data vertically (that is, to split by observations) use partition. This is commonly applied to a vector of observation indices, but can also be applied to datasets themselves, provided they are vectors, matrices or tables.\n\nTo split tabular data horizontally (i.e., break up a table based on feature names) use unpack.","category":"section"},{"location":"preparing_data/#Bridging-the-gap-between-data-type-and-model-requirements","page":"Preparing Data","title":"Bridging the gap between data type and model requirements","text":"As outlined in Getting Started, it is important that the scientific type of data matches the requirements of the model of interest. For example, while the majority of supervised learning models require input features to be Continuous, newcomers to MLJ are sometimes surprised at the disappointing results of model queries such as this one:\n\nX = (height   = [185, 153, 163, 114, 180],\n     time     = [2.3, 4.5, 4.2, 1.8, 7.1],\n     mark     = [\"D\", \"A\", \"C\", \"B\", \"A\"],\n     admitted = [\"yes\", \"no\", missing, \"yes\"]);\ny = [12.4, 12.5, 12.0, 31.9, 43.0]\nmodels(matching(X, y))\n\nOr are unsure about the source of the following warning:\n\njulia> Tree = @load DecisionTreeRegressor pkg=DecisionTree verbosity=0;\njulia> tree = Tree();\n\njulia> machine(tree, X, y)\n┌ Warning: The scitype of `X`, in `machine(model, X, ...)` is incompatible with `model=DecisionTreeRegressor @378`:\n│ scitype(X) = Table{Union{AbstractVector{Continuous}, AbstractVector{Count}, AbstractVector{Textual}, AbstractVector{Union{Missing, Textual}}}}\n│ input_scitype(model) = Table{var\"#s46\"} where var\"#s46\"<:Union{AbstractVector{var\"#s9\"} where var\"#s9\"<:Continuous, AbstractVector{var\"#s9\"} where var\"#s9\"<:Count, AbstractVector{var\"#s9\"} where var\"#s9\"<:OrderedFactor}.\n└ @ MLJBase ~/Dropbox/Julia7/MLJ/MLJBase/src/machines.jl:103\nMachine{DecisionTreeRegressor,…} @198 trained 0 times; caches data\nargs:\n1:  Source @628 ⏎ `Table{Union{AbstractVector{Continuous}, AbstractVector{Count}, AbstractVector{Textual}, AbstractVector{Union{Missing, Textual}}}}`\n2:  Source @544 ⏎ `AbstractVector{Continuous}`\n\nThe meaning of the warning is:\n\nThe input X is a table with column scitypes Continuous, Count, and Textual and Union{Missing, Textual}, which can also see by inspecting the schema:\nschema(X)\nThe model requires a table whose column element scitypes subtype Continuous, an incompatibility.","category":"section"},{"location":"preparing_data/#Common-data-preprocessing-workflows","page":"Preparing Data","title":"Common data preprocessing workflows","text":"There are two tools for addressing data-model type mismatches like the above, with links to further documentation given below:\n\nScientific type coercion: We coerce machine types to obtain the intended scientific interpretation. If height in the above example is intended to be Continuous, mark is supposed to be OrderedFactor, and admitted a (binary) Multiclass, then we can do\n\nX_coerced = coerce(X, :height=>Continuous, :mark=>OrderedFactor, :admitted=>Multiclass);\nschema(X_coerced)\n\nData transformations: We carry out conventional data transformations, such as missing value imputation and feature encoding:\n\nimputer = FillImputer()\nmach = machine(imputer, X_coerced) |> fit!\nX_imputed = transform(mach, X_coerced);\nschema(X_imputed)\n\nencoder = ContinuousEncoder()\nmach = machine(encoder, X_imputed) |> fit!\nX_encoded = transform(mach, X_imputed)\n\nschema(X_encoded)\n\nSuch transformations can also be combined in a pipeline; see Linear Pipelines.","category":"section"},{"location":"preparing_data/#Scientific-type-coercion","page":"Preparing Data","title":"Scientific type coercion","text":"Scientific type coercion is documented in detail at ScientificTypes.jl. See also the tutorial at the this MLJ Workshop (specifically, here) and this Data Science in Julia tutorial.\n\nAlso relevant is the section, Working with Categorical Data.","category":"section"},{"location":"preparing_data/#Data-transformation","page":"Preparing Data","title":"Data transformation","text":"MLJ's Built-in transformers are documented at Transformers and Other Unsupervised Models. Several of these provide methods of converting categorical features (i.e., those with OrderedFactor or Multiclass element scitype) to Continous features. \n\nVery commonly applied examples are: ContinuousEncoder, OneHotEncoder, FeatureSelector and FillImputer. A Gaussian mixture models imputer is provided by BetaML, which can be loaded with\n\nMissingImputator = @load MissingImputator pkg=BetaML\n\nThis MLJ Workshop, the \"End-to-end examples\" in Data Science in Julia tutorials, and the tutorials in MLJTransforms.jl give further illustrations of data preprocessing in MLJ.","category":"section"},{"location":"preparing_data/#MLJBase.partition","page":"Preparing Data","title":"MLJBase.partition","text":"partition(X, fractions...;\n          shuffle=nothing,\n          rng=Random.GLOBAL_RNG,\n          stratify=nothing,\n          multi=false)\n\nSplits the vector, matrix or table X into a tuple of objects of the same type, whose vertical concatenation is X. The number of rows in each component of the return value is determined by the corresponding fractions of length(nrows(X)), where valid fractions are floats between 0 and 1 whose sum is less than one. The last fraction is not provided, as it is inferred from the preceding ones.\n\nFor synchronized partitioning of multiple objects, use the multi=true option.\n\njulia> partition(1:1000, 0.8)\n([1,...,800], [801,...,1000])\n\njulia> partition(1:1000, 0.2, 0.7)\n([1,...,200], [201,...,900], [901,...,1000])\n\njulia> partition(reshape(1:10, 5, 2), 0.2, 0.4)\n([1 6], [2 7; 3 8], [4 9; 5 10])\n\njulia> X, y = make_blobs() # a table and vector\njulia> Xtrain, Xtest = partition(X, 0.8, stratify=y)\n\nHere's an example of synchronized partitioning of multiple objects:\n\njulia> (Xtrain, Xtest), (ytrain, ytest) = partition((X, y), 0.8, rng=123, multi=true)\n\nKeywords\n\nshuffle=nothing: if set to true, shuffles the rows before taking fractions.\nrng=Random.GLOBAL_RNG: specifies the random number generator to be used, can be an integer seed. If specified, and shuffle === nothing is interpreted as true.\nstratify=nothing: if a vector is specified, the partition will match the stratification of the given vector. In that case, shuffle cannot be false.\nmulti=false: if true then X is expected to be a tuple of objects sharing a common length, which are each partitioned separately using the same specified fractions and the same row shuffling. Returns a tuple of partitions (a tuple of tuples).\n\n\n\n\n\n","category":"function"},{"location":"preparing_data/#MLJBase.unpack","page":"Preparing Data","title":"MLJBase.unpack","text":"unpack(table, f1, f2, ... fk;\n       wrap_singles=false,\n       shuffle=false,\n       rng::Union{AbstractRNG,Int,Nothing}=nothing,\n       coerce_options...)\n\nHorizontally split any Tables.jl compatible table into smaller tables or vectors by making column selections determined by the predicates f1, f2, ..., fk. Selection from the column names is without replacement. A predicate is any object f such that f(name) is true or false for each column name::Symbol of table.\n\nReturns a tuple of tables/vectors with length one greater than the number of supplied predicates, with the last component including all previously unselected columns.\n\njulia> table = DataFrame(x=[1,2], y=['a', 'b'], z=[10.0, 20.0], w=[\"A\", \"B\"])\n2×4 DataFrame\n Row │ x      y     z        w\n     │ Int64  Char  Float64  String\n─────┼──────────────────────────────\n   1 │     1  a        10.0  A\n   2 │     2  b        20.0  B\n\njulia> Z, XY, W = unpack(table, ==(:z), !=(:w));\njulia> Z\n2-element Vector{Float64}:\n 10.0\n 20.0\n\njulia> XY\n2×2 DataFrame\n Row │ x      y\n     │ Int64  Char\n─────┼─────────────\n   1 │     1  a\n   2 │     2  b\n\njulia> W  # the column(s) left over\n2-element Vector{String}:\n \"A\"\n \"B\"\n\nWhenever a returned table contains a single column, it is converted to a vector unless wrap_singles=true.\n\nIf coerce_options are specified then table is first replaced with coerce(table, coerce_options). See ScientificTypes.coerce for details.\n\nIf shuffle=true then the rows of table are first shuffled, using the global RNG, unless rng is specified; if rng is an integer, it specifies the seed of an automatically generated Mersenne twister. If rng is specified then shuffle=true is implicit.\n\n\n\n\n\n","category":"function"},{"location":"models/AgglomerativeClustering_MLJScikitLearnInterface/#AgglomerativeClustering_MLJScikitLearnInterface","page":"AgglomerativeClustering","title":"AgglomerativeClustering","text":"AgglomerativeClustering\n\nA model type for constructing a agglomerative clustering, based on MLJScikitLearnInterface.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nAgglomerativeClustering = @load AgglomerativeClustering pkg=MLJScikitLearnInterface\n\nDo model = AgglomerativeClustering() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in AgglomerativeClustering(n_clusters=...).\n\nRecursively merges the pair of clusters that minimally increases a given linkage distance. Note: there is no predict or transform. Instead, inspect the fitted_params.","category":"section"},{"location":"","page":"Home","title":"Home","text":"<script async defer src=\"https://buttons.github.io/buttons.js\"></script>\n\n<div style=\"font-size:1.25em;font-weight:bold;\">\n  <a href=\"about_mlj\"\n    style=\"color: #389826;\">About</a>           &nbsp;|&nbsp;\n  <a href=\"https://JuliaAI.github.io/MLJ.jl/dev/about_mlj/#Installation\" \n    style=\"color: #389826;\">Install</a>         &nbsp;|&nbsp;\n  <a href=\"learning_mlj\"   style=\"color: #389826;\">Learn</a>            &nbsp;|&nbsp;\n  <a href=\"mlj_cheatsheet\" style=\"color: #9558B2;\">Cheatsheet</a>       &nbsp;|&nbsp;\n  <a href=\"common_mlj_workflows\" style=\"color: #9558B2;\">Workflows</a>\n</div>\n\n<span style=\"color: #9558B2;font-size:4.5em;\">\nMLJ</span>\n<br>\n<span style=\"color: #9558B2;font-size:2.25em;font-style:italic;\">\nA Machine Learning Framework for Julia</span>\n\nTo support MLJ development, please cite these works or star the repo:\n\n(Image: DOI)  (Image: arXiv)\n\n<a class=\"github-button\" \n  href=\"https://github.com/JuliaAI/MLJ.jl\" \n  data-icon=\"octicon-star\" \n  data-size=\"large\" \n  data-show-count=\"true\" \n  aria-label=\"Star JuliaAI/MLJ.jl on GitHub\">\n  Star</a>","category":"section"},{"location":"#Model-Browser","page":"Home","title":"Model Browser","text":"Text Based | Graphical ","category":"section"},{"location":"#Reference-Manual","page":"Home","title":"Reference Manual","text":"","category":"section"},{"location":"#Basics","page":"Home","title":"Basics","text":"Getting Started |  Working with Categorical Data |  Common MLJ Workflows | Machines | MLJ Cheatsheet ","category":"section"},{"location":"#Data","page":"Home","title":"Data","text":"Working with Categorical Data |  Preparing Data | Generating Synthetic Data | OpenML Integration | Correcting Class Imbalance","category":"section"},{"location":"#Models","page":"Home","title":"Models","text":"Model Search | Loading Model Code | Transformers and Other Unsupervised Models | Feature Selection | Simple User Defined Models | List of Supported Models | Third Party Packages ","category":"section"},{"location":"#Meta-algorithms","page":"Home","title":"Meta-algorithms","text":"Evaluating Model Performance | Tuning Models | Composing Models | Controlling Iterative Models | Learning Curves| Correcting Class Imbalance | Thresholding Probabilistic Predictors | Recursive feature elimination","category":"section"},{"location":"#Composition","page":"Home","title":"Composition","text":"Composing Models | Linear Pipelines | Target Transformations | Homogeneous Ensembles | Model Stacking | Learning Networks| Correcting Class Imbalance","category":"section"},{"location":"#Integration","page":"Home","title":"Integration","text":"Logging Workflows | OpenML Integration","category":"section"},{"location":"#Customization-and-Extension","page":"Home","title":"Customization and Extension","text":"Simple User Defined Models | Quick-Start Guide to Adding Models | Adding Models for General Use | Composing Models | Internals | Modifying Behavior","category":"section"},{"location":"#Miscellaneous","page":"Home","title":"Miscellaneous","text":"Weights | Acceleration and Parallelism | Performance Measures ","category":"section"},{"location":"models/SVMNuClassifier_MLJScikitLearnInterface/#SVMNuClassifier_MLJScikitLearnInterface","page":"SVMNuClassifier","title":"SVMNuClassifier","text":"SVMNuClassifier\n\nA model type for constructing a nu-support vector classifier, based on MLJScikitLearnInterface.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nSVMNuClassifier = @load SVMNuClassifier pkg=MLJScikitLearnInterface\n\nDo model = SVMNuClassifier() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in SVMNuClassifier(nu=...).","category":"section"},{"location":"models/SVMNuClassifier_MLJScikitLearnInterface/#Hyper-parameters","page":"SVMNuClassifier","title":"Hyper-parameters","text":"nu = 0.5\nkernel = rbf\ndegree = 3\ngamma = scale\ncoef0 = 0.0\nshrinking = true\ntol = 0.001\ncache_size = 200\nmax_iter = -1\ndecision_function_shape = ovr\nrandom_state = nothing","category":"section"},{"location":"models/KernelPCA_MultivariateStats/#KernelPCA_MultivariateStats","page":"KernelPCA","title":"KernelPCA","text":"KernelPCA\n\nA model type for constructing a kernel prinicipal component analysis model, based on MultivariateStats.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nKernelPCA = @load KernelPCA pkg=MultivariateStats\n\nDo model = KernelPCA() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in KernelPCA(maxoutdim=...).\n\nIn kernel PCA the linear operations of ordinary principal component analysis are performed in a reproducing Hilbert space.","category":"section"},{"location":"models/KernelPCA_MultivariateStats/#Training-data","page":"KernelPCA","title":"Training data","text":"In MLJ or MLJBase, bind an instance model to data with\n\nmach = machine(model, X)\n\nHere:\n\nX is any table of input features (eg, a DataFrame) whose columns are of scitype Continuous; check column scitypes with schema(X).\n\nTrain the machine using fit!(mach, rows=...).","category":"section"},{"location":"models/KernelPCA_MultivariateStats/#Hyper-parameters","page":"KernelPCA","title":"Hyper-parameters","text":"maxoutdim=0: Controls the the dimension (number of columns) of the output, outdim. Specifically, outdim = min(n, indim, maxoutdim), where n is the number of observations and indim the input dimension.\nkernel::Function=(x,y)->x'y: The kernel function, takes in 2 vector arguments x and y, returns a scalar value. Defaults to the dot product of x and y.\nsolver::Symbol=:eig: solver to use for the eigenvalues, one of :eig(default, uses LinearAlgebra.eigen), :eigs(uses Arpack.eigs).\ninverse::Bool=true: perform calculations needed for inverse transform\nbeta::Real=1.0: strength of the ridge regression that learns the inverse transform when inverse is true.\ntol::Real=0.0: Convergence tolerance for eigenvalue solver.\nmaxiter::Int=300: maximum number of iterations for eigenvalue solver.","category":"section"},{"location":"models/KernelPCA_MultivariateStats/#Operations","page":"KernelPCA","title":"Operations","text":"transform(mach, Xnew): Return a lower dimensional projection of the input Xnew, which   should have the same scitype as X above.\ninverse_transform(mach, Xsmall): For a dimension-reduced table Xsmall, such as returned by transform, reconstruct a table, having same the number of columns as the original training data X, that transforms to Xsmall.  Mathematically, inverse_transform is a right-inverse for the PCA projection map, whose image is orthogonal to the kernel of that map. In particular, if Xsmall = transform(mach, Xnew), then inverse_transform(Xsmall) is only an approximation to Xnew.","category":"section"},{"location":"models/KernelPCA_MultivariateStats/#Fitted-parameters","page":"KernelPCA","title":"Fitted parameters","text":"The fields of fitted_params(mach) are:\n\nprojection: Returns the projection matrix, which has size (indim, outdim), where indim and outdim are the number of features of the input and ouput respectively.","category":"section"},{"location":"models/KernelPCA_MultivariateStats/#Report","page":"KernelPCA","title":"Report","text":"The fields of report(mach) are:\n\nindim: Dimension (number of columns) of the training data and new data to be transformed.\noutdim: Dimension of transformed data.\nprincipalvars: The variance of the principal components.","category":"section"},{"location":"models/KernelPCA_MultivariateStats/#Examples","page":"KernelPCA","title":"Examples","text":"using MLJ\nusing LinearAlgebra\n\nKernelPCA = @load KernelPCA pkg=MultivariateStats\n\nX, y = @load_iris ## a table and a vector\n\nfunction rbf_kernel(length_scale)\n    return (x,y) -> norm(x-y)^2 / ((2 * length_scale)^2)\nend\n\nmodel = KernelPCA(maxoutdim=2, kernel=rbf_kernel(1))\nmach = machine(model, X) |> fit!\n\nXproj = transform(mach, X)\n\nSee also PCA, ICA, FactorAnalysis, PPCA","category":"section"},{"location":"models/StableRulesClassifier_SIRUS/#StableRulesClassifier_SIRUS","page":"StableRulesClassifier","title":"StableRulesClassifier","text":"StableRulesClassifier\n\nA model type for constructing a stable rules classifier, based on SIRUS.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nStableRulesClassifier = @load StableRulesClassifier pkg=SIRUS\n\nDo model = StableRulesClassifier() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in StableRulesClassifier(rng=...).\n\nStableRulesClassifier implements the explainable rule-based model based on a random forest.","category":"section"},{"location":"models/StableRulesClassifier_SIRUS/#Training-data","page":"StableRulesClassifier","title":"Training data","text":"In MLJ or MLJBase, bind an instance model to data with\n\nmach = machine(model, X, y)\n\nwhere\n\nX: any table of input features (eg, a DataFrame) whose columns each have one of the following element scitypes: Continuous, Count, or <:OrderedFactor; check column scitypes with schema(X)\ny: the target, which can be any AbstractVector whose element scitype is <:OrderedFactor or <:Multiclass; check the scitype with scitype(y)\n\nTrain the machine with fit!(mach, rows=...).","category":"section"},{"location":"models/StableRulesClassifier_SIRUS/#Hyperparameters","page":"StableRulesClassifier","title":"Hyperparameters","text":"rng::AbstractRNG=default_rng(): Random number generator.   Using a StableRNG from StableRNGs.jl is advised.\npartial_sampling::Float64=0.7:   Ratio of samples to use in each subset of the data.   The default should be fine for most cases.\nn_trees::Int=1000:   The number of trees to use.   It is advisable to use at least thousand trees to for a better rule selection, and   in turn better predictive performance.\nmax_depth::Int=2:   The depth of the tree.   A lower depth decreases model complexity and can therefore improve accuracy when the sample size is small (reduce overfitting).\nq::Int=10: Number of cutpoints to use per feature.   The default value should be fine for most situations.\nmin_data_in_leaf::Int=5: Minimum number of data points per leaf.\nmax_rules::Int=10:   This is the most important hyperparameter after lambda.   The more rules, the more accurate the model should be.   If this is not the case, tune lambda first.   However, more rules will also decrease model interpretability.   So, it is important to find a good balance here.   In most cases, 10 to 40 rules should provide reasonable accuracy while remaining interpretable.\nlambda::Float64=1.0:   The weights of the final rules are determined via a regularized regression over each rule as a binary feature.   This hyperparameter specifies the strength of the ridge (L2) regularizer.   SIRUS is very sensitive to the choice of this hyperparameter.   Ensure that you try the full range from 10^-4 to 10^4 (e.g., 0.001, 0.01, ..., 100).   When trying the range, one good check is to verify that an increase in max_rules increases performance.   If this is not the case, then try a different value for lambda.","category":"section"},{"location":"models/StableRulesClassifier_SIRUS/#Fitted-parameters","page":"StableRulesClassifier","title":"Fitted parameters","text":"The fields of fitted_params(mach) are:\n\nfitresult: A StableRules object.","category":"section"},{"location":"models/StableRulesClassifier_SIRUS/#Operations","page":"StableRulesClassifier","title":"Operations","text":"predict(mach, Xnew): Return a vector of predictions for each row of Xnew.","category":"section"},{"location":"quick_start_guide_to_adding_models/#Quick-Start-Guide-to-Adding-Models","page":"Quick-Start Guide to Adding Models","title":"Quick-Start Guide to Adding Models","text":"This guide has moved to this section of the MLJModelInterface.jl documentation.\n\nFor quick-and-dirty user-defined models, not intended for registering with the MLJ Model Registry, see Simple User Defined Models. ","category":"section"},{"location":"target_transformations/#Target-Transformations","page":"Target Transformations","title":"Target Transformations","text":"Some supervised models work best if the target variable has been standardized, i.e., rescaled to have zero mean and unit variance. Such a target transformation is learned from the values of the training target variable. In particular, one generally learns a different transformation when training on a proper subset of the training data. Good data hygiene prescribes that a new transformation should be computed each time the supervised model is trained on new data - for example in cross-validation.\n\nAdditionally, one generally wants to inverse transform the predictions of the supervised model for the final target predictions to be on the original scale.\n\nAll these concerns are addressed by wrapping the supervised model using TransformedTargetModel:\n\nRidge = @load RidgeRegressor pkg=MLJLinearModels verbosity=0\nridge = Ridge(fit_intercept=false)\nridge2 = TransformedTargetModel(ridge, transformer=Standardizer())\n\nNote that all the original hyperparameters, as well as those of the Standardizer, are accessible as nested hyper-parameters of the wrapped model, which can be trained or evaluated like any other:\n\nX, y = make_regression(rng=1234, intercept=false)\ny = y*10^5\nmach = machine(ridge2, X, y)\nfit!(mach, rows=1:60, verbosity=0)\npredict(mach, rows=61:62)\n\nTraining and predicting using ridge2 as above means:\n\nStandardizing the target y using the first 60 rows to get a new target z\nTraining the original ridge model using the first 60 rows of X and z\nCalling predict on the machine trained in Step 2 on rows 61:62 of X\nApplying the inverse scaling learned in Step 1 to those predictions (to get the final output shown above)\n\nSince both ridge and ridge2 return predictions on the original scale, we can meaningfully compare the corresponding mean absolute errors, which are indeed different in this case.\n\nevaluate(ridge, X, y, measure=l1)\n\nevaluate(ridge2, X, y, measure=l1)\n\nOrdinary functions can also be used in target transformations but an inverse must be explicitly specified:\n\nridge3 = TransformedTargetModel(ridge, transformer=y->log.(y), inverse=z->exp.(z))\nX, y = @load_boston\nevaluate(ridge3, X, y, measure=l1)","category":"section"},{"location":"target_transformations/#MLJBase.TransformedTargetModel","page":"Target Transformations","title":"MLJBase.TransformedTargetModel","text":"TransformedTargetModel(model; transformer=nothing, inverse=nothing, cache=true)\n\nWrap the supervised or semi-supervised model in a transformation of the target variable.\n\nHere transformer one of the following:\n\nThe Unsupervised model that is to transform the training target. By default (inverse=nothing) the parameters learned by this transformer are also used to inverse-transform the predictions of model, which means transformer must implement the inverse_transform method. If this is not the case, specify inverse=identity to suppress inversion.\nA callable object for transforming the target, such as y -> log.(y). In this case a callable inverse, such as z -> exp.(z), should be specified.\n\nSpecify cache=false to prioritize memory over speed, or to guarantee data anonymity.\n\nSpecify inverse=identity if model is a probabilistic predictor, as inverse-transforming sample spaces is not supported. Alternatively, replace model with a deterministic model, such as Pipeline(model, y -> mode.(y)).\n\nExamples\n\nA model that normalizes the target before applying ridge regression, with predictions returned on the original scale:\n\n@load RidgeRegressor pkg=MLJLinearModels\nmodel = RidgeRegressor()\ntmodel = TransformedTargetModel(model, transformer=Standardizer())\n\nA model that applies a static log transformation to the data, again returning predictions to the original scale:\n\ntmodel2 = TransformedTargetModel(model, transformer=y->log.(y), inverse=z->exp.(y))\n\n\n\n\n\n","category":"function"},{"location":"models/SVMClassifier_MLJScikitLearnInterface/#SVMClassifier_MLJScikitLearnInterface","page":"SVMClassifier","title":"SVMClassifier","text":"SVMClassifier\n\nA model type for constructing a C-support vector classifier, based on MLJScikitLearnInterface.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nSVMClassifier = @load SVMClassifier pkg=MLJScikitLearnInterface\n\nDo model = SVMClassifier() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in SVMClassifier(C=...).","category":"section"},{"location":"models/SVMClassifier_MLJScikitLearnInterface/#Hyper-parameters","page":"SVMClassifier","title":"Hyper-parameters","text":"C = 1.0\nkernel = rbf\ndegree = 3\ngamma = scale\ncoef0 = 0.0\nshrinking = true\ntol = 0.001\ncache_size = 200\nmax_iter = -1\ndecision_function_shape = ovr\nrandom_state = nothing","category":"section"},{"location":"models/PCADetector_OutlierDetectionPython/#PCADetector_OutlierDetectionPython","page":"PCADetector","title":"PCADetector","text":"PCADetector(n_components = nothing,\n               n_selected_components = nothing,\n               copy = true,\n               whiten = false,\n               svd_solver = \"auto\",\n               tol = 0.0\n               iterated_power = \"auto\",\n               standardization = true,\n               weighted = true,\n               random_state = nothing)\n\nhttps://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.pca","category":"section"},{"location":"models/RandomForestClassifier_DecisionTree/#RandomForestClassifier_DecisionTree","page":"RandomForestClassifier","title":"RandomForestClassifier","text":"RandomForestClassifier\n\nA model type for constructing a CART random forest classifier, based on DecisionTree.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nRandomForestClassifier = @load RandomForestClassifier pkg=DecisionTree\n\nDo model = RandomForestClassifier() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in RandomForestClassifier(max_depth=...).\n\nRandomForestClassifier implements the standard Random Forest algorithm, originally published in Breiman, L. (2001): \"Random Forests.\", Machine Learning, vol. 45, pp. 5–32.","category":"section"},{"location":"models/RandomForestClassifier_DecisionTree/#Training-data","page":"RandomForestClassifier","title":"Training data","text":"In MLJ or MLJBase, bind an instance model to data with\n\nmach = machine(model, X, y)\n\nwhere\n\nX: any table of input features (eg, a DataFrame) whose columns each have one of the following element scitypes: Continuous, Count, or <:OrderedFactor; check column scitypes with schema(X)\ny: the target, which can be any AbstractVector whose element scitype is <:OrderedFactor or <:Multiclass; check the scitype with scitype(y)\n\nTrain the machine with fit!(mach, rows=...).","category":"section"},{"location":"models/RandomForestClassifier_DecisionTree/#Hyperparameters","page":"RandomForestClassifier","title":"Hyperparameters","text":"max_depth=-1:          max depth of the decision tree (-1=any)\nmin_samples_leaf=1:    min number of samples each leaf needs to have\nmin_samples_split=2:   min number of samples needed for a split\nmin_purity_increase=0: min purity needed for a split\nn_subfeatures=-1: number of features to select at random (0 for all, -1 for square root of number of features)\nn_trees=10:            number of trees to train\nsampling_fraction=0.7  fraction of samples to train each tree on\nfeature_importance: method to use for computing feature importances. One of (:impurity, :split)\nrng=Random.GLOBAL_RNG: random number generator or seed","category":"section"},{"location":"models/RandomForestClassifier_DecisionTree/#Operations","page":"RandomForestClassifier","title":"Operations","text":"predict(mach, Xnew): return predictions of the target given features Xnew having the same scitype as X above. Predictions are probabilistic, but uncalibrated.\npredict_mode(mach, Xnew): instead return the mode of each prediction above.","category":"section"},{"location":"models/RandomForestClassifier_DecisionTree/#Fitted-parameters","page":"RandomForestClassifier","title":"Fitted parameters","text":"The fields of fitted_params(mach) are:\n\nforest: the Ensemble object returned by the core DecisionTree.jl algorithm","category":"section"},{"location":"models/RandomForestClassifier_DecisionTree/#Report","page":"RandomForestClassifier","title":"Report","text":"The fields of report(mach) are:\n\nfeatures: the names of the features encountered in training","category":"section"},{"location":"models/RandomForestClassifier_DecisionTree/#Accessor-functions","page":"RandomForestClassifier","title":"Accessor functions","text":"feature_importances(mach) returns a vector of (feature::Symbol => importance) pairs; the type of importance is determined by the hyperparameter feature_importance (see above)","category":"section"},{"location":"models/RandomForestClassifier_DecisionTree/#Examples","page":"RandomForestClassifier","title":"Examples","text":"using MLJ\nForest = @load RandomForestClassifier pkg=DecisionTree\nforest = Forest(min_samples_split=6, n_subfeatures=3)\n\nX, y = @load_iris\nmach = machine(forest, X, y) |> fit!\n\nXnew = (sepal_length = [6.4, 7.2, 7.4],\n        sepal_width = [2.8, 3.0, 2.8],\n        petal_length = [5.6, 5.8, 6.1],\n        petal_width = [2.1, 1.6, 1.9],)\nyhat = predict(mach, Xnew) ## probabilistic predictions\npredict_mode(mach, Xnew)   ## point predictions\npdf.(yhat, \"virginica\")    ## probabilities for the \"verginica\" class\n\nfitted_params(mach).forest ## raw `Ensemble` object from DecisionTrees.jl\n\nfeature_importances(mach)  ## `:impurity` feature importances\nforest.feature_importance = :split\nfeature_importance(mach)   ## `:split` feature importances\n\n\nSee also DecisionTree.jl and the unwrapped model type MLJDecisionTreeInterface.DecisionTree.RandomForestClassifier.","category":"section"},{"location":"models/LADRegressor_MLJLinearModels/#LADRegressor_MLJLinearModels","page":"LADRegressor","title":"LADRegressor","text":"LADRegressor\n\nA model type for constructing a lad regressor, based on MLJLinearModels.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nLADRegressor = @load LADRegressor pkg=MLJLinearModels\n\nDo model = LADRegressor() to construct an instance with default hyper-parameters.\n\nLeast absolute deviation regression is a linear model with objective function\n\n$\n\n∑ρ(Xθ - y) + n⋅λ|θ|₂² + n⋅γ|θ|₁ $\n\nwhere ρ is the absolute loss and n is the number of observations.\n\nIf scale_penalty_with_samples = false the objective function is instead\n\n$\n\n∑ρ(Xθ - y) + λ|θ|₂² + γ|θ|₁ $\n\n.\n\nDifferent solver options exist, as indicated under \"Hyperparameters\" below. ","category":"section"},{"location":"models/LADRegressor_MLJLinearModels/#Training-data","page":"LADRegressor","title":"Training data","text":"In MLJ or MLJBase, bind an instance model to data with\n\nmach = machine(model, X, y)\n\nwhere:\n\nX is any table of input features (eg, a DataFrame) whose columns have Continuous scitype; check column scitypes with schema(X)\ny is the target, which can be any AbstractVector whose element scitype is Continuous; check the scitype with scitype(y)\n\nTrain the machine using fit!(mach, rows=...).","category":"section"},{"location":"models/LADRegressor_MLJLinearModels/#Hyperparameters","page":"LADRegressor","title":"Hyperparameters","text":"See also RobustRegressor.","category":"section"},{"location":"models/LADRegressor_MLJLinearModels/#Parameters","page":"LADRegressor","title":"Parameters","text":"lambda::Real: strength of the regularizer if penalty is :l2 or :l1.     Strength of the L2 regularizer if penalty is :en. Default: 1.0\ngamma::Real: strength of the L1 regularizer if penalty is :en. Default: 0.0\npenalty::Union{String, Symbol}: the penalty to use, either :l2, :l1, :en (elastic net) or :none. Default: :l2\nfit_intercept::Bool: whether to fit the intercept or not. Default: true\npenalize_intercept::Bool: whether to penalize the intercept. Default: false\nscale_penalty_with_samples::Bool: whether to scale the penalty with the number of observations. Default: true\nsolver::Union{Nothing, MLJLinearModels.Solver}: some instance of MLJLinearModels.S where S is one of: LBFGS, IWLSCG, if penalty = :l2, and ProxGrad otherwise.\nIf solver = nothing (default) then LBFGS() is used, if penalty = :l2, and otherwise ProxGrad(accel=true) (FISTA) is used.\nSolver aliases: FISTA(; kwargs...) = ProxGrad(accel=true, kwargs...), ISTA(; kwargs...) = ProxGrad(accel=false, kwargs...) Default: nothing","category":"section"},{"location":"models/LADRegressor_MLJLinearModels/#Example","page":"LADRegressor","title":"Example","text":"using MLJ\nX, y = make_regression()\nmach = fit!(machine(LADRegressor(), X, y))\npredict(mach, X)\nfitted_params(mach)","category":"section"},{"location":"models/RidgeRegressor_MLJLinearModels/#RidgeRegressor_MLJLinearModels","page":"RidgeRegressor","title":"RidgeRegressor","text":"RidgeRegressor\n\nA model type for constructing a ridge regressor, based on MLJLinearModels.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nRidgeRegressor = @load RidgeRegressor pkg=MLJLinearModels\n\nDo model = RidgeRegressor() to construct an instance with default hyper-parameters.\n\nRidge regression is a linear model with objective function\n\n$\n\n|Xθ - y|₂²/2 + n⋅λ|θ|₂²/2 $\n\nwhere n is the number of observations.\n\nIf scale_penalty_with_samples = false then the objective function is instead\n\n$\n\n|Xθ - y|₂²/2 + λ|θ|₂²/2 $\n\n.\n\nDifferent solver options exist, as indicated under \"Hyperparameters\" below. ","category":"section"},{"location":"models/RidgeRegressor_MLJLinearModels/#Training-data","page":"RidgeRegressor","title":"Training data","text":"In MLJ or MLJBase, bind an instance model to data with\n\nmach = machine(model, X, y)\n\nwhere:\n\nX is any table of input features (eg, a DataFrame) whose columns have Continuous scitype; check column scitypes with schema(X)\ny is the target, which can be any AbstractVector whose element scitype is Continuous; check the scitype with scitype(y)\n\nTrain the machine using fit!(mach, rows=...).","category":"section"},{"location":"models/RidgeRegressor_MLJLinearModels/#Hyperparameters","page":"RidgeRegressor","title":"Hyperparameters","text":"lambda::Real: strength of the L2 regularization. Default: 1.0\nfit_intercept::Bool: whether to fit the intercept or not. Default: true\npenalize_intercept::Bool: whether to penalize the intercept. Default: false\nscale_penalty_with_samples::Bool: whether to scale the penalty with the number of observations. Default: true\nsolver::Union{Nothing, MLJLinearModels.Solver}: any instance of MLJLinearModels.Analytical. Use Analytical() for Cholesky and CG()=Analytical(iterative=true) for conjugate-gradient. If solver = nothing (default) then Analytical() is used.  Default: nothing","category":"section"},{"location":"models/RidgeRegressor_MLJLinearModels/#Example","page":"RidgeRegressor","title":"Example","text":"using MLJ\nX, y = make_regression()\nmach = fit!(machine(RidgeRegressor(), X, y))\npredict(mach, X)\nfitted_params(mach)\n\nSee also ElasticNetRegressor.","category":"section"},{"location":"models/KNNDetector_OutlierDetectionPython/#KNNDetector_OutlierDetectionPython","page":"KNNDetector","title":"KNNDetector","text":"KNNDetector(n_neighbors = 5,\n               method = \"largest\",\n               radius = 1.0,\n               algorithm = \"auto\",\n               leaf_size = 30,\n               metric = \"minkowski\",\n               p = 2,\n               metric_params = nothing,\n               n_jobs = 1)\n\nhttps://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.knn","category":"section"},{"location":"models/LinearRegressor_MultivariateStats/#LinearRegressor_MultivariateStats","page":"LinearRegressor","title":"LinearRegressor","text":"LinearRegressor\n\nA model type for constructing a linear regressor, based on MultivariateStats.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nLinearRegressor = @load LinearRegressor pkg=MultivariateStats\n\nDo model = LinearRegressor() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in LinearRegressor(bias=...).\n\nLinearRegressor assumes the target is a Continuous variable and trains a linear prediction function using the least squares algorithm. Options exist to specify a bias  term.","category":"section"},{"location":"models/LinearRegressor_MultivariateStats/#Training-data","page":"LinearRegressor","title":"Training data","text":"In MLJ or MLJBase, bind an instance model to data with\n\nmach = machine(model, X, y)\n\nHere:\n\nX is any table of input features (eg, a DataFrame) whose columns are of scitype    Continuous; check the column scitypes with schema(X).\ny is the target, which can be any AbstractVector whose element scitype is    Continuous; check the scitype with scitype(y).\n\nTrain the machine using fit!(mach, rows=...).","category":"section"},{"location":"models/LinearRegressor_MultivariateStats/#Hyper-parameters","page":"LinearRegressor","title":"Hyper-parameters","text":"bias=true: Include the bias term if true, otherwise fit without bias term.","category":"section"},{"location":"models/LinearRegressor_MultivariateStats/#Operations","page":"LinearRegressor","title":"Operations","text":"predict(mach, Xnew): Return predictions of the target given new features Xnew, which    should have the same scitype as X above.","category":"section"},{"location":"models/LinearRegressor_MultivariateStats/#Fitted-parameters","page":"LinearRegressor","title":"Fitted parameters","text":"The fields of fitted_params(mach) are:\n\ncoefficients: The linear coefficients determined by the model.\nintercept: The intercept determined by the model.","category":"section"},{"location":"models/LinearRegressor_MultivariateStats/#Examples","page":"LinearRegressor","title":"Examples","text":"using MLJ\n\nLinearRegressor = @load LinearRegressor pkg=MultivariateStats\nlinear_regressor = LinearRegressor()\n\nX, y = make_regression(100, 2) ## a table and a vector (synthetic data)\nmach = machine(linear_regressor, X, y) |> fit!\n\nXnew, _ = make_regression(3, 2)\nyhat = predict(mach, Xnew) ## new predictions\n\nSee also MultitargetLinearRegressor, RidgeRegressor, MultitargetRidgeRegressor","category":"section"},{"location":"models/QuantileRegressor_MLJLinearModels/#QuantileRegressor_MLJLinearModels","page":"QuantileRegressor","title":"QuantileRegressor","text":"QuantileRegressor\n\nA model type for constructing a quantile regressor, based on MLJLinearModels.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nQuantileRegressor = @load QuantileRegressor pkg=MLJLinearModels\n\nDo model = QuantileRegressor() to construct an instance with default hyper-parameters.\n\nThis model coincides with RobustRegressor, with the exception that the robust loss, rho, is fixed to QuantileRho(delta), where delta is a new hyperparameter.\n\nDifferent solver options exist, as indicated under \"Hyperparameters\" below. ","category":"section"},{"location":"models/QuantileRegressor_MLJLinearModels/#Training-data","page":"QuantileRegressor","title":"Training data","text":"In MLJ or MLJBase, bind an instance model to data with\n\nmach = machine(model, X, y)\n\nwhere:\n\nX is any table of input features (eg, a DataFrame) whose columns have Continuous scitype; check column scitypes with schema(X)\ny is the target, which can be any AbstractVector whose element scitype is Continuous; check the scitype with scitype(y)\n\nTrain the machine using fit!(mach, rows=...).","category":"section"},{"location":"models/QuantileRegressor_MLJLinearModels/#Hyperparameters","page":"QuantileRegressor","title":"Hyperparameters","text":"delta::Real: parameterizes the QuantileRho function (indicating the quantile to use     with default 0.5 for the median regression) Default: 0.5\nlambda::Real: strength of the regularizer if penalty is :l2 or :l1.     Strength of the L2 regularizer if penalty is :en. Default: 1.0\ngamma::Real: strength of the L1 regularizer if penalty is :en. Default: 0.0\npenalty::Union{String, Symbol}: the penalty to use, either :l2, :l1, :en (elastic net) or :none. Default: :l2\nfit_intercept::Bool: whether to fit the intercept or not. Default: true\npenalize_intercept::Bool: whether to penalize the intercept. Default: false\nscale_penalty_with_samples::Bool: whether to scale the penalty with the number of observations. Default: true\nsolver::Union{Nothing, MLJLinearModels.Solver}: some instance of MLJLinearModels.S where S is one of: LBFGS, IWLSCG, if penalty = :l2, and ProxGrad otherwise.\nIf solver = nothing (default) then LBFGS() is used, if penalty = :l2, and otherwise ProxGrad(accel=true) (FISTA) is used.\nSolver aliases: FISTA(; kwargs...) = ProxGrad(accel=true, kwargs...), ISTA(; kwargs...) = ProxGrad(accel=false, kwargs...) Default: nothing","category":"section"},{"location":"models/QuantileRegressor_MLJLinearModels/#Example","page":"QuantileRegressor","title":"Example","text":"using MLJ\nX, y = make_regression()\nmach = fit!(machine(QuantileRegressor(), X, y))\npredict(mach, X)\nfitted_params(mach)\n\nSee also RobustRegressor, HuberRegressor.","category":"section"},{"location":"mlj_cheatsheet/#MLJ-Cheatsheet","page":"MLJ Cheatsheet","title":"MLJ Cheatsheet","text":"","category":"section"},{"location":"mlj_cheatsheet/#Starting-an-interactive-MLJ-session","page":"MLJ Cheatsheet","title":"Starting an interactive MLJ session","text":"using MLJ\nMLJ_VERSION","category":"section"},{"location":"mlj_cheatsheet/#Model-search-and-code-loading","page":"MLJ Cheatsheet","title":"Model search and code loading","text":"info(\"PCA\") retrieves registry metadata for the model called \"PCA\"\n\ninfo(\"RidgeRegressor\", pkg=\"MultivariateStats\") retrieves metadata for \"RidgeRegresssor\", which is provided by multiple packages\n\ndoc(\"DecisionTreeClassifier\", pkg=\"DecisionTree\") retrieves the model document string for the classifier, without loading model code\n\nmodels() lists metadata of every registered model.\n\nmodels(\"Tree\") lists models with \"Tree\" in the model or package name.\n\nmodels(x -> x.is_supervised && x.is_pure_julia) lists all supervised models written in pure julia.\n\nmodels(matching(X)) lists all unsupervised models compatible with input X.\n\nmodels(matching(X, y)) lists all supervised models compatible with input/target X/y.\n\nWith additional conditions:\n\nmodels() do model\n    matching(model, X, y) &&\n    model.prediction_type == :probabilistic &&\n    model.is_pure_julia\nend\n\nTree = @load DecisionTreeClassifier pkg=DecisionTree\n\nimports \"DecisionTreeClassifier\" type and binds it to Tree.\n\ntree = Tree() to instantiate a Tree.\n\ntree2 = Tree(max_depth=2) instantiates a tree with different hyperparameter\n\nRidge = @load RidgeRegressor pkg=MultivariateStats imports a type for a model provided by multiple packages\n\nFor interactive loading instead, use @iload","category":"section"},{"location":"mlj_cheatsheet/#Scitypes-and-coercion","page":"MLJ Cheatsheet","title":"Scitypes and coercion","text":"scitype(x) is the scientific type of x. For example scitype(2.4) == Continuous\n\n(Image: scitypes_small.png)\n\ntype scitype\nAbstractFloat Continuous\nInteger Count\nCategoricalValue and CategoricalString Multiclass or OrderedFactor\nAbstractString Textual\n\nFigure and Table for common scalar scitypes\n\nUse schema(X) to get the column scitypes of a table X\n\nTo coerce the data into different scitypes, use the coerce function:\n\ncoerce(y, Multiclass) attempts coercion of all elements of y into scitype Multiclass\ncoerce(X, :x1 => Continuous, :x2 => OrderedFactor) to coerce columns :x1 and :x2 of table X.\ncoerce(X, Count => Continuous) to coerce all columns with Count scitype to Continuous.","category":"section"},{"location":"mlj_cheatsheet/#Ingesting-data","page":"MLJ Cheatsheet","title":"Ingesting data","text":"Split the table channing into target y (the :Exit column) and features X (everything else), after a seeded row shuffling:\n\nusing RDatasets\nchanning = dataset(\"boot\", \"channing\")\ny, X =  unpack(channing, ==(:Exit); rng=123)\n\nSame as above but exclude :Time column from X:\n\nusing RDatasets\nchanning = dataset(\"boot\", \"channing\")\ny, X = unpack(channing,\n              ==(:Exit),\n              !=(:Time);\n              rng=123)\n\nHere, y is assigned the :Exit column, and X is assigned the rest, except :Time.\n\nSplitting row indices into train/validation/test, with seeded shuffling:\n\ntrain, valid, test = partition(eachindex(y), 0.7, 0.2, rng=1234) # for 70:20:10 ratio\n\nFor a stratified split:\n\ntrain, test = partition(eachindex(y), 0.8, stratify=y)\n\nSplit a table or matrix X, instead of indices:\n\nXtrain, Xvalid, Xtest = partition(X, 0.5, 0.3, rng=123)\n\nSimultaneous splitting (needs multi=true):\n\n(Xtrain, Xtest), (ytrain, ytest) = partition((X, y), 0.8, rng=123, multi=true)\n\nGetting data from OpenML:\n\ntable = OpenML.load(91)\n\nCreating synthetic classification data:\n\nX, y = make_blobs(100, 2)\n\n(also: make_moons, make_circles, make_regression)\n\nCreating synthetic regression data:\n\nX, y = make_regression(100, 2)","category":"section"},{"location":"mlj_cheatsheet/#Machine-construction","page":"MLJ Cheatsheet","title":"Machine construction","text":"Supervised case:\n\nmodel = KNNRegressor(K=1)\nmach = machine(model, X, y)\n\nUnsupervised case:\n\nmodel = OneHotEncoder()\nmach = machine(model, X)","category":"section"},{"location":"mlj_cheatsheet/#Fitting","page":"MLJ Cheatsheet","title":"Fitting","text":"The fit! function can be used to fit a machine (defaults shown):\n\nfit!(mach, rows=1:100, verbosity=1, force=false)","category":"section"},{"location":"mlj_cheatsheet/#Prediction","page":"MLJ Cheatsheet","title":"Prediction","text":"Supervised case: predict(mach, Xnew) or predict(mach, rows=1:100)\nFor probabilistic models: predict_mode, predict_mean and predict_median.\nUnsupervised case: W = transform(mach, Xnew) or inverse_transform(mach, W), etc.","category":"section"},{"location":"mlj_cheatsheet/#Inspecting-objects","page":"MLJ Cheatsheet","title":"Inspecting objects","text":"info(ConstantRegressor()), info(\"PCA\"), info(\"RidgeRegressor\", pkg=\"MultivariateStats\") gets all properties (aka traits) of registered models\n\nschema(X) get column names, types and scitypes, and nrows, of a table X\n\nscitype(X) gets the scientific type of X\n\nfitted_params(mach) gets learned parameters of the fitted machine\n\nreport(mach) gets other training results (e.g. feature rankings)","category":"section"},{"location":"mlj_cheatsheet/#Saving-and-retrieving-machines-using-Julia-serializer","page":"MLJ Cheatsheet","title":"Saving and retrieving machines using Julia serializer","text":"MLJ.save(\"my_machine.jls\", mach) to save machine mach (without data)\n\npredict_only_mach = machine(\"my_machine.jls\") to deserialize.","category":"section"},{"location":"mlj_cheatsheet/#Performance-estimation","page":"MLJ Cheatsheet","title":"Performance estimation","text":"evaluate(model, X, y, resampling=CV(), measure=rms)\n\nevaluate!(mach, resampling=Holdout(), measure=[rms, mav])\n\nevaluate!(mach, resampling=[(fold1, fold2), (fold2, fold1)], measure=rms)","category":"section"},{"location":"mlj_cheatsheet/#Resampling-strategies-(resampling...)","page":"MLJ Cheatsheet","title":"Resampling strategies (resampling=...)","text":"Holdout(fraction_train=0.7, rng=1234) for simple holdout\n\nCV(nfolds=6, rng=1234) for cross-validation\n\nStratifiedCV(nfolds=6, rng=1234) for stratified cross-validation\n\nTimeSeriesCV(nfolds=4) for time-series cross-validation\n\nInSample(): test set = train set\n\nor a list of pairs of row indices:\n\n[(train1, eval1), (train2, eval2), ... (traink, evalk)]","category":"section"},{"location":"mlj_cheatsheet/#Tuning-model-wrapper","page":"MLJ Cheatsheet","title":"Tuning model wrapper","text":"tuned_model = TunedModel(model; tuning=RandomSearch(), resampling=Holdout(), measure=…, range=…)","category":"section"},{"location":"mlj_cheatsheet/#Ranges-for-tuning-(range...)","page":"MLJ Cheatsheet","title":"Ranges for tuning (range=...)","text":"If r = range(KNNRegressor(), :K, lower=1, upper = 20, scale=:log)\n\nthen Grid() search uses iterator(r, 6) == [1, 2, 3, 6, 11, 20].\n\nlower=-Inf and upper=Inf are allowed.\n\nNon-numeric ranges: r = range(model, :parameter, values=…)\n\nInstead of model, declare type: r = range(Char, :c; values=['a', 'b'])\n\nNested ranges: Use dot syntax, as in r = range(EnsembleModel(atom=tree), :(atom.max_depth), ...)\n\nSpecify multiple ranges, as in range=[r1, r2, r3]. For more range options do ?Grid or ?RandomSearch","category":"section"},{"location":"mlj_cheatsheet/#Tuning-strategies","page":"MLJ Cheatsheet","title":"Tuning strategies","text":"RandomSearch(rng=1234) for basic random search\n\nGrid(resolution=10) or Grid(goal=50) for basic grid search\n\nAlso available: LatinHyperCube, Explicit (built-in), MLJTreeParzenTuning, ParticleSwarm, AdaptiveParticleSwarm (3rd-party packages)","category":"section"},{"location":"mlj_cheatsheet/#Learning-curves","page":"MLJ Cheatsheet","title":"Learning curves","text":"For generating a plot of performance against parameter specified by range:\n\ncurve = learning_curve(mach, resolution=30, resampling=Holdout(), measure=…, range=…, n=1)\n\ncurve = learning_curve(model, X, y, resolution=30, resampling=Holdout(), measure=…, range=…, n=1)\n\nIf using Plots.jl:\n\nplot(curve.parameter_values, curve.measurements, xlab=curve.parameter_name, xscale=curve.parameter_scale)","category":"section"},{"location":"mlj_cheatsheet/#Controlling-iterative-models","page":"MLJ Cheatsheet","title":"Controlling iterative models","text":"Requires: using MLJIteration\n\niterated_model = IteratedModel(model=…, resampling=Holdout(), measure=…, controls=…, retrain=false)","category":"section"},{"location":"mlj_cheatsheet/#Controls","page":"MLJ Cheatsheet","title":"Controls","text":"Increment training: Step(n=1)\n\nStopping: TimeLimit(t=0.5) (in hours), NumberLimit(n=100), NumberSinceBest(n=6), NotANumber(), Threshold(value=0.0), GL(alpha=2.0), PQ(alpha=0.75, k=5), Patience(n=5)\n\nLogging: Info(f=identity), Warn(f=\"\"), Error(predicate, f=\"\")\n\nCallbacks: Callback(f=mach->nothing), WithNumberDo(f=n->@info(n)), WithIterationsDo(f=i->@info(\"num iterations: $i\")), WithLossDo(f=x->@info(\"loss: $x\")), WithTrainingLossesDo(f=v->@info(v))\n\nSnapshots: Save(filename=\"machine.jlso\")\n\nWraps: MLJIteration.skip(control, predicate=1), IterationControl.with_state_do(control)","category":"section"},{"location":"mlj_cheatsheet/#Performance-measures-(metrics)","page":"MLJ Cheatsheet","title":"Performance measures (metrics)","text":"Do measures() to get full list.\n\nDo measures(\"log\") to list measures with \"log\" in doc-string.","category":"section"},{"location":"mlj_cheatsheet/#Transformers","page":"MLJ Cheatsheet","title":"Transformers","text":"Built-ins include: Standardizer, OneHotEncoder, UnivariateBoxCoxTransformer, FeatureSelector, FillImputer, UnivariateDiscretizer, ContinuousEncoder, UnivariateTimeTypeToContinuous\n\nExternals include: PCA (in MultivariateStats), KMeans, KMedoids (in Clustering).\n\nmodels(m -> !m.is_supervised) to get full list","category":"section"},{"location":"mlj_cheatsheet/#Ensemble-model-wrapper","page":"MLJ Cheatsheet","title":"Ensemble model wrapper","text":"EnsembleModel(model; weights=Float64[], bagging_fraction=0.8, rng=GLOBAL_RNG, n=100, parallel=true, out_of_bag_measure=[])","category":"section"},{"location":"mlj_cheatsheet/#Target-transformation-wrapper","page":"MLJ Cheatsheet","title":"Target transformation wrapper","text":"TransformedTargetModel(model; target=Standardizer())","category":"section"},{"location":"mlj_cheatsheet/#Pipelines","page":"MLJ Cheatsheet","title":"Pipelines","text":"pipe = (X -> coerce(X, :height=>Continuous)) |> OneHotEncoder |> KNNRegressor(K=3)\n\nUnsupervised:\npipe = Standardizer |> OneHotEncoder\nConcatenation:\npipe1 |> pipe2 or model |> pipe or pipe |> model, etc.","category":"section"},{"location":"mlj_cheatsheet/#Advanced-model-composition-techniques","page":"MLJ Cheatsheet","title":"Advanced model composition techniques","text":"See the Composing Models section of the MLJ manual.","category":"section"},{"location":"models/ExtraTreesClassifier_MLJScikitLearnInterface/#ExtraTreesClassifier_MLJScikitLearnInterface","page":"ExtraTreesClassifier","title":"ExtraTreesClassifier","text":"ExtraTreesClassifier\n\nA model type for constructing a extra trees classifier, based on MLJScikitLearnInterface.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nExtraTreesClassifier = @load ExtraTreesClassifier pkg=MLJScikitLearnInterface\n\nDo model = ExtraTreesClassifier() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in ExtraTreesClassifier(n_estimators=...).\n\nExtra trees classifier, fits a number of randomized decision trees on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.","category":"section"},{"location":"models/SGDRegressor_MLJScikitLearnInterface/#SGDRegressor_MLJScikitLearnInterface","page":"SGDRegressor","title":"SGDRegressor","text":"SGDRegressor\n\nA model type for constructing a stochastic gradient descent-based regressor, based on MLJScikitLearnInterface.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nSGDRegressor = @load SGDRegressor pkg=MLJScikitLearnInterface\n\nDo model = SGDRegressor() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in SGDRegressor(loss=...).","category":"section"},{"location":"models/SGDRegressor_MLJScikitLearnInterface/#Hyper-parameters","page":"SGDRegressor","title":"Hyper-parameters","text":"loss = squared_error\npenalty = l2\nalpha = 0.0001\nl1_ratio = 0.15\nfit_intercept = true\nmax_iter = 1000\ntol = 0.001\nshuffle = true\nverbose = 0\nepsilon = 0.1\nrandom_state = nothing\nlearning_rate = invscaling\neta0 = 0.01\npower_t = 0.25\nearly_stopping = false\nvalidation_fraction = 0.1\nn_iter_no_change = 5\nwarm_start = false\naverage = false","category":"section"},{"location":"models/LassoCVRegressor_MLJScikitLearnInterface/#LassoCVRegressor_MLJScikitLearnInterface","page":"LassoCVRegressor","title":"LassoCVRegressor","text":"LassoCVRegressor\n\nA model type for constructing a lasso regressor with built-in cross-validation, based on MLJScikitLearnInterface.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nLassoCVRegressor = @load LassoCVRegressor pkg=MLJScikitLearnInterface\n\nDo model = LassoCVRegressor() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in LassoCVRegressor(eps=...).","category":"section"},{"location":"models/LassoCVRegressor_MLJScikitLearnInterface/#Hyper-parameters","page":"LassoCVRegressor","title":"Hyper-parameters","text":"eps = 0.001\nn_alphas = 100\nalphas = nothing\nfit_intercept = true\nprecompute = auto\nmax_iter = 1000\ntol = 0.0001\ncopy_X = true\ncv = 5\nverbose = false\nn_jobs = nothing\npositive = false\nrandom_state = nothing\nselection = cyclic","category":"section"},{"location":"models/BorderlineSMOTE1_Imbalance/#BorderlineSMOTE1_Imbalance","page":"BorderlineSMOTE1","title":"BorderlineSMOTE1","text":"Initiate a BorderlineSMOTE1 model with the given hyper-parameters.\n\nBorderlineSMOTE1\n\nA model type for constructing a borderline smot e1, based on Imbalance.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nBorderlineSMOTE1 = @load BorderlineSMOTE1 pkg=Imbalance\n\nDo model = BorderlineSMOTE1() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in BorderlineSMOTE1(m=...).\n\nBorderlineSMOTE1 implements the BorderlineSMOTE1 algorithm to correct for class imbalance as in Han, H., Wang, W.-Y., & Mao, B.-H. (2005). Borderline-SMOTE: A new over-sampling method in imbalanced data sets learning.  In D.S. Huang, X.-P. Zhang, & G.-B. Huang (Eds.), Advances in Intelligent Computing (pp. 878-887). Springer. ","category":"section"},{"location":"models/BorderlineSMOTE1_Imbalance/#Training-data","page":"BorderlineSMOTE1","title":"Training data","text":"In MLJ or MLJBase, wrap the model in a machine by\n\nmach = machine(model)\n\nThere is no need to provide any data here because the model is a static transformer.\n\nLikewise, there is no need to fit!(mach).\n\nFor default values of the hyper-parameters, model can be constructed by\n\nmodel = BorderlineSMOTE1()","category":"section"},{"location":"models/BorderlineSMOTE1_Imbalance/#Hyperparameters","page":"BorderlineSMOTE1","title":"Hyperparameters","text":"m::Integer=5: The number of neighbors to consider while checking the BorderlineSMOTE1 condition. Should be within the range   0 < m < N where N is the number of observations in the data. It will be automatically set to N-1 if N ≤ m.\nk::Integer=5: Number of nearest neighbors to consider in the SMOTE part of the algorithm. Should be within the range  0 < k < n where n is the number of observations in the smallest class. It will be automatically set to  l-1 for any class with l points where l ≤ k.\nratios=1.0: A parameter that controls the amount of oversampling to be done for each class\nCan be a float and in this case each class will be oversampled to the size of the majority class times the float. By default, all classes are oversampled to the size of the majority class\nCan be a dictionary mapping each class label to the float ratio for that class\nrng::Union{AbstractRNG, Integer}=default_rng(): Either an AbstractRNG object or an Integer    seed to be used with Xoshiro if the Julia VERSION supports it. Otherwise, uses MersenneTwister`.\nverbosity::Integer=1: Whenever higher than 0 info regarding the points that will participate in oversampling is logged.","category":"section"},{"location":"models/BorderlineSMOTE1_Imbalance/#Transform-Inputs","page":"BorderlineSMOTE1","title":"Transform Inputs","text":"X: A matrix or table of floats where each row is an observation from the dataset\ny: An abstract vector of labels (e.g., strings) that correspond to the observations in X","category":"section"},{"location":"models/BorderlineSMOTE1_Imbalance/#Transform-Outputs","page":"BorderlineSMOTE1","title":"Transform Outputs","text":"Xover: A matrix or table that includes original data and the new observations    due to oversampling. depending on whether the input X is a matrix or table respectively\nyover: An abstract vector of labels corresponding to Xover","category":"section"},{"location":"models/BorderlineSMOTE1_Imbalance/#Operations","page":"BorderlineSMOTE1","title":"Operations","text":"transform(mach, X, y): resample the data X and y using BorderlineSMOTE1, returning both the new and original observations","category":"section"},{"location":"models/BorderlineSMOTE1_Imbalance/#Example","page":"BorderlineSMOTE1","title":"Example","text":"using MLJ\nimport Imbalance\n\n## set probability of each class\nclass_probs = [0.5, 0.2, 0.3]                         \nnum_rows, num_continuous_feats = 1000, 5\n## generate a table and categorical vector accordingly\nX, y = Imbalance.generate_imbalanced_data(num_rows, num_continuous_feats; \n                                stds=[0.1 0.1 0.1], min_sep=0.01, class_probs, rng=42)            \n\njulia> Imbalance.checkbalance(y)\n1: ▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 200 (40.8%) \n2: ▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 310 (63.3%) \n0: ▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 490 (100.0%) \n\n## load BorderlineSMOTE1\nBorderlineSMOTE1 = @load BorderlineSMOTE1 pkg=Imbalance\n\n## wrap the model in a machine\noversampler = BorderlineSMOTE1(m=3, k=5, ratios=Dict(0=>1.0, 1=> 0.9, 2=>0.8), rng=42)\nmach = machine(oversampler)\n\n## provide the data to transform (there is nothing to fit)\nXover, yover = transform(mach, X, y)\n\n\njulia> Imbalance.checkbalance(yover)\n2: ▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 392 (80.0%) \n1: ▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 441 (90.0%) \n0: ▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 490 (100.0%) ","category":"section"},{"location":"models/DecisionTreeClassifier_BetaML/#DecisionTreeClassifier_BetaML","page":"DecisionTreeClassifier","title":"DecisionTreeClassifier","text":"mutable struct DecisionTreeClassifier <: MLJModelInterface.Probabilistic\n\nA simple Decision Tree model for classification with support for Missing data, from the Beta Machine Learning Toolkit (BetaML).","category":"section"},{"location":"models/DecisionTreeClassifier_BetaML/#Hyperparameters:","page":"DecisionTreeClassifier","title":"Hyperparameters:","text":"max_depth::Int64: The maximum depth the tree is allowed to reach. When this is reached the node is forced to become a leaf [def: 0, i.e. no limits]\nmin_gain::Float64: The minimum information gain to allow for a node's partition [def: 0]\nmin_records::Int64: The minimum number of records a node must holds to consider for a partition of it [def: 2]\nmax_features::Int64: The maximum number of (random) features to consider at each partitioning [def: 0, i.e. look at all features]\nsplitting_criterion::Function: This is the name of the function to be used to compute the information gain of a specific partition. This is done by measuring the difference betwwen the \"impurity\" of the labels of the parent node with those of the two child nodes, weighted by the respective number of items. [def: gini]. Either gini, entropy or a custom function. It can also be an anonymous function.\nrng::Random.AbstractRNG: A Random Number Generator to be used in stochastic parts of the code [deafult: Random.GLOBAL_RNG]","category":"section"},{"location":"models/DecisionTreeClassifier_BetaML/#Example:","page":"DecisionTreeClassifier","title":"Example:","text":"julia> using MLJ\n\njulia> X, y        = @load_iris;\n\njulia> modelType   = @load DecisionTreeClassifier pkg = \"BetaML\" verbosity=0\nBetaML.Trees.DecisionTreeClassifier\n\njulia> model       = modelType()\nDecisionTreeClassifier(\n  max_depth = 0, \n  min_gain = 0.0, \n  min_records = 2, \n  max_features = 0, \n  splitting_criterion = BetaML.Utils.gini, \n  rng = Random._GLOBAL_RNG())\n\njulia> mach        = machine(model, X, y);\n\njulia> fit!(mach);\n[ Info: Training machine(DecisionTreeClassifier(max_depth = 0, …), …).\n\njulia> cat_est    = predict(mach, X)\n150-element CategoricalDistributions.UnivariateFiniteVector{Multiclass{3}, String, UInt32, Float64}:\n UnivariateFinite{Multiclass{3}}(setosa=>1.0, versicolor=>0.0, virginica=>0.0)\n UnivariateFinite{Multiclass{3}}(setosa=>1.0, versicolor=>0.0, virginica=>0.0)\n ⋮\n UnivariateFinite{Multiclass{3}}(setosa=>0.0, versicolor=>0.0, virginica=>1.0)\n UnivariateFinite{Multiclass{3}}(setosa=>0.0, versicolor=>0.0, virginica=>1.0)\n UnivariateFinite{Multiclass{3}}(setosa=>0.0, versicolor=>0.0, virginica=>1.0)","category":"section"},{"location":"loading_model_code/#Loading-Model-Code","page":"Loading Model Code","title":"Loading Model Code","text":"Once the name of a model, and the package providing that model, have been identified (see Model Search) one can either import the model type interactively with @iload, as shown under Installation, or use @load as shown below. The @load macro works from within a module, a package or a function, provided the relevant package providing the MLJ interface has been added to your package environment. It will attempt to load the model type into the global namespace of the module in which @load is invoked (Main if invoked at the REPL).\n\nIn general, the code providing core functionality for the model (living in a package you should consult for documentation) may be different from the package providing the MLJ interface. Since the core package is a dependency of the interface package, only the interface package needs to be added to your environment.\n\nFor instance, suppose you have activated a Julia package environment my_env that you wish to use for your MLJ project; for example, you have run:\n\nusing Pkg\nPkg.activate(\"my_env\", shared=true)\n\nFurthermore, suppose you want to use DecisionTreeClassifier, provided by the DecisionTree.jl package. Then, to determine which package provides the MLJ interface you call load_path:\n\njulia> load_path(\"DecisionTreeClassifier\", pkg=\"DecisionTree\")\n\"MLJDecisionTreeInterface.DecisionTreeClassifier\"\n\nIn this case, we see that the package required is MLJDecisionTreeInterface.jl. If this package is not in my_env (do Pkg.status() to check) you add it by running\n\njulia> Pkg.add(\"MLJDecisionTreeInterface\")\n\nSo long as my_env is the active environment, this action need never be repeated (unless you run Pkg.rm(\"MLJDecisionTreeInterface\")). You are now ready to instantiate a decision tree classifier:\n\njulia> Tree = @load DecisionTree pkg=DecisionTree\njulia> tree = Tree()\n\nwhich is equivalent to\n\njulia> import MLJDecisionTreeInterface.DecisionTreeClassifier\njulia> Tree = MLJDecisionTreeInterface.DecisionTreeClassifier\njulia> tree = Tree()\n\nTip. The specification pkg=... above can be dropped for the many models that are provided by only a single package.","category":"section"},{"location":"loading_model_code/#API","page":"Loading Model Code","title":"API","text":"","category":"section"},{"location":"loading_model_code/#StatisticalTraits.load_path","page":"Loading Model Code","title":"StatisticalTraits.load_path","text":"load_path(model_name::String, pkg=nothing)\n\nReturn the load path for model type with name model_name, specifying the algorithm=providing package name pkg to resolve name conflicts, if necessary.\n\nload_path(proxy::NamedTuple)\n\nReturn the load path for the model whose name is proxy.name and whose algorithm-providing package has name proxy.package_name. For example, proxy could be any element of the vector returned by models().\n\nload_path(model)\n\nReturn the load path of a model instance or type. Usually requires necessary model code to have been separately loaded. Supply strings as above if code is not loaded.\n\n\n\n\n\n","category":"function"},{"location":"loading_model_code/#MLJModels.@load","page":"Loading Model Code","title":"MLJModels.@load","text":"@load ModelName pkg=nothing verbosity=0 add=false\n\nImport the model type the model named in the first argument into the calling module, specfying pkg in the case of an ambiguous name (to packages providing a model type with the same name). Returns the model type.\n\nWarning In older versions of MLJ/MLJModels, @load returned an instance instead.\n\nTo automatically add required interface packages to the current environment, specify add=true. For interactive loading, use @iload instead.\n\nExamples\n\nTree = @load DecisionTreeRegressor\ntree = Tree()\ntree2 = Tree(min_samples_split=6)\n\nSVM = @load SVC pkg=LIBSVM\nsvm = SVM()\n\nSee also @iload\n\n\n\n\n\n","category":"macro"},{"location":"loading_model_code/#MLJModels.@iload","page":"Loading Model Code","title":"MLJModels.@iload","text":"@iload ModelName\n\nInteractive alternative to @load. Provides user with an optioin to install (add) the required interface package to the current environment, and to choose the relevant model-providing package in ambiguous cases.  See @load\n\n\n\n\n\n","category":"macro"},{"location":"models/MCDDetector_OutlierDetectionPython/#MCDDetector_OutlierDetectionPython","page":"MCDDetector","title":"MCDDetector","text":"MCDDetector(store_precision = true,\n               assume_centered = false,\n               support_fraction = nothing,\n               random_state = nothing)\n\nhttps://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.mcd","category":"section"},{"location":"models/OneClassSVM_LIBSVM/#OneClassSVM_LIBSVM","page":"OneClassSVM","title":"OneClassSVM","text":"OneClassSVM\n\nA model type for constructing a one-class support vector machine, based on LIBSVM.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nOneClassSVM = @load OneClassSVM pkg=LIBSVM\n\nDo model = OneClassSVM() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in OneClassSVM(kernel=...).\n\nReference for algorithm and core C-library: C.-C. Chang and C.-J. Lin (2011): \"LIBSVM: a library for support vector machines.\" ACM Transactions on Intelligent Systems and Technology, 2(3):27:1–27:27. Updated at https://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf. \n\nThis model is an outlier detection model delivering raw scores based on the decision function of a support vector machine. Like the NuSVC classifier, it uses the nu re-parameterization of the cost parameter appearing in standard support vector classification SVC.\n\nTo extract normalized scores (\"probabilities\") wrap the model using ProbabilisticDetector from OutlierDetection.jl. For threshold-based classification, wrap the probabilistic model using MLJ's BinaryThresholdPredictor. Examples of wrapping appear below.","category":"section"},{"location":"models/OneClassSVM_LIBSVM/#Training-data","page":"OneClassSVM","title":"Training data","text":"In MLJ or MLJBase, bind an instance model to data with:\n\nmach = machine(model, X, y)\n\nwhere\n\nX: any table of input features (eg, a DataFrame) whose columns each have Continuous element scitype; check column scitypes with schema(X)\n\nTrain the machine using fit!(mach, rows=...).","category":"section"},{"location":"models/OneClassSVM_LIBSVM/#Hyper-parameters","page":"OneClassSVM","title":"Hyper-parameters","text":"kernel=LIBSVM.Kernel.RadialBasis: either an object that can be called, as in kernel(x1, x2), or one of the built-in kernels from the LIBSVM.jl package listed below.  Here x1 and x2 are vectors whose lengths match the number of columns of the training data X (see \"Examples\" below).\nLIBSVM.Kernel.Linear: (x1, x2) -> x1'*x2\nLIBSVM.Kernel.Polynomial: (x1, x2) -> gamma*x1'*x2 + coef0)^degree\nLIBSVM.Kernel.RadialBasis: (x1, x2) -> (exp(-gamma*norm(x1 - x2)^2))\nLIBSVM.Kernel.Sigmoid: (x1, x2) - > tanh(gamma*x1'*x2 + coef0)\nHere gamma, coef0, degree are other hyper-parameters. Serialization of models with user-defined kernels comes with some restrictions. See LIVSVM.jl issue91\ngamma = 0.0: kernel parameter (see above); if gamma==-1.0 then gamma = 1/nfeatures is used in training, where nfeatures is the number of features (columns of X).  If gamma==0.0 then gamma = 1/(var(Tables.matrix(X))*nfeatures) is used. Actual value used appears in the report (see below).\ncoef0 = 0.0: kernel parameter (see above)\ndegree::Int32 = Int32(3): degree in polynomial kernel (see above)\nnu=0.5 (range (0, 1]): An upper bound on the fraction of margin errors and a lower bound of the fraction of support vectors. Denoted ν in the cited paper. Changing nu changes the thickness of the margin (a neighborhood of the decision surface) and a margin error is said to have occurred if a training observation lies on the wrong side of the surface or within the margin.\ncachesize=200.0 cache memory size in MB\ntolerance=0.001: tolerance for the stopping criterion\nshrinking=true: whether to use shrinking heuristics","category":"section"},{"location":"models/OneClassSVM_LIBSVM/#Operations","page":"OneClassSVM","title":"Operations","text":"transform(mach, Xnew): return scores for outlierness, given features Xnew having the same scitype as X above. The greater the score, the more likely it is an outlier. This score is based on the SVM decision function. For normalized scores, wrap model using ProbabilisticDetector from OutlierDetection.jl and call predict instead, and for threshold-based classification, wrap again using BinaryThresholdPredictor. See the examples below.","category":"section"},{"location":"models/OneClassSVM_LIBSVM/#Fitted-parameters","page":"OneClassSVM","title":"Fitted parameters","text":"The fields of fitted_params(mach) are:\n\nlibsvm_model: the trained model object created by the LIBSVM.jl package\norientation: this equals 1 if the decision function for libsvm_model is increasing with increasing outlierness, and -1 if it is decreasing instead. Correspondingly, the libsvm_model attaches true to outliers in the first case, and false in the second. (The scores given in the MLJ report and generated by MLJ.transform already correct for this ambiguity, which is therefore only an issue for users directly accessing libsvm_model.)","category":"section"},{"location":"models/OneClassSVM_LIBSVM/#Report","page":"OneClassSVM","title":"Report","text":"The fields of report(mach) are:\n\ngamma: actual value of the kernel parameter gamma used in training","category":"section"},{"location":"models/OneClassSVM_LIBSVM/#Examples","page":"OneClassSVM","title":"Examples","text":"","category":"section"},{"location":"models/OneClassSVM_LIBSVM/#Generating-raw-scores-for-outlierness","page":"OneClassSVM","title":"Generating raw scores for outlierness","text":"using MLJ\nimport LIBSVM\nimport StableRNGs.StableRNG\n\nOneClassSVM = @load OneClassSVM pkg=LIBSVM           ## model type\nmodel = OneClassSVM(kernel=LIBSVM.Kernel.Polynomial) ## instance\n\nrng = StableRNG(123)\nXmatrix = randn(rng, 5, 3)\nXmatrix[1, 1] = 100.0\nX = MLJ.table(Xmatrix)\n\nmach = machine(model, X) |> fit!\n\n## training scores (outliers have larger scores):\njulia> report(mach).scores\n5-element Vector{Float64}:\n  6.711689156091755e-7\n -6.740101976655081e-7\n -6.711632439648446e-7\n -6.743015858874887e-7\n -6.745393717880104e-7\n\n## scores for new data:\nXnew = MLJ.table(rand(rng, 2, 3))\n\njulia> transform(mach, rand(rng, 2, 3))\n2-element Vector{Float64}:\n -6.746293022511047e-7\n -6.744289265348623e-7","category":"section"},{"location":"models/OneClassSVM_LIBSVM/#Generating-probabilistic-predictions-of-outlierness","page":"OneClassSVM","title":"Generating probabilistic predictions of outlierness","text":"Continuing the previous example:\n\nusing OutlierDetection\npmodel = ProbabilisticDetector(model)\npmach = machine(pmodel, X) |> fit!\n\n## probabilistic predictions on new data:\n\njulia> y_prob = predict(pmach, Xnew)\n2-element UnivariateFiniteVector{OrderedFactor{2}, String, UInt8, Float64}:\n UnivariateFinite{OrderedFactor{2}}(normal=>1.0, outlier=>9.57e-5)\n UnivariateFinite{OrderedFactor{2}}(normal=>1.0, outlier=>0.0)\n\n## probabilities for outlierness:\n\njulia> pdf.(y_prob, \"outlier\")\n2-element Vector{Float64}:\n 9.572583265925801e-5\n 0.0\n\n## raw scores are still available using `transform`:\n\njulia> transform(pmach, Xnew)\n2-element Vector{Float64}:\n 9.572583265925801e-5\n 0.0","category":"section"},{"location":"models/OneClassSVM_LIBSVM/#Outlier-classification-using-a-probability-threshold:","page":"OneClassSVM","title":"Outlier classification using a probability threshold:","text":"Continuing the previous example:\n\ndmodel = BinaryThresholdPredictor(pmodel, threshold=0.9)\ndmach = machine(dmodel, X) |> fit!\n\njulia> yhat = predict(dmach, Xnew)\n2-element CategoricalArrays.CategoricalArray{String,1,UInt8}:\n \"normal\"\n \"normal\"","category":"section"},{"location":"models/OneClassSVM_LIBSVM/#User-defined-kernels","page":"OneClassSVM","title":"User-defined kernels","text":"Continuing the first example:\n\nk(x1, x2) = x1'*x2 ## equivalent to `LIBSVM.Kernel.Linear`\nmodel = OneClassSVM(kernel=k)\nmach = machine(model, X) |> fit!\n\njulia> yhat = transform(mach, Xnew)\n2-element Vector{Float64}:\n -0.4825363352732942\n -0.4848772169720227\n\nSee also LIVSVM.jl and the original C implementation documentation. For an alternative source of outlier detection models with an MLJ interface, see OutlierDetection.jl.","category":"section"},{"location":"models/KNeighborsClassifier_MLJScikitLearnInterface/#KNeighborsClassifier_MLJScikitLearnInterface","page":"KNeighborsClassifier","title":"KNeighborsClassifier","text":"KNeighborsClassifier\n\nA model type for constructing a K-nearest neighbors classifier, based on MLJScikitLearnInterface.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nKNeighborsClassifier = @load KNeighborsClassifier pkg=MLJScikitLearnInterface\n\nDo model = KNeighborsClassifier() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in KNeighborsClassifier(n_neighbors=...).","category":"section"},{"location":"models/KNeighborsClassifier_MLJScikitLearnInterface/#Hyper-parameters","page":"KNeighborsClassifier","title":"Hyper-parameters","text":"n_neighbors = 5\nweights = uniform\nalgorithm = auto\nleaf_size = 30\np = 2\nmetric = minkowski\nmetric_params = nothing\nn_jobs = nothing","category":"section"},{"location":"models/BayesianLDA_MLJScikitLearnInterface/#BayesianLDA_MLJScikitLearnInterface","page":"BayesianLDA","title":"BayesianLDA","text":"BayesianLDA\n\nA model type for constructing a Bayesian linear discriminant analysis, based on MLJScikitLearnInterface.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nBayesianLDA = @load BayesianLDA pkg=MLJScikitLearnInterface\n\nDo model = BayesianLDA() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in BayesianLDA(solver=...).","category":"section"},{"location":"models/BayesianLDA_MLJScikitLearnInterface/#Hyper-parameters","page":"BayesianLDA","title":"Hyper-parameters","text":"solver = svd\nshrinkage = nothing\npriors = nothing\nn_components = nothing\nstore_covariance = false\ntol = 0.0001\ncovariance_estimator = nothing","category":"section"},{"location":"models/LaplaceClassifier_LaplaceRedux/#LaplaceClassifier_LaplaceRedux","page":"LaplaceClassifier","title":"LaplaceClassifier","text":"LaplaceClassifier\n\nA model type for constructing a laplace classifier, based on LaplaceRedux.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nLaplaceClassifier = @load LaplaceClassifier pkg=LaplaceRedux\n\nDo model = LaplaceClassifier() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in LaplaceClassifier(model=...).\n\nLaplaceClassifier implements the Laplace Redux – Effortless Bayesian Deep Learning, originally published in Daxberger, E., Kristiadi, A., Immer, A., Eschenhagen, R., Bauer, M., Hennig, P. (2021): \"Laplace Redux – Effortless Bayesian Deep Learning.\", NIPS'21: Proceedings of the 35th International Conference on Neural Information Processing Systems*, Article No. 1537, pp. 20089–20103 for classification models.","category":"section"},{"location":"models/LaplaceClassifier_LaplaceRedux/#Training-data","page":"LaplaceClassifier","title":"Training data","text":"In MLJ or MLJBase, given a dataset X,y and a Flux_Chain adapted to the dataset, pass the chain to the model\n\nlaplace_model = LaplaceClassifier(model = Flux_Chain,kwargs...)\n\nthen bind an instance laplace_model to data with\n\nmach = machine(laplace_model, X, y)\n\nwhere\n\nX: any table of input features (eg, a DataFrame) whose columns each have one of the following element scitypes: Continuous, Count, or <:OrderedFactor; check column scitypes with schema(X)\ny: is the target, which can be any AbstractVector whose element scitype is <:OrderedFactor or <:Multiclass; check the scitype with scitype(y)\n\nTrain the machine using fit!(mach, rows=...).","category":"section"},{"location":"models/LaplaceClassifier_LaplaceRedux/#Hyperparameters-(format:-name-type-default-value-restrictions)","page":"LaplaceClassifier","title":"Hyperparameters (format: name-type-default value-restrictions)","text":"model::Union{Flux.Chain,Nothing} = nothing:                                                     Either nothing or a Flux model provided by the user and compatible with the dataset. In the former case, LaplaceRedux will use a standard MLP with 2 hidden layers with 20 neurons each.\nflux_loss = Flux.Losses.logitcrossentropy :                                                     a Flux loss function\noptimiser = Adam()                                                                              a Flux optimiser\nepochs::Integer = 1000::(_ > 0):                                                                the number of training epochs.\nbatch_size::Integer = 32::(_ > 0):                                                              the batch size.\nsubset_of_weights::Symbol = :all::(_ in (:all, :last_layer, :subnetwork)):                      the subset of weights to use, either :all, :last_layer, or :subnetwork.\nsubnetwork_indices = nothing:                                                                   the indices of the subnetworks.\nhessian_structure::Union{HessianStructure,Symbol,String} = :full::(_ in (:full, :diagonal)):    the structure of the Hessian matrix, either :full or :diagonal.\nbackend::Symbol = :GGN::(_ in (:GGN, :EmpiricalFisher)):                                        the backend to use, either :GGN or :EmpiricalFisher.\nobservational_noise (alias σ)::Float64 = 1.0:                                                   the standard deviation of the prior distribution.\nprior_mean (alias μ₀)::Float64 = 0.0:                                                           the mean of the prior distribution.\nprior_precision_matrix (alias P₀)::Union{AbstractMatrix,UniformScaling,Nothing} = nothing:      the covariance matrix of the prior distribution.\nfit_prior_nsteps::Int = 100::(_ > 0):                                                          the number of steps used to fit the priors.\nlink_approx::Symbol = :probit::(_ in (:probit, :plugin)):                                       the approximation to adopt to compute the probabilities.","category":"section"},{"location":"models/LaplaceClassifier_LaplaceRedux/#Operations","page":"LaplaceClassifier","title":"Operations","text":"predict(mach, Xnew): return predictions of the target given features Xnew having the same scitype as X above. Predictions are probabilistic, but uncalibrated.\npredict_mode(mach, Xnew): instead return the mode of each prediction above.","category":"section"},{"location":"models/LaplaceClassifier_LaplaceRedux/#Fitted-parameters","page":"LaplaceClassifier","title":"Fitted parameters","text":"The fields of fitted_params(mach) are:\n\nmean: The mean of the posterior distribution.\nH: The Hessian of the posterior distribution.\nP: The precision matrix of the posterior distribution.\ncov_matrix: The covariance matrix of the posterior distribution.\nn_data: The number of data points.\nn_params: The number of parameters.\nn_out: The number of outputs.\nloss: The loss value of the posterior distribution.","category":"section"},{"location":"models/LaplaceClassifier_LaplaceRedux/#Report","page":"LaplaceClassifier","title":"Report","text":"The fields of report(mach) are:\n\nloss_history: an array containing the total loss per epoch.","category":"section"},{"location":"models/LaplaceClassifier_LaplaceRedux/#Accessor-functions","page":"LaplaceClassifier","title":"Accessor functions","text":"training_losses(mach): return the loss history from report","category":"section"},{"location":"models/LaplaceClassifier_LaplaceRedux/#Examples","page":"LaplaceClassifier","title":"Examples","text":"using MLJ\nLaplaceClassifier = @load LaplaceClassifier pkg=LaplaceRedux\n\nX, y = @load_iris\n\n## Define the Flux Chain model\nusing Flux\nmodel = Chain(\n    Dense(4, 10, relu),\n    Dense(10, 10, relu),\n    Dense(10, 3)\n)\n\n#Define the LaplaceClassifier\nmodel = LaplaceClassifier(model=model)\n\nmach = machine(model, X, y) |> fit!\n\nXnew = (sepal_length = [6.4, 7.2, 7.4],\n        sepal_width = [2.8, 3.0, 2.8],\n        petal_length = [5.6, 5.8, 6.1],\n        petal_width = [2.1, 1.6, 1.9],)\nyhat = predict(mach, Xnew) ## probabilistic predictions\npredict_mode(mach, Xnew)   ## point predictions\ntraining_losses(mach)      ## loss history per epoch\npdf.(yhat, \"virginica\")    ## probabilities for the \"verginica\" class\nfitted_params(mach)        ## NamedTuple with the fitted params of Laplace\n\n\nSee also LaplaceRedux.jl.","category":"section"},{"location":"models/LassoLarsCVRegressor_MLJScikitLearnInterface/#LassoLarsCVRegressor_MLJScikitLearnInterface","page":"LassoLarsCVRegressor","title":"LassoLarsCVRegressor","text":"LassoLarsCVRegressor\n\nA model type for constructing a Lasso model fit with least angle regression (LARS) with built-in cross-validation, based on MLJScikitLearnInterface.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nLassoLarsCVRegressor = @load LassoLarsCVRegressor pkg=MLJScikitLearnInterface\n\nDo model = LassoLarsCVRegressor() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in LassoLarsCVRegressor(fit_intercept=...).","category":"section"},{"location":"models/LassoLarsCVRegressor_MLJScikitLearnInterface/#Hyper-parameters","page":"LassoLarsCVRegressor","title":"Hyper-parameters","text":"fit_intercept = true\nverbose = false\nmax_iter = 500\nprecompute = auto\ncv = 5\nmax_n_alphas = 1000\nn_jobs = nothing\neps = 2.220446049250313e-16\ncopy_X = true\npositive = false","category":"section"},{"location":"models/OrthogonalMatchingPursuitCVRegressor_MLJScikitLearnInterface/#OrthogonalMatchingPursuitCVRegressor_MLJScikitLearnInterface","page":"OrthogonalMatchingPursuitCVRegressor","title":"OrthogonalMatchingPursuitCVRegressor","text":"OrthogonalMatchingPursuitCVRegressor\n\nA model type for constructing a orthogonal ,atching pursuit (OMP) model with built-in cross-validation, based on MLJScikitLearnInterface.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nOrthogonalMatchingPursuitCVRegressor = @load OrthogonalMatchingPursuitCVRegressor pkg=MLJScikitLearnInterface\n\nDo model = OrthogonalMatchingPursuitCVRegressor() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in OrthogonalMatchingPursuitCVRegressor(copy=...).","category":"section"},{"location":"models/OrthogonalMatchingPursuitCVRegressor_MLJScikitLearnInterface/#Hyper-parameters","page":"OrthogonalMatchingPursuitCVRegressor","title":"Hyper-parameters","text":"copy = true\nfit_intercept = true\nmax_iter = nothing\ncv = 5\nn_jobs = 1\nverbose = false","category":"section"},{"location":"models/KMeansClusterer_BetaML/#KMeansClusterer_BetaML","page":"KMeansClusterer","title":"KMeansClusterer","text":"mutable struct KMeansClusterer <: MLJModelInterface.Unsupervised\n\nThe classical KMeansClusterer clustering algorithm, from the Beta Machine Learning Toolkit (BetaML).","category":"section"},{"location":"models/KMeansClusterer_BetaML/#Parameters:","page":"KMeansClusterer","title":"Parameters:","text":"n_classes::Int64: Number of classes to discriminate the data [def: 3]\ndist::Function: Function to employ as distance. Default to the Euclidean distance. Can be one of the predefined distances (l1_distance, l2_distance, l2squared_distance),  cosine_distance), any user defined function accepting two vectors and returning a scalar or an anonymous function with the same characteristics. Attention that, contrary to KMedoidsClusterer, the KMeansClusterer algorithm is not guaranteed to converge with other distances than the Euclidean one.\ninitialisation_strategy::String: The computation method of the vector of the initial representatives. One of the following:\n\"random\": randomly in the X space\n\"grid\": using a grid approach\n\"shuffle\": selecting randomly within the available points [default]\n\"given\": using a provided set of initial representatives provided in the initial_representatives parameter\ninitial_representatives::Union{Nothing, Matrix{Float64}}: Provided (K x D) matrix of initial representatives (useful only with initialisation_strategy=\"given\") [default: nothing]\nrng::Random.AbstractRNG: Random Number Generator [deafult: Random.GLOBAL_RNG]","category":"section"},{"location":"models/KMeansClusterer_BetaML/#Notes:","page":"KMeansClusterer","title":"Notes:","text":"data must be numerical\nonline fitting (re-fitting with new data) is supported","category":"section"},{"location":"models/KMeansClusterer_BetaML/#Example:","page":"KMeansClusterer","title":"Example:","text":"julia> using MLJ\n\njulia> X, y        = @load_iris;\n\njulia> modelType   = @load KMeansClusterer pkg = \"BetaML\" verbosity=0\nBetaML.Clustering.KMeansClusterer\n\njulia> model       = modelType()\nKMeansClusterer(\n  n_classes = 3, \n  dist = BetaML.Clustering.var\"#34#36\"(), \n  initialisation_strategy = \"shuffle\", \n  initial_representatives = nothing, \n  rng = Random._GLOBAL_RNG())\n\njulia> mach        = machine(model, X);\n\njulia> fit!(mach);\n[ Info: Training machine(KMeansClusterer(n_classes = 3, …), …).\n\njulia> classes_est = predict(mach, X);\n\njulia> hcat(y,classes_est)\n150×2 CategoricalArrays.CategoricalArray{Union{Int64, String},2,UInt32}:\n \"setosa\"     2\n \"setosa\"     2\n \"setosa\"     2\n ⋮            \n \"virginica\"  3\n \"virginica\"  3\n \"virginica\"  1","category":"section"},{"location":"models/LassoRegressor_MLJLinearModels/#LassoRegressor_MLJLinearModels","page":"LassoRegressor","title":"LassoRegressor","text":"LassoRegressor\n\nA model type for constructing a lasso regressor, based on MLJLinearModels.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nLassoRegressor = @load LassoRegressor pkg=MLJLinearModels\n\nDo model = LassoRegressor() to construct an instance with default hyper-parameters.\n\nLasso regression is a linear model with objective function\n\n$\n\n|Xθ - y|₂²/2 + n⋅λ|θ|₁ $\n\nwhere n is the number of observations.\n\nIf scale_penalty_with_samples = false the objective function is\n\n$\n\n|Xθ - y|₂²/2 + λ|θ|₁ $\n\n.\n\nDifferent solver options exist, as indicated under \"Hyperparameters\" below. ","category":"section"},{"location":"models/LassoRegressor_MLJLinearModels/#Training-data","page":"LassoRegressor","title":"Training data","text":"In MLJ or MLJBase, bind an instance model to data with\n\nmach = machine(model, X, y)\n\nwhere:\n\nX is any table of input features (eg, a DataFrame) whose columns have Continuous scitype; check column scitypes with schema(X)\ny is the target, which can be any AbstractVector whose element scitype is Continuous; check the scitype with scitype(y)\n\nTrain the machine using fit!(mach, rows=...).","category":"section"},{"location":"models/LassoRegressor_MLJLinearModels/#Hyperparameters","page":"LassoRegressor","title":"Hyperparameters","text":"lambda::Real: strength of the L1 regularization. Default: 1.0\nfit_intercept::Bool: whether to fit the intercept or not. Default: true\npenalize_intercept::Bool: whether to penalize the intercept. Default: false\nscale_penalty_with_samples::Bool: whether to scale the penalty with the number of observations. Default: true\nsolver::Union{Nothing, MLJLinearModels.Solver}: any instance of MLJLinearModels.ProxGrad. If solver=nothing (default) then ProxGrad(accel=true) (FISTA) is used. Solver aliases: FISTA(; kwargs...) = ProxGrad(accel=true, kwargs...), ISTA(; kwargs...) = ProxGrad(accel=false, kwargs...).  Default: nothing","category":"section"},{"location":"models/LassoRegressor_MLJLinearModels/#Example","page":"LassoRegressor","title":"Example","text":"using MLJ\nX, y = make_regression()\nmach = fit!(machine(LassoRegressor(), X, y))\npredict(mach, X)\nfitted_params(mach)\n\nSee also ElasticNetRegressor.","category":"section"},{"location":"model_stacking/#Model-Stacking","page":"Model Stacking","title":"Model Stacking","text":"In a model stack, as introduced by Wolpert (1992), an adjudicating model learns the best way to combine the predictions of multiple base models. In MLJ, such models are constructed using the Stack constructor. To learn more about stacking and to see how to construct a stack \"by hand\" using Learning Networks, see this Data Science in Julia tutorial","category":"section"},{"location":"model_stacking/#MLJBase.Stack","page":"Model Stacking","title":"MLJBase.Stack","text":"Stack(; metalearner=nothing, name1=model1, name2=model2, ..., keyword_options...)\n\nImplements the two-layer generalized stack algorithm introduced by Wolpert (1992) and generalized by Van der Laan et al (2007). Returns an instance of type ProbabilisticStack or DeterministicStack, depending on the prediction type of metalearner.\n\nWhen training a machine bound to such an instance:\n\nThe data is split into training/validation sets according to the specified resampling strategy.\nEach base model model1, model2, ... is trained on each training subset and outputs predictions on the corresponding validation sets. The multi-fold predictions are spliced together into a so-called out-of-sample prediction for each model.\nThe adjudicating model, metalearner, is subsequently trained on the out-of-sample predictions to learn the best combination of base model predictions.\nEach base model is retrained on all supplied data for purposes of passing on new production data onto the adjudicator for making new predictions\n\nArguments\n\nmetalearner::Supervised: The model that will optimize the desired criterion based on its internals.  For instance, a LinearRegression model will optimize the squared error.\nresampling: The resampling strategy used to prepare out-of-sample predictions of the base learners.\nmeasures: A measure or iterable over measures, to perform an internal evaluation of the learners in the Stack while training. This is not for the evaluation of the Stack itself.\ncache: Whether machines created in the learning network will cache data or not.\nacceleration: A supported AbstractResource to define the training parallelization mode of the stack.\nname1=model1, name2=model2, ...: the Supervised model instances to be used as base learners.  The provided names become properties of the instance created to allow hyper-parameter access\n\nExample\n\nThe following code defines a DeterministicStack instance for learning a Continuous target, and demonstrates that:\n\nBase models can be Probabilistic models even if the stack itself is Deterministic (predict_mean is applied in such cases).\nAs an alternative to hyperparameter optimization, one can stack multiple copies of given model, mutating the hyper-parameter used in each copy.\n\nusing MLJ\n\nDecisionTreeRegressor = @load DecisionTreeRegressor pkg=DecisionTree\nEvoTreeRegressor = @load EvoTreeRegressor\nXGBoostRegressor = @load XGBoostRegressor\nKNNRegressor = @load KNNRegressor pkg=NearestNeighborModels\nLinearRegressor = @load LinearRegressor pkg=MLJLinearModels\n\nX, y = make_regression(500, 5)\n\nstack = Stack(;metalearner=LinearRegressor(),\n                resampling=CV(),\n                measures=rmse,\n                constant=ConstantRegressor(),\n                tree_2=DecisionTreeRegressor(max_depth=2),\n                tree_3=DecisionTreeRegressor(max_depth=3),\n                evo=EvoTreeRegressor(),\n                knn=KNNRegressor(),\n                xgb=XGBoostRegressor())\n\nmach = machine(stack, X, y)\nevaluate!(mach; resampling=Holdout(), measure=rmse)\n\n\nThe internal evaluation report can be accessed like this and provides a PerformanceEvaluation object for each model:\n\nreport(mach).cv_report\n\n\n\n\n\n","category":"type"},{"location":"models/LGBMClassifier_LightGBM/#LGBMClassifier_LightGBM","page":"LGBMClassifier","title":"LGBMClassifier","text":"LGBMClassifier\n\nA model type for constructing a LightGBM classifier, based on LightGBM.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nLGBMClassifier = @load LGBMClassifier pkg=LightGBM\n\nDo model = LGBMClassifier() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in LGBMClassifier(objective=...).\n\n`LightGBM, short for light gradient-boosting machine, is a framework for gradient boosting based on decision tree algorithms and used for classification and other machine learning tasks, with a focus on performance and scalability. This model in particular is used for various types of classification tasks.","category":"section"},{"location":"models/LGBMClassifier_LightGBM/#Training-data-In-MLJ-or-MLJBase,-bind-an-instance-model-to-data-with","page":"LGBMClassifier","title":"Training data In MLJ or MLJBase, bind an instance model to data with","text":"mach = machine(model, X, y) \n\nHere:\n\nX is any table of input features (eg, a DataFrame) whose columns are of scitype Continuous; check the column scitypes with schema(X); alternatively, X is any AbstractMatrix with Continuous elements; check the scitype with scitype(X).\ny is a vector of targets whose items are of scitype Continuous. Check the scitype with scitype(y).\n\nTrain the machine using fit!(mach, rows=...).","category":"section"},{"location":"models/LGBMClassifier_LightGBM/#Operations","page":"LGBMClassifier","title":"Operations","text":"predict(mach, Xnew): return predictions of the target given new features Xnew, which should have the same scitype as X above.","category":"section"},{"location":"models/LGBMClassifier_LightGBM/#Hyper-parameters","page":"LGBMClassifier","title":"Hyper-parameters","text":"See https://lightgbm.readthedocs.io/en/v3.3.5/Parameters.html.","category":"section"},{"location":"models/LGBMClassifier_LightGBM/#Fitted-parameters","page":"LGBMClassifier","title":"Fitted parameters","text":"The fields of fitted_params(mach) are:\n\nfitresult: Fitted model information, contains a LGBMClassification object, a CategoricalArray of the input class names, and the classifier with all its parameters","category":"section"},{"location":"models/LGBMClassifier_LightGBM/#Report","page":"LGBMClassifier","title":"Report","text":"The fields of report(mach) are:\n\ntraining_metrics: A dictionary containing all training metrics.\nimportance: A namedtuple containing:\ngain: The total gain of each split used by the model\nsplit: The number of times each feature is used by the model.","category":"section"},{"location":"models/LGBMClassifier_LightGBM/#Examples","page":"LGBMClassifier","title":"Examples","text":"\nusing DataFrames\nusing MLJ\n\n## load the model\nLGBMClassifier = @load LGBMClassifier pkg=LightGBM \n\nX, y = @load_iris \nX = DataFrame(X)\ntrain, test = partition(collect(eachindex(y)), 0.70, shuffle=true)\n\nfirst(X, 3)\nlgb = LGBMClassifier() ## initialise a model with default params\nmach = machine(lgb, X[train, :], y[train]) |> fit!\n\npredict(mach, X[test, :])\n\n## access feature importances\nmodel_report = report(mach)\ngain_importance = model_report.importance.gain\nsplit_importance = model_report.importance.split\n\nSee also LightGBM.jl and the unwrapped model type LightGBM.LGBMClassification","category":"section"},{"location":"models/Birch_MLJScikitLearnInterface/#Birch_MLJScikitLearnInterface","page":"Birch","title":"Birch","text":"Birch\n\nA model type for constructing a birch, based on MLJScikitLearnInterface.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nBirch = @load Birch pkg=MLJScikitLearnInterface\n\nDo model = Birch() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in Birch(threshold=...).\n\nMemory-efficient, online-learning algorithm provided as an alternative to MiniBatchKMeans. Note: noisy samples are given the label -1.","category":"section"},{"location":"models/UnivariateFillImputer_MLJTransforms/#UnivariateFillImputer_MLJTransforms","page":"UnivariateFillImputer","title":"UnivariateFillImputer","text":"UnivariateFillImputer\n\nA model type for constructing a single variable fill imputer, based on MLJTransforms.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nUnivariateFillImputer = @load UnivariateFillImputer pkg=MLJTransforms\n\nDo model = UnivariateFillImputer() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in UnivariateFillImputer(continuous_fill=...).\n\nUse this model to imputing missing values in a vector with a fixed value learned from the non-missing values of training vector.\n\nFor imputing missing values in tabular data, use FillImputer instead.","category":"section"},{"location":"models/UnivariateFillImputer_MLJTransforms/#Training-data","page":"UnivariateFillImputer","title":"Training data","text":"In MLJ or MLJBase, bind an instance model to data with\n\nmach = machine(model, x)\n\nwhere\n\nx: any abstract vector with element scitype Union{Missing, T} where T is a subtype of Continuous, Multiclass, OrderedFactor or Count; check scitype using scitype(x)\n\nTrain the machine using fit!(mach, rows=...).","category":"section"},{"location":"models/UnivariateFillImputer_MLJTransforms/#Hyper-parameters","page":"UnivariateFillImputer","title":"Hyper-parameters","text":"continuous_fill: function or other callable to determine value to be imputed in the case of Continuous (abstract float) data; default is to apply median after skipping missing values\ncount_fill: function or other callable to determine value to be imputed in the case of Count (integer) data; default is to apply rounded median after skipping missing values\nfinite_fill: function or other callable to determine value to be imputed in the case of Multiclass or OrderedFactor data (categorical vectors); default is to apply mode after skipping missing values","category":"section"},{"location":"models/UnivariateFillImputer_MLJTransforms/#Operations","page":"UnivariateFillImputer","title":"Operations","text":"transform(mach, xnew): return xnew with missing values imputed with the fill values learned when fitting mach","category":"section"},{"location":"models/UnivariateFillImputer_MLJTransforms/#Fitted-parameters","page":"UnivariateFillImputer","title":"Fitted parameters","text":"The fields of fitted_params(mach) are:\n\nfiller: the fill value to be imputed in all new data","category":"section"},{"location":"models/UnivariateFillImputer_MLJTransforms/#Examples","page":"UnivariateFillImputer","title":"Examples","text":"using MLJ\nimputer = UnivariateFillImputer()\n\nx_continuous = [1.0, 2.0, missing, 3.0]\nx_multiclass = coerce([\"y\", \"n\", \"y\", missing, \"y\"], Multiclass)\nx_count = [1, 1, 1, 2, missing, 3, 3]\n\nmach = machine(imputer, x_continuous)\nfit!(mach)\n\njulia> fitted_params(mach)\n(filler = 2.0,)\n\njulia> transform(mach, [missing, missing, 101.0])\n3-element Vector{Float64}:\n 2.0\n 2.0\n 101.0\n\nmach2 = machine(imputer, x_multiclass) |> fit!\n\njulia> transform(mach2, x_multiclass)\n5-element CategoricalArray{String,1,UInt32}:\n \"y\"\n \"n\"\n \"y\"\n \"y\"\n \"y\"\n\nmach3 = machine(imputer, x_count) |> fit!\n\njulia> transform(mach3, [missing, missing, 5])\n3-element Vector{Int64}:\n 2\n 2\n 5\n\nFor imputing tabular data, use FillImputer.","category":"section"},{"location":"models/MultitargetGaussianMixtureRegressor_BetaML/#MultitargetGaussianMixtureRegressor_BetaML","page":"MultitargetGaussianMixtureRegressor","title":"MultitargetGaussianMixtureRegressor","text":"mutable struct MultitargetGaussianMixtureRegressor <: MLJModelInterface.Deterministic\n\nA non-linear regressor derived from fitting the data on a probabilistic model (Gaussian Mixture Model). Relatively fast but generally not very precise, except for data with a structure matching the chosen underlying mixture.\n\nThis is the multi-target version of the model. If you want to predict a single label (y), use the MLJ model GaussianMixtureRegressor.","category":"section"},{"location":"models/MultitargetGaussianMixtureRegressor_BetaML/#Hyperparameters:","page":"MultitargetGaussianMixtureRegressor","title":"Hyperparameters:","text":"n_classes::Int64: Number of mixtures (latent classes) to consider [def: 3]\ninitial_probmixtures::Vector{Float64}: Initial probabilities of the categorical distribution (n_classes x 1) [default: []]\nmixtures::Union{Type, Vector{<:BetaML.GMM.AbstractMixture}}: An array (of length n_classes) of the mixtures to employ (see the [?GMM](@ref GMM) module). Each mixture object can be provided with or without its parameters (e.g. mean and variance for the gaussian ones). Fully qualified mixtures are useful only if theinitialisationstrategyparameter is  set to \"gived\" This parameter can also be given symply in term of a _type. In this case it is automatically extended to a vector of n_classesmixtures of the specified type. Note that mixing of different mixture types is not currently supported. [def:[DiagonalGaussian() for i in 1:n_classes]`]\ntol::Float64: Tolerance to stop the algorithm [default: 10^(-6)]\nminimum_variance::Float64: Minimum variance for the mixtures [default: 0.05]\nminimum_covariance::Float64: Minimum covariance for the mixtures with full covariance matrix [default: 0]. This should be set different than minimum_variance (see notes).\ninitialisation_strategy::String: The computation method of the vector of the initial mixtures. One of the following:\n\"grid\": using a grid approach\n\"given\": using the mixture provided in the fully qualified mixtures parameter\n\"kmeans\": use first kmeans (itself initialised with a \"grid\" strategy) to set the initial mixture centers [default]\nNote that currently \"random\" and \"shuffle\" initialisations are not supported in gmm-based algorithms.\nmaximum_iterations::Int64: Maximum number of iterations [def: typemax(Int64), i.e. ∞]\nrng::Random.AbstractRNG: Random Number Generator [deafult: Random.GLOBAL_RNG]","category":"section"},{"location":"models/MultitargetGaussianMixtureRegressor_BetaML/#Example:","page":"MultitargetGaussianMixtureRegressor","title":"Example:","text":"julia> using MLJ\n\njulia> X, y        = @load_boston;\n\njulia> ydouble     = hcat(y, y .*2  .+5);\n\njulia> modelType   = @load MultitargetGaussianMixtureRegressor pkg = \"BetaML\" verbosity=0\nBetaML.GMM.MultitargetGaussianMixtureRegressor\n\njulia> model       = modelType()\nMultitargetGaussianMixtureRegressor(\n  n_classes = 3, \n  initial_probmixtures = Float64[], \n  mixtures = BetaML.GMM.DiagonalGaussian{Float64}[BetaML.GMM.DiagonalGaussian{Float64}(nothing, nothing), BetaML.GMM.DiagonalGaussian{Float64}(nothing, nothing), BetaML.GMM.DiagonalGaussian{Float64}(nothing, nothing)], \n  tol = 1.0e-6, \n  minimum_variance = 0.05, \n  minimum_covariance = 0.0, \n  initialisation_strategy = \"kmeans\", \n  maximum_iterations = 9223372036854775807, \n  rng = Random._GLOBAL_RNG())\n\njulia> mach        = machine(model, X, ydouble);\n\njulia> fit!(mach);\n[ Info: Training machine(MultitargetGaussianMixtureRegressor(n_classes = 3, …), …).\nIter. 1:        Var. of the post  20.46947926187522       Log-likelihood -23662.72770575145\n\njulia> ŷdouble    = predict(mach, X)\n506×2 Matrix{Float64}:\n 23.3358  51.6717\n 23.3358  51.6717\n  ⋮       \n 16.6843  38.3686\n 16.6843  38.3686","category":"section"},{"location":"models/OrdinalEncoder_MLJTransforms/#OrdinalEncoder_MLJTransforms","page":"OrdinalEncoder","title":"OrdinalEncoder","text":"OrdinalEncoder\n\nA model type for constructing a ordinal encoder, based on MLJTransforms.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nOrdinalEncoder = @load OrdinalEncoder pkg=MLJTransforms\n\nDo model = OrdinalEncoder() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in OrdinalEncoder(features=...).\n\nOrdinalEncoder implements ordinal encoding which replaces the categorical values in the specified     categorical features with integers (ordered arbitrarily). This will create an implicit ordering between     categories which may not be a proper modelling assumption.","category":"section"},{"location":"models/OrdinalEncoder_MLJTransforms/#Training-data","page":"OrdinalEncoder","title":"Training data","text":"In MLJ (or MLJBase) bind an instance unsupervised model to data with\n\nmach = machine(model, X)\n\nHere:\n\nX is any table of input features (eg, a DataFrame). Features to be transformed must  have element scitype Multiclass or OrderedFactor. Use schema(X) to  check scitypes.\n\nTrain the machine using fit!(mach, rows=...).","category":"section"},{"location":"models/OrdinalEncoder_MLJTransforms/#Hyper-parameters","page":"OrdinalEncoder","title":"Hyper-parameters","text":"features=[]: A list of names of categorical features given as symbols to exclude or in clude from encoding, according to the value of ignore, or a single symbol (which is treated as a vector with one symbol), or a callable that returns true for features to be included/excluded.\nignore=true: Whether to exclude or include the features given in features\nordered_factor=false: Whether to encode OrderedFactor or ignore them\noutput_type: The numerical concrete type of the encoded features. Default is Float32.","category":"section"},{"location":"models/OrdinalEncoder_MLJTransforms/#Operations","page":"OrdinalEncoder","title":"Operations","text":"transform(mach, Xnew): Apply ordinal encoding to selected Multiclass or OrderedFactor features ofXnewspecified by hyper-parameters, and   return the new table.   Features that are neitherMulticlassnorOrderedFactor`  are always left unchanged.","category":"section"},{"location":"models/OrdinalEncoder_MLJTransforms/#Fitted-parameters","page":"OrdinalEncoder","title":"Fitted parameters","text":"The fields of fitted_params(mach) are:\n\nindex_given_feat_level: A dictionary that maps each level for each column in a subset of the categorical features of X into an integer.","category":"section"},{"location":"models/OrdinalEncoder_MLJTransforms/#Report","page":"OrdinalEncoder","title":"Report","text":"The fields of report(mach) are:\n\nencoded_features: The subset of the categorical features of X that were encoded","category":"section"},{"location":"models/OrdinalEncoder_MLJTransforms/#Examples","page":"OrdinalEncoder","title":"Examples","text":"using MLJ\n\n## Define categorical features\nA = [\"g\", \"b\", \"g\", \"r\", \"r\",]  \nB = [1.0, 2.0, 3.0, 4.0, 5.0,]\nC = [\"f\", \"f\", \"f\", \"m\", \"f\",]  \nD = [true, false, true, false, true,]\nE = [1, 2, 3, 4, 5,]\n\n## Combine into a named tuple\nX = (A = A, B = B, C = C, D = D, E = E)\n\n## Coerce A, C, D to multiclass and B to continuous and E to ordinal\nX = coerce(X,\n:A => Multiclass,\n:B => Continuous,\n:C => Multiclass,\n:D => Multiclass,\n:E => OrderedFactor,\n)\n\n## Check scitype coercion:\nschema(X)\n\nencoder = OrdinalEncoder(ordered_factor = false)\nmach = fit!(machine(encoder, X))\nXnew = transform(mach, X)\n\njulia > Xnew\n    (A = [2, 1, 2, 3, 3],\n    B = [1.0, 2.0, 3.0, 4.0, 5.0],\n    C = [1, 1, 1, 2, 1],\n    D = [2, 1, 2, 1, 2],\n    E = CategoricalArrays.CategoricalValue{Int64, UInt32}[1, 2, 3, 4, 5],)\n\nSee also TargetEncoder","category":"section"},{"location":"models/DBSCAN_MLJScikitLearnInterface/#DBSCAN_MLJScikitLearnInterface","page":"DBSCAN","title":"DBSCAN","text":"DBSCAN\n\nA model type for constructing a dbscan, based on MLJScikitLearnInterface.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nDBSCAN = @load DBSCAN pkg=MLJScikitLearnInterface\n\nDo model = DBSCAN() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in DBSCAN(eps=...).\n\nDensity-Based Spatial Clustering of Applications with Noise. Finds core samples of high density and expands clusters from them. Good for data which contains clusters of similar density.","category":"section"},{"location":"models/KNNRegressor_NearestNeighborModels/#KNNRegressor_NearestNeighborModels","page":"KNNRegressor","title":"KNNRegressor","text":"KNNRegressor\n\nA model type for constructing a K-nearest neighbor regressor, based on NearestNeighborModels.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nKNNRegressor = @load KNNRegressor pkg=NearestNeighborModels\n\nDo model = KNNRegressor() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in KNNRegressor(K=...).\n\nKNNRegressor implements K-Nearest Neighbors regressor  which is non-parametric algorithm that predicts the response associated with a new point  by taking an weighted average of the response of the K-nearest points.","category":"section"},{"location":"models/KNNRegressor_NearestNeighborModels/#Training-data","page":"KNNRegressor","title":"Training data","text":"In MLJ or MLJBase, bind an instance model to data with\n\nmach = machine(model, X, y)\n\nOR\n\nmach = machine(model, X, y, w)\n\nHere:\n\nX is any table of input features (eg, a DataFrame) whose columns are of scitype Continuous; check column scitypes with schema(X).\ny is the target, which can be any table of responses whose element scitype is    Continuous; check the scitype with scitype(y).\nw is the observation weights which can either be nothing(default) or an  AbstractVector whoose element scitype is Count or Continuous. This is different  from weights kernel which is an hyperparameter to the model, see below.\n\nTrain the machine using fit!(mach, rows=...).","category":"section"},{"location":"models/KNNRegressor_NearestNeighborModels/#Hyper-parameters","page":"KNNRegressor","title":"Hyper-parameters","text":"K::Int=5 : number of neighbors\nalgorithm::Symbol = :kdtree : one of (:kdtree, :brutetree, :balltree)\nmetric::Metric = Euclidean() : any Metric from    Distances.jl for the    distance between points. For algorithm = :kdtree only metrics which are    instances of Distances.UnionMinkowskiMetric are supported.\nleafsize::Int = algorithm == 10 : determines the number of points    at which to stop splitting the tree. This option is ignored and always taken as 0    for algorithm = :brutetree, since brutetree isn't actually a tree.\nreorder::Bool = true : if true then points which are close in    distance are placed close in memory. In this case, a copy of the original data    will be made so that the original data is left unmodified. Setting this to true    can significantly improve performance of the specified algorithm    (except :brutetree). This option is ignored and always taken as false for    algorithm = :brutetree.\nweights::KNNKernel=Uniform() : kernel used in assigning weights to the    k-nearest neighbors for each observation. An instance of one of the types in    list_kernels(). User-defined weighting functions can be passed by wrapping the    function in a UserDefinedKernel kernel (do ?NearestNeighborModels.UserDefinedKernel for more    info). If observation weights w are passed during machine construction then the    weight assigned to each neighbor vote is the product of the kernel generated    weight for that neighbor and the corresponding observation weight.","category":"section"},{"location":"models/KNNRegressor_NearestNeighborModels/#Operations","page":"KNNRegressor","title":"Operations","text":"predict(mach, Xnew): Return predictions of the target given features Xnew, which should have same scitype as X above.","category":"section"},{"location":"models/KNNRegressor_NearestNeighborModels/#Fitted-parameters","page":"KNNRegressor","title":"Fitted parameters","text":"The fields of fitted_params(mach) are:\n\ntree: An instance of either KDTree, BruteTree or BallTree depending on the  value of the algorithm hyperparameter (See hyper-parameters section above).  These are data structures that stores the training data with the view of making  quicker nearest neighbor searches on test data points.","category":"section"},{"location":"models/KNNRegressor_NearestNeighborModels/#Examples","page":"KNNRegressor","title":"Examples","text":"using MLJ\nKNNRegressor = @load KNNRegressor pkg=NearestNeighborModels\nX, y = @load_boston; ## loads the crabs dataset from MLJBase\n## view possible kernels\nNearestNeighborModels.list_kernels()\nmodel = KNNRegressor(weights = NearestNeighborModels.Inverse()) #KNNRegressor instantiation\nmach = machine(model, X, y) |> fit! ## wrap model and required data in an MLJ machine and fit\ny_hat = predict(mach, X)\n\n\nSee also MultitargetKNNRegressor","category":"section"},{"location":"models/ARDRegressor_MLJScikitLearnInterface/#ARDRegressor_MLJScikitLearnInterface","page":"ARDRegressor","title":"ARDRegressor","text":"ARDRegressor\n\nA model type for constructing a Bayesian ARD regressor, based on MLJScikitLearnInterface.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nARDRegressor = @load ARDRegressor pkg=MLJScikitLearnInterface\n\nDo model = ARDRegressor() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in ARDRegressor(max_iter=...).","category":"section"},{"location":"models/ARDRegressor_MLJScikitLearnInterface/#Hyper-parameters","page":"ARDRegressor","title":"Hyper-parameters","text":"max_iter = 300\ntol = 0.001\nalpha_1 = 1.0e-6\nalpha_2 = 1.0e-6\nlambda_1 = 1.0e-6\nlambda_2 = 1.0e-6\ncompute_score = false\nthreshold_lambda = 10000.0\nfit_intercept = true\ncopy_X = true\nverbose = false","category":"section"},{"location":"models/LinearRegressor_MLJScikitLearnInterface/#LinearRegressor_MLJScikitLearnInterface","page":"LinearRegressor","title":"LinearRegressor","text":"LinearRegressor\n\nA model type for constructing a ordinary least-squares regressor (OLS), based on MLJScikitLearnInterface.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nLinearRegressor = @load LinearRegressor pkg=MLJScikitLearnInterface\n\nDo model = LinearRegressor() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in LinearRegressor(fit_intercept=...).","category":"section"},{"location":"models/LinearRegressor_MLJScikitLearnInterface/#Hyper-parameters","page":"LinearRegressor","title":"Hyper-parameters","text":"fit_intercept = true\ncopy_X = true\nn_jobs = nothing","category":"section"},{"location":"models/SVMNuRegressor_MLJScikitLearnInterface/#SVMNuRegressor_MLJScikitLearnInterface","page":"SVMNuRegressor","title":"SVMNuRegressor","text":"SVMNuRegressor\n\nA model type for constructing a nu-support vector regressor, based on MLJScikitLearnInterface.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nSVMNuRegressor = @load SVMNuRegressor pkg=MLJScikitLearnInterface\n\nDo model = SVMNuRegressor() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in SVMNuRegressor(nu=...).","category":"section"},{"location":"models/SVMNuRegressor_MLJScikitLearnInterface/#Hyper-parameters","page":"SVMNuRegressor","title":"Hyper-parameters","text":"nu = 0.5\nC = 1.0\nkernel = rbf\ndegree = 3\ngamma = scale\ncoef0 = 0.0\nshrinking = true\ntol = 0.001\ncache_size = 200\nmax_iter = -1","category":"section"},{"location":"models/LinearRegressor_MLJLinearModels/#LinearRegressor_MLJLinearModels","page":"LinearRegressor","title":"LinearRegressor","text":"LinearRegressor\n\nA model type for constructing a linear regressor, based on MLJLinearModels.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nLinearRegressor = @load LinearRegressor pkg=MLJLinearModels\n\nDo model = LinearRegressor() to construct an instance with default hyper-parameters.\n\nThis model provides standard linear regression with objective function\n\n$\n\n|Xθ - y|₂²/2 $\n\nDifferent solver options exist, as indicated under \"Hyperparameters\" below. ","category":"section"},{"location":"models/LinearRegressor_MLJLinearModels/#Training-data","page":"LinearRegressor","title":"Training data","text":"In MLJ or MLJBase, bind an instance model to data with\n\nmach = machine(model, X, y)\n\nwhere:\n\nX is any table of input features (eg, a DataFrame) whose columns have Continuous scitype; check column scitypes with schema(X)\ny is the target, which can be any AbstractVector whose element scitype is Continuous; check the scitype with scitype(y)\n\nTrain the machine using fit!(mach, rows=...).","category":"section"},{"location":"models/LinearRegressor_MLJLinearModels/#Hyperparameters","page":"LinearRegressor","title":"Hyperparameters","text":"fit_intercept::Bool: whether to fit the intercept or not. Default: true\nsolver::Union{Nothing, MLJLinearModels.Solver}: \"any instance of MLJLinearModels.Analytical. Use Analytical() for Cholesky and CG()=Analytical(iterative=true) for conjugate-gradient.\nIf solver = nothing (default) then Analytical() is used.  Default: nothing","category":"section"},{"location":"models/LinearRegressor_MLJLinearModels/#Example","page":"LinearRegressor","title":"Example","text":"using MLJ\nX, y = make_regression()\nmach = fit!(machine(LinearRegressor(), X, y))\npredict(mach, X)\nfitted_params(mach)","category":"section"},{"location":"models/FrequencyEncoder_MLJTransforms/#FrequencyEncoder_MLJTransforms","page":"FrequencyEncoder","title":"FrequencyEncoder","text":"FrequencyEncoder\n\nA model type for constructing a frequency encoder, based on MLJTransforms.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nFrequencyEncoder = @load FrequencyEncoder pkg=MLJTransforms\n\nDo model = FrequencyEncoder() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in FrequencyEncoder(features=...).\n\nFrequencyEncoder implements frequency encoding which replaces the categorical values in the specified     categorical features with their (normalized or raw) frequencies of occurrence in the dataset. ","category":"section"},{"location":"models/FrequencyEncoder_MLJTransforms/#Training-data","page":"FrequencyEncoder","title":"Training data","text":"In MLJ (or MLJBase) bind an instance unsupervised model to data with\n\nmach = machine(model, X)\n\nHere:\n\nX is any table of input features (eg, a DataFrame). Features to be transformed must  have element scitype Multiclass or OrderedFactor. Use schema(X) to  check scitypes.\n\nTrain the machine using fit!(mach, rows=...).","category":"section"},{"location":"models/FrequencyEncoder_MLJTransforms/#Hyper-parameters","page":"FrequencyEncoder","title":"Hyper-parameters","text":"features=[]: A list of names of categorical features given as symbols to exclude or in clude from encoding, according to the value of ignore, or a single symbol (which is treated as a vector with one symbol), or a callable that returns true for features to be included/excluded.\nignore=true: Whether to exclude or include the features given in features\nordered_factor=false: Whether to encode OrderedFactor or ignore them\nnormalize=false: Whether to use normalized frequencies that sum to 1 over category values or to use raw counts.\noutput_type=Float32: The type of the output values. The default is Float32, but you can set it to Float64 or any other type that can hold the frequency values.","category":"section"},{"location":"models/FrequencyEncoder_MLJTransforms/#Operations","page":"FrequencyEncoder","title":"Operations","text":"transform(mach, Xnew): Apply frequency encoding to selected Multiclass or OrderedFactor features ofXnewspecified by hyper-parameters, and   return the new table.   Features that are neitherMulticlassnorOrderedFactor`  are always left unchanged.","category":"section"},{"location":"models/FrequencyEncoder_MLJTransforms/#Fitted-parameters","page":"FrequencyEncoder","title":"Fitted parameters","text":"The fields of fitted_params(mach) are:\n\nstatistic_given_feat_val: A dictionary that maps each level for each column in a subset of the categorical features of X into its frequency.","category":"section"},{"location":"models/FrequencyEncoder_MLJTransforms/#Report","page":"FrequencyEncoder","title":"Report","text":"The fields of report(mach) are:\n\nencoded_features: The subset of the categorical features of X that were encoded","category":"section"},{"location":"models/FrequencyEncoder_MLJTransforms/#Examples","page":"FrequencyEncoder","title":"Examples","text":"using MLJ\n\n## Define categorical features\nA = [\"g\", \"b\", \"g\", \"r\", \"r\",]  \nB = [1.0, 2.0, 3.0, 4.0, 5.0,]\nC = [\"f\", \"f\", \"f\", \"m\", \"f\",]  \nD = [true, false, true, false, true,]\nE = [1, 2, 3, 4, 5,]\n\n## Combine into a named tuple\nX = (A = A, B = B, C = C, D = D, E = E)\n\n## Coerce A, C, D to multiclass and B to continuous and E to ordinal\nX = coerce(X,\n:A => Multiclass,\n:B => Continuous,\n:C => Multiclass,\n:D => Multiclass,\n:E => OrderedFactor,\n)\n\n## Check scitype coercions:\nschema(X)\n\nencoder = FrequencyEncoder(ordered_factor = false, normalize=true)\nmach = fit!(machine(encoder, X))\nXnew = transform(mach, X)\n\njulia > Xnew\n    (A = [2, 1, 2, 2, 2],\n    B = [1.0, 2.0, 3.0, 4.0, 5.0],\n    C = [4, 4, 4, 1, 4],\n    D = [3, 2, 3, 2, 3],\n    E = CategoricalArrays.CategoricalValue{Int64, UInt32}[1, 2, 3, 4, 5],)\n\nSee also TargetEncoder","category":"section"}]
}
