<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Common MLJ Workflows · MLJ</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.svg" alt="MLJ logo"/></a><div class="docs-package-name"><span class="docs-autofit">MLJ</span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Introduction</a></li><li><a class="tocitem" href="../getting_started/">Getting Started</a></li><li class="is-active"><a class="tocitem" href>Common MLJ Workflows</a><ul class="internal"><li><a class="tocitem" href="#Data-ingestion-1"><span>Data ingestion</span></a></li><li><a class="tocitem" href="#Model-search-1"><span>Model search</span></a></li><li><a class="tocitem" href="#Instantiating-a-model-1"><span>Instantiating a model</span></a></li><li><a class="tocitem" href="#Evaluating-a-model-1"><span>Evaluating a model</span></a></li><li><a class="tocitem" href="#Basic-fit/evaluate/predict-by-hand:-1"><span>Basic fit/evaluate/predict by hand:</span></a></li><li><a class="tocitem" href="#More-performance-evaluation-examples-1"><span>More performance evaluation examples</span></a></li><li><a class="tocitem" href="#Inspecting-training-results-1"><span>Inspecting training results</span></a></li><li><a class="tocitem" href="#Basic-fit/transform-for-unsupervised-models-1"><span>Basic fit/transform for unsupervised models</span></a></li><li><a class="tocitem" href="#Inverting-learned-transformations-1"><span>Inverting learned transformations</span></a></li><li><a class="tocitem" href="#Nested-hyperparameter-tuning-1"><span>Nested hyperparameter tuning</span></a></li><li><a class="tocitem" href="#Constructing-a-linear-pipeline-1"><span>Constructing a linear pipeline</span></a></li><li><a class="tocitem" href="#Creating-a-homogeneous-ensemble-of-models-1"><span>Creating a homogeneous ensemble of models</span></a></li><li><a class="tocitem" href="#Performance-curves-1"><span>Performance curves</span></a></li></ul></li><li><a class="tocitem" href="../working_with_categorical_data/">Working with Categorical Data</a></li><li><a class="tocitem" href="../model_search/">Model Search</a></li><li><a class="tocitem" href="../machines/">Machines</a></li><li><a class="tocitem" href="../evaluating_model_performance/">Evaluating Model Performance</a></li><li><a class="tocitem" href="../performance_measures/">Performance Measures</a></li><li><a class="tocitem" href="../tuning_models/">Tuning Models</a></li><li><a class="tocitem" href="../learning_curves/">Learning Curves</a></li><li><a class="tocitem" href="../transformers/">Transformers and other unsupervised models</a></li><li><a class="tocitem" href="../composing_models/">Composing Models</a></li><li><a class="tocitem" href="../homogeneous_ensembles/">Homogeneous Ensembles</a></li><li><a class="tocitem" href="../generating_synthetic_data/">Generating Synthetic Data</a></li><li><a class="tocitem" href="../openml_integration/">OpenML Integration</a></li><li><a class="tocitem" href="../acceleration_and_parallelism/">Acceleration and Parallelism</a></li><li><a class="tocitem" href="../simple_user_defined_models/">Simple User Defined Models</a></li><li><a class="tocitem" href="../quick_start_guide_to_adding_models/">Quick-Start Guide to Adding Models</a></li><li><a class="tocitem" href="../adding_models_for_general_use/">Adding Models for General Use</a></li><li><a class="tocitem" href="../benchmarking/">Benchmarking</a></li><li><a class="tocitem" href="../internals/">Internals</a></li><li><a class="tocitem" href="../list_of_supported_models/">List of Supported Models</a></li><li><a class="tocitem" href="../third_party_packages/">Third Party Packages</a></li><li><a class="tocitem" href="../glossary/">Glossary</a></li><li><a class="tocitem" href="../mlj_cheatsheet/">MLJ Cheatsheet</a></li><li><a class="tocitem" href="../frequently_asked_questions/">FAQ</a></li><li><a class="tocitem" href="../julia_blogpost/">Julia BlogPost</a></li><li><a class="tocitem" href="../api/">Index of Methods</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Common MLJ Workflows</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Common MLJ Workflows</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/alan-turing-institute/MLJ.jl/blob/master/docs/src/common_mlj_workflows.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Common-MLJ-Workflows-1"><a class="docs-heading-anchor" href="#Common-MLJ-Workflows-1">Common MLJ Workflows</a><a class="docs-heading-anchor-permalink" href="#Common-MLJ-Workflows-1" title="Permalink"></a></h1><h2 id="Data-ingestion-1"><a class="docs-heading-anchor" href="#Data-ingestion-1">Data ingestion</a><a class="docs-heading-anchor-permalink" href="#Data-ingestion-1" title="Permalink"></a></h2><pre><code class="language-julia">import RDatasets
channing = RDatasets.dataset(&quot;boot&quot;, &quot;channing&quot;)
first(channing, 4)</code></pre><table class="data-frame"><thead><tr><th></th><th>Sex</th><th>Entry</th><th>Exit</th><th>Time</th><th>Cens</th></tr><tr><th></th><th>Cat…</th><th>Int32</th><th>Int32</th><th>Int32</th><th>Int32</th></tr></thead><tbody><p>4 rows × 5 columns</p><tr><th>1</th><td>Male</td><td>782</td><td>909</td><td>127</td><td>1</td></tr><tr><th>2</th><td>Male</td><td>1020</td><td>1128</td><td>108</td><td>1</td></tr><tr><th>3</th><td>Male</td><td>856</td><td>969</td><td>113</td><td>1</td></tr><tr><th>4</th><td>Male</td><td>915</td><td>957</td><td>42</td><td>1</td></tr></tbody></table><p>Inspecting metadata, including column scientific types:</p><pre><code class="language-julia">schema(channing)</code></pre><pre><code class="language-none">┌─────────┬────────────────────────────────┬───────────────┐
│ _.names │ _.types                        │ _.scitypes    │
├─────────┼────────────────────────────────┼───────────────┤
│ Sex     │ CategoricalValue{String,UInt8} │ Multiclass{2} │
│ Entry   │ Int32                          │ Count         │
│ Exit    │ Int32                          │ Count         │
│ Time    │ Int32                          │ Count         │
│ Cens    │ Int32                          │ Count         │
└─────────┴────────────────────────────────┴───────────────┘
_.nrows = 462
</code></pre><p>Unpacking data and correcting for wrong scitypes:</p><pre><code class="language-julia">y, X =  unpack(channing,
               ==(:Exit),            # y is the :Exit column
               !=(:Time);            # X is the rest, except :Time
               :Exit=&gt;Continuous,
               :Entry=&gt;Continuous,
               :Cens=&gt;Multiclass)
first(X, 4)</code></pre><table class="data-frame"><thead><tr><th></th><th>Sex</th><th>Entry</th><th>Cens</th></tr><tr><th></th><th>Cat…</th><th>Float64</th><th>Cat…</th></tr></thead><tbody><p>4 rows × 3 columns</p><tr><th>1</th><td>Male</td><td>782.0</td><td>1</td></tr><tr><th>2</th><td>Male</td><td>1020.0</td><td>1</td></tr><tr><th>3</th><td>Male</td><td>856.0</td><td>1</td></tr><tr><th>4</th><td>Male</td><td>915.0</td><td>1</td></tr></tbody></table><p><em>Note:</em> Before julia 1.2, replace <code>!=(:Time)</code> with <code>col -&gt; col != :Time</code>.</p><pre><code class="language-julia">y[1:4]</code></pre><pre><code class="language-none">4-element Array{Float64,1}:
  909.0
 1128.0
  969.0
  957.0</code></pre><p>Loading a built-in supervised dataset:</p><pre><code class="language-julia">X, y = @load_iris;
selectrows(X, 1:4) # selectrows works for any Tables.jl table</code></pre><pre><code class="language-none">(sepal_length = [5.1, 4.9, 4.7, 4.6],
 sepal_width = [3.5, 3.0, 3.2, 3.1],
 petal_length = [1.4, 1.4, 1.3, 1.5],
 petal_width = [0.2, 0.2, 0.2, 0.2],)</code></pre><pre><code class="language-julia">y[1:4]</code></pre><pre><code class="language-none">4-element CategoricalArray{String,1,UInt32}:
 &quot;setosa&quot;
 &quot;setosa&quot;
 &quot;setosa&quot;
 &quot;setosa&quot;</code></pre><h2 id="Model-search-1"><a class="docs-heading-anchor" href="#Model-search-1">Model search</a><a class="docs-heading-anchor-permalink" href="#Model-search-1" title="Permalink"></a></h2><p><em>Reference:</em>   <a href="../model_search/">Model Search</a></p><p>Searching for a supervised model:</p><pre><code class="language-julia">X, y = @load_boston
models(matching(X, y))</code></pre><pre><code class="language-none">57-element Array{NamedTuple{(:name, :package_name, :is_supervised, :docstring, :hyperparameter_ranges, :hyperparameter_types, :hyperparameters, :implemented_methods, :is_pure_julia, :is_wrapper, :load_path, :package_license, :package_url, :package_uuid, :prediction_type, :supports_online, :supports_weights, :input_scitype, :target_scitype, :output_scitype),T} where T&lt;:Tuple,1}:
 (name = ARDRegressor, package_name = ScikitLearn, ... )
 (name = AdaBoostRegressor, package_name = ScikitLearn, ... )
 (name = BaggingRegressor, package_name = ScikitLearn, ... )
 (name = BayesianRidgeRegressor, package_name = ScikitLearn, ... )
 (name = ConstantRegressor, package_name = MLJModels, ... )
 (name = DecisionTreeRegressor, package_name = DecisionTree, ... )
 (name = DeterministicConstantRegressor, package_name = MLJModels, ... )
 (name = DummyRegressor, package_name = ScikitLearn, ... )
 (name = ElasticNetCVRegressor, package_name = ScikitLearn, ... )
 (name = ElasticNetRegressor, package_name = MLJLinearModels, ... )
 ⋮
 (name = RidgeRegressor, package_name = MultivariateStats, ... )
 (name = RidgeRegressor, package_name = ScikitLearn, ... )
 (name = RobustRegressor, package_name = MLJLinearModels, ... )
 (name = SGDRegressor, package_name = ScikitLearn, ... )
 (name = SVMLinearRegressor, package_name = ScikitLearn, ... )
 (name = SVMNuRegressor, package_name = ScikitLearn, ... )
 (name = SVMRegressor, package_name = ScikitLearn, ... )
 (name = TheilSenRegressor, package_name = ScikitLearn, ... )
 (name = XGBoostRegressor, package_name = XGBoost, ... )</code></pre><pre><code class="language-julia">models(matching(X, y))[6]</code></pre><pre><code class="language-none">CART decision tree regressor.
→ based on [DecisionTree](https://github.com/bensadeghi/DecisionTree.jl).
→ do `@load DecisionTreeRegressor pkg=&quot;DecisionTree&quot;` to use the model.
→ do `?DecisionTreeRegressor` for documentation.
(name = &quot;DecisionTreeRegressor&quot;,
 package_name = &quot;DecisionTree&quot;,
 is_supervised = true,
 docstring = &quot;CART decision tree regressor.\n→ based on [DecisionTree](https://github.com/bensadeghi/DecisionTree.jl).\n→ do `@load DecisionTreeRegressor pkg=\&quot;DecisionTree\&quot;` to use the model.\n→ do `?DecisionTreeRegressor` for documentation.&quot;,
 hyperparameter_ranges = (nothing, nothing, nothing, nothing, nothing, nothing, nothing),
 hyperparameter_types = (&quot;Int64&quot;, &quot;Int64&quot;, &quot;Int64&quot;, &quot;Float64&quot;, &quot;Int64&quot;, &quot;Bool&quot;, &quot;Float64&quot;),
 hyperparameters = (:max_depth, :min_samples_leaf, :min_samples_split, :min_purity_increase, :n_subfeatures, :post_prune, :merge_purity_threshold),
 implemented_methods = [:predict, :clean!, :fit, :fitted_params],
 is_pure_julia = true,
 is_wrapper = false,
 load_path = &quot;MLJDecisionTreeInterface.DecisionTreeRegressor&quot;,
 package_license = &quot;MIT&quot;,
 package_url = &quot;https://github.com/bensadeghi/DecisionTree.jl&quot;,
 package_uuid = &quot;7806a523-6efd-50cb-b5f6-3fa6f1930dbb&quot;,
 prediction_type = :deterministic,
 supports_online = false,
 supports_weights = false,
 input_scitype = Table{_s24} where _s24&lt;:Union{AbstractArray{_s23,1} where _s23&lt;:Continuous, AbstractArray{_s23,1} where _s23&lt;:Count, AbstractArray{_s23,1} where _s23&lt;:OrderedFactor},
 target_scitype = AbstractArray{Continuous,1},
 output_scitype = Unknown,)</code></pre><p>More refined searches:</p><pre><code class="language-julia">models() do model
    matching(model, X, y) &amp;&amp;
    model.prediction_type == :deterministic &amp;&amp;
    model.is_pure_julia
end</code></pre><pre><code class="language-none">18-element Array{NamedTuple{(:name, :package_name, :is_supervised, :docstring, :hyperparameter_ranges, :hyperparameter_types, :hyperparameters, :implemented_methods, :is_pure_julia, :is_wrapper, :load_path, :package_license, :package_url, :package_uuid, :prediction_type, :supports_online, :supports_weights, :input_scitype, :target_scitype, :output_scitype),T} where T&lt;:Tuple,1}:
 (name = DecisionTreeRegressor, package_name = DecisionTree, ... )
 (name = DeterministicConstantRegressor, package_name = MLJModels, ... )
 (name = ElasticNetRegressor, package_name = MLJLinearModels, ... )
 (name = EvoTreeRegressor, package_name = EvoTrees, ... )
 (name = HuberRegressor, package_name = MLJLinearModels, ... )
 (name = KNNRegressor, package_name = NearestNeighbors, ... )
 (name = KPLSRegressor, package_name = PartialLeastSquaresRegressor, ... )
 (name = LADRegressor, package_name = MLJLinearModels, ... )
 (name = LassoRegressor, package_name = MLJLinearModels, ... )
 (name = LinearRegressor, package_name = MLJLinearModels, ... )
 (name = LinearRegressor, package_name = MultivariateStats, ... )
 (name = NeuralNetworkRegressor, package_name = MLJFlux, ... )
 (name = PLSRegressor, package_name = PartialLeastSquaresRegressor, ... )
 (name = QuantileRegressor, package_name = MLJLinearModels, ... )
 (name = RandomForestRegressor, package_name = DecisionTree, ... )
 (name = RidgeRegressor, package_name = MLJLinearModels, ... )
 (name = RidgeRegressor, package_name = MultivariateStats, ... )
 (name = RobustRegressor, package_name = MLJLinearModels, ... )</code></pre><p>Searching for an unsupervised model:</p><pre><code class="language-julia">models(matching(X))</code></pre><pre><code class="language-none">24-element Array{NamedTuple{(:name, :package_name, :is_supervised, :docstring, :hyperparameter_ranges, :hyperparameter_types, :hyperparameters, :implemented_methods, :is_pure_julia, :is_wrapper, :load_path, :package_license, :package_url, :package_uuid, :prediction_type, :supports_online, :supports_weights, :input_scitype, :target_scitype, :output_scitype),T} where T&lt;:Tuple,1}:
 (name = AffinityPropagation, package_name = ScikitLearn, ... )
 (name = AgglomerativeClustering, package_name = ScikitLearn, ... )
 (name = Birch, package_name = ScikitLearn, ... )
 (name = ContinuousEncoder, package_name = MLJModels, ... )
 (name = DBSCAN, package_name = ScikitLearn, ... )
 (name = FactorAnalysis, package_name = MultivariateStats, ... )
 (name = FeatureAgglomeration, package_name = ScikitLearn, ... )
 (name = FeatureSelector, package_name = MLJModels, ... )
 (name = FillImputer, package_name = MLJModels, ... )
 (name = ICA, package_name = MultivariateStats, ... )
 ⋮
 (name = MeanShift, package_name = ScikitLearn, ... )
 (name = MiniBatchKMeans, package_name = ScikitLearn, ... )
 (name = OPTICS, package_name = ScikitLearn, ... )
 (name = OneClassSVM, package_name = LIBSVM, ... )
 (name = OneHotEncoder, package_name = MLJModels, ... )
 (name = PCA, package_name = MultivariateStats, ... )
 (name = PPCA, package_name = MultivariateStats, ... )
 (name = SpectralClustering, package_name = ScikitLearn, ... )
 (name = Standardizer, package_name = MLJModels, ... )</code></pre><p>Getting the metadata entry for a given model type:</p><pre><code class="language-julia">info(&quot;PCA&quot;)
info(&quot;RidgeRegressor&quot;, pkg=&quot;MultivariateStats&quot;) # a model type in multiple packages</code></pre><pre><code class="language-none">Ridge regressor with regularization parameter lambda. Learns a
linear regression with a penalty on the l2 norm of the coefficients.

→ based on [MultivariateStats](https://github.com/JuliaStats/MultivariateStats.jl).
→ do `@load RidgeRegressor pkg=&quot;MultivariateStats&quot;` to use the model.
→ do `?RidgeRegressor` for documentation.
(name = &quot;RidgeRegressor&quot;,
 package_name = &quot;MultivariateStats&quot;,
 is_supervised = true,
 docstring = &quot;Ridge regressor with regularization parameter lambda. Learns a\nlinear regression with a penalty on the l2 norm of the coefficients.\n\n→ based on [MultivariateStats](https://github.com/JuliaStats/MultivariateStats.jl).\n→ do `@load RidgeRegressor pkg=\&quot;MultivariateStats\&quot;` to use the model.\n→ do `?RidgeRegressor` for documentation.&quot;,
 hyperparameter_ranges = (nothing, nothing),
 hyperparameter_types = (&quot;Union{Real, Union{AbstractArray{T,1}, AbstractArray{T,2}} where T}&quot;, &quot;Bool&quot;),
 hyperparameters = (:lambda, :bias),
 implemented_methods = [:predict, :clean!, :fit, :fitted_params],
 is_pure_julia = true,
 is_wrapper = false,
 load_path = &quot;MLJMultivariateStatsInterface.RidgeRegressor&quot;,
 package_license = &quot;MIT&quot;,
 package_url = &quot;https://github.com/JuliaStats/MultivariateStats.jl&quot;,
 package_uuid = &quot;6f286f6a-111f-5878-ab1e-185364afe411&quot;,
 prediction_type = :deterministic,
 supports_online = false,
 supports_weights = false,
 input_scitype = Table{_s24} where _s24&lt;:(AbstractArray{_s23,1} where _s23&lt;:Continuous),
 target_scitype = Union{AbstractArray{Continuous,1}, Table{_s24} where _s24&lt;:(AbstractArray{_s23,1} where _s23&lt;:Continuous)},
 output_scitype = Unknown,)</code></pre><h2 id="Instantiating-a-model-1"><a class="docs-heading-anchor" href="#Instantiating-a-model-1">Instantiating a model</a><a class="docs-heading-anchor-permalink" href="#Instantiating-a-model-1" title="Permalink"></a></h2><p><em>Reference:</em>   <a href="../">Getting Started</a></p><pre><code class="language-julia">@load DecisionTreeClassifier
model = DecisionTreeClassifier(min_samples_split=5, max_depth=4)</code></pre><pre><code class="language-none">DecisionTreeClassifier(
    max_depth = 4,
    min_samples_leaf = 1,
    min_samples_split = 5,
    min_purity_increase = 0.0,
    n_subfeatures = 0,
    post_prune = false,
    merge_purity_threshold = 1.0,
    pdf_smoothing = 0.0,
    display_depth = 5) @212</code></pre><p>or</p><pre><code class="language-">model = @load DecisionTreeClassifier
model.min_samples_split = 5
model.max_depth = 4</code></pre><h2 id="Evaluating-a-model-1"><a class="docs-heading-anchor" href="#Evaluating-a-model-1">Evaluating a model</a><a class="docs-heading-anchor-permalink" href="#Evaluating-a-model-1" title="Permalink"></a></h2><p><em>Reference:</em>   <a href="../evaluating_model_performance/">Evaluating Model Performance</a></p><pre><code class="language-julia">X, y = @load_boston
model = @load KNNRegressor
evaluate(model, X, y, resampling=CV(nfolds=5), measure=[rms, mav])</code></pre><pre><code class="language-none">┌───────────────────────────┬───────────────┬───────────────────────────────┐
│ _.measure                 │ _.measurement │ _.per_fold                    │
├───────────────────────────┼───────────────┼───────────────────────────────┤
│ RootMeanSquaredError @154 │ 8.77          │ [8.53, 8.8, 10.7, 9.43, 5.59] │
│ MeanAbsoluteError @996    │ 6.02          │ [6.52, 5.7, 7.65, 6.09, 4.11] │
└───────────────────────────┴───────────────┴───────────────────────────────┘
_.per_observation = [missing, missing]
_.fitted_params_per_fold = [ … ]
_.report_per_fold = [ … ]
</code></pre><h2 id="Basic-fit/evaluate/predict-by-hand:-1"><a class="docs-heading-anchor" href="#Basic-fit/evaluate/predict-by-hand:-1">Basic fit/evaluate/predict by hand:</a><a class="docs-heading-anchor-permalink" href="#Basic-fit/evaluate/predict-by-hand:-1" title="Permalink"></a></h2><p><em>Reference:</em>   <a href="../">Getting Started</a>, <a href="../machines/">Machines</a>, <a href="../evaluating_model_performance/">Evaluating Model Performance</a>, <a href="../performance_measures/">Performance Measures</a></p><pre><code class="language-julia">import RDatasets
vaso = RDatasets.dataset(&quot;robustbase&quot;, &quot;vaso&quot;); # a DataFrame
first(vaso, 3)</code></pre><table class="data-frame"><thead><tr><th></th><th>Volume</th><th>Rate</th><th>Y</th></tr><tr><th></th><th>Float64</th><th>Float64</th><th>Int64</th></tr></thead><tbody><p>3 rows × 3 columns</p><tr><th>1</th><td>3.7</td><td>0.825</td><td>1</td></tr><tr><th>2</th><td>3.5</td><td>1.09</td><td>1</td></tr><tr><th>3</th><td>1.25</td><td>2.5</td><td>1</td></tr></tbody></table><pre><code class="language-julia">y, X = unpack(vaso, ==(:Y), c -&gt; true; :Y =&gt; Multiclass)

tree_model = @load DecisionTreeClassifier</code></pre><pre><code class="language-none">[ Info: For silent loading, specify `verbosity=0`.
[ Info: Model code for DecisionTreeClassifier already loaded
(MLJDecisionTreeInterface.DecisionTreeClassifier)() ✔</code></pre><p>Bind the model and data together in a <em>machine</em> , which will additionally store the learned parameters (<em>fitresults</em>) when fit:</p><pre><code class="language-julia">tree = machine(tree_model, X, y)</code></pre><pre><code class="language-none">Machine{DecisionTreeClassifier} @030 trained 0 times.
  args: 
    1:	Source @519 ⏎ `Table{AbstractArray{Continuous,1}}`
    2:	Source @513 ⏎ `AbstractArray{Multiclass{2},1}`
</code></pre><p>Split row indices into training and evaluation rows:</p><pre><code class="language-julia">train, test = partition(eachindex(y), 0.7, shuffle=true, rng=1234); # 70:30 split</code></pre><pre><code class="language-none">([27, 28, 30, 31, 32, 18, 21, 9, 26, 14  …  7, 39, 2, 37, 1, 8, 19, 25, 35, 34], [22, 13, 11, 4, 10, 16, 3, 20, 29, 23, 12, 24])</code></pre><p>Fit on train and evaluate on test:</p><pre><code class="language-julia">fit!(tree, rows=train)
yhat = predict(tree, X[test,:])
mean(cross_entropy(yhat, y[test]))</code></pre><pre><code class="language-none">6.5216583816514975</code></pre><p>Predict on new data:</p><pre><code class="language-julia">Xnew = (Volume=3*rand(3), Rate=3*rand(3))
predict(tree, Xnew)      # a vector of distributions</code></pre><pre><code class="language-none">3-element MLJBase.UnivariateFiniteArray{Multiclass{2},Int64,UInt32,Float64,1}:
 UnivariateFinite{Multiclass{2}}(0=&gt;0.0, 1=&gt;1.0)
 UnivariateFinite{Multiclass{2}}(0=&gt;0.9, 1=&gt;0.1)
 UnivariateFinite{Multiclass{2}}(0=&gt;0.273, 1=&gt;0.727)</code></pre><pre><code class="language-julia">predict_mode(tree, Xnew) # a vector of point-predictions</code></pre><pre><code class="language-none">3-element CategoricalArray{Int64,1,UInt32}:
 1
 0
 1</code></pre><h2 id="More-performance-evaluation-examples-1"><a class="docs-heading-anchor" href="#More-performance-evaluation-examples-1">More performance evaluation examples</a><a class="docs-heading-anchor-permalink" href="#More-performance-evaluation-examples-1" title="Permalink"></a></h2><pre><code class="language-julia">import LossFunctions.ZeroOneLoss</code></pre><p>Evaluating model + data directly:</p><pre><code class="language-julia">evaluate(tree_model, X, y,
         resampling=Holdout(fraction_train=0.7, shuffle=true, rng=1234),
         measure=[cross_entropy, ZeroOneLoss()])</code></pre><pre><code class="language-none">┌───────────────────────┬───────────────┬────────────┐
│ _.measure             │ _.measurement │ _.per_fold │
├───────────────────────┼───────────────┼────────────┤
│ LogLoss{Float64} @209 │ 6.52          │ [6.52]     │
│ ZeroOneLoss           │ 0.417         │ [0.417]    │
└───────────────────────┴───────────────┴────────────┘
_.per_observation = [[[0.105, 36.0, ..., 1.3]], [[0.0, 1.0, ..., 1.0]]]
_.fitted_params_per_fold = [ … ]
_.report_per_fold = [ … ]
</code></pre><p>If a machine is already defined, as above:</p><pre><code class="language-julia">evaluate!(tree,
          resampling=Holdout(fraction_train=0.7, shuffle=true, rng=1234),
          measure=[cross_entropy, ZeroOneLoss()])</code></pre><pre><code class="language-none">┌───────────────────────┬───────────────┬────────────┐
│ _.measure             │ _.measurement │ _.per_fold │
├───────────────────────┼───────────────┼────────────┤
│ LogLoss{Float64} @209 │ 6.52          │ [6.52]     │
│ ZeroOneLoss           │ 0.417         │ [0.417]    │
└───────────────────────┴───────────────┴────────────┘
_.per_observation = [[[0.105, 36.0, ..., 1.3]], [[0.0, 1.0, ..., 1.0]]]
_.fitted_params_per_fold = [ … ]
_.report_per_fold = [ … ]
</code></pre><p>Using cross-validation:</p><pre><code class="language-julia">evaluate!(tree, resampling=CV(nfolds=5, shuffle=true, rng=1234),
          measure=[cross_entropy, ZeroOneLoss()])</code></pre><pre><code class="language-none">┌───────────────────────┬───────────────┬───────────────────────────────────┐
│ _.measure             │ _.measurement │ _.per_fold                        │
├───────────────────────┼───────────────┼───────────────────────────────────┤
│ LogLoss{Float64} @209 │ 2.47          │ [9.25, 0.598, 0.912, 1.07, 0.546] │
│ ZeroOneLoss           │ 0.432         │ [0.5, 0.375, 0.5, 0.5, 0.286]     │
└───────────────────────┴───────────────┴───────────────────────────────────┘
_.per_observation = [[[2.22e-16, 0.944, ..., 2.22e-16], [0.847, 0.56, ..., 0.56], [0.194, 2.22e-16, ..., 0.223], [2.01, 2.01, ..., 0.143], [0.405, 0.405, ..., 1.1]], [[0.0, 1.0, ..., 0.0], [1.0, 0.0, ..., 0.0], [0.0, 0.0, ..., 0.0], [1.0, 1.0, ..., 0.0], [0.0, 0.0, ..., 1.0]]]
_.fitted_params_per_fold = [ … ]
_.report_per_fold = [ … ]
</code></pre><p>With user-specified train/test pairs of row indices:</p><pre><code class="language-julia">f1, f2, f3 = 1:13, 14:26, 27:36
pairs = [(f1, vcat(f2, f3)), (f2, vcat(f3, f1)), (f3, vcat(f1, f2))];
evaluate!(tree,
          resampling=pairs,
          measure=[cross_entropy, ZeroOneLoss()])</code></pre><pre><code class="language-none">┌───────────────────────┬───────────────┬───────────────────────┐
│ _.measure             │ _.measurement │ _.per_fold            │
├───────────────────────┼───────────────┼───────────────────────┤
│ LogLoss{Float64} @209 │ 5.88          │ [2.16, 11.0, 4.51]    │
│ ZeroOneLoss           │ 0.241         │ [0.304, 0.304, 0.115] │
└───────────────────────┴───────────────┴───────────────────────┘
_.per_observation = [[[0.154, 0.154, ..., 0.154], [2.22e-16, 36.0, ..., 2.22e-16], [2.22e-16, 2.22e-16, ..., 0.693]], [[0.0, 0.0, ..., 0.0], [0.0, 1.0, ..., 0.0], [0.0, 0.0, ..., 0.0]]]
_.fitted_params_per_fold = [ … ]
_.report_per_fold = [ … ]
</code></pre><p>Changing a hyperparameter and re-evaluating:</p><pre><code class="language-julia">tree_model.max_depth = 3
evaluate!(tree,
          resampling=CV(nfolds=5, shuffle=true, rng=1234),
          measure=[cross_entropy, ZeroOneLoss()])</code></pre><pre><code class="language-none">┌───────────────────────┬───────────────┬────────────────────────────────────┐
│ _.measure             │ _.measurement │ _.per_fold                         │
├───────────────────────┼───────────────┼────────────────────────────────────┤
│ LogLoss{Float64} @209 │ 2.25          │ [9.18, 0.484, 0.427, 0.564, 0.624] │
│ ZeroOneLoss           │ 0.336         │ [0.375, 0.25, 0.25, 0.375, 0.429]  │
└───────────────────────┴───────────────┴────────────────────────────────────┘
_.per_observation = [[[2.22e-16, 1.32, ..., 2.22e-16], [2.22e-16, 0.318, ..., 0.318], [0.405, 2.22e-16, ..., 2.22e-16], [1.5, 1.5, ..., 2.22e-16], [1.22, 2.22e-16, ..., 0.348]], [[0.0, 1.0, ..., 0.0], [0.0, 0.0, ..., 0.0], [0.0, 0.0, ..., 0.0], [1.0, 1.0, ..., 0.0], [1.0, 0.0, ..., 0.0]]]
_.fitted_params_per_fold = [ … ]
_.report_per_fold = [ … ]
</code></pre><h2 id="Inspecting-training-results-1"><a class="docs-heading-anchor" href="#Inspecting-training-results-1">Inspecting training results</a><a class="docs-heading-anchor-permalink" href="#Inspecting-training-results-1" title="Permalink"></a></h2><p>Fit a ordinary least square model to some synthetic data:</p><pre><code class="language-julia">x1 = rand(100)
x2 = rand(100)

X = (x1=x1, x2=x2)
y = x1 - 2x2 + 0.1*rand(100);

ols_model = @load LinearRegressor pkg=GLM
ols =  machine(ols_model, X, y)
fit!(ols)</code></pre><pre><code class="language-none">Machine{LinearRegressor} @120 trained 1 time.
  args: 
    1:	Source @411 ⏎ `Table{AbstractArray{Continuous,1}}`
    2:	Source @805 ⏎ `AbstractArray{Continuous,1}`
</code></pre><p>Get a named tuple representing the learned parameters, human-readable if appropriate:</p><pre><code class="language-julia">fitted_params(ols)</code></pre><pre><code class="language-none">(coef = [1.0011087756779795, -1.981962982443257],
 intercept = 0.042502059308033614,)</code></pre><p>Get other training-related information:</p><pre><code class="language-julia">report(ols)</code></pre><pre><code class="language-none">(deviance = 0.0786435679682587,
 dof_residual = 97.0,
 stderror = [0.009867222906037327, 0.010513616258229374, 0.00795441943523589],
 vcov = [9.736208787742771e-5 1.152411002218949e-5 -5.575667105378673e-5; 1.152411002218949e-5 0.000110536126825305 -5.696418843513146e-5; -5.575667105378673e-5 -5.696418843513146e-5 6.327278855165845e-5],)</code></pre><h2 id="Basic-fit/transform-for-unsupervised-models-1"><a class="docs-heading-anchor" href="#Basic-fit/transform-for-unsupervised-models-1">Basic fit/transform for unsupervised models</a><a class="docs-heading-anchor-permalink" href="#Basic-fit/transform-for-unsupervised-models-1" title="Permalink"></a></h2><p>Load data:</p><pre><code class="language-julia">X, y = @load_iris
train, test = partition(eachindex(y), 0.97, shuffle=true, rng=123)</code></pre><pre><code class="language-none">([125, 100, 130, 9, 70, 148, 39, 64, 6, 107  …  110, 59, 139, 21, 112, 144, 140, 72, 109, 41], [106, 147, 47, 5])</code></pre><p>Instantiate and fit the model/machine:</p><pre><code class="language-julia">@load PCA
pca_model = PCA(maxoutdim=2)
pca = machine(pca_model, X)
fit!(pca, rows=train)</code></pre><pre><code class="language-none">Machine{PCA} @043 trained 1 time.
  args: 
    1:	Source @103 ⏎ `Table{AbstractArray{Continuous,1}}`
</code></pre><p>Transform selected data bound to the machine:</p><pre><code class="language-julia">transform(pca, rows=test);</code></pre><pre><code class="language-none">(x1 = [-3.3942826854483243, -1.5219827578765068, 2.538247455185219, 2.7299639893931373],
 x2 = [0.5472450223745241, -0.36842368617126214, 0.5199299511335698, 0.3448466122232363],)</code></pre><p>Transform new data:</p><pre><code class="language-julia">Xnew = (sepal_length=rand(3), sepal_width=rand(3),
        petal_length=rand(3), petal_width=rand(3));
transform(pca, Xnew)</code></pre><pre><code class="language-none">(x1 = [4.757616461818688, 4.895341653189153, 5.256481063011105],
 x2 = [-4.757244999089303, -4.043842354135277, -4.524167273523684],)</code></pre><h2 id="Inverting-learned-transformations-1"><a class="docs-heading-anchor" href="#Inverting-learned-transformations-1">Inverting learned transformations</a><a class="docs-heading-anchor-permalink" href="#Inverting-learned-transformations-1" title="Permalink"></a></h2><pre><code class="language-julia">y = rand(100);
stand_model = UnivariateStandardizer()
stand = machine(stand_model, y)
fit!(stand)
z = transform(stand, y);
@assert inverse_transform(stand, z) ≈ y # true</code></pre><pre><code class="language-none">[ Info: Training Machine{UnivariateStandardizer} @965.</code></pre><h2 id="Nested-hyperparameter-tuning-1"><a class="docs-heading-anchor" href="#Nested-hyperparameter-tuning-1">Nested hyperparameter tuning</a><a class="docs-heading-anchor-permalink" href="#Nested-hyperparameter-tuning-1" title="Permalink"></a></h2><p><em>Reference:</em>   <a href="../tuning_models/">Tuning Models</a></p><p>Define a model with nested hyperparameters:</p><pre><code class="language-julia">tree_model = @load DecisionTreeClassifier
forest_model = EnsembleModel(atom=tree_model, n=300)</code></pre><pre><code class="language-none">ProbabilisticEnsembleModel(
    atom = DecisionTreeClassifier(
            max_depth = -1,
            min_samples_leaf = 1,
            min_samples_split = 2,
            min_purity_increase = 0.0,
            n_subfeatures = 0,
            post_prune = false,
            merge_purity_threshold = 1.0,
            pdf_smoothing = 0.0,
            display_depth = 5),
    atomic_weights = Float64[],
    bagging_fraction = 0.8,
    rng = Random._GLOBAL_RNG(),
    n = 300,
    acceleration = CPU1{Nothing}(nothing),
    out_of_bag_measure = Any[]) @427</code></pre><p>Inspect all hyperparameters, even nested ones (returns nested named tuple):</p><pre><code class="language-julia">params(forest_model)</code></pre><pre><code class="language-none">(atom = (max_depth = -1,
         min_samples_leaf = 1,
         min_samples_split = 2,
         min_purity_increase = 0.0,
         n_subfeatures = 0,
         post_prune = false,
         merge_purity_threshold = 1.0,
         pdf_smoothing = 0.0,
         display_depth = 5,),
 atomic_weights = Float64[],
 bagging_fraction = 0.8,
 rng = Random._GLOBAL_RNG(),
 n = 300,
 acceleration = CPU1{Nothing}(nothing),
 out_of_bag_measure = Any[],)</code></pre><p>Define ranges for hyperparameters to be tuned:</p><pre><code class="language-julia">r1 = range(forest_model, :bagging_fraction, lower=0.5, upper=1.0, scale=:log10)</code></pre><pre><code class="language-none">MLJBase.NumericRange(Float64, :bagging_fraction, ... )</code></pre><pre><code class="language-julia">r2 = range(forest_model, :(atom.n_subfeatures), lower=1, upper=4) # nested</code></pre><pre><code class="language-none">MLJBase.NumericRange(Int64, :(atom.n_subfeatures), ... )</code></pre><p>Wrap the model in a tuning strategy:</p><pre><code class="language-julia">tuned_forest = TunedModel(model=forest_model,
                          tuning=Grid(resolution=12),
                          resampling=CV(nfolds=6),
                          ranges=[r1, r2],
                          measure=cross_entropy)</code></pre><pre><code class="language-none">ProbabilisticTunedModel(
    model = ProbabilisticEnsembleModel(
            atom = DecisionTreeClassifier @761,
            atomic_weights = Float64[],
            bagging_fraction = 0.8,
            rng = Random._GLOBAL_RNG(),
            n = 300,
            acceleration = CPU1{Nothing}(nothing),
            out_of_bag_measure = Any[]),
    tuning = Grid(
            goal = nothing,
            resolution = 12,
            shuffle = true,
            rng = Random._GLOBAL_RNG()),
    resampling = CV(
            nfolds = 6,
            shuffle = false,
            rng = Random._GLOBAL_RNG()),
    measure = LogLoss(
            tol = 2.220446049250313e-16),
    weights = nothing,
    operation = MLJModelInterface.predict,
    range = MLJBase.NumericRange{T,MLJBase.Bounded,Symbol} where T[NumericRange{Float64,…} @488, NumericRange{Int64,…} @406],
    selection_heuristic = MLJTuning.NaiveSelection(nothing),
    train_best = true,
    repeats = 1,
    n = nothing,
    acceleration = CPU1{Nothing}(nothing),
    acceleration_resampling = CPU1{Nothing}(nothing),
    check_measure = true) @181</code></pre><p>Bound the wrapped model to data:</p><pre><code class="language-julia">tuned = machine(tuned_forest, X, y)</code></pre><pre><code class="language-none">Machine{ProbabilisticTunedModel{Grid,…}} @648 trained 0 times.
  args: 
    1:	Source @683 ⏎ `Table{AbstractArray{Continuous,1}}`
    2:	Source @106 ⏎ `AbstractArray{Multiclass{3},1}`
</code></pre><p>Fitting the resultant machine optimizes the hyperparameters specified in <code>range</code>, using the specified <code>tuning</code> and <code>resampling</code> strategies and performance <code>measure</code> (possibly a vector of measures), and retrains on all data bound to the machine:</p><pre><code class="language-julia">fit!(tuned)</code></pre><pre><code class="language-none">Machine{ProbabilisticTunedModel{Grid,…}} @648 trained 1 time.
  args: 
    1:	Source @683 ⏎ `Table{AbstractArray{Continuous,1}}`
    2:	Source @106 ⏎ `AbstractArray{Multiclass{3},1}`
</code></pre><p>Inspecting the optimal model:</p><pre><code class="language-julia">F = fitted_params(tuned)</code></pre><pre><code class="language-none">(best_model = ProbabilisticEnsembleModel{DecisionTreeClassifier} @749,
 best_fitted_params = (fitresult = WrappedEnsemble{Tuple{Node{Float64,…},…},…} @062,),)</code></pre><pre><code class="language-julia">F.best_model</code></pre><pre><code class="language-none">ProbabilisticEnsembleModel(
    atom = DecisionTreeClassifier(
            max_depth = -1,
            min_samples_leaf = 1,
            min_samples_split = 2,
            min_purity_increase = 0.0,
            n_subfeatures = 3,
            post_prune = false,
            merge_purity_threshold = 1.0,
            pdf_smoothing = 0.0,
            display_depth = 5),
    atomic_weights = Float64[],
    bagging_fraction = 0.5,
    rng = Random._GLOBAL_RNG(),
    n = 300,
    acceleration = CPU1{Nothing}(nothing),
    out_of_bag_measure = Any[]) @749</code></pre><p>Inspecting details of tuning procedure:</p><pre><code class="language-julia">report(tuned)</code></pre><pre><code class="language-none">(best_model = ProbabilisticEnsembleModel{DecisionTreeClassifier} @749,
 best_history_entry = (model = ProbabilisticEnsembleModel{DecisionTreeClassifier} @749,
                       measure = LogLoss{Float64}[LogLoss{Float64} @209],
                       measurement = [0.1544233654685739],
                       per_fold = [[3.663735981263026e-15, 0.00026711210124483257, 0.23096738915191511, 0.24990004669352694, 0.2270868622496832, 0.21831878261506973]],),
 history = NamedTuple{(:model, :measure, :measurement, :per_fold),Tuple{MLJ.ProbabilisticEnsembleModel{MLJDecisionTreeInterface.DecisionTreeClassifier},Array{LogLoss{Float64},1},Array{Float64,1},Array{Array{Float64,1},1}}}[(model = ProbabilisticEnsembleModel{DecisionTreeClassifier} @970, measure = [LogLoss{Float64} @209], measurement = [0.2115773566464334], per_fold = [[0.07128683709576461, 0.024459710104442398, 0.314484079263343, 0.2510193976094461, 0.3149344694437086, 0.2932796463618957]]), (model = ProbabilisticEnsembleModel{DecisionTreeClassifier} @972, measure = [LogLoss{Float64} @209], measurement = [0.16950900797314436], per_fold = [[0.02849399555681907, 0.00498968226102696, 0.25052938013779347, 0.22702753998114134, 0.2584061101145397, 0.24760733978754565]]), (model = ProbabilisticEnsembleModel{DecisionTreeClassifier} @837, measure = [LogLoss{Float64} @209], measurement = [0.1924161478158445], per_fold = [[3.663735981263026e-15, 3.663735981263026e-15, 0.2774868349536034, 0.3040462244431442, 0.28810544941774624, 0.284858378080566]]), (model = ProbabilisticEnsembleModel{DecisionTreeClassifier} @120, measure = [LogLoss{Float64} @209], measurement = [0.41367922256081074], per_fold = [[3.663735981263026e-15, 3.663735981263026e-15, 0.25693547601109434, 1.6382702853637645, 0.3038272305729403, 0.2830423434170579]]), (model = ProbabilisticEnsembleModel{DecisionTreeClassifier} @181, measure = [LogLoss{Float64} @209], measurement = [0.6664945987782859], per_fold = [[3.663735981263026e-15, 3.663735981263026e-15, 0.4047507616938802, 1.6472202879076416, 1.5899951100046552, 0.357001433063531]]), (model = ProbabilisticEnsembleModel{DecisionTreeClassifier} @893, measure = [LogLoss{Float64} @209], measurement = [0.6467291829668016], per_fold = [[3.663735981263026e-15, 3.663735981263026e-15, 1.5790275600059485, 0.40629481422539493, 1.56265301261687, 0.3323997109525885]]), (model = ProbabilisticEnsembleModel{DecisionTreeClassifier} @530, measure = [LogLoss{Float64} @209], measurement = [0.20397373569489385], per_fold = [[0.06164487300511071, 0.029581461619074823, 0.31095414650095154, 0.24280464235005766, 0.2914283164557683, 0.28742897423840014]]), (model = ProbabilisticEnsembleModel{DecisionTreeClassifier} @749, measure = [LogLoss{Float64} @209], measurement = [0.1544233654685739], per_fold = [[3.663735981263026e-15, 0.00026711210124483257, 0.23096738915191511, 0.24990004669352694, 0.2270868622496832, 0.21831878261506973]]), (model = ProbabilisticEnsembleModel{DecisionTreeClassifier} @179, measure = [LogLoss{Float64} @209], measurement = [0.20993405387616884], per_fold = [[0.07003479518511707, 0.022673953809944605, 0.28854919423473957, 0.24125586317732706, 0.3334920286980416, 0.3035984881518431]]), (model = ProbabilisticEnsembleModel{DecisionTreeClassifier} @008, measure = [LogLoss{Float64} @209], measurement = [0.22733311692042635], per_fold = [[3.663735981263026e-15, 3.663735981263026e-15, 0.35661389389635884, 0.3905076991108403, 0.31320644947378307, 0.3036706590415685]])  …  (model = ProbabilisticEnsembleModel{DecisionTreeClassifier} @951, measure = [LogLoss{Float64} @209], measurement = [0.1668744175655764], per_fold = [[3.663735981263026e-15, 3.663735981263026e-15, 0.22203645445585743, 0.2886415654754664, 0.229737538429412, 0.26083094703271525]]), (model = ProbabilisticEnsembleModel{DecisionTreeClassifier} @435, measure = [LogLoss{Float64} @209], measurement = [0.19972937068408267], per_fold = [[0.056026383031831044, 0.019730698392026867, 0.28901469321961815, 0.21784311278913393, 0.3179025163462579, 0.29785882032562816]]), (model = ProbabilisticEnsembleModel{DecisionTreeClassifier} @094, measure = [LogLoss{Float64} @209], measurement = [0.18436411489824422], per_fold = [[3.663735981263026e-15, 3.663735981263026e-15, 0.24793446690054677, 0.3308753805882974, 0.2620310728429529, 0.2653437690576611]]), (model = ProbabilisticEnsembleModel{DecisionTreeClassifier} @989, measure = [LogLoss{Float64} @209], measurement = [0.17089807403067336], per_fold = [[0.03776944253793757, 0.006489352903215799, 0.22317473838712373, 0.24837195443552193, 0.26914355910081855, 0.2404393968194227]]), (model = ProbabilisticEnsembleModel{DecisionTreeClassifier} @510, measure = [LogLoss{Float64} @209], measurement = [0.15956064960400007], per_fold = [[0.0018729166822267019, 3.663735981263026e-15, 0.2036529675208513, 0.27105476311486826, 0.2273683776659733, 0.25341487264007717]]), (model = ProbabilisticEnsembleModel{DecisionTreeClassifier} @676, measure = [LogLoss{Float64} @209], measurement = [1.319300923217029], per_fold = [[3.663735981263026e-15, 3.663735981263026e-15, 2.949176582391892, 2.885544002904876, 1.6280348168679262, 0.45305013713747205]]), (model = ProbabilisticEnsembleModel{DecisionTreeClassifier} @100, measure = [LogLoss{Float64} @209], measurement = [0.1833090004977327], per_fold = [[3.663735981263026e-15, 3.663735981263026e-15, 0.24236939485421047, 0.27767467355659936, 0.3035198495137422, 0.2762900850618367]]), (model = ProbabilisticEnsembleModel{DecisionTreeClassifier} @744, measure = [LogLoss{Float64} @209], measurement = [0.2110715043143114], per_fold = [[0.06816986715592367, 0.032621365906290216, 0.32083470047883805, 0.2670112467306891, 0.30509970424002014, 0.2726921413741072]]), (model = ProbabilisticEnsembleModel{DecisionTreeClassifier} @491, measure = [LogLoss{Float64} @209], measurement = [0.6330482707658336], per_fold = [[3.663735981263026e-15, 3.663735981263026e-15, 0.32829240390846165, 1.6153048213732293, 1.5563407008700085, 0.29835169844329484]]), (model = ProbabilisticEnsembleModel{DecisionTreeClassifier} @619, measure = [LogLoss{Float64} @209], measurement = [0.17764150320716107], per_fold = [[0.028216062508844553, 0.007632013708947325, 0.2562673091752543, 0.2590254965569613, 0.26638960028551667, 0.2483185370074423]])],
 best_report = (measures = Any[],
                oob_measurements = missing,),
 plotting = (parameter_names = [&quot;bagging_fraction&quot;, &quot;atom.n_subfeatures&quot;],
             parameter_scales = [:log10, :linear],
             parameter_values = Any[0.6851754923600618 1; 0.6040447222022236 2; … ; 0.8277532798848107 4; 0.6433324490047159 2],
             measurements = [0.2115773566464334, 0.16950900797314436, 0.1924161478158445, 0.41367922256081074, 0.6664945987782859, 0.6467291829668016, 0.20397373569489385, 0.1544233654685739, 0.20993405387616884, 0.22733311692042635  …  0.1668744175655764, 0.19972937068408267, 0.18436411489824422, 0.17089807403067336, 0.15956064960400007, 1.319300923217029, 0.1833090004977327, 0.2110715043143114, 0.6330482707658336, 0.17764150320716107],),)</code></pre><p>Visualizing these results:</p><pre><code class="language-julia">using Plots
plot(tuned)</code></pre><p><img src="../img/workflows_tuning_plot.png" alt/></p><p>Predicting on new data using the optimized model:</p><pre><code class="language-julia">predict(tuned, Xnew)</code></pre><pre><code class="language-none">3-element Array{UnivariateFinite{Multiclass{3},String,UInt32,Float64},1}:
 UnivariateFinite{Multiclass{3}}(versicolor=&gt;0.0767, virginica=&gt;0.0167, setosa=&gt;0.907)
 UnivariateFinite{Multiclass{3}}(versicolor=&gt;0.0, virginica=&gt;0.0, setosa=&gt;1.0)
 UnivariateFinite{Multiclass{3}}(versicolor=&gt;0.0, virginica=&gt;0.0, setosa=&gt;1.0)</code></pre><h2 id="Constructing-a-linear-pipeline-1"><a class="docs-heading-anchor" href="#Constructing-a-linear-pipeline-1">Constructing a linear pipeline</a><a class="docs-heading-anchor-permalink" href="#Constructing-a-linear-pipeline-1" title="Permalink"></a></h2><p><em>Reference:</em>   <a href="../composing_models/">Composing Models</a></p><p>Constructing a linear (unbranching) pipeline with a <em>learned</em> target transformation/inverse transformation:</p><pre><code class="language-julia">X, y = @load_reduced_ames
@load KNNRegressor
pipe = @pipeline(X -&gt; coerce(X, :age=&gt;Continuous),
                 OneHotEncoder,
                 KNNRegressor(K=3),
                 target = UnivariateStandardizer)</code></pre><pre><code class="language-none">Pipeline259(
    one_hot_encoder = OneHotEncoder(
            features = Symbol[],
            drop_last = false,
            ordered_factor = true,
            ignore = false),
    knn_regressor = KNNRegressor(
            K = 3,
            algorithm = :kdtree,
            metric = Distances.Euclidean(0.0),
            leafsize = 10,
            reorder = true,
            weights = :uniform),
    target = UnivariateStandardizer()) @769</code></pre><p>Evaluating the pipeline (just as you would any other model):</p><pre><code class="language-julia">pipe.knn_regressor.K = 2
pipe.one_hot_encoder.drop_last = true
evaluate(pipe, X, y, resampling=Holdout(), measure=rms, verbosity=2)</code></pre><pre><code class="language-none">┌───────────────────────────┬───────────────┬────────────┐
│ _.measure                 │ _.measurement │ _.per_fold │
├───────────────────────────┼───────────────┼────────────┤
│ RootMeanSquaredError @154 │ 53100.0       │ [53100.0]  │
└───────────────────────────┴───────────────┴────────────┘
_.per_observation = [missing]
_.fitted_params_per_fold = [ … ]
_.report_per_fold = [ … ]
</code></pre><p>Inspecting the learned parameters in a pipeline:</p><pre><code class="language-julia">mach = machine(pipe, X, y) |&gt; fit!
F = fitted_params(mach)
F.one_hot_encoder</code></pre><pre><code class="language-none">(fitresult = OneHotEncoderResult @826,)</code></pre><p>Constructing a linear (unbranching) pipeline with a <em>static</em> (unlearned) target transformation/inverse transformation:</p><pre><code class="language-julia">@load DecisionTreeRegressor
pipe2 = @pipeline(X -&gt; coerce(X, :age=&gt;Continuous),
                  OneHotEncoder,
                  DecisionTreeRegressor(max_depth=4),
                  target = y -&gt; log.(y),
                  inverse = z -&gt; exp.(z))</code></pre><pre><code class="language-none">Pipeline270(
    one_hot_encoder = OneHotEncoder(
            features = Symbol[],
            drop_last = false,
            ordered_factor = true,
            ignore = false),
    decision_tree_regressor = DecisionTreeRegressor(
            max_depth = 4,
            min_samples_leaf = 5,
            min_samples_split = 2,
            min_purity_increase = 0.0,
            n_subfeatures = 0,
            post_prune = false,
            merge_purity_threshold = 1.0),
    target = WrappedFunction(
            f = Main.ex-workflows.var&quot;#28#29&quot;()),
    inverse = WrappedFunction(
            f = Main.ex-workflows.var&quot;#30#31&quot;())) @139</code></pre><h2 id="Creating-a-homogeneous-ensemble-of-models-1"><a class="docs-heading-anchor" href="#Creating-a-homogeneous-ensemble-of-models-1">Creating a homogeneous ensemble of models</a><a class="docs-heading-anchor-permalink" href="#Creating-a-homogeneous-ensemble-of-models-1" title="Permalink"></a></h2><p><em>Reference:</em> <a href="../homogeneous_ensembles/">Homogeneous Ensembles</a></p><pre><code class="language-julia">X, y = @load_iris
tree_model = @load DecisionTreeClassifier
forest_model = EnsembleModel(atom=tree_model, bagging_fraction=0.8, n=300)
forest = machine(forest_model, X, y)
evaluate!(forest, measure=cross_entropy)</code></pre><pre><code class="language-none">┌───────────────────────┬───────────────┬───────────────────────────────────────
│ _.measure             │ _.measurement │ _.per_fold                           ⋯
├───────────────────────┼───────────────┼───────────────────────────────────────
│ LogLoss{Float64} @209 │ 0.623         │ [3.66e-15, 3.66e-15, 0.256, 1.64, 1. ⋯
└───────────────────────┴───────────────┴───────────────────────────────────────
                                                                1 column omitted
_.per_observation = [[[3.66e-15, 3.66e-15, ..., 3.66e-15], [3.66e-15, 3.66e-15, ..., 3.66e-15], [0.0305, 3.66e-15, ..., 3.66e-15], [3.66e-15, 0.236, ..., 3.66e-15], [3.66e-15, 0.027, ..., 3.66e-15], [0.0168, 0.436, ..., 0.027]]]
_.fitted_params_per_fold = [ … ]
_.report_per_fold = [ … ]
</code></pre><h2 id="Performance-curves-1"><a class="docs-heading-anchor" href="#Performance-curves-1">Performance curves</a><a class="docs-heading-anchor-permalink" href="#Performance-curves-1" title="Permalink"></a></h2><p>Generate a plot of performance, as a function of some hyperparameter (building on the preceding example)</p><p>Single performance curve:</p><pre><code class="language-julia">r = range(forest_model, :n, lower=1, upper=1000, scale=:log10)
curve = learning_curve(forest,
                            range=r,
                            resampling=Holdout(),
                            resolution=50,
                            measure=cross_entropy,
                            verbosity=0)</code></pre><pre><code class="language-none">(parameter_name = &quot;n&quot;,
 parameter_scale = :log10,
 parameter_values = [1, 2, 3, 4, 5, 6, 7, 8, 10, 11  …  281, 324, 373, 429, 494, 569, 655, 754, 869, 1000],
 measurements = [9.611640903764574, 4.359125602188101, 4.289104451886073, 2.817038132768318, 2.822262474764587, 2.834466798504303, 2.8488030394291313, 2.857975211337237, 2.889449738486046, 2.897397652248809  …  0.5794036635901676, 0.5716075985908329, 0.5760061512191447, 0.5869139564274185, 0.5842663168652249, 0.5809748629851776, 0.5853673101123474, 0.5928670524745373, 0.5930022686282439, 0.5919100756995038],)</code></pre><pre><code class="language-julia">using Plots
plot(curve.parameter_values, curve.measurements, xlab=curve.parameter_name, xscale=curve.parameter_scale)</code></pre><p><img src="../img/workflows_learning_curve.png" alt/></p><p>Multiple curves:</p><pre><code class="language-julia">curve = learning_curve(forest,
                       range=r,
                       resampling=Holdout(),
                       measure=cross_entropy,
                       resolution=50,
                       rng_name=:rng,
                       rngs=4,
                       verbosity=0)</code></pre><pre><code class="language-none">(parameter_name = &quot;n&quot;,
 parameter_scale = :log10,
 parameter_values = [1, 2, 3, 4, 5, 6, 7, 8, 10, 11  …  281, 324, 373, 429, 494, 569, 655, 754, 869, 1000],
 measurements = [9.611640903764574 7.20873067782343 15.218431430960575 4.004850376568572; 8.040507294495367 7.20873067782343 15.218431430960575 4.004850376568572; … ; 1.2625503959471174 1.2276161494674176 1.2519656520891125 1.2295350828703766; 1.2653080834621215 1.2265981396765404 1.2512713145188792 1.231142130329592],)</code></pre><pre><code class="language-julia">plot(curve.parameter_values, curve.measurements,
xlab=curve.parameter_name, xscale=curve.parameter_scale)</code></pre><p><img src="../img/workflows_learning_curves.png" alt/></p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../getting_started/">« Getting Started</a><a class="docs-footer-nextpage" href="../working_with_categorical_data/">Working with Categorical Data »</a></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Tuesday 5 January 2021 23:19">Tuesday 5 January 2021</span>. Using Julia version 1.5.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
