<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Adding New Models · MLJ</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link href="../assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><h1>MLJ</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="../search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="../">Getting Started</a></li><li><a class="toctext" href="../scientific_data_types/">Scientific Data Types</a></li><li><a class="toctext" href="../learning_networks/">Learning Networks</a></li><li><span class="toctext">Adding New Models</span><ul><li class="current"><a class="toctext" href>Adding New Models</a><ul class="internal"></ul></li><li><a class="toctext" href="../the_simplified_model_api/">The Simplified Model API</a></li></ul></li><li><a class="toctext" href="../internals/">Internals</a></li><li><a class="toctext" href="../glossary/">Glossary</a></li><li><a class="toctext" href="../api/">API</a></li><li><a class="toctext" href="../frequently_asked_questions/">FAQ</a></li><li><a class="toctext" href="../NEWS/">MLJ News</a></li></ul></nav><article id="docs"><header><nav><ul><li>Adding New Models</li><li><a href>Adding New Models</a></li></ul><a class="edit-page" href="https://github.com/alan-turing-institute/MLJ.jl/blob/master/docs/src/adding_new_models.md"><span class="fa"></span> Edit on GitHub</a></nav><hr/><div id="topbar"><span>Adding New Models</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="Adding-New-Models-1" href="#Adding-New-Models-1">Adding New Models</a></h1><p>This guide outlines the specification of the MLJ model interface and provides guidelines for implementing the interface for models defined in external packages. For sample implementations, see <a href="https://github.com/alan-turing-institute/MLJModels.jl/tree/master/src">MLJModels/src</a>.</p><p>The machine learning tools provided by MLJ can be applied to the models in any package that imports the package <a href="https://github.com/alan-turing-institute/MLJBase.jl">MLJBase</a> and implements the API defined there, as outlined in detail below. For a quick-and-dirty implementation of user-defined models see <a href="../the_simplified_model_api/">The Simplified Model API</a>.  To make new models available to all MLJ users, see <a href="#Where-to-place-code-implementing-new-models">Where to place code implementing new models</a>.</p><p>It is assumed the reader has read <a href="../">Getting Started</a>. To implement the API described here, some familiarity with the following packages is also helpful:</p><ul><li><a href="https://github.com/JuliaStats/Distributions.jl">Distributions.jl</a></li></ul><p>(for probabilistic predictions)</p><ul><li><a href="https://github.com/JuliaData/CategoricalArrays.jl">CategoricalArrays.jl</a></li></ul><p>(essential if you are implementing a model handling data of <code>Multiclass</code> or <code>FiniteOrderedFactor</code> scitype)</p><ul><li><a href="https://github.com/JuliaData/Tables.jl">Tables.jl</a> (if you&#39;re</li></ul><p>algorithm needs input data in a novel format).</p><p>&lt;!– As a temporary measure, –&gt; &lt;!– the MLJ package also implements the MLJ interface for some –&gt; &lt;!– non-compliant packages, using lazily loaded modules (&quot;glue code&quot;) –&gt; &lt;!– residing in –&gt; &lt;!– <a href="https://github.com/alan-turing-institute/MLJ.jl/tree/master/src/interfaces">src/interfaces</a> –&gt; &lt;!– of the MLJ.jl repository. A checklist for adding models in this latter –&gt; &lt;!– way is given at the end; a template is given here: –&gt; &lt;!– <a href="https://github.com/alan-turing-institute/MLJ.jl/tree/master/src/interfaces/DecisionTree.jl">&quot;src/interfaces/DecisionTree.jl&quot;</a>. –&gt;</p><p>In MLJ, the basic interface exposed to the user, built atop the model interface described here, is the <em>machine interface</em>. After a first reading of this document, the reader may wish to refer to <a href="../internals/">MLJ Internals</a> for context.</p><h3><a class="nav-anchor" id="Overview-1" href="#Overview-1">Overview</a></h3><p>A <em>model</em> is an object storing hyperparameters associated with some machine learning algorithm, where &quot;learning algorithm&quot; is broadly interpreted.  In MLJ, hyperparameters include configuration parameters, like the number of threads, and special instructions, such as &quot;compute feature rankings&quot;, which may or may not affect the final learning outcome.  However, the logging level (<code>verbosity</code> below) is excluded.</p><p>The name of the Julia type associated with a model indicates the associated algorithm (e.g., <code>DecisionTreeClassifier</code>). The outcome of training a learning algorithm is here called a <em>fit-result</em>. For ordinary multilinear regression, for example, this would be the coefficients and intercept. For a general supervised model, it is the (generally minimal) information needed to make new predictions.</p><p>The ultimate supertype of all models is <code>MLJBase.Model</code>, which has two abstract subtypes:</p><pre><code class="language-julia">abstract type Supervised{R} &lt;: Model end
abstract type Unsupervised &lt;: Model end</code></pre><p>Here the parameter <code>R</code> refers to a fit-result type. By declaring a model to be a subtype of <code>MLJBase.Supervised{R}</code> you guarantee the fit-result to be of type <code>R</code> and, if <code>R</code> is concrete, one may improve the performance of homogeneous ensembles of the model (as defined by the built-in MLJ <code>EnsembleModel</code> wrapper). There is no abstract type for fit-results because these types are generally declared outside of MLJBase.</p><blockquote><p>The necessity to declare the fitresult type <code>R</code> may disappear in the future (issue #93).</p></blockquote><p><code>Supervised</code> models are further divided according to whether they are able to furnish probabilistic predictions of the target(s) (which they will do so by default) or directly predict &quot;point&quot; estimates, for each new input pattern:</p><pre><code class="language-julia">abstract type Probabilistic{R} &lt;: Supervised{R} end
abstract type Deterministic{R} &lt;: Supervised{R} end</code></pre><p>Further division of model types is realized through <a href="#Trait-Declarations">trait declarations</a>.</p><p>Associated with every concrete subtype of <code>Model</code> there must be a <code>fit</code> method, which implements the associated algorithm to produce the fit-result. Additionally, every <code>Supervised</code> model has a <code>predict</code> method, while <code>Unsupervised</code> models must have a <code>transform</code> method. More generally, methods such as these, that are dispatched on a model instance and a fit-result (plus other data), are called <em>operations</em>. <code>Probabilistic</code> supervised models optionally implement a <code>predict_mode</code> operation (in the case of classifiers) or a <code>predict_mean</code> and/or <code>predict_median</code> operations (in the case of regressors) overriding obvious fallbacks provided by <code>MLJBase</code>. <code>Unsupervised</code> models may implement an <code>inverse_transform</code> operation.</p><h3><a class="nav-anchor" id="New-model-type-declarations-and-optional-clean!-method-1" href="#New-model-type-declarations-and-optional-clean!-method-1">New model type declarations and optional clean! method</a></h3><p>Here is an example of a concrete supervised model type declaration, made after defining an appropriate fit-result type (an optional step):</p><pre><code class="language-julia">import MLJ

struct LinearFitResult{F&lt;:AbstractFloat} &lt;: MLJBase.MLJType
    coefficients::Vector{F}
    bias::F
end

mutable struct RidgeRegressor{F} &lt;: MLJBase.Deterministic{LinearFitResult{F}}
    target_type::Type{F}
    lambda::Float64
end</code></pre><p><strong>Note.</strong> Model fields may be of any type except <code>NamedTuple</code>.  (This is because named tuples are used to represented the nested hyperparameters  of composite models (models that have other models as fields.)</p><p>Models (which are mutable) should not be given internal constructors. It is recommended that they be given an external lazy keyword constructor of the same name. This constructor defines default values for every field, and optionally corrects invalid field values by calling a <code>clean!</code> method (whose fallback returns an empty message string):</p><pre><code class="language-julia">function MLJ.clean!(model::RidgeRegressor)
    warning = &quot;&quot;
    if model.lambda &lt; 0
        warning *= &quot;Need lambda ≥ 0. Resetting lambda=0. &quot;
        model.lambda = 0
    end
    return warning
end

# keyword constructor
function RidgeRegressor(; target_type=Float64, lambda=0.0)

    model = RidgeRegressor(target_type, lambda)

    message = MLJBase.clean!(model)
    isempty(message) || @warn message

    return model
    
end</code></pre><h3><a class="nav-anchor" id="The-model-API-for-supervised-models-1" href="#The-model-API-for-supervised-models-1">The model API for supervised models</a></h3><p>Below we describe the compulsory and optional methods to be specified for each concrete type <code>SomeSupervisedModelType{R} &lt;: MLJBase.Supervised{R}</code>. </p><h4><a class="nav-anchor" id="The-form-of-data-for-fitting-and-predicting-1" href="#The-form-of-data-for-fitting-and-predicting-1">The form of data for fitting and predicting</a></h4><p>In every circumstance, the argument <code>X</code> passed to the <code>fit</code> method described below, and the argument <code>Xnew</code> of the <code>predict</code> method, will be some table supporting the <a href="https://github.com/JuliaData/Tables.jl">Tables.jl</a> API. The interface implementer can control the scientific type of data appearing in <code>X</code> with an appropriate <code>input_scitype</code> declaration (see <a href="#Trait-Declarations">Trait Declarations</a> below). If the core algorithm requires data in a different or more specific form, then <code>fit</code> will need to coerce the table into the form desired. To this end, MLJ provides the convenience method <code>MLJBase.matrix</code>; <code>MLJBase.matrix(Xtable)</code> is a two-dimensional <code>Array{T}</code> where <code>T</code> is the tightest common type of elements of <code>Xtable</code>, and <code>Xtable</code> is any table.</p><blockquote><p>Tables.jl has recently added a <code>matrix</code> method as well.</p></blockquote><p>Other convenience methods provided by MLJBase for handling tabular data are: <code>selectrows</code>, <code>selectcols</code>, <code>select</code>, <code>schema</code> (for extracting the size, names and eltypes of a table) and <code>table</code> (for materializing an abstract matrix, or named tuple of vectors, as a table matching a given prototype). Query the doc-strings for details.</p><p>Note that generally the same type coercions applied to <code>X</code> by <code>fit</code> will need to be applied by <code>predict</code> to <code>Xnew</code>. </p><p><strong>Important convention</strong> It is to be understood that the columns of the table <code>X</code> correspond to features and the rows to patterns.</p><p>The form of the target data <code>y</code> passed to <code>fit</code> is constrained by the <code>target_scitype</code> trait declaration. All elements of <code>y</code> will satisfy <code>scitype(y) &lt;: target_scitype(SomeSupervisedModelType)</code>. Furthermore, for univariate targets, <code>y</code> is always a <code>Vector</code> or <code>CategoricalVector</code>, according to the value of the trait:</p><table><tr><th style="text-align: right"><code>target_scitype(SomeSupervisedModelType)</code></th><th style="text-align: right">type of <code>y</code></th><th style="text-align: right">tightest known supertype of <code>eltype(y)</code></th></tr><tr><td style="text-align: right"><code>Continuous</code></td><td style="text-align: right"><code>Vector</code></td><td style="text-align: right"><code>Real</code></td></tr><tr><td style="text-align: right"><code>&lt;: Multiclass</code></td><td style="text-align: right"><code>CategoricalVector</code></td><td style="text-align: right"><code>Union{CategoricalString, CategoricalValue}</code></td></tr><tr><td style="text-align: right"><code>&lt;: FiniteOrderedFactor</code></td><td style="text-align: right"><code>CategoricalVector</code></td><td style="text-align: right"><code>Union{CategoricalString, CategoricalValue}</code></td></tr><tr><td style="text-align: right"><code>Count</code></td><td style="text-align: right"><code>Vector</code></td><td style="text-align: right"><code>Integer</code></td></tr></table><p>So, for example, if your model is a binary classifier, you declare</p><pre><code class="language-julia">target_scitype(SomeSupervisedModelType)=Multiclass{2}</code></pre><p>If it can predict any number of classes, you might instead declare</p><pre><code class="language-julia">target_scitype(SomeSupervisedModelType)=Union{Multiclass, FiniteOrderedFactor}</code></pre><p>See also the table in <a href="../">Getting Started</a>.</p><p>For multivariate targets, <code>y</code> will be a table whose columns have the scitypes indicated in the <code>Tuple</code> type returned by <code>target_scitype</code>; for example, if you declare <code>target_scitype(SomeSupervisedModelType) = Tuple{Continuous,Count}</code>, then <code>y</code> will have two columns, the first with <code>Real</code> elements, the second with <code>Integer</code> elements.</p><h4><a class="nav-anchor" id="The-fit-method-1" href="#The-fit-method-1">The fit method</a></h4><p>A compulsory <code>fit</code> method returns three objects:</p><pre><code class="language-julia">MLJBase.fit(model::SomeSupervisedModelType, verbosity::Int, X, y) -&gt; fitresult, cache, report</code></pre><p>Note: The <code>Int</code> typing of <code>verbosity</code> cannot be omitted.</p><ol><li><p><code>fitresult::R</code> is the fit-result in the sense above (which becomes an  argument for <code>predict</code> discussed below).</p></li><li><p><code>report</code> is a (possibly empty) <code>NamedTuple</code>, for example,  <code>report=(deviance=..., dof_residual=..., stderror=..., vcov=...)</code>.  Any training-related statistics, such as internal estimates of the  generalization error, and feature rankings, should be returned in  the <code>report</code> tuple. How, or if, these are generated should be  controlled by hyperparameters (the fields of <code>model</code>). Fitted  parameters, such as the coefficients of a linear model, do not go  in the report as they will be extractable from <code>fitresult</code> (and  accessible to MLJ through the <code>fitted_params</code> method, see below).</p></li></ol><p>3.	The value of <code>cache</code> can be <code>nothing</code>, unless one is also defining    an <code>update</code> method (see below). The Julia type of <code>cache</code> is not    presently restricted.</p><p>It is not necessary for <code>fit</code> to provide dimension checks or to call <code>clean!</code> on the model; MLJ will carry out such checks.</p><p>The method <code>fit</code> should never alter hyperparameter values. If the package is able to suggest better hyperparameters, as a byproduct of training, return these in the report field.</p><p>One should test that actual fit-results have the type declared in the model <code>mutable struct</code> declaration. To help with this, <code>MLJBase.fitresult_type(m)</code> returns the declared type, for any supervised model (or model type) <code>m</code>.</p><p>The <code>verbosity</code> level (0 for silent) is for passing to learning algorithm itself. A <code>fit</code> method wrapping such an algorithm should generally avoid doing any of its own logging.</p><h4><a class="nav-anchor" id="The-fitted_params-method-1" href="#The-fitted_params-method-1">The fitted_params method</a></h4><p>A <code>fitted_params</code> method may be optionally overloaded. It&#39;s purpose is to provide MLJ accesss to a user-friendly representation of the learned parameters of the model (as opposed to the hyperparameters). They must be extractable from <code>fitresult</code>.</p><pre><code class="language-julia">MLJBase.fitted_params(model::SomeSupervisedModelType, fitresult) -&gt; friendly_fitresult::NamedTuple</code></pre><p>For a linear model, for example, one might declare something like <code>friendly_fitresult=(coefs=[...], bias=...)</code>.</p><p>The fallback is to return <code>(fitresult=fitresult,)</code>.</p><h4><a class="nav-anchor" id="The-predict-method-1" href="#The-predict-method-1">The predict method</a></h4><p>A compulsory <code>predict</code> method has the form</p><pre><code class="language-julia">MLJBase.predict(model::SomeSupervisedModelType, fitresult, Xnew) -&gt; yhat</code></pre><p>Here <code>Xnew</code> is an any table whose entries satisfy the same scitype constraints as discussed for <code>X</code> above.</p><p><strong>Prediction types for deterministic responses.</strong> In the case of <code>Deterministic</code> models, <code>yhat</code> must have the same form as the target <code>y</code> passed to the <code>fit</code> method (see above discussion on the form of data for fitting), with one exception: If predicting a <code>Count</code>, the prediction may be <code>Continuous</code>. For all models predicting a <code>Multiclass</code> or <code>FiniteOrderedFactor</code>, the categorical vectors returned by <code>predict</code> <strong>must have the levels in the categorical pool of the target data presented in training</strong>, even if not all levels appear in the training data or prediction itself. That is, we must have <code>levels(yhat) == levels(y)</code>.</p><p>Unfortunately, code not written with the preservation of categorical levels in mind poses special problems. To help with this, MLJ provides a utility <code>CategoricalDecoder</code> which can decode a <code>CategoricalArray</code> into a plain array, and re-encode a prediction with the original levels intact. The <code>CategoricalDecoder</code> object created during <code>fit</code> will need to be bundled with <code>fitresult</code> to make it available to <code>predict</code> during re-encoding. </p><p>So, for example, if the core algorithm being wrapped by <code>fit</code> expects a nominal target <code>yint</code> of type <code>Vector{Int64}</code> then a <code>fit</code> method may look something like this:</p><pre><code class="language-julia">function MLJBase.fit(model::SomeSupervisedModelType, verbosity, X, y)
    decoder = MLJBase.CategoricalDecoder(y, Int64)
    yint = transform(decoder, y)
    core_fitresult = SomePackage.fit(X, yint, verbosity=verbosity)
    fitresult = (decoder, core_fitresult)
    cache = nothing
    report = nothing
    return fitresult, cache, report
end</code></pre><p>while a corresponding deterministic <code>predict</code> operation might look like this:</p><pre><code class="language-julia">function MLJBase.predict(model::SomeSupervisedModelType, fitresult, Xnew)
    decoder, core_fitresult = fitresult
    yhat = SomePackage.predict(core_fitresult, Xnew)
    return inverse_transform(decoder, yhat)
end</code></pre><p>Query <code>?MLJBase.DecodeCategorical</code> for more information.</p><p>If you are coding a learning algorithm from scratch, rather than wrapping an existing one, conversions may be unnecessary. It may suffice to record the pool of <code>y</code> and bundle that with the fitresult for <code>predict</code> to append to the levels of its categorical output.</p><p><strong>Prediction types for probabilistic responses.</strong> In the case of <code>Probabilistic</code> models with univariate targets, <code>yhat</code> must be a <code>Vector</code> whose elements are distributions (one distribution per row of <code>Xnew</code>).</p><p>A <em>distribution</em> is any instance of a subtype of <code>Distributions.Distribution</code> from the package Distributions.jl, or any instance of the additional types <code>UnivariateNominal</code> and <code>MultivariateNominal</code> defined in MLJBase.jl (or any other type <code>D</code> you define for which <code>MLJBase.isdistribution(::D) = true</code>, meaning <code>Base.rand</code> and <code>Distributions.pdf</code> are implemented, as well <code>Distributions.mean</code>/<code>Distribution.median</code> or <code>Distributions.mode</code>).</p><p>Use <code>UnivariateNominal</code> for <code>Probabilistic</code> models predicting <code>Multiclass</code> or <code>FiniteOrderedFactor</code> targets. For example, suppose <code>levels(y)=[&quot;yes&quot;, &quot;no&quot;, &quot;maybe&quot;]</code> and set <code>L=levels(y)</code>. Then, if the predicted probabilities for some input pattern are <code>[0.1, 0.7, 0.2]</code>, respectively, then the prediction returned for that pattern will be <code>UnivariateNominal(L, [0.1, 0.7, 0.2])</code>. Query <code>?UnivariateNominal</code> for more information.</p><p>The <code>predict</code> method will need access to all levels in the pool of the target variable presented <code>y</code> presented for training, which consequently need to be encoded in the <code>fitresult</code> returned by <code>fit</code>. If a <code>CategoricalDecoder</code> object, <code>decoder</code>, has been bundled in <code>fitresult</code>, as in the deterministic example above, then the levels are given by <code>levels(decoder)</code>. Levels not observed in the training data  (i.e., only in its pool) should be assigned probability zero.</p><h4><a class="nav-anchor" id="Trait-declarations-1" href="#Trait-declarations-1">Trait declarations</a></h4><p>There are a number of recommended trait declarations for each model mutable structure <code>SomeSupervisedModelType &lt;: Supervised</code> you define. Basic fitting, resampling and tuning in MLJ does not require these traits but some advanced MLJ meta-algorithms may require them now, or in the future. In particular, MLJ&#39;s <code>models(::Task)</code> method (matching models to user-specified tasks) can only identify models having a complete set of trait declarations. A full set of declarations is shown below for the <code>DecisionTreeClassifier</code> type (defined in the submodule DecisionTree_ of MLJModels):</p><pre><code class="language-julia">MLJBase.load_path(::Type{&lt;:DecisionTreeClassifier}) = &quot;MLJModels.DecisionTree_.DecisionTreeClassifier&quot; 
MLJBase.package_name(::Type{&lt;:DecisionTreeClassifier}) = &quot;DecisionTree&quot;
MLJBase.package_uuid(::Type{&lt;:DecisionTreeClassifier}) = &quot;7806a523-6efd-50cb-b5f6-3fa6f1930dbb&quot;
MLJBase.package_url(::Type{&lt;:DecisionTreeClassifier}) = &quot;https://github.com/bensadeghi/DecisionTree.jl&quot;
MLJBase.is_pure_julia(::Type{&lt;:DecisionTreeClassifier}) = true
MLJBase.input_is_multivariate(::Type{&lt;:DecisionTreeClassifier}) = true
MLJBase.input_scitypes(::Type{&lt;:DecisionTreeClassifier}) = MLJBase.Continuous
MLJBase.target_scitype(::Type{&lt;:DecisionTreeClassifier}) = MLJBase.Multiclass</code></pre><p>Note that models predicting multivariate targets will need to need to have <code>target_scitype</code> return an appropriate <code>Tuple</code> type. </p><p>For an explanation of <code>Found</code> and <code>Other</code> in the table below, see <a href="../scientific_data_types/">Scientific Types</a>.</p><table><tr><th style="text-align: right">method</th><th style="text-align: right">return type</th><th style="text-align: right">declarable return values</th><th style="text-align: right">default value</th></tr><tr><td style="text-align: right"><code>target_scitype</code></td><td style="text-align: right"><code>DataType</code></td><td style="text-align: right">subtype of <code>Found</code> or tuple of such types</td><td style="text-align: right"><code>Union{Found,NTuple{&lt;:Found}}</code></td></tr><tr><td style="text-align: right"><code>input_scitypes</code></td><td style="text-align: right"><code>DataType</code></td><td style="text-align: right">subtype of <code>Union{Missing,Found}</code></td><td style="text-align: right"><code>Union{Missing,Found}</code></td></tr><tr><td style="text-align: right"><code>input_is_multivariate</code></td><td style="text-align: right"><code>Bool</code></td><td style="text-align: right"><code>true</code> or <code>false</code></td><td style="text-align: right"><code>true</code></td></tr><tr><td style="text-align: right"><code>is_pure_julia</code></td><td style="text-align: right"><code>Bool</code></td><td style="text-align: right"><code>true</code> or <code>false</code></td><td style="text-align: right"><code>false</code></td></tr><tr><td style="text-align: right"><code>load_path</code></td><td style="text-align: right"><code>String</code></td><td style="text-align: right">unrestricted</td><td style="text-align: right">&quot;unknown&quot;</td></tr><tr><td style="text-align: right"><code>package_name</code></td><td style="text-align: right"><code>String</code></td><td style="text-align: right">unrestricted</td><td style="text-align: right">&quot;unknown&quot;</td></tr><tr><td style="text-align: right"><code>package_uuid</code></td><td style="text-align: right"><code>String</code></td><td style="text-align: right">unrestricted</td><td style="text-align: right">&quot;unknown&quot;</td></tr><tr><td style="text-align: right"><code>package_url</code></td><td style="text-align: right"><code>String</code></td><td style="text-align: right">unrestricted</td><td style="text-align: right">&quot;unknown&quot;</td></tr></table><p>You can test declarations of traits by calling <code>info(SomeModelType)</code>.</p><h4><a class="nav-anchor" id="The-update!-method-1" href="#The-update!-method-1">The update! method</a></h4><p>An <code>update</code> method may be optionally overloaded to enable a call by MLJ to retrain a model (on the same training data) to avoid repeating computations unnecessarily.</p><pre><code class="language-julia">MLJBase.update(model::SomeSupervisedModelType, verbosity, old_fitresult, old_cache, X, y) -&gt; fitresult, cache, report</code></pre><p>If an MLJ <code>Machine</code> is being <code>fit!</code> and it is not the first time, then <code>update</code> is called instead of <code>fit</code> unless <code>fit!</code> has been called with new rows. However, <code>MLJBase</code> defines a fallback for <code>update</code> which just calls <code>fit</code>. For context, see <a href="../internals/">MLJ Internals</a>. </p><p>Learning networks wrapped as models constitute one use-case: One would like each component model to be retrained only when hyperparameter changes &quot;upstream&quot; make this necessary. In this case MLJ provides a fallback (specifically, the fallback is for any subtype of <code>Supervised{Node}</code>). A second important use-case is iterative models, where calls to increase the number of iterations only restarts the iterative procedure if other hyperparameters have also changed. For an example, see <code>builtins/Ensembles.jl</code>.</p><p>In the event that the argument <code>fitresult</code> (returned by a preceding call to <code>fit</code>) is not sufficient for performing an update, the author can arrange for <code>fit</code> to output in its <code>cache</code> return value any additional information required, as this is also passed as an argument to the <code>update</code> method.</p><h4><a class="nav-anchor" id="Multivariate-models-1" href="#Multivariate-models-1">Multivariate models</a></h4><p>TODO</p><h3><a class="nav-anchor" id="Unsupervised-models-1" href="#Unsupervised-models-1">Unsupervised models</a></h3><p>TODO</p><h3><a class="nav-anchor" id="Where-to-place-code-implementing-new-models-1" href="#Where-to-place-code-implementing-new-models-1">Where to place code implementing new models</a></h3><p>Note that different packages can implement models having the same name without causing conflicts, although an MLJ user cannot simultaneously <em>load</em> two such models.</p><p>There are two options for making a new model implementation available to all MLJ users:</p><ol><li><p><strong>Native implementations</strong> (preferred option). The implementation code lives in the same package that contains the learning algorithms implementing the interface. In this case, it is sufficient to open an issue at <a href="https://github.com/alan-turing-institute/MLJRegistry.jl">MLJRegistry</a> requesting the package to be registered with MLJ. Registering a package allows the MLJ user to access its models&#39; metadata and to selectively load them.</p></li><li><p><strong>External implementations</strong> (short-term alternative). The model implementation code is necessarily separate from the package <code>SomePkg</code> defining the learning algorithm being wrapped. In this case, the recommended procedure is to include the implementation code at <a href="https://github.com/alan-turing-institute/MLJModels.jl/tree/master/src">MLJModels/src</a> via a pull-request, and test code at <a href="https://github.com/alan-turing-institute/MLJModels.jl/tree/master/test">MLJModels/test</a>. Assuming <code>SomePkg</code> is the only package imported by the implementation code, one needs to: (i) register <code>SomePkg</code> at MLJRegistry as explained above; and (ii) add a corresponding <code>@require</code> line in the PR to <a href="https://github.com/alan-turing-institute/MLJModels.jl/tree/master/src/MLJModels.jl">MLJModels/src/MLJModels.jl</a> to enable lazy-loading of that package by MLJ (following the pattern of existing additions). If other packages must be imported, add them to the MLJModels project file after checking they are not already there. If it is really necessary, packages can be also added to Project.toml for testing purposes.</p></li></ol><p>Additionally, one needs to ensure that the implementation code defines the <code>package_name</code> and <code>load_path</code> model traits appropriately, so that <code>MLJ</code>&#39;s <code>@load</code> macro can find the necessary code (see <a href="https://github.com/alan-turing-institute/MLJModels.jl/tree/master/src">MLJModels/src</a> for examples). The <code>@load</code> command can only be tested after registration. If changes are made, lodge an issue at <a href="https://github.com/alan-turing-institute/MLJRegistry.jl">MLJRegistry</a> to make the changes available to MLJ.  </p><footer><hr/><a class="previous" href="../learning_networks/"><span class="direction">Previous</span><span class="title">Learning Networks</span></a><a class="next" href="../the_simplified_model_api/"><span class="direction">Next</span><span class="title">The Simplified Model API</span></a></footer></article></body></html>
