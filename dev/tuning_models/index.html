<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Tuning Models · MLJ</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit">MLJ</span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Getting Started</a></li><li><a class="tocitem" href="../common_mlj_workflows/">Common MLJ Workflows</a></li><li><a class="tocitem" href="../model_search/">Model Search</a></li><li><a class="tocitem" href="../machines/">Machines</a></li><li><a class="tocitem" href="../evaluating_model_performance/">Evaluating Model Performance</a></li><li><a class="tocitem" href="../performance_measures/">Performance Measures</a></li><li class="is-active"><a class="tocitem" href>Tuning Models</a><ul class="internal"><li><a class="tocitem" href="#Tuning-a-single-hyperparameter-using-a-grid-search-1"><span>Tuning a single hyperparameter using a grid search</span></a></li><li><a class="tocitem" href="#Tuning-multiple-nested-hyperparameters-1"><span>Tuning multiple nested hyperparameters</span></a></li><li><a class="tocitem" href="#API-1"><span>API</span></a></li></ul></li><li><a class="tocitem" href="../learning_curves/">Learning Curves</a></li><li><a class="tocitem" href="../built_in_transformers/">Built-in Transformers</a></li><li><a class="tocitem" href="../composing_models/">Composing Models</a></li><li><a class="tocitem" href="../homogeneous_ensembles/">Homogeneous Ensembles</a></li><li><a class="tocitem" href="../openml_integration/">OpenML Integration</a></li><li><a class="tocitem" href="../simple_user_defined_models/">Simple User Defined Models</a></li><li><a class="tocitem" href="../quick_start_guide_to_adding_models/">Quick-Start Guide to Adding Models</a></li><li><a class="tocitem" href="../adding_models_for_general_use/">Adding Models for General Use</a></li><li><a class="tocitem" href="../benchmarking/">Benchmarking</a></li><li><a class="tocitem" href="../internals/">Internals</a></li><li><a class="tocitem" href="../glossary/">Glossary</a></li><li><a class="tocitem" href="../mlj_cheatsheet/">MLJ Cheatsheet</a></li><li><a class="tocitem" href="../NEWS/">MLJ News</a></li><li><a class="tocitem" href="../frequently_asked_questions/">FAQ</a></li><li><a class="tocitem" href="../julia_blogpost/">Julia BlogPost</a></li><li><a class="tocitem" href="../acceleration_and_parallelism/">Acceleration and Parallelism</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Tuning Models</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Tuning Models</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/alan-turing-institute/MLJ.jl/blob/master/docs/src/tuning_models.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Tuning-models-1"><a class="docs-heading-anchor" href="#Tuning-models-1">Tuning models</a><a class="docs-heading-anchor-permalink" href="#Tuning-models-1" title="Permalink"></a></h1><p>In MLJ tuning is implemented as a model wrapper. After wrapping a model in a tuning strategy and binding the wrapped model to data in a machine, <code>mach</code>, calling <code>fit!(mach)</code> instigates a search for optimal model hyperparameters, within a specified <code>range</code>, and then uses all supplied data to train the best model. To predict using the optimal model, one just calls <code>predict(mach, Xnew)</code>. In this way the wrapped model may be viewed as a &quot;self-tuning&quot; version of the unwrapped model.</p><h2 id="Tuning-a-single-hyperparameter-using-a-grid-search-1"><a class="docs-heading-anchor" href="#Tuning-a-single-hyperparameter-using-a-grid-search-1">Tuning a single hyperparameter using a grid search</a><a class="docs-heading-anchor-permalink" href="#Tuning-a-single-hyperparameter-using-a-grid-search-1" title="Permalink"></a></h2><pre><code class="language-julia-repl">julia&gt; using MLJ

julia&gt; X = MLJ.table(rand(100, 10));

julia&gt; y = 2X.x1 - X.x2 + 0.05*rand(100);

julia&gt; tree_model = @load DecisionTreeRegressor;</code></pre><p>Let&#39;s tune <code>min_purity_increase</code> in the model above, using a grid-search. To do so we will use the simplest <code>range</code> object, a one-dimensional range object constructed using the <code>range</code> method:</p><pre><code class="language-julia-repl">julia&gt; r = range(tree_model, :min_purity_increase, lower=0.001, upper=1.0, scale=:log);

julia&gt; self_tuning_tree_model = TunedModel(model=tree_model,
                                           resampling = CV(nfolds=3),
                                           tuning = Grid(resolution=10),
                                           range = r,
                                           measure = rms);</code></pre><p>Incidentally, a grid is generated internally &quot;over the range&quot; by calling the <code>iterator</code> method with an appropriate resolution:</p><pre><code class="language-julia-repl">julia&gt; iterator(r, 5)
5-element Array{Float64,1}:
 0.0010000000000000002
 0.005623413251903492
 0.0316227766016838
 0.1778279410038923
 1.0</code></pre><p>Non-numeric hyperparameters are handled a little differently:</p><pre><code class="language-julia-repl">julia&gt; selector_model = FeatureSelector();

julia&gt; r2 = range(selector_model, :features, values = [[:x1,], [:x1, :x2]]);

julia&gt; iterator(r2)
2-element Array{Array{Symbol,1},1}:
 [:x1]
 [:x1, :x2]</code></pre><p>Unbounded ranges are also permitted. See the <code>range</code> and <code>iterator</code> docstrings below for details.</p><p>Returning to the wrapped tree model:</p><pre><code class="language-julia-repl">julia&gt; self_tuning_tree = machine(self_tuning_tree_model, X, y);

julia&gt; fit!(self_tuning_tree, verbosity=0);</code></pre><p>We can inspect the detailed results of the grid search with <code>report(self_tuning_tree)</code> or just retrieve the optimal model, as here:</p><pre><code class="language-julia-repl">julia&gt; fitted_params(self_tuning_tree).best_model
DecisionTreeRegressor(
    max_depth = -1,
    min_samples_leaf = 5,
    min_samples_split = 2,
    min_purity_increase = 0.0021544346900318843,
    n_subfeatures = 0,
    post_prune = false,
    merge_purity_threshold = 1.0) @ 1…17</code></pre><p>Predicting on new input observations using the optimal model:</p><pre><code class="language-julia-repl">julia&gt; Xnew  = MLJ.table(rand(3, 10));

julia&gt; predict(self_tuning_tree, Xnew)
3-element Array{Float64,1}:
 0.2015433410116656
 0.2015433410116656
 0.6067397365458543</code></pre><h2 id="Tuning-multiple-nested-hyperparameters-1"><a class="docs-heading-anchor" href="#Tuning-multiple-nested-hyperparameters-1">Tuning multiple nested hyperparameters</a><a class="docs-heading-anchor-permalink" href="#Tuning-multiple-nested-hyperparameters-1" title="Permalink"></a></h2><p>The following model has another model, namely a <code>DecisionTreeRegressor</code>, as a hyperparameter:</p><pre><code class="language-julia">julia&gt; tree_model = DecisionTreeRegressor()
julia&gt; forest_model = EnsembleModel(atom=tree_model);</code></pre><p>Nested hyperparameters can be inspected using <code>params</code> (or just type <code>@more</code> in the REPL after instantiating <code>forest_model</code>):</p><pre><code class="language-julia-repl">julia&gt; params(forest_model)
(atom = (max_depth = -1,
         min_samples_leaf = 5,
         min_samples_split = 2,
         min_purity_increase = 0.0,
         n_subfeatures = 0,
         post_prune = false,
         merge_purity_threshold = 1.0,),
 atomic_weights = Float64[],
 bagging_fraction = 0.8,
 rng = MersenneTwister(UInt32[0x000004d2]) @ 740,
 n = 100,
 acceleration = CPU1{Nothing}(nothing),
 out_of_bag_measure = Any[],)</code></pre><p>Ranges for nested hyperparameters are specified using dot syntax. In this case we will specify a <code>goal</code> for the total number of grid points, rather than a per-dimension resolution:</p><pre><code class="language-julia-repl">julia&gt; r1 = range(forest_model, :(atom.n_subfeatures), lower=1, upper=9);

julia&gt; r2 = range(forest_model, :bagging_fraction, lower=0.4, upper=1.0);

julia&gt; self_tuning_forest_model = TunedModel(model=forest_model,
                                             tuning=Grid(goal=30),
                                             resampling=CV(nfolds=6),
                                             range=[r1, r2],
                                             measure=rms);

julia&gt; self_tuning_forest = machine(self_tuning_forest_model, X, y);

julia&gt; fit!(self_tuning_forest, verbosity=1)
[ Info: Training Machine{DeterministicTunedModel{Grid,…}} @ 8…56.
[ Info: Attempting to evaluate 25 models.
Machine{DeterministicTunedModel{Grid,…}} @ 8…56</code></pre><p>In this two-parameter case, a plot of the grid search results is also available:</p><pre><code class="language-julia">using Plots
plot(self_tuning_forest)</code></pre><p><img src="../img/tuning_plot.png" alt/></p><p>For more options in a grid search, see the <code>Grid</code> docstring below.</p><h2 id="API-1"><a class="docs-heading-anchor" href="#API-1">API</a><a class="docs-heading-anchor-permalink" href="#API-1" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="Base.range" href="#Base.range"><code>Base.range</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">r = range(model, :hyper; values=nothing)</code></pre><p>Define a one-dimensional <code>NominalRange</code> object for a field <code>hyper</code> of <code>model</code>. Note that <code>r</code> is not directly iterable but <code>iterator(r)</code> is. </p><p>The behaviour of range methods depends on the type of the value of the hyperparameter at <code>model</code> during range construction. To override (or if <code>model</code> is not available) specify a type in place of <code>model</code>.</p><p>A nested hyperparameter is specified using dot notation. For example, <code>:(atom.max_depth)</code> specifies the <code>max_depth</code> hyperparameter of the submodel <code>model.atom</code>.</p><pre><code class="language-none">r = range(model, :hyper; upper=nothing, lower=nothing,
          scale=nothing, values=nothing)</code></pre><p>Assuming <code>values</code> is not specified, define a one-dimensional <code>NumericRange</code> object for a <code>Real</code> field <code>hyper</code> of <code>model</code>.  Note that <code>r</code> is not directly iteratable but <code>iterator(r, n)</code>is an iterator of length <code>n</code>. To generate random elements from <code>r</code>, instead apply <code>rand</code> methods to <code>sampler(r)</code>. The supported scales are <code>:linear</code>,<code>:log</code>, <code>:logminus</code>, <code>:log10</code>, <code>:log2</code>, or a callable object.</p><p>A nested hyperparameter is specified using dot notation (see above).</p><p>If <code>scale</code> is unspecified, it is set to <code>:linear</code>, <code>:log</code>, <code>:logminus</code>, or <code>:linear</code>, according to whether the interval <code>(lower, upper)</code> is bounded, right-unbounded, left-unbounded, or doubly unbounded, respectively.  Note <code>upper=Inf</code> and <code>lower=-Inf</code> are allowed.</p><p>If <code>values</code> is specified, the other keyword arguments are ignored and a <code>NominalRange</code> object is returned (see above).</p><p>See also: <a href="#MLJBase.iterator"><code>iterator</code></a>, <a href="@ref"><code>sampler</code></a></p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.iterator" href="#MLJBase.iterator"><code>MLJBase.iterator</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">iterator([rng, ], r::NominalRange, [,n])
iterator([rng, ], r::NumericRange, n)</code></pre><p>Return an iterator (currently a vector) for a <code>ParamRange</code> object <code>r</code>. In the first case iteration is over all <code>values</code> stored in the range (or just the first <code>n</code>, if <code>n</code> is specified). In the second case, the iteration is over approximately <code>n</code> ordered values, generated as follows:</p><p>(i) First, exactly <code>n</code> values are generated between <code>U</code> and <code>L</code>, with a spacing determined by <code>r.scale</code> (uniform if <code>scale=:linear</code>) where <code>U</code> and <code>L</code> are given by the following table:</p><table><tr><th style="text-align: right"><code>r.lower</code></th><th style="text-align: right"><code>r.upper</code></th><th style="text-align: right"><code>L</code></th><th style="text-align: right"><code>U</code></th></tr><tr><td style="text-align: right">finite</td><td style="text-align: right">finite</td><td style="text-align: right"><code>r.lower</code></td><td style="text-align: right"><code>r.upper</code></td></tr><tr><td style="text-align: right"><code>-Inf</code></td><td style="text-align: right">finite</td><td style="text-align: right"><code>r.upper - 2r.unit</code></td><td style="text-align: right"><code>r.upper</code></td></tr><tr><td style="text-align: right">finite</td><td style="text-align: right"><code>Inf</code></td><td style="text-align: right"><code>r.lower</code></td><td style="text-align: right"><code>r.lower + 2r.unit</code></td></tr><tr><td style="text-align: right"><code>-Inf</code></td><td style="text-align: right"><code>Inf</code></td><td style="text-align: right"><code>r.origin - r.unit</code></td><td style="text-align: right"><code>r.origin + r.unit</code></td></tr></table><p>(ii) If a callable <code>f</code> is provided as <code>scale</code>, then a uniform spacing is always applied in (i) but <code>f</code> is broadcast over the results. (Unlike ordinary scales, this alters the effective range of values generated, instead of just altering the spacing.)</p><p>(iii) If <code>r</code> is a discrete numeric range (<code>r isa NumericRange{&lt;:Integer}</code>) then the values are additionally rounded, with any duplicate values removed. Otherwise all the values are used (and there are exacltly <code>n</code> of them).</p><p>(iv) Finally, if a random number generator <code>rng</code> is specified, then the values are returned in random order (sampling without replacement), and otherwise they are returned in numeric order, or in the order provided to the range constructor, in the case of a <code>NominalRange</code>.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJTuning.TunedModel" href="#MLJTuning.TunedModel"><code>MLJTuning.TunedModel</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">tuned_model = TunedModel(; model=nothing,
                         tuning=Grid(),
                         resampling=Holdout(),
                         measure=nothing,
                         weights=nothing,
                         repeats=1,
                         operation=predict,
                         range=nothing,
                         n=default_n(tuning, range),
                         train_best=true,
                         acceleration=default_resource(),
                         acceleration_resampling=CPU1(),
                         check_measure=true)</code></pre><p>Construct a model wrapper for hyperparameter optimization of a supervised learner.</p><p>Calling <code>fit!(mach)</code> on a machine <code>mach=machine(tuned_model, X, y)</code> or <code>mach=machine(tuned_model, X, y, w)</code> will:</p><ul><li><p>Instigate a search, over clones of <code>model</code>, with the hyperparameter mutations specified by <code>range</code>, for a model optimizing the specified <code>measure</code>, using performance evaluations carried out using the specified <code>tuning</code> strategy and <code>resampling</code> strategy.</p></li><li><p>Fit an internal machine, based on the optimal model <code>fitted_params(mach).best_model</code>, wrapping the optimal <code>model</code> object in <em>all</em> the provided data <code>X</code>, <code>y</code>(, <code>w</code>). Calling <code>predict(mach, Xnew)</code> then returns predictions on <code>Xnew</code> of this internal machine. The final train can be supressed by setting <code>train_best=false</code>.</p></li></ul><p>The <code>range</code> objects supported depend on the <code>tuning</code> strategy specified. Query the <code>strategy</code> docstring for details. To optimize over an explicit list <code>v</code> of models of the same type, use <code>strategy=Explicit()</code> and specify <code>model=v[1]</code> and <code>range=v</code>.</p><p>The number of models searched is specified by <code>n</code>. If unspecified, then <code>MLJTuning.default_n(tuning, range)</code> is used. When <code>n</code> is increased and <code>fit!(mach)</code> called again, the old search history is re-instated and the search continues where it left off.</p><p>If <code>measure</code> supports weights (<code>supports_weights(measure) == true</code>) then any <code>weights</code> specified will be passed to the measure. If more than one <code>measure</code> is specified, then only the first is optimized (unless <code>strategy</code> is multi-objective) but the performance against every measure specified will be computed and reported in <code>report(mach).best_performance</code> and other relevant attributes of the generated report.</p><p>Specify <code>repeats &gt; 1</code> for repeated resampling per model evaluation. See <a href="../evaluating_model_performance/#MLJBase.evaluate!"><code>evaluate!</code></a> options for details.</p><p><em>Important.</em> If a custom <code>measure</code> is used, and the measure is a score, rather than a loss, be sure to check that <code>MLJ.orientation(measure) == :score</code> to ensure maximization of the measure, rather than minimization. Override an incorrect value with <code>MLJ.orientation(::typeof(measure)) = :score</code>.</p><p><em>Important:</em> If <code>weights</code> are left unspecified, and <code>measure</code> supports sample weights, then any weight vector <code>w</code> used in constructing a corresponding tuning machine, as in <code>tuning_machine = machine(tuned_model, X, y, w)</code> (which is then used in <em>training</em> each model in the search) will also be passed to <code>measure</code> for evaluation.</p><p>In the case of two-parameter tuning, a Plots.jl plot of performance estimates is returned by <code>plot(mach)</code> or <code>heatmap(mach)</code>.</p><p>Once a tuning machine <code>mach</code> has bee trained as above, then <code>fitted_params(mach)</code> has these keys/values:</p><table><tr><th style="text-align: right">key</th><th style="text-align: right">value</th></tr><tr><td style="text-align: right"><code>best_model</code></td><td style="text-align: right">optimal model instance</td></tr><tr><td style="text-align: right"><code>best_fitted_params</code></td><td style="text-align: right">learned parameters of the optimal model</td></tr></table><p>The named tuple <code>report(mach)</code> has these keys/values:</p><table><tr><th style="text-align: right">key</th><th style="text-align: right">value</th></tr><tr><td style="text-align: right"><code>best_model</code></td><td style="text-align: right">optimal model instance</td></tr><tr><td style="text-align: right"><code>best_result</code></td><td style="text-align: right">corresponding &quot;result&quot; entry in the history</td></tr><tr><td style="text-align: right"><code>best_report</code></td><td style="text-align: right">report generated by fitting the optimal model</td></tr></table><p>plus others specific to the <code>tuning</code> strategy, such as <code>history=...</code>.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJTuning.Grid" href="#MLJTuning.Grid"><code>MLJTuning.Grid</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">Grid(goal=nothing, resolution=10, rng=Random.GLOBAL_RNG, shuffle=true)</code></pre><p>Instantiate a Cartesian grid-based hyperparameter tuning strategy with a specified number of grid points as <code>goal</code>, or using a specified default <code>resolution</code> in each numeric dimension.</p><p><strong>Supported ranges:</strong></p><ul><li><p>A single one-dimensional range (<code>ParamRange</code> object) <code>r</code>, or pair of the form <code>(r, res)</code> where <code>res</code> specifies a resolution to override the default <code>resolution</code>.</p></li><li><p>Any vector of objects of the above form</p></li></ul><p><code>ParamRange</code> objects are constructed using the <code>range</code> method.</p><p>Example 1:</p><pre><code class="language-none">range(model, :hyper1, lower=1, origin=2, unit=1)</code></pre><p>Example 2:</p><pre><code class="language-none">[(range(model, :hyper1, lower=1, upper=10), 15),
  range(model, :hyper2, lower=2, upper=4),
  range(model, :hyper3, values=[:ball, :tree])]</code></pre><p>Note: All the <code>field</code> values of the <code>ParamRange</code> objects (<code>:hyper1</code>, <code>:hyper2</code>, <code>:hyper3</code> in the preceding example) must refer to field names a of single model (the <code>model</code> specified during <code>TunedModel</code> construction).</p><p><strong>Algorithm</strong></p><p>This is a standard grid search with the following specifics: In all cases all <code>values</code> of each specified <code>NominalRange</code> are exhausted. If <code>goal</code> is specified, then all resolutions are ignored, and a global resolution is applied to the <code>NumericRange</code> objects that maximizes the number of grid points, subject to the restriction that this not exceed <code>goal</code>. Otherwise the default <code>resolution</code> and any parameter-specific resolutions apply.</p><p>In all cases the models generated are shuffled using <code>rng</code>, unless <code>shuffle=false</code>.</p><p>See also <a href="#MLJTuning.TunedModel"><code>TunedModel</code></a>, <a href="#Base.range"><code>range</code></a>.</p></div></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../performance_measures/">« Performance Measures</a><a class="docs-footer-nextpage" href="../learning_curves/">Learning Curves »</a></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Wednesday 11 March 2020 07:45">Wednesday 11 March 2020</span>. Using Julia version 1.2.0.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
