<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Machines · MLJ</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.svg" alt="MLJ logo"/></a><div class="docs-package-name"><span class="docs-autofit">MLJ</span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../about_mlj/">About MLJ</a></li><li><a class="tocitem" href="../getting_started/">Getting Started</a></li><li><a class="tocitem" href="../common_mlj_workflows/">Common MLJ Workflows</a></li><li><a class="tocitem" href="../working_with_categorical_data/">Working with Categorical Data</a></li><li><a class="tocitem" href="../model_search/">Model Search</a></li><li><a class="tocitem" href="../loading_model_code/">Loading Model Code</a></li><li class="is-active"><a class="tocitem" href>Machines</a><ul class="internal"><li class="toplevel"><a class="tocitem" href="#Warm-restarts"><span>Warm restarts</span></a></li><li><a class="tocitem" href="#Inspecting-machines"><span>Inspecting machines</span></a></li><li><a class="tocitem" href="#Constructing-machines"><span>Constructing machines</span></a></li><li><a class="tocitem" href="#Lowering-memory-demands"><span>Lowering memory demands</span></a></li><li><a class="tocitem" href="#Saving-machines"><span>Saving machines</span></a></li><li><a class="tocitem" href="#Internals"><span>Internals</span></a></li><li><a class="tocitem" href="#API-Reference"><span>API Reference</span></a></li></ul></li><li><a class="tocitem" href="../evaluating_model_performance/">Evaluating Model Performance</a></li><li><a class="tocitem" href="../performance_measures/">Performance Measures</a></li><li><a class="tocitem" href="../weights/">Weights</a></li><li><a class="tocitem" href="../tuning_models/">Tuning Models</a></li><li><a class="tocitem" href="../learning_curves/">Learning Curves</a></li><li><a class="tocitem" href="../preparing_data/">Preparing Data</a></li><li><a class="tocitem" href="../transformers/">Transformers and Other Unsupervised models</a></li><li><a class="tocitem" href="../more_on_probabilistic_predictors/">More on Probablistic Predictors</a></li><li><a class="tocitem" href="../composing_models/">Composing Models</a></li><li><a class="tocitem" href="../linear_pipelines/">Linear Pipelines</a></li><li><a class="tocitem" href="../target_transformations/">Target Transformations</a></li><li><a class="tocitem" href="../homogeneous_ensembles/">Homogeneous Ensembles</a></li><li><a class="tocitem" href="../model_stacking/">Model Stacking</a></li><li><a class="tocitem" href="../controlling_iterative_models/">Controlling Iterative Models</a></li><li><a class="tocitem" href="../generating_synthetic_data/">Generating Synthetic Data</a></li><li><a class="tocitem" href="../openml_integration/">OpenML Integration</a></li><li><a class="tocitem" href="../acceleration_and_parallelism/">Acceleration and Parallelism</a></li><li><a class="tocitem" href="../simple_user_defined_models/">Simple User Defined Models</a></li><li><a class="tocitem" href="../quick_start_guide_to_adding_models/">Quick-Start Guide to Adding Models</a></li><li><a class="tocitem" href="../adding_models_for_general_use/">Adding Models for General Use</a></li><li><a class="tocitem" href="../modifying_behavior/">Modifying Behavior</a></li><li><a class="tocitem" href="../internals/">Internals</a></li><li><a class="tocitem" href="../list_of_supported_models/">List of Supported Models</a></li><li><a class="tocitem" href="../third_party_packages/">Third Party Packages</a></li><li><a class="tocitem" href="../glossary/">Glossary</a></li><li><a class="tocitem" href="../mlj_cheatsheet/">MLJ Cheatsheet</a></li><li><a class="tocitem" href="../known_issues/">Known Issues</a></li><li><a class="tocitem" href="../frequently_asked_questions/">FAQ</a></li><li><a class="tocitem" href="../julia_blogpost/">Julia BlogPost</a></li><li><a class="tocitem" href="../api/">Index of Methods</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Machines</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Machines</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/alan-turing-institute/MLJ.jl/blob/master/docs/src/machines.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Machines"><a class="docs-heading-anchor" href="#Machines">Machines</a><a id="Machines-1"></a><a class="docs-heading-anchor-permalink" href="#Machines" title="Permalink"></a></h1><p>Recall from <a href="../getting_started/#Getting-Started">Getting Started</a> that a machine binds a model (i.e., a choice of algorithm + hyperparameters) to data (see more at <a href="#Constructing-machines">Constructing machines</a> below). A machine is also the object storing <em>learned</em> parameters.  Under the hood, calling <code>fit!</code> on a machine calls either <code>MLJBase.fit</code> or <code>MLJBase.update</code>, depending on the machine&#39;s internal state (as recorded in private fields <code>old_model</code> and <code>old_rows</code>). These lower-level <code>fit</code> and <code>update</code> methods, which are not ordinarily called directly by the user, dispatch on the model and a view of the data defined by the optional <code>rows</code> keyword argument of <code>fit!</code> (all rows by default). </p><h1 id="Warm-restarts"><a class="docs-heading-anchor" href="#Warm-restarts">Warm restarts</a><a id="Warm-restarts-1"></a><a class="docs-heading-anchor-permalink" href="#Warm-restarts" title="Permalink"></a></h1><p>If a model <code>update</code> method has been implemented for the model, calls to <code>fit!</code> will avoid redundant calculations for certain kinds of model mutations. The main use-case is increasing an iteration parameter, such as the number of epochs in a neural network. To test if <code>SomeIterativeModel</code> supports this feature, check <code>iteration_parameter(SomeIterativeModel)</code> is different from <code>nothing</code>.</p><pre><code class="language-">using MLJ; color_off() # hide
tree = (@load DecisionTreeClassifier pkg=DecisionTree verbosity=0)()
forest = EnsembleModel(model=tree, n=10);
X, y = @load_iris;
mach = machine(forest, X, y)
fit!(mach, verbosity=2);</code></pre><p>Generally, changing a hyperparameter triggers retraining on calls to subsequent <code>fit!</code>:</p><pre><code class="language-julia-repl">julia&gt; forest.bagging_fraction=0.5
0.5

julia&gt; fit!(mach, verbosity=2);
ERROR: UndefVarError: mach not defined</code></pre><p>However, for this iterative model, increasing the iteration parameter only adds models to the existing ensemble:</p><pre><code class="language-julia-repl">julia&gt; forest.n=15
15

julia&gt; fit!(mach, verbosity=2);
ERROR: UndefVarError: mach not defined</code></pre><p>Call <code>fit!</code> again without making a change and no retraining occurs:</p><pre><code class="language-julia-repl">julia&gt; fit!(mach);
ERROR: UndefVarError: mach not defined</code></pre><p>However, retraining can be forced:</p><pre><code class="language-julia-repl">julia&gt; fit!(mach, force=true);
ERROR: UndefVarError: mach not defined</code></pre><p>And is re-triggered if the view of the data changes:</p><pre><code class="language-julia-repl">julia&gt; fit!(mach, rows=1:100);
ERROR: UndefVarError: mach not defined</code></pre><pre><code class="language-julia-repl">julia&gt; fit!(mach, rows=1:100);
ERROR: UndefVarError: mach not defined</code></pre><p>If an iterative model exposes it&#39;s iteration parameter as a hyper-parameter, and it implements the warm restart behaviour above, then it can be wrapped in a &quot;control strategy&quot;, like an early stopping critetion. See <a href="../controlling_iterative_models/#Controlling-Iterative-Models">Controlling Iterative Models</a> for details.</p><h2 id="Inspecting-machines"><a class="docs-heading-anchor" href="#Inspecting-machines">Inspecting machines</a><a id="Inspecting-machines-1"></a><a class="docs-heading-anchor-permalink" href="#Inspecting-machines" title="Permalink"></a></h2><p>There are two methods for inspecting the outcomes of training in MLJ. To obtain a named-tuple describing the learned parameters (in a user-friendly way where possible) use <code>fitted_params(mach)</code>. All other training-related outcomes are inspected with <code>report(mach)</code>.</p><pre><code class="language-">X, y = @load_iris
pca = (@load PCA verbosity=0)()
mach = machine(pca, X)
fit!(mach)</code></pre><pre><code class="language-julia-repl">julia&gt; fitted_params(mach)
ERROR: UndefVarError: mach not defined

julia&gt; report(mach)
ERROR: UndefVarError: mach not defined</code></pre><article class="docstring"><header><a class="docstring-binding" id="MLJModelInterface.fitted_params" href="#MLJModelInterface.fitted_params"><code>MLJModelInterface.fitted_params</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">fitted_params(model, fitresult) -&gt; human_readable_fitresult # named_tuple</code></pre><p>Models may overload <code>fitted_params</code>. The fallback returns <code>(fitresult=fitresult,)</code>.</p><p>Other training-related outcomes should be returned in the <code>report</code> part of the tuple returned by <code>fit</code>.</p></div></section><section><div><pre><code class="language-none">fitted_params(mach)</code></pre><p>Return the learned parameters for a machine <code>mach</code> that has been <code>fit!</code>, for example the coefficients in a linear model.</p><p>This is a named tuple and human-readable if possible.</p><p>If <code>mach</code> is a machine for a composite model, such as a model constructed using <code>@pipeline</code>, then the returned named tuple has the composite type&#39;s field names as keys. The corresponding value is the fitted parameters for the machine in the underlying learning network bound to that model. (If multiple machines share the same model, then the value is a vector.)</p><pre><code class="language-julia">using MLJ
@load LogisticClassifier pkg=MLJLinearModels
X, y = @load_crabs;
pipe = @pipeline Standardizer LogisticClassifier
mach = machine(pipe, X, y) |&gt; fit!

julia&gt; fitted_params(mach).logistic_classifier
(classes = CategoricalArrays.CategoricalValue{String,UInt32}[&quot;B&quot;, &quot;O&quot;],
 coefs = Pair{Symbol,Float64}[:FL =&gt; 3.7095037897680405, :RW =&gt; 0.1135739140854546, :CL =&gt; -1.6036892745322038, :CW =&gt; -4.415667573486482, :BD =&gt; 3.238476051092471],
 intercept = 0.0883301599726305,)</code></pre><p>Additional keys, <code>machines</code> and <code>fitted_params_given_machine</code>, give a list of <em>all</em> machines in the underlying network, and a dictionary of fitted parameters keyed on those machines.</p><p>```</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.report" href="#MLJBase.report"><code>MLJBase.report</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">report(mach)</code></pre><p>Return the report for a machine <code>mach</code> that has been <code>fit!</code>, for example the coefficients in a linear model.</p><p>This is a named tuple and human-readable if possible.</p><p>If <code>mach</code> is a machine for a composite model, such as a model constructed using <code>@pipeline</code>, then the returned named tuple has the composite type&#39;s field names as keys. The corresponding value is the report for the machine in the underlying learning network bound to that model. (If multiple machines share the same model, then the value is a vector.)</p><pre><code class="language-julia">using MLJ
@load LinearBinaryClassifier pkg=GLM
X, y = @load_crabs;
pipe = @pipeline Standardizer LinearBinaryClassifier
mach = machine(pipe, X, y) |&gt; fit!

julia&gt; report(mach).linear_binary_classifier
(deviance = 3.8893386087844543e-7,
 dof_residual = 195.0,
 stderror = [18954.83496713119, 6502.845740757159, 48484.240246060406, 34971.131004997274, 20654.82322484894, 2111.1294584763386],
 vcov = [3.592857686311793e8 9.122732393971942e6 … -8.454645589364915e7 5.38856837634321e6; 9.122732393971942e6 4.228700272808351e7 … -4.978433790526467e7 -8.442545425533723e6; … ; -8.454645589364915e7 -4.978433790526467e7 … 4.2662172244975924e8 2.1799125705781363e7; 5.38856837634321e6 -8.442545425533723e6 … 2.1799125705781363e7 4.456867590446599e6],)
</code></pre><p>Additional keys, <code>machines</code> and <code>report_given_machine</code>, give a list of <em>all</em> machines in the underlying network, and a dictionary of reports keyed on those machines.</p><p>```</p></div></section></article><h2 id="Constructing-machines"><a class="docs-heading-anchor" href="#Constructing-machines">Constructing machines</a><a id="Constructing-machines-1"></a><a class="docs-heading-anchor-permalink" href="#Constructing-machines" title="Permalink"></a></h2><p>A machine is constructed with the syntax <code>machine(model, args...)</code> where the possibilities for <code>args</code> (called <em>training arguments</em>) are summarized in table below. Here <code>X</code> and <code>y</code> represent inputs and target, respectively, and <code>Xout</code> the output of a <code>transform</code> call. Machines for supervised models may have additional training arguments, such as a vector of per-observation weights (in which case <code>supports_weights(model) == true</code>).</p><table><tr><th style="text-align: right"><code>model</code> supertype</th><th style="text-align: right"><code>machine</code> constructor calls</th><th style="text-align: right">operation calls (first compulsory)</th></tr><tr><td style="text-align: right"><code>Deterministic &lt;: Supervised</code></td><td style="text-align: right"><code>machine(model, X, y, extras...)</code></td><td style="text-align: right"><code>predict(mach, Xnew)</code>, <code>transform(mach, Xnew)</code>, <code>inverse_transform(mach, Xout)</code></td></tr><tr><td style="text-align: right"><code>Probabilistic &lt;: Supervised</code></td><td style="text-align: right"><code>machine(model, X, y, extras...)</code></td><td style="text-align: right"><code>predict(mach, Xnew)</code>, <code>predict_mean(mach, Xnew)</code>, <code>predict_median(mach, Xnew)</code>, <code>predict_mode(mach, Xnew)</code>, <code>transform(mach, Xnew)</code>, <code>inverse_transform(mach, Xout)</code></td></tr><tr><td style="text-align: right"><code>Unsupervised</code> (except <code>Static</code>)</td><td style="text-align: right"><code>machine(model, X)</code></td><td style="text-align: right"><code>transform(mach, Xnew)</code>, <code>inverse_transform(mach, Xout)</code>, <code>predict(mach, Xnew)</code></td></tr><tr><td style="text-align: right"><code>Static</code></td><td style="text-align: right"><code>machine(model)</code></td><td style="text-align: right"><code>transform(mach, Xnews...)</code>, <code>inverse_transform(mach, Xout)</code></td></tr></table><p>All operations on machines (<code>predict</code>, <code>transform</code>, etc) have exactly one argument (<code>Xnew</code> or <code>Xout</code> above) after <code>mach</code>, the machine instance. An exception is a machine bound to a <code>Static</code> model, which can have any number of arguments after <code>mach</code>. For more on <code>Static</code> transformers (which have no <em>training</em> arguments) see <a href="../transformers/#Static-transformers">Static transformers</a>.</p><p>A machine is reconstructed from a file using the syntax <code>machine(&quot;my_machine.jlso&quot;)</code>, or <code>machine(&quot;my_machine.jlso&quot;, args...)</code> if retraining using new data. See <a href="#Saving-machines">Saving machines</a> below.</p><h2 id="Lowering-memory-demands"><a class="docs-heading-anchor" href="#Lowering-memory-demands">Lowering memory demands</a><a id="Lowering-memory-demands-1"></a><a class="docs-heading-anchor-permalink" href="#Lowering-memory-demands" title="Permalink"></a></h2><p>For large data sets you may be able to save memory by suppressing data caching that some models perform to increase speed. To do this, specify <code>cache=false</code>, as in</p><pre><code class="language-julia">machine(model, X, y, cache=false)</code></pre><h3 id="Constructing-machines-in-learning-networks"><a class="docs-heading-anchor" href="#Constructing-machines-in-learning-networks">Constructing machines in learning networks</a><a id="Constructing-machines-in-learning-networks-1"></a><a class="docs-heading-anchor-permalink" href="#Constructing-machines-in-learning-networks" title="Permalink"></a></h3><p>Instead of data <code>X</code>, <code>y</code>, etc,  the <code>machine</code> constructor is provided <code>Node</code> or <code>Source</code> objects (&quot;dynamic data&quot;) when building a learning network. See <a href="../composing_models/">Composing Models</a> for more on this advanced feature. One also uses <code>machine</code> to wrap a machine around a whole learning network; see <a href="../composing_models/#Learning-network-machines">Learning network machines</a>.</p><h2 id="Saving-machines"><a class="docs-heading-anchor" href="#Saving-machines">Saving machines</a><a id="Saving-machines-1"></a><a class="docs-heading-anchor-permalink" href="#Saving-machines" title="Permalink"></a></h2><p>To save a machine to file, use the <code>MLJ.save</code> command:</p><pre><code class="language-julia">tree = (@load DecisionTreeClassifier pkg=DecisionTree verbosity=0)()
mach = fit!(machine(tree, X, y))
MLJ.save(&quot;my_machine.jlso&quot;, mach)</code></pre><p>To de-serialize, one uses the <code>machine</code> constructor:</p><pre><code class="language-julia">mach2 = machine(&quot;my_machine.jlso&quot;)
predict(mach2, Xnew);</code></pre><p>The machine <code>mach2</code> cannot be retrained; however, by providing data to the constructor one can enable retraining using the saved model hyperparameters (which overwrites the saved learned parameters):</p><pre><code class="language-julia">mach3 = machine(&quot;my_machine.jlso&quot;, Xnew, ynew)
fit!(mach3)</code></pre><h2 id="Internals"><a class="docs-heading-anchor" href="#Internals">Internals</a><a id="Internals-1"></a><a class="docs-heading-anchor-permalink" href="#Internals" title="Permalink"></a></h2><p>For a supervised machine the <code>predict</code> method calls a lower-level <code>MLJBase.predict</code> method, dispatched on the underlying model and the <code>fitresult</code> (see below). To see <code>predict</code> in action, as well as its unsupervised cousins <code>transform</code> and <code>inverse_transform</code>, see <a href="../">Getting Started</a>.</p><p>The fields of a <code>Machine</code> instance (which should not generally be accessed by the user) are:</p><ul><li><p><code>model</code> - the struct containing the hyperparameters to be used in calls to <code>fit!</code></p></li><li><p><code>fitresult</code> - the learned parameters in a raw form, initially undefined</p></li><li><p><code>args</code> - a tuple of the data, each element wrapped in a source node; see <a href="../composing_models/#Learning-Networks">Learning Networks</a> (in the supervised learning example above, <code>args = (source(X), source(y))</code>)</p></li><li><p><code>report</code> - outputs of training not encoded in <code>fitresult</code> (eg, feature rankings)</p></li><li><p><code>old_model</code> - a deep copy of the model used in the last call to <code>fit!</code></p></li><li><p><code>old_rows</code> -  a copy of the row indices used in last call to <code>fit!</code></p></li><li><p><code>cache</code></p></li></ul><p>The interested reader can learn more on machine internals by examining the simplified code excerpt in <a href="../internals/">Internals</a>.</p><h2 id="API-Reference"><a class="docs-heading-anchor" href="#API-Reference">API Reference</a><a id="API-Reference-1"></a><a class="docs-heading-anchor-permalink" href="#API-Reference" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="MLJBase.machine" href="#MLJBase.machine"><code>MLJBase.machine</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">machine(model, args...; cache=true)</code></pre><p>Construct a <code>Machine</code> object binding a <code>model</code>, storing hyper-parameters of some machine learning algorithm, to some data, <code>args</code>. Calling <code>fit!</code> on a <code>Machine</code> object stores in the machine object the outcomes of applying the algorithm. This in turn enables generalization to new data using operations such as <code>predict</code> or <code>transform</code>:</p><pre><code class="language-julia">using MLJModels
X, y = make_regression()

PCA = @load PCA pkg=MultivariateStats
model = PCA()
mach = machine(model, X)
fit!(mach, rows=1:50)
transform(mach, selectrows(X, 51:100)) # or transform(mach, rows=51:100)

DecisionTreeRegressor = @load DecisionTreeRegressor pkg=DecisionTree
model = DecisionTreeRegressor()
mach = machine(model, X, y)
fit!(mach, rows=1:50)
predict(mach, selectrows(X, 51:100)) # or predict(mach, rows=51:100)</code></pre><p>Specify <code>cache=false</code> to prioritize memory management over speed, and to guarantee data anonymity when serializing composite models.</p><p>When building a learning network, <code>Node</code> objects can be substituted for the concrete data.</p><p><strong>Learning network machines</strong></p><pre><code class="language-none">machine(Xs; oper1=node1, oper2=node2, ...)
machine(Xs, ys; oper1=node1, oper2=node2, ...)
machine(Xs, ys, extras...; oper1=node1, oper2=node2, ...)</code></pre><p>Construct a special machine called a <em>learning network machine</em>, that wraps a learning network, usually in preparation to export the network as a stand-alone composite model type. The keyword arguments declare what nodes are called when operations, such as <code>predict</code> and <code>transform</code>, are called on the machine. An advanced option allows one to additionally pass the output of any node to the machine&#39;s report; see below.</p><p>In addition to the operations named in the constructor, the methods <code>fit!</code>, <code>report</code>, and <code>fitted_params</code> can be applied as usual to the machine constructed.</p><pre><code class="language-none">machine(Probabilistic(), args...; kwargs...)
machine(Deterministic(), args...; kwargs...)
machine(Unsupervised(), args...; kwargs...)
machine(Static(), args...; kwargs...)</code></pre><p>Same as above, but specifying explicitly the kind of model the learning network is to meant to represent.</p><p>Learning network machines are not to be confused with an ordinary machine that happens to be bound to a stand-alone composite model (i.e., an <em>exported</em> learning network).</p><p><strong>Examples of learning network machines</strong></p><p>Supposing a supervised learning network&#39;s final predictions are obtained by calling a node <code>yhat</code>, then the code</p><pre><code class="language-julia">mach = machine(Deterministic(), Xs, ys; predict=yhat)
fit!(mach; rows=train)
predictions = predict(mach, Xnew) # `Xnew` concrete data</code></pre><p>is  equivalent to</p><pre><code class="language-julia">fit!(yhat, rows=train)
predictions = yhat(Xnew)</code></pre><p>Here <code>Xs</code> and <code>ys</code> are the source nodes receiving, respectively, the input and target data.</p><p>In a unsupervised learning network for clustering, with single source node <code>Xs</code> for inputs, and in which the node <code>Xout</code> delivers the output of dimension reduction, and <code>yhat</code> the class labels, one can write</p><pre><code class="language-julia">mach = machine(Unsupervised(), Xs; transform=Xout, predict=yhat)
fit!(mach)
transformed = transform(mach, Xnew) # `Xnew` concrete data
predictions = predict(mach, Xnew)</code></pre><p>which is equivalent to</p><pre><code class="language-julia">fit!(Xout)
fit!(yhat)
transformed = Xout(Xnew)
predictions = yhat(Xnew)</code></pre><p><strong>Including a node&#39;s output in the report</strong></p><p>The return value of a node called with no arguments can be included in a learning network machine&#39;s report, and so in the report of any composite model type constructed by exporting a learning network. This is useful for exposing byproducts of network training that are not readily deduced from the <code>report</code>s and <code>fitted_params</code> of the component machines (which are automatically exposed).</p><p>The following example shows how to expose <code>err1()</code> and <code>err2()</code>, where <code>err1</code> are <code>err2</code> are nodes in the network delivering training errors.</p><pre><code class="language-julia">X, y = make_moons()
Xs = source(X)
ys = source(y)

model = ConstantClassifier()
mach = machine(model, Xs, ys)
yhat = predict(mach, Xs)
err1 = @node auc(yhat, ys)
err2 = @node accuracy(yhat, ys)

network_mach = machine(Probabilistic(),
                       Xs,
                       ys,
                       predict=yhat,
                       report=(auc=err1, accuracy=err2))

fit!(network_mach)
r = report(network_mach)
@assert r.auc == auc(yhat(), ys())
@assert r.accuracy == accuracy(yhat(), ys())</code></pre></div></section></article><article class="docstring"><header><a class="docstring-binding" id="StatsBase.fit!" href="#StatsBase.fit!"><code>StatsBase.fit!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">fit!(mach::Machine, rows=nothing, verbosity=1, force=false)</code></pre><p>Fit the machine <code>mach</code>. In the case that <code>mach</code> has <code>Node</code> arguments, first train all other machines on which <code>mach</code> depends.</p><p>To attempt to fit a machine without touching any other machine, use <code>fit_only!</code>. For more on the internal logic of fitting see <a href="#MLJBase.fit_only!"><code>fit_only!</code></a></p></div></section><section><div><pre><code class="language-none">fit!(N::Node;
     rows=nothing,
     verbosity=1,
     force=false,
     acceleration=CPU1())</code></pre><p>Train all machines required to call the node <code>N</code>, in an appropriate order.  These machines are those returned by <code>machines(N)</code>.</p></div></section><section><div><pre><code class="language-none">fit!(mach::Machine{&lt;:Surrogate};
     rows=nothing,
     acceleration=CPU1(),
     verbosity=1,
     force=false))</code></pre><p>Train the complete learning network wrapped by the machine <code>mach</code>.</p><p>More precisely, if <code>s</code> is the learning network signature used to construct <code>mach</code>, then call <code>fit!(N)</code>, where <code>N</code> is a greatest lower bound of the nodes appearing in the signature (values in the signature that are not <code>AbstractNode</code> are ignored). For example, if <code>s = (predict=yhat, transform=W)</code>, then call <code>fit!(glb(yhat, W))</code>.</p><p>See also <a href="#MLJBase.machine"><code>machine</code></a></p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.fit_only!" href="#MLJBase.fit_only!"><code>MLJBase.fit_only!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">MLJBase.fit_only!(mach::Machine; rows=nothing, verbosity=1, force=false)</code></pre><p>Without mutating any other machine on which it may depend, perform one of the following actions to the machine <code>mach</code>, using the data and model bound to it, and restricting the data to <code>rows</code> if specified:</p><ul><li><p><em>Ab initio training.</em> Ignoring any previous learned parameters and cache, compute and store new learned parameters. Increment <code>mach.state</code>.</p></li><li><p><em>Training update.</em> Making use of previous learned parameters and/or  cache, replace or mutate existing learned parameters. The effect is  the same (or nearly the same) as in ab initio training, but may be  faster or use less memory, assuming the model supports an update  option (implements <code>MLJBase.update</code>). Increment <code>mach.state</code>.</p></li><li><p><em>No-operation.</em> Leave existing learned parameters untouched. Do not  increment <code>mach.state</code>.</p></li></ul><p><strong>Training action logic</strong></p><p>For the action to be a no-operation, either <code>mach.frozen == true</code> or or none of the following apply:</p><ul><li><p>(i) <code>mach</code> has never been trained (<code>mach.state == 0</code>).</p></li><li><p>(ii) <code>force == true</code>.</p></li><li><p>(iii) The <code>state</code> of some other machine on which <code>mach</code> depends has changed since the last time <code>mach</code> was trained (ie, the last time <code>mach.state</code> was last incremented).</p></li><li><p>(iv) The specified <code>rows</code> have changed since the last retraining and <code>mach.model</code> does not have <code>Static</code> type.</p></li><li><p>(v) <code>mach.model</code> has changed since the last retraining.</p></li></ul><p>In any of the cases (i) - (iv), <code>mach</code> is trained ab initio. If only (v) fails, then a training update is applied.</p><p>To freeze or unfreeze <code>mach</code>, use <code>freeze!(mach)</code> or <code>thaw!(mach)</code>.</p><p><strong>Implementation detail</strong></p><p>The data to which a machine is bound is stored in <code>mach.args</code>. Each element of <code>args</code> is either a <code>Node</code> object, or, in the case that concrete data was bound to the machine, it is concrete data wrapped in a <code>Source</code> node. In all cases, to obtain concrete data for actual training, each argument <code>N</code> is called, as in <code>N()</code> or <code>N(rows=rows)</code>, and either <code>MLJBase.fit</code> (ab initio training) or <code>MLJBase.update</code> (training update) is dispatched on <code>mach.model</code> and this data. See the &quot;Adding models for general use&quot; section of the MLJ documentation for more on these lower-level training methods.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJModelInterface.save" href="#MLJModelInterface.save"><code>MLJModelInterface.save</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">MLJ.save(filename, mach::Machine; kwargs...)
MLJ.save(io, mach::Machine; kwargs...)

MLJBase.save(filename, mach::Machine; kwargs...)
MLJBase.save(io, mach::Machine; kwargs...)</code></pre><p>Serialize the machine <code>mach</code> to a file with path <code>filename</code>, or to an input/output stream <code>io</code> (at least <code>IOBuffer</code> instances are supported).</p><p>The format is JLSO (a wrapper for julia native or BSON serialization). For some model types, a custom serialization will be additionally performed.</p><p><strong>Keyword arguments</strong></p><p>These keyword arguments are passed to the JLSO serializer:</p><table><tr><th style="text-align: right">keyword</th><th style="text-align: right">values</th><th style="text-align: right">default</th></tr><tr><td style="text-align: right"><code>format</code></td><td style="text-align: right"><code>:julia_serialize</code>, <code>:BSON</code></td><td style="text-align: right"><code>:julia_serialize</code></td></tr><tr><td style="text-align: right"><code>compression</code></td><td style="text-align: right"><code>:gzip</code>, <code>:none</code></td><td style="text-align: right"><code>:none</code></td></tr></table><p>See <a href="https://github.com/invenia/JLSO.jl">https://github.com/invenia/JLSO.jl</a> for details.</p><p>Any additional keyword arguments are passed to model-specific serializers.</p><p>Machines are de-serialized using the <code>machine</code> constructor as shown in the example below. Data (or nodes) may be optionally passed to the constructor for retraining on new data using the saved model.</p><p><strong>Example</strong></p><pre><code class="language-none">using MLJ
tree = @load DecisionTreeClassifier
X, y = @load_iris
mach = fit!(machine(tree, X, y))

MLJ.save(&quot;tree.jlso&quot;, mach, compression=:none)
mach_predict_only = machine(&quot;tree.jlso&quot;)
predict(mach_predict_only, X)

mach2 = machine(&quot;tree.jlso&quot;, selectrows(X, 1:100), y[1:100])
predict(mach2, X) # same as above

fit!(mach2) # saved learned parameters are over-written
predict(mach2, X) # not same as above

# using a buffer:
io = IOBuffer()
MLJ.save(io, mach)
seekstart(io)
predict_only_mach = machine(io)
predict(predict_only_mach, X)</code></pre><div class="admonition is-warning"><header class="admonition-header">Only load files from trusted sources</header><div class="admonition-body"><p>Maliciously constructed JLSO files, like pickles, and most other general purpose serialization formats, can allow for arbitrary code execution during loading. This means it is possible for someone to use a JLSO file that looks like a serialized MLJ machine as a <a href="https://en.wikipedia.org/wiki/Trojan_horse_(computing)">Trojan horse</a>.</p></div></div></div></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../loading_model_code/">« Loading Model Code</a><a class="docs-footer-nextpage" href="../evaluating_model_performance/">Evaluating Model Performance »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Monday 3 January 2022 21:51">Monday 3 January 2022</span>. Using Julia version 1.7.1.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
