<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Machines · MLJ</title><meta name="title" content="Machines · MLJ"/><meta property="og:title" content="Machines · MLJ"/><meta property="twitter:title" content="Machines · MLJ"/><meta name="description" content="Documentation for MLJ."/><meta property="og:description" content="Documentation for MLJ."/><meta property="twitter:description" content="Documentation for MLJ."/><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../search_index.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.svg" alt="MLJ logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../">MLJ</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../model_browser/">Model Browser</a></li><li><a class="tocitem" href="../about_mlj/">About MLJ</a></li><li><a class="tocitem" href="../learning_mlj/">Learning MLJ</a></li><li><a class="tocitem" href="../getting_started/">Getting Started</a></li><li><a class="tocitem" href="../common_mlj_workflows/">Common MLJ Workflows</a></li><li><a class="tocitem" href="../working_with_categorical_data/">Working with Categorical Data</a></li><li><a class="tocitem" href="../model_search/">Model Search</a></li><li><a class="tocitem" href="../loading_model_code/">Loading Model Code</a></li><li class="is-active"><a class="tocitem" href>Machines</a><ul class="internal"><li class="toplevel"><a class="tocitem" href="#Warm-restarts"><span>Warm restarts</span></a></li><li><a class="tocitem" href="#Inspecting-machines"><span>Inspecting machines</span></a></li><li><a class="tocitem" href="#Constructing-machines"><span>Constructing machines</span></a></li><li><a class="tocitem" href="#Lowering-memory-demands"><span>Lowering memory demands</span></a></li><li><a class="tocitem" href="#Saving-machines"><span>Saving machines</span></a></li><li><a class="tocitem" href="#Internals"><span>Internals</span></a></li><li><a class="tocitem" href="#API-Reference"><span>API Reference</span></a></li></ul></li><li><a class="tocitem" href="../evaluating_model_performance/">Evaluating Model Performance</a></li><li><a class="tocitem" href="../performance_measures/">Performance Measures</a></li><li><a class="tocitem" href="../weights/">Weights</a></li><li><a class="tocitem" href="../tuning_models/">Tuning Models</a></li><li><a class="tocitem" href="../learning_curves/">Learning Curves</a></li><li><a class="tocitem" href="../preparing_data/">Preparing Data</a></li><li><a class="tocitem" href="../transformers/">Transformers and Other Unsupervised models</a></li><li><a class="tocitem" href="../more_on_probabilistic_predictors/">More on Probabilistic Predictors</a></li><li><a class="tocitem" href="../composing_models/">Composing Models</a></li><li><a class="tocitem" href="../linear_pipelines/">Linear Pipelines</a></li><li><a class="tocitem" href="../target_transformations/">Target Transformations</a></li><li><a class="tocitem" href="../homogeneous_ensembles/">Homogeneous Ensembles</a></li><li><a class="tocitem" href="../correcting_class_imbalance/">Correcting Class Imbalance</a></li><li><a class="tocitem" href="../model_stacking/">Model Stacking</a></li><li><a class="tocitem" href="../learning_networks/">Learning Networks</a></li><li><a class="tocitem" href="../controlling_iterative_models/">Controlling Iterative Models</a></li><li><a class="tocitem" href="../generating_synthetic_data/">Generating Synthetic Data</a></li><li><a class="tocitem" href="../logging_workflows/">Logging Workflows</a></li><li><a class="tocitem" href="../openml_integration/">OpenML Integration</a></li><li><a class="tocitem" href="../acceleration_and_parallelism/">Acceleration and Parallelism</a></li><li><a class="tocitem" href="../simple_user_defined_models/">Simple User Defined Models</a></li><li><a class="tocitem" href="../quick_start_guide_to_adding_models/">Quick-Start Guide to Adding Models</a></li><li><a class="tocitem" href="../adding_models_for_general_use/">Adding Models for General Use</a></li><li><a class="tocitem" href="../modifying_behavior/">Modifying Behavior</a></li><li><a class="tocitem" href="../internals/">Internals</a></li><li><a class="tocitem" href="../list_of_supported_models/">List of Supported Models</a></li><li><a class="tocitem" href="../third_party_packages/">Third Party Packages</a></li><li><a class="tocitem" href="../glossary/">Glossary</a></li><li><a class="tocitem" href="../mlj_cheatsheet/">MLJ Cheatsheet</a></li><li><a class="tocitem" href="../frequently_asked_questions/">FAQ</a></li><li><a class="tocitem" href="../api/">Index of Methods</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Machines</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Machines</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/alan-turing-institute/MLJ.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/alan-turing-institute/MLJ.jl/blob/dev/docs/src/machines.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Machines"><a class="docs-heading-anchor" href="#Machines">Machines</a><a id="Machines-1"></a><a class="docs-heading-anchor-permalink" href="#Machines" title="Permalink"></a></h1><p>Recall from <a href="../getting_started/#Getting-Started">Getting Started</a> that a machine binds a model (i.e., a choice of algorithm + hyperparameters) to data (see more at <a href="#Constructing-machines">Constructing machines</a> below). A machine is also the object storing <em>learned</em> parameters.  Under the hood, calling <code>fit!</code> on a machine calls either <code>MLJBase.fit</code> or <code>MLJBase.update</code>, depending on the machine&#39;s internal state (as recorded in private fields <code>old_model</code> and <code>old_rows</code>). These lower-level <code>fit</code> and <code>update</code> methods, which are not ordinarily called directly by the user, dispatch on the model and a view of the data defined by the optional <code>rows</code> keyword argument of <code>fit!</code> (all rows by default).</p><h1 id="Warm-restarts"><a class="docs-heading-anchor" href="#Warm-restarts">Warm restarts</a><a id="Warm-restarts-1"></a><a class="docs-heading-anchor-permalink" href="#Warm-restarts" title="Permalink"></a></h1><p>If a model <code>update</code> method has been implemented for the model, calls to <code>fit!</code> will avoid redundant calculations for certain kinds of model mutations. The main use-case is increasing an iteration parameter, such as the number of epochs in a neural network. To test if <code>SomeIterativeModel</code> supports this feature, check <code>iteration_parameter(SomeIterativeModel)</code> is different from <code>nothing</code>.</p><pre><code class="language-julia hljs">tree = (@load DecisionTreeClassifier pkg=DecisionTree verbosity=0)()
forest = EnsembleModel(model=tree, n=10);
X, y = @load_iris;
mach = machine(forest, X, y)
fit!(mach, verbosity=2);</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">trained Machine; caches model-specific representations of data
  model: ProbabilisticEnsembleModel(model = DecisionTreeClassifier(max_depth = -1, …), …)
  args: 
    1:	Source @749 ⏎ Table{AbstractVector{Continuous}}
    2:	Source @777 ⏎ AbstractVector{Multiclass{3}}
</code></pre><p>Generally, changing a hyperparameter triggers retraining on calls to subsequent <code>fit!</code>:</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; forest.bagging_fraction=0.5</code><code class="nohighlight hljs ansi" style="display:block;">0.5</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; fit!(mach, verbosity=2);</code><code class="nohighlight hljs ansi" style="display:block;">[ Info: Updating machine(ProbabilisticEnsembleModel(model = DecisionTreeClassifier(max_depth = -1, …), …), …).
[ Info: Truncating existing ensemble.</code></pre><p>However, for this iterative model, increasing the iteration parameter only adds models to the existing ensemble:</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; forest.n=15</code><code class="nohighlight hljs ansi" style="display:block;">15</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; fit!(mach, verbosity=2);</code><code class="nohighlight hljs ansi" style="display:block;">[ Info: Updating machine(ProbabilisticEnsembleModel(model = DecisionTreeClassifier(max_depth = -1, …), …), …).
[ Info: Building on existing ensemble of length 10
[ Info: One hash per new atom trained: 
#####</code></pre><p>Call <code>fit!</code> again without making a change and no retraining occurs:</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; fit!(mach);</code><code class="nohighlight hljs ansi" style="display:block;">[ Info: Not retraining machine(ProbabilisticEnsembleModel(model = DecisionTreeClassifier(max_depth = -1, …), …), …). Use `force=true` to force.</code></pre><p>However, retraining can be forced:</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; fit!(mach, force=true);</code><code class="nohighlight hljs ansi" style="display:block;">[ Info: Training machine(ProbabilisticEnsembleModel(model = DecisionTreeClassifier(max_depth = -1, …), …), …).</code></pre><p>And is re-triggered if the view of the data changes:</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; fit!(mach, rows=1:100);</code><code class="nohighlight hljs ansi" style="display:block;">[ Info: Training machine(ProbabilisticEnsembleModel(model = DecisionTreeClassifier(max_depth = -1, …), …), …).</code></pre><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; fit!(mach, rows=1:100);</code><code class="nohighlight hljs ansi" style="display:block;">[ Info: Not retraining machine(ProbabilisticEnsembleModel(model = DecisionTreeClassifier(max_depth = -1, …), …), …). Use `force=true` to force.</code></pre><p>If an iterative model exposes its iteration parameter as a hyperparameter, and it implements the warm restart behavior above, then it can be wrapped in a &quot;control strategy&quot;, like an early stopping criterion. See <a href="../controlling_iterative_models/#Controlling-Iterative-Models">Controlling Iterative Models</a> for details.</p><h2 id="Inspecting-machines"><a class="docs-heading-anchor" href="#Inspecting-machines">Inspecting machines</a><a id="Inspecting-machines-1"></a><a class="docs-heading-anchor-permalink" href="#Inspecting-machines" title="Permalink"></a></h2><p>There are two principal methods for inspecting the outcomes of training in MLJ. To obtain a named-tuple describing the learned parameters (in a user-friendly way where possible) use <code>fitted_params(mach)</code>. All other training-related outcomes are inspected with <code>report(mach)</code>.</p><pre><code class="language-julia hljs">X, y = @load_iris
pca = (@load PCA verbosity=0)()
mach = machine(pca, X)
fit!(mach)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">trained Machine; caches model-specific representations of data
  model: PCA(maxoutdim = 0, …)
  args: 
    1:	Source @167 ⏎ Table{AbstractVector{Continuous}}
</code></pre><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; fitted_params(mach)</code><code class="nohighlight hljs ansi" style="display:block;">(projection = [-0.36158967738145 0.6565398832858296 0.5809972798276162; 0.08226888989221415 0.7297123713264985 -0.5964180879380994; -0.8565721052905275 -0.175767403428653 -0.07252407548695988; -0.3588439262482158 -0.07470647013503479 -0.5490609107266099],)</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; report(mach)</code><code class="nohighlight hljs ansi" style="display:block;">(indim = 4,
 outdim = 3,
 tprincipalvar = 4.545608248041779,
 tresidualvar = 0.023683027126000233,
 tvar = 4.569291275167779,
 mean = [5.843333333333334, 3.0540000000000003, 3.758666666666667, 1.198666666666667],
 principalvars = [4.224840768320109, 0.24224357162751498, 0.0785239080941545],
 loadings = [-0.7432265175592332 0.3231374133069471 0.16280774164399525; 0.16909891062391016 0.3591516283038468 -0.16712897864451629; -1.7606340630732822 -0.0865096325959021 -0.02032278180089568; -0.73758278605778 -0.03676921407410996 -0.15385849470227703],)</code></pre><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="MLJModelInterface.fitted_params-Tuple{Machine}" href="#MLJModelInterface.fitted_params-Tuple{Machine}"><code>MLJModelInterface.fitted_params</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">fitted_params(mach)</code></pre><p>Return the learned parameters for a machine <code>mach</code> that has been <code>fit!</code>, for example the coefficients in a linear model.</p><p>This is a named tuple and human-readable if possible.</p><p>If <code>mach</code> is a machine for a composite model, such as a model constructed using the pipeline syntax <code>model1 |&gt; model2 |&gt; ...</code>, then the returned named tuple has the composite type&#39;s field names as keys. The corresponding value is the fitted parameters for the machine in the underlying learning network bound to that model. (If multiple machines share the same model, then the value is a vector.)</p><pre><code class="language-julia hljs">using MLJ
@load LogisticClassifier pkg=MLJLinearModels
X, y = @load_crabs;
pipe = Standardizer() |&gt; LogisticClassifier()
mach = machine(pipe, X, y) |&gt; fit!

julia&gt; fitted_params(mach).logistic_classifier
(classes = CategoricalArrays.CategoricalValue{String,UInt32}[&quot;B&quot;, &quot;O&quot;],
 coefs = Pair{Symbol,Float64}[:FL =&gt; 3.7095037897680405, :RW =&gt; 0.1135739140854546, :CL =&gt; -1.6036892745322038, :CW =&gt; -4.415667573486482, :BD =&gt; 3.238476051092471],
 intercept = 0.0883301599726305,)</code></pre><p>Additional keys, <code>machines</code> and <code>fitted_params_given_machine</code>, give a list of <em>all</em> machines in the underlying network, and a dictionary of fitted parameters keyed on those machines.</p><p>See also <a href="#MLJBase.report-Tuple{Machine}"><code>report</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJBase.jl/blob/v1.1.1/src/machines.jl#L784-L817">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="MLJBase.report-Tuple{Machine}" href="#MLJBase.report-Tuple{Machine}"><code>MLJBase.report</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">report(mach)</code></pre><p>Return the report for a machine <code>mach</code> that has been <code>fit!</code>, for example the coefficients in a linear model.</p><p>This is a named tuple and human-readable if possible.</p><p>If <code>mach</code> is a machine for a composite model, such as a model constructed using the pipeline syntax <code>model1 |&gt; model2 |&gt; ...</code>, then the returned named tuple has the composite type&#39;s field names as keys. The corresponding value is the report for the machine in the underlying learning network bound to that model. (If multiple machines share the same model, then the value is a vector.)</p><pre><code class="language-julia hljs">using MLJ
@load LinearBinaryClassifier pkg=GLM
X, y = @load_crabs;
pipe = Standardizer() |&gt; LinearBinaryClassifier()
mach = machine(pipe, X, y) |&gt; fit!

julia&gt; report(mach).linear_binary_classifier
(deviance = 3.8893386087844543e-7,
 dof_residual = 195.0,
 stderror = [18954.83496713119, 6502.845740757159, 48484.240246060406, 34971.131004997274, 20654.82322484894, 2111.1294584763386],
 vcov = [3.592857686311793e8 9.122732393971942e6 … -8.454645589364915e7 5.38856837634321e6; 9.122732393971942e6 4.228700272808351e7 … -4.978433790526467e7 -8.442545425533723e6; … ; -8.454645589364915e7 -4.978433790526467e7 … 4.2662172244975924e8 2.1799125705781363e7; 5.38856837634321e6 -8.442545425533723e6 … 2.1799125705781363e7 4.456867590446599e6],)
</code></pre><p>Additional keys, <code>machines</code> and <code>report_given_machine</code>, give a list of <em>all</em> machines in the underlying network, and a dictionary of reports keyed on those machines.</p><p>See also <a href="#MLJModelInterface.fitted_params-Tuple{Machine}"><code>fitted_params</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJBase.jl/blob/v1.1.1/src/machines.jl#L826-L861">source</a></section></article><h3 id="Training-losses-and-feature-importances"><a class="docs-heading-anchor" href="#Training-losses-and-feature-importances">Training losses and feature importances</a><a id="Training-losses-and-feature-importances-1"></a><a class="docs-heading-anchor-permalink" href="#Training-losses-and-feature-importances" title="Permalink"></a></h3><p>Training losses and feature importances, if reported by a model, will be available in the machine&#39;s report (see above). However, there are also direct access methods where supported:</p><pre><code class="language-julia hljs">training_losses(mach::Machine) -&gt; vector_of_losses</code></pre><p>Here <code>vector_of_losses</code> will be in historical order (most recent loss last). This kind of access is supported for <code>model = mach.model</code> if <code>supports_training_losses(model) == true</code>.</p><pre><code class="language-julia hljs">feature_importances(mach::Machine) -&gt; vector_of_pairs</code></pre><p>Here a <code>vector_of_pairs</code> is a vector of elements of the form <code>feature =&gt; importance_value</code>, where <code>feature</code> is a symbol. For example, <code>vector_of_pairs = [:gender =&gt; 0.23, :height =&gt; 0.7, :weight =&gt; 0.1]</code>. If a model does not support feature importances for some model hyperparameters, every <code>importance_value</code> will be zero. This kind of access is supported for <code>model = mach.model</code> if <code>reports_feature_importances(model) == true</code>.</p><p>If a model can report multiple types of feature importances, then there will be a model hyper-parameter controlling the active type.</p><h2 id="Constructing-machines"><a class="docs-heading-anchor" href="#Constructing-machines">Constructing machines</a><a id="Constructing-machines-1"></a><a class="docs-heading-anchor-permalink" href="#Constructing-machines" title="Permalink"></a></h2><p>A machine is constructed with the syntax <code>machine(model, args...)</code> where the possibilities for <code>args</code> (called <em>training arguments</em>) are summarized in the table below. Here <code>X</code> and <code>y</code> represent inputs and target, respectively, and <code>Xout</code> is the output of a <code>transform</code> call. Machines for supervised models may have additional training arguments, such as a vector of per-observation weights (in which case <code>supports_weights(model) == true</code>).</p><table><tr><th style="text-align: right"><code>model</code> supertype</th><th style="text-align: right"><code>machine</code> constructor calls</th><th style="text-align: right">operation calls (first compulsory)</th></tr><tr><td style="text-align: right"><code>Deterministic &lt;: Supervised</code></td><td style="text-align: right"><code>machine(model, X, y, extras...)</code></td><td style="text-align: right"><code>predict(mach, Xnew)</code>, <code>transform(mach, Xnew)</code>, <code>inverse_transform(mach, Xout)</code></td></tr><tr><td style="text-align: right"><code>Probabilistic &lt;: Supervised</code></td><td style="text-align: right"><code>machine(model, X, y, extras...)</code></td><td style="text-align: right"><code>predict(mach, Xnew)</code>, <code>predict_mean(mach, Xnew)</code>, <code>predict_median(mach, Xnew)</code>, <code>predict_mode(mach, Xnew)</code>, <code>transform(mach, Xnew)</code>, <code>inverse_transform(mach, Xout)</code></td></tr><tr><td style="text-align: right"><code>Unsupervised</code> (except <code>Static</code>)</td><td style="text-align: right"><code>machine(model, X)</code></td><td style="text-align: right"><code>transform(mach, Xnew)</code>, <code>inverse_transform(mach, Xout)</code>, <code>predict(mach, Xnew)</code></td></tr><tr><td style="text-align: right"><code>Static</code></td><td style="text-align: right"><code>machine(model)</code></td><td style="text-align: right"><code>transform(mach, Xnews...)</code>, <code>inverse_transform(mach, Xout)</code></td></tr></table><p>All operations on machines (<code>predict</code>, <code>transform</code>, etc) have exactly one argument (<code>Xnew</code> or <code>Xout</code> above) after <code>mach</code>, the machine instance. An exception is a machine bound to a <code>Static</code> model, which can have any number of arguments after <code>mach</code>. For more on <code>Static</code> transformers (which have no <em>training</em> arguments) see <a href="../transformers/#Static-transformers">Static transformers</a>.</p><p>A machine is reconstructed from a file using the syntax <code>machine(&quot;my_machine.jlso&quot;)</code>, or <code>machine(&quot;my_machine.jlso&quot;, args...)</code> if retraining using new data. See <a href="#Saving-machines">Saving machines</a> below.</p><h2 id="Lowering-memory-demands"><a class="docs-heading-anchor" href="#Lowering-memory-demands">Lowering memory demands</a><a id="Lowering-memory-demands-1"></a><a class="docs-heading-anchor-permalink" href="#Lowering-memory-demands" title="Permalink"></a></h2><p>For large data sets, you may be able to save memory by suppressing data caching that some models perform to increase speed. To do this, specify <code>cache=false</code>, as in</p><pre><code class="language-julia hljs">machine(model, X, y, cache=false)</code></pre><h3 id="Constructing-machines-in-learning-networks"><a class="docs-heading-anchor" href="#Constructing-machines-in-learning-networks">Constructing machines in learning networks</a><a id="Constructing-machines-in-learning-networks-1"></a><a class="docs-heading-anchor-permalink" href="#Constructing-machines-in-learning-networks" title="Permalink"></a></h3><p>Instead of data <code>X</code>, <code>y</code>, etc, the <code>machine</code> constructor is provided <code>Node</code> or <code>Source</code> objects (&quot;dynamic data&quot;) when building a learning network. See <a href="../learning_networks/#Learning-Networks">Learning Networks</a> for more on this advanced feature.</p><h2 id="Saving-machines"><a class="docs-heading-anchor" href="#Saving-machines">Saving machines</a><a id="Saving-machines-1"></a><a class="docs-heading-anchor-permalink" href="#Saving-machines" title="Permalink"></a></h2><p>Users can save and restore MLJ machines using any external serialization package by suitably preparing their <code>Machine</code> object, and applying a post-processing step to the deserialized object. This is explained under <a href="#Using-an-arbitrary-serializer">Using an arbitrary serializer</a> below.</p><p>However, if a user is happy to use Julia&#39;s standard library Serialization module, there is a simplified workflow described first.</p><p>The usual serialization provisos apply. For example, when deserializing you need to have all code on which the serialization object depended loaded at the time of deserialization also. If a hyper-parameter happens to be a user-defined function, then that function must be defined at deserialization. And you should only deserialize objects from trusted sources.</p><h3 id="Using-Julia&#39;s-native-serializer"><a class="docs-heading-anchor" href="#Using-Julia&#39;s-native-serializer">Using Julia&#39;s native serializer</a><a id="Using-Julia&#39;s-native-serializer-1"></a><a class="docs-heading-anchor-permalink" href="#Using-Julia&#39;s-native-serializer" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="MLJModelInterface.save" href="#MLJModelInterface.save"><code>MLJModelInterface.save</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">MLJ.save(filename, mach::Machine)
MLJ.save(io, mach::Machine)

MLJBase.save(filename, mach::Machine)
MLJBase.save(io, mach::Machine)</code></pre><p>Serialize the machine <code>mach</code> to a file with path <code>filename</code>, or to an input/output stream <code>io</code> (at least <code>IOBuffer</code> instances are supported) using the Serialization module.</p><p>To serialise using a different format, see <a href="#MLJBase.serializable"><code>serializable</code></a>.</p><p>Machines are deserialized using the <code>machine</code> constructor as shown in the example below.</p><blockquote><p>The implementation of <code>save</code> for machines changed in MLJ 0.18  (MLJBase 0.20). You can only restore a machine saved using older  versions of MLJ using an older version.</p></blockquote><p><strong>Example</strong></p><pre><code class="nohighlight hljs">using MLJ
Tree = @load DecisionTreeClassifier
X, y = @load_iris
mach = fit!(machine(Tree(), X, y))

MLJ.save(&quot;tree.jls&quot;, mach)
mach_predict_only = machine(&quot;tree.jls&quot;)
predict(mach_predict_only, X)

# using a buffer:
io = IOBuffer()
MLJ.save(io, mach)
seekstart(io)
predict_only_mach = machine(io)
predict(predict_only_mach, X)</code></pre><div class="admonition is-warning"><header class="admonition-header">Only load files from trusted sources</header><div class="admonition-body"><p>Maliciously constructed JLS files, like pickles, and most other general purpose serialization formats, can allow for arbitrary code execution during loading. This means it is possible for someone to use a JLS file that looks like a serialized MLJ machine as a <a href="https://en.wikipedia.org/wiki/Trojan_horse_(computing)">Trojan horse</a>.</p></div></div><p>See also <a href="#MLJBase.serializable"><code>serializable</code></a>, <a href="#MLJBase.machine"><code>machine</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJBase.jl/blob/v1.1.1/src/machines.jl#L1028-L1076">source</a></section></article><h3 id="Using-an-arbitrary-serializer"><a class="docs-heading-anchor" href="#Using-an-arbitrary-serializer">Using an arbitrary serializer</a><a id="Using-an-arbitrary-serializer-1"></a><a class="docs-heading-anchor-permalink" href="#Using-an-arbitrary-serializer" title="Permalink"></a></h3><p>Since machines contain training data, serializing a machine directly is not recommended. Also, the learned parameters of models implemented in a language other than Julia may not have persistent representations, which means serializing them is useless. To address these two issues, users:</p><ul><li><p>Call <code>serializable(mach)</code> on a machine <code>mach</code> they wish to save (to remove data and create persistent learned parameters)</p></li><li><p>Serialize the returned object using <code>SomeSerializationPkg</code></p></li></ul><p>To restore the original machine (minus training data) they:</p><ul><li>Deserialize using <code>SomeSerializationPkg</code> to obtain a new object <code>mach</code></li><li>Call <code>restore!(mach)</code> to ensure <code>mach</code> can be used to predict or transform new data.</li></ul><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="MLJBase.serializable" href="#MLJBase.serializable"><code>MLJBase.serializable</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">serializable(mach::Machine)</code></pre><p>Returns a shallow copy of the machine to make it serializable. In particular, all training data is removed and, if necessary, learned parameters are replaced with persistent representations.</p><p>Any general purpose Julia serializer may be applied to the output of <code>serializable</code> (eg, JLSO, BSON, JLD) but you must call <code>restore!(mach)</code> on the deserialised object <code>mach</code> before using it. See the example below.</p><p>If using Julia&#39;s standard Serialization library, a shorter workflow is available using the <a href="#MLJModelInterface.save"><code>MLJBase.save</code></a> (or <code>MLJ.save</code>) method.</p><p>A machine returned by <code>serializable</code> is characterized by the property <code>mach.state == -1</code>.</p><p><strong>Example using <a href="https://invenia.github.io/JLSO.jl/stable/">JLSO</a></strong></p><pre><code class="nohighlight hljs">using MLJ
using JLSO
Tree = @load DecisionTreeClassifier
tree = Tree()
X, y = @load_iris
mach = fit!(machine(tree, X, y))

# This machine can now be serialized
smach = serializable(mach)
JLSO.save(&quot;machine.jlso&quot;, :machine =&gt; smach)

# Deserialize and restore learned parameters to useable form:
loaded_mach = JLSO.load(&quot;machine.jlso&quot;)[:machine]
restore!(loaded_mach)

predict(loaded_mach, X)
predict(mach, X)</code></pre><p>See also <a href="#MLJBase.restore!"><code>restore!</code></a>, <a href="#MLJModelInterface.save"><code>MLJBase.save</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJBase.jl/blob/v1.1.1/src/machines.jl#L937-L977">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="MLJBase.restore!" href="#MLJBase.restore!"><code>MLJBase.restore!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">restore!(mach::Machine)</code></pre><p>Restore the state of a machine that is currently serializable but which may not be otherwise usable. For such a machine, <code>mach</code>, one has <code>mach.state=1</code>. Intended for restoring deserialized machine objects to a useable form.</p><p>For an example see <a href="#MLJBase.serializable"><code>serializable</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJBase.jl/blob/v1.1.1/src/machines.jl#L1009-L1019">source</a></section></article><h2 id="Internals"><a class="docs-heading-anchor" href="#Internals">Internals</a><a id="Internals-1"></a><a class="docs-heading-anchor-permalink" href="#Internals" title="Permalink"></a></h2><p>For a supervised machine, the <code>predict</code> method calls a lower-level <code>MLJBase.predict</code> method, dispatched on the underlying model and the <code>fitresult</code> (see below). To see <code>predict</code> in action, as well as its unsupervised cousins <code>transform</code> and <code>inverse_transform</code>, see <a href="../">Getting Started</a>.</p><p>Except for <code>model</code>, a <code>Machine</code> instance has several fields which the user should not directly access; these include:</p><ul><li><p><code>model</code> - the struct containing the hyperparameters to be used in calls to <code>fit!</code></p></li><li><p><code>fitresult</code> - the learned parameters in a raw form, initially undefined</p></li><li><p><code>args</code> - a tuple of the data, each element wrapped in a source node; see <a href="../learning_networks/#Learning-Networks">Learning Networks</a> (in the supervised learning example above, <code>args = (source(X), source(y))</code>)</p></li><li><p><code>report</code> - outputs of training not encoded in <code>fitresult</code> (eg, feature rankings), initially undefined</p></li><li><p><code>old_model</code> - a deep copy of the model used in the last call to <code>fit!</code></p></li><li><p><code>old_rows</code> -  a copy of the row indices used in the last call to <code>fit!</code></p></li><li><p><code>cache</code></p></li></ul><p>The interested reader can learn more about machine internals by examining the simplified code excerpt in <a href="../internals/">Internals</a>.</p><h2 id="API-Reference"><a class="docs-heading-anchor" href="#API-Reference">API Reference</a><a id="API-Reference-1"></a><a class="docs-heading-anchor-permalink" href="#API-Reference" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="MLJBase.machine" href="#MLJBase.machine"><code>MLJBase.machine</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">machine(model, args...; cache=true, scitype_check_level=1)</code></pre><p>Construct a <code>Machine</code> object binding a <code>model</code>, storing hyper-parameters of some machine learning algorithm, to some data, <code>args</code>. Calling <a href="#StatsAPI.fit!"><code>fit!</code></a> on a <code>Machine</code> instance <code>mach</code> stores outcomes of applying the algorithm in <code>mach</code>, which can be inspected using <code>fitted_params(mach)</code> (learned paramters) and <code>report(mach)</code> (other outcomes). This in turn enables generalization to new data using operations such as <code>predict</code> or <code>transform</code>:</p><pre><code class="language-julia hljs">using MLJModels
X, y = make_regression()

PCA = @load PCA pkg=MultivariateStats
model = PCA()
mach = machine(model, X)
fit!(mach, rows=1:50)
transform(mach, selectrows(X, 51:100)) # or transform(mach, rows=51:100)

DecisionTreeRegressor = @load DecisionTreeRegressor pkg=DecisionTree
model = DecisionTreeRegressor()
mach = machine(model, X, y)
fit!(mach, rows=1:50)
predict(mach, selectrows(X, 51:100)) # or predict(mach, rows=51:100)</code></pre><p>Specify <code>cache=false</code> to prioritize memory management over speed.</p><p>When building a learning network, <code>Node</code> objects can be substituted for the concrete data but no type or dimension checks are applied.</p><p><strong>Checks on the types of training data</strong></p><p>A model articulates its data requirements using <a href="https://juliaai.github.io/ScientificTypes.jl/dev/">scientific types</a>, i.e., using the <a href="@ref"><code>scitype</code></a> function instead of the <code>typeof</code> function.</p><p>If <code>scitype_check_level &gt; 0</code> then the scitype of each <code>arg</code> in <code>args</code> is computed, and this is compared with the scitypes expected by the model, unless <code>args</code> contains <code>Unknown</code> scitypes and <code>scitype_check_level &lt; 4</code>, in which case no further action is taken. Whether warnings are issued or errors thrown depends the level. For details, see <a href="@ref"><code>default_scitype_check_level</code></a>, a method to inspect or change the default level (<code>1</code> at startup).</p><p><strong>Machines with model placeholders</strong></p><p>A symbol can be substituted for a model in machine constructors to act as a placeholder for a model specified at training time. The symbol must be the field name for a struct whose corresponding value is a model, as shown in the following example:</p><pre><code class="language-julia hljs">mutable struct MyComposite
    transformer
    classifier
end

my_composite = MyComposite(Standardizer(), ConstantClassifier)

X, y = make_blobs()
mach = machine(:classifier, X, y)
fit!(mach, composite=my_composite)</code></pre><p>The last two lines are equivalent to</p><pre><code class="language-julia hljs">mach = machine(ConstantClassifier(), X, y)
fit!(mach)</code></pre><p>Delaying model specification is used when exporting learning networks as new stand-alone model types. See <a href="../learning_networks/#MLJBase.prefit"><code>prefit</code></a> and the MLJ documentation on learning networks.</p><p>See also <a href="#StatsAPI.fit!"><code>fit!</code></a>, <a href="@ref"><code>default_scitype_check_level</code></a>, <a href="#MLJModelInterface.save"><code>MLJBase.save</code></a>, <a href="#MLJBase.serializable"><code>serializable</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJBase.jl/blob/v1.1.1/src/machines.jl#L248-L327">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="StatsAPI.fit!" href="#StatsAPI.fit!"><code>StatsAPI.fit!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">fit!(mach::Machine, rows=nothing, verbosity=1, force=false, composite=nothing)</code></pre><p>Fit the machine <code>mach</code>. In the case that <code>mach</code> has <code>Node</code> arguments, first train all other machines on which <code>mach</code> depends.</p><p>To attempt to fit a machine without touching any other machine, use <code>fit_only!</code>. For more on options and the the internal logic of fitting see <a href="#MLJBase.fit_only!"><code>fit_only!</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJBase.jl/blob/v1.1.1/src/machines.jl#L763-L774">source</a></section><section><div><pre><code class="language-julia hljs">fit!(N::Node;
     rows=nothing,
     verbosity=1,
     force=false,
     acceleration=CPU1())</code></pre><p>Train all machines required to call the node <code>N</code>, in an appropriate order, but parallelizing where possible using specified <code>acceleration</code> mode.  These machines are those returned by <code>machines(N)</code>.</p><p>Supported modes of <code>acceleration</code>: <code>CPU1()</code>, <code>CPUThreads()</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJBase.jl/blob/v1.1.1/src/composition/learning_networks/nodes.jl#L195-L208">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="MLJBase.fit_only!" href="#MLJBase.fit_only!"><code>MLJBase.fit_only!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">MLJBase.fit_only!(
    mach::Machine;
    rows=nothing,
    verbosity=1,
    force=false,
    composite=nothing,
)</code></pre><p>Without mutating any other machine on which it may depend, perform one of the following actions to the machine <code>mach</code>, using the data and model bound to it, and restricting the data to <code>rows</code> if specified:</p><ul><li><p><em>Ab initio training.</em> Ignoring any previous learned parameters and cache, compute and store new learned parameters. Increment <code>mach.state</code>.</p></li><li><p><em>Training update.</em> Making use of previous learned parameters and/or  cache, replace or mutate existing learned parameters. The effect is  the same (or nearly the same) as in ab initio training, but may be  faster or use less memory, assuming the model supports an update  option (implements <code>MLJBase.update</code>). Increment <code>mach.state</code>.</p></li><li><p><em>No-operation.</em> Leave existing learned parameters untouched. Do not  increment <code>mach.state</code>.</p></li></ul><p>If the model, <code>model</code>, bound to <code>mach</code> is a symbol, then instead perform the action using the true model given by <code>getproperty(composite, model)</code>. See also <a href="#MLJBase.machine"><code>machine</code></a>.</p><p><strong>Training action logic</strong></p><p>For the action to be a no-operation, either <code>mach.frozen == true</code> or or none of the following apply:</p><ul><li><p>(i) <code>mach</code> has never been trained (<code>mach.state == 0</code>).</p></li><li><p>(ii) <code>force == true</code>.</p></li><li><p>(iii) The <code>state</code> of some other machine on which <code>mach</code> depends has changed since the last time <code>mach</code> was trained (ie, the last time <code>mach.state</code> was last incremented).</p></li><li><p>(iv) The specified <code>rows</code> have changed since the last retraining and <code>mach.model</code> does not have <code>Static</code> type.</p></li><li><p>(v) <code>mach.model</code> is a model and different from the last model used for training, but has the same type.</p></li><li><p>(vi) <code>mach.model</code> is a model but has a type different from the last model used for training.</p></li><li><p>(vii) <code>mach.model</code> is a symbol and <code>(composite, mach.model)</code> is different from the last model used for training, but has the same type.</p></li><li><p>(viii) <code>mach.model</code> is a symbol and <code>(composite, mach.model)</code> has a different type from the last model used for training.</p></li></ul><p>In any of the cases (i) - (iv), (vi), or (viii), <code>mach</code> is trained ab initio. If (v) or (vii) is true, then a training update is applied.</p><p>To freeze or unfreeze <code>mach</code>, use <code>freeze!(mach)</code> or <code>thaw!(mach)</code>.</p><p><strong>Implementation details</strong></p><p>The data to which a machine is bound is stored in <code>mach.args</code>. Each element of <code>args</code> is either a <code>Node</code> object, or, in the case that concrete data was bound to the machine, it is concrete data wrapped in a <code>Source</code> node. In all cases, to obtain concrete data for actual training, each argument <code>N</code> is called, as in <code>N()</code> or <code>N(rows=rows)</code>, and either <code>MLJBase.fit</code> (ab initio training) or <code>MLJBase.update</code> (training update) is dispatched on <code>mach.model</code> and this data. See the &quot;Adding models for general use&quot; section of the MLJ documentation for more on these lower-level training methods.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJBase.jl/blob/v1.1.1/src/machines.jl#L531-L606">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../loading_model_code/">« Loading Model Code</a><a class="docs-footer-nextpage" href="../evaluating_model_performance/">Evaluating Model Performance »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="auto">Automatic (OS)</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.2.1 on <span class="colophon-date" title="Monday 26 February 2024 22:46">Monday 26 February 2024</span>. Using Julia version 1.10.1.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
