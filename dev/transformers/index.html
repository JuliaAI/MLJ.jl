<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Transformers and Other Unsupervised models · MLJ</title><meta name="title" content="Transformers and Other Unsupervised models · MLJ"/><meta property="og:title" content="Transformers and Other Unsupervised models · MLJ"/><meta property="twitter:title" content="Transformers and Other Unsupervised models · MLJ"/><meta name="description" content="Documentation for MLJ."/><meta property="og:description" content="Documentation for MLJ."/><meta property="twitter:description" content="Documentation for MLJ."/><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../search_index.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script><link href="../assets/favicon.ico" rel="icon" type="image/x-icon"/><link href="https://fonts.googleapis.com/css2?family=Lato:ital,wght@0,100;0,300;0,400;0,700;0,900;1,100;1,300;1,400;1,700;1,900&amp;family=Montserrat:ital,wght@0,100..900;1,100..900&amp;display=swap" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.svg" alt="MLJ logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../">MLJ</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../model_browser/">Model Browser</a></li><li><a class="tocitem" href="../about_mlj/">About MLJ</a></li><li><a class="tocitem" href="../learning_mlj/">Learning MLJ</a></li><li><input class="collapse-toggle" id="menuitem-5" type="checkbox"/><label class="tocitem" for="menuitem-5"><span class="docs-label">Basics</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../getting_started/">Getting Started</a></li><li><a class="tocitem" href="../common_mlj_workflows/">Common MLJ Workflows</a></li><li><a class="tocitem" href="../machines/">Machines</a></li><li><a class="tocitem" href="../mlj_cheatsheet/">MLJ Cheatsheet</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-6" type="checkbox"/><label class="tocitem" for="menuitem-6"><span class="docs-label">Data</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../working_with_categorical_data/">Working with Categorical Data</a></li><li><a class="tocitem" href="../preparing_data/">Preparing Data</a></li><li><a class="tocitem" href="../generating_synthetic_data/">Generating Synthetic Data</a></li><li><a class="tocitem" href="../openml_integration/">OpenML Integration</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-7" type="checkbox" checked/><label class="tocitem" for="menuitem-7"><span class="docs-label">Models</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../model_search/">Model Search</a></li><li><a class="tocitem" href="../loading_model_code/">Loading Model Code</a></li><li class="is-active"><a class="tocitem" href>Transformers and Other Unsupervised models</a><ul class="internal"><li><a class="tocitem" href="#Built-in-transformers"><span>Built-in transformers</span></a></li><li><a class="tocitem" href="#Static-transformers"><span>Static transformers</span></a></li><li><a class="tocitem" href="#Transformers-that-also-predict"><span>Transformers that also predict</span></a></li><li><a class="tocitem" href="#Reference"><span>Reference</span></a></li></ul></li><li><a class="tocitem" href="../feature_selection/">Feature Selection</a></li><li><a class="tocitem" href="../list_of_supported_models/">List of Supported Models</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-8" type="checkbox"/><label class="tocitem" for="menuitem-8"><span class="docs-label">Meta-algorithms</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../evaluating_model_performance/">Evaluating Model Performance</a></li><li><a class="tocitem" href="../tuning_models/">Tuning Models</a></li><li><a class="tocitem" href="../learning_curves/">Learning Curves</a></li><li><a class="tocitem" href="../controlling_iterative_models/">Controlling Iterative Models</a></li><li><a class="tocitem" href="../correcting_class_imbalance/">Correcting Class Imbalance</a></li><li><a class="tocitem" href="../thresholding_probabilistic_predictors/">Thresholding Probabilistic Predictors</a></li><li><a class="tocitem" href="../target_transformations/">Target Transformations</a></li><li><a class="tocitem" href="../homogeneous_ensembles/">Homogeneous Ensembles</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-9" type="checkbox"/><label class="tocitem" for="menuitem-9"><span class="docs-label">Model Composition</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../composing_models/">Composing Models</a></li><li><a class="tocitem" href="../linear_pipelines/">Linear Pipelines</a></li><li><a class="tocitem" href="../model_stacking/">Model Stacking</a></li><li><a class="tocitem" href="../learning_networks/">Learning Networks</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-10" type="checkbox"/><label class="tocitem" for="menuitem-10"><span class="docs-label">Third Party Tools</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../logging_workflows/">Logging Workflows</a></li><li><a class="tocitem" href="../third_party_packages/">Third Party Packages</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-11" type="checkbox"/><label class="tocitem" for="menuitem-11"><span class="docs-label">Customization and Extension</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../simple_user_defined_models/">Simple User Defined Models</a></li><li><a class="tocitem" href="../quick_start_guide_to_adding_models/">Quick-Start Guide to Adding Models</a></li><li><a class="tocitem" href="../adding_models_for_general_use/">Adding Models for General Use</a></li><li><a class="tocitem" href="../modifying_behavior/">Modifying Behavior</a></li><li><a class="tocitem" href="../internals/">Internals</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-12" type="checkbox"/><label class="tocitem" for="menuitem-12"><span class="docs-label">Miscellaneous</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../performance_measures/">Performance Measures</a></li><li><a class="tocitem" href="../weights/">Weights</a></li><li><a class="tocitem" href="../acceleration_and_parallelism/">Acceleration and Parallelism</a></li><li><a class="tocitem" href="../glossary/">Glossary</a></li><li><a class="tocitem" href="../frequently_asked_questions/">FAQ</a></li></ul></li><li><a class="tocitem" href="../api/">Index of Methods</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Models</a></li><li class="is-active"><a href>Transformers and Other Unsupervised models</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Transformers and Other Unsupervised models</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/JuliaAI/MLJ.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/JuliaAI/MLJ.jl/blob/dev/docs/src/transformers.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Transformers-and-Other-Unsupervised-Models"><a class="docs-heading-anchor" href="#Transformers-and-Other-Unsupervised-Models">Transformers and Other Unsupervised Models</a><a id="Transformers-and-Other-Unsupervised-Models-1"></a><a class="docs-heading-anchor-permalink" href="#Transformers-and-Other-Unsupervised-Models" title="Permalink"></a></h1><p>Several unsupervised models used to perform common transformations, such as one-hot encoding, missing value imputation, and categorical encoding, are available in MLJ out-of-the-box (no need to load code with <code>@load</code>). They are detailed in <a href="#Built-in-transformers">Built-in transformers</a> below.</p><p>A transformer is <em>static</em> if it has no learned parameters. While such a transformer is tantamount to an ordinary function, realizing it as an MLJ static transformer (a subtype of <code>Static &lt;: Unsupervised</code>) can be useful, especially if the function depends on parameters the user would like to manipulate (which become <em>hyper-parameters</em> of the model). The necessary syntax for defining your own static transformers is described in <a href="#Static-transformers">Static transformers</a> below.</p><p>Some unsupervised models, such as clustering algorithms, have a <code>predict</code> method in addition to a <code>transform</code> method. We give an example of this in <a href="#Transformers-that-also-predict">Transformers that also predict</a></p><h2 id="Built-in-transformers"><a class="docs-heading-anchor" href="#Built-in-transformers">Built-in transformers</a><a id="Built-in-transformers-1"></a><a class="docs-heading-anchor-permalink" href="#Built-in-transformers" title="Permalink"></a></h2><p>For tutorials on the transformers below, refer to the <a href="https://github.com/JuliaAI/MLJTransforms.jl">MLJTransforms documentation</a>. </p><table><tr><th style="text-align: center">Transformer</th><th style="text-align: center">Brief Description</th></tr><tr><td style="text-align: center"><a href="#MLJTransforms.Standardizer"><code>Standardizer</code></a></td><td style="text-align: center">Transforming columns of numerical features by standardization</td></tr><tr><td style="text-align: center"><a href="#MLJTransforms.UnivariateBoxCoxTransformer"><code>UnivariateBoxCoxTransformer</code></a></td><td style="text-align: center">Apply BoxCox transformation given a single vector</td></tr><tr><td style="text-align: center"><a href="#MLJTransforms.InteractionTransformer"><code>InteractionTransformer</code></a></td><td style="text-align: center">Transforming columns of numerical features to create new interaction features</td></tr><tr><td style="text-align: center"><a href="#MLJTransforms.UnivariateDiscretizer"><code>UnivariateDiscretizer</code></a></td><td style="text-align: center">Discretize a continuous vector into an ordered factor</td></tr><tr><td style="text-align: center"><a href="#MLJTransforms.FillImputer"><code>FillImputer</code></a></td><td style="text-align: center">Fill in missing values of features belonging to any scientific type</td></tr><tr><td style="text-align: center"><a href="#MLJTransforms.UnivariateFillImputer"><code>UnivariateFillImputer</code></a></td><td style="text-align: center">Fill in missing values in a single vector</td></tr><tr><td style="text-align: center"><a href="#MLJTransforms.UnivariateTimeTypeToContinuous"><code>UnivariateTimeTypeToContinuous</code></a></td><td style="text-align: center">Transform a vector of time type into continuous type</td></tr><tr><td style="text-align: center"><a href="#MLJTransforms.OneHotEncoder"><code>OneHotEncoder</code></a></td><td style="text-align: center">Encode categorical variables into one-hot vectors</td></tr><tr><td style="text-align: center"><a href="#MLJTransforms.ContinuousEncoder"><code>ContinuousEncoder</code></a></td><td style="text-align: center">Adds type casting functionality to OnehotEncoder</td></tr><tr><td style="text-align: center"><a href="#MLJTransforms.OrdinalEncoder"><code>OrdinalEncoder</code></a></td><td style="text-align: center">Encode categorical variables into ordered integers</td></tr><tr><td style="text-align: center"><a href="#MLJTransforms.FrequencyEncoder"><code>FrequencyEncoder</code></a></td><td style="text-align: center">Encode categorical variables into their normalized or unormalized frequencies</td></tr><tr><td style="text-align: center"><a href="#MLJTransforms.TargetEncoder"><code>TargetEncoder</code></a></td><td style="text-align: center">Encode categorical variables into relevant target statistics</td></tr><tr><td style="text-align: center"><a href="#MLJTransforms.ContrastEncoder"><code>ContrastEncoder</code></a></td><td style="text-align: center">Allows defining a custom contrast encoder via a contrast matrix</td></tr><tr><td style="text-align: center"><a href="#MLJTransforms.CardinalityReducer"><code>CardinalityReducer</code></a></td><td style="text-align: center">Reduce cardinality of high cardinality categorical features by grouping infrequent categories</td></tr><tr><td style="text-align: center"><a href="#MLJTransforms.MissingnessEncoder"><code>MissingnessEncoder</code></a></td><td style="text-align: center">Encode missing values of categorical features into new values</td></tr></table><h2 id="Static-transformers"><a class="docs-heading-anchor" href="#Static-transformers">Static transformers</a><a id="Static-transformers-1"></a><a class="docs-heading-anchor-permalink" href="#Static-transformers" title="Permalink"></a></h2><p>A <em>static transformer</em> is a model for transforming data that does not generalize to new data (does not &quot;learn&quot;) but which nevertheless has hyperparameters. For example, the <code>DBSAN</code> clustering model from Clustering.jl can assign labels to some collection of observations, cannot directly assign a label to some new observation.</p><p>The general user may define their own static models. The main use-case is insertion into a <a href="../linear_pipelines/#Linear-Pipelines">Linear Pipelines</a> some parameter-dependent transformation. (If a static transformer has no hyper-parameters, it is tantamount to an ordinary function. An ordinary function can be inserted directly into a pipeline; the situation for learning networks is only <a href="../learning_networks/#node_overloading">slightly more complicated</a>.</p><p>The following example defines a new model type <code>Averager</code> to perform the weighted average of two vectors (target predictions, for example). We suppose the weighting is normalized, and therefore controlled by a single hyper-parameter, <code>mix</code>.</p><pre><code class="language-julia hljs">mutable struct Averager &lt;: Static
    mix::Float64
end

MLJ.transform(a::Averager, _, y1, y2) = (1 - a.mix)*y1 + a.mix*y2</code></pre><p><em>Important.</em> Note the sub-typing <code>&lt;: Static</code>.</p><p>Such static transformers with (unlearned) parameters can have arbitrarily many inputs, but only one output. In the single input case, an <code>inverse_transform</code> can also be defined. Since they have no real learned parameters, you bind a static transformer to a machine without specifying training arguments; there is no need to <code>fit!</code> the machine:</p><pre><code class="language-julia hljs">mach = machine(Averager(0.5))
transform(mach, [1, 2, 3], [3, 2, 1])</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">3-element Vector{Float64}:
 2.0
 2.0
 2.0</code></pre><p>Let&#39;s see how we can include our <code>Averager</code> in a <a href="../learning_networks/#Learning-Networks">learning network</a> to mix the predictions of two regressors, with one-hot encoding of the inputs. Here&#39;s two regressors for mixing, and some dummy data for testing our learning network:</p><pre><code class="language-julia hljs">ridge = (@load RidgeRegressor pkg=MultivariateStats)()
knn = (@load KNNRegressor)()

import Random.seed!
seed!(112)
X = (
    x1=coerce(rand(&quot;ab&quot;, 100), Multiclass),
    x2=rand(100),
)
y = X.x2 + 0.05*rand(100)
schema(X)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">┌───────┬───────────────┬────────────────────────────────┐
│ names │ scitypes      │ types                          │
├───────┼───────────────┼────────────────────────────────┤
│ x1    │ Multiclass{2} │ CategoricalValue{Char, UInt32} │
│ x2    │ Continuous    │ Float64                        │
└───────┴───────────────┴────────────────────────────────┘
</code></pre><p>And the learning network:</p><pre><code class="language-julia hljs">Xs = source(X)
ys = source(y)

averager = Averager(0.5)

mach0 = machine(OneHotEncoder(), Xs)
W = transform(mach0, Xs) # one-hot encode the input

mach1 = machine(ridge, W, ys)
y1 = predict(mach1, W)

mach2 = machine(knn, W, ys)
y2 = predict(mach2, W)

mach4= machine(averager)
yhat = transform(mach4, y1, y2)

# test:
fit!(yhat)
Xnew = selectrows(X, 1:3)
yhat(Xnew)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">3-element Vector{Float64}:
 0.6403223210037916
 0.9607694439597683
 0.8159225346205365</code></pre><p>We next &quot;export&quot; the learning network as a standalone composite model type. First we need a struct for the composite model. Since we are restricting to <code>Deterministic</code> component regressors, the composite will also make deterministic predictions, and so gets the supertype <code>DeterministicNetworkComposite</code>:</p><pre><code class="language-julia hljs">mutable struct DoubleRegressor &lt;: DeterministicNetworkComposite
    regressor1
    regressor2
    averager
end</code></pre><p>As described in <a href="../learning_networks/#Learning-Networks">Learning Networks</a>, we next paste the learning network into a <code>prefit</code> declaration, replace the component models with symbolic placeholders, and add a learning network &quot;interface&quot;:</p><pre><code class="language-julia hljs">import MLJBase
function MLJBase.prefit(composite::DoubleRegressor, verbosity, X, y)
    Xs = source(X)
    ys = source(y)

    mach0 = machine(OneHotEncoder(), Xs)
    W = transform(mach0, Xs) # one-hot encode the input

    mach1 = machine(:regressor1, W, ys)
    y1 = predict(mach1, W)

    mach2 = machine(:regressor2, W, ys)
    y2 = predict(mach2, W)

    mach4= machine(:averager)
    yhat = transform(mach4, y1, y2)

    # learning network interface:
    (; predict=yhat)
end</code></pre><p>The new model type can be evaluated like any other supervised model:</p><pre><code class="language-julia hljs">X, y = @load_reduced_ames;
composite = DoubleRegressor(ridge, knn, Averager(0.5))</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">DoubleRegressor(
  regressor1 = RidgeRegressor(
        lambda = 1.0, 
        bias = true), 
  regressor2 = KNNRegressor(
        K = 5, 
        algorithm = :kdtree, 
        metric = Distances.Euclidean(0.0), 
        leafsize = 10, 
        reorder = true, 
        weights = NearestNeighborModels.Uniform()), 
  averager = Averager(
        mix = 0.5))</code></pre><pre><code class="language-julia hljs">composite.averager.mix = 0.25 # adjust mix from default of 0.5
evaluate(composite, X, y, measure=l1)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">PerformanceEvaluation object with these fields:
  model, measure, operation,
  measurement, per_fold, per_observation,
  fitted_params_per_fold, report_per_fold,
  train_test_rows, resampling, repeats
Extract:
┌──────────┬───────────┬─────────────┐
│ measure  │ operation │ measurement │
├──────────┼───────────┼─────────────┤
│ LPLoss(  │ predict   │ 17200.0     │
│   p = 1) │           │             │
└──────────┴───────────┴─────────────┘
┌────────────────────────────────────────────────────────┬─────────┐
│ per_fold                                               │ 1.96*SE │
├────────────────────────────────────────────────────────┼─────────┤
│ [15200.0, 15800.0, 18500.0, 16400.0, 18600.0, 18500.0] │ 1350.0  │
└────────────────────────────────────────────────────────┴─────────┘
</code></pre><p>A static transformer can also expose byproducts of the transform computation in the report of any associated machine. See <a href="#Static-transformers">Static transformers</a> for details.</p><h2 id="Transformers-that-also-predict"><a class="docs-heading-anchor" href="#Transformers-that-also-predict">Transformers that also predict</a><a id="Transformers-that-also-predict-1"></a><a class="docs-heading-anchor-permalink" href="#Transformers-that-also-predict" title="Permalink"></a></h2><p>Some clustering algorithms learn to label data by identifying a collection of &quot;centroids&quot; in the training data. Any new input observation is labeled with the cluster to which it is closest (this is the output of <code>predict</code>) while the vector of all distances from the centroids defines a lower-dimensional representation of the observation (the output of <code>transform</code>). In the following example a K-means clustering algorithm assigns one of three labels 1, 2, 3 to the input features of the iris data set and compares them with the actual species recorded in the target (not seen by the algorithm).</p><pre><code class="language-julia hljs">import Random.seed!
seed!(123)

X, y = @load_iris
KMeans = @load KMeans pkg=Clustering
kmeans = KMeans()
mach = machine(kmeans, X) |&gt; fit!</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">[ Info: For silent loading, specify `verbosity=0`.
import MLJClusteringInterface ✔
[ Info: Training machine(KMeans(k = 3, …), …).</code></pre><p>Transforming:</p><pre><code class="language-julia hljs">Xsmall = transform(mach)
selectrows(Xsmall, 1:4) |&gt; pretty</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">┌────────────┬────────────┬────────────┐
│ x1         │ x2         │ x3         │
│ Float64    │ Float64    │ Float64    │
│ Continuous │ Continuous │ Continuous │
├────────────┼────────────┼────────────┤
│ 11.6913    │ 0.021592   │ 25.599     │
│ 11.5503    │ 0.191992   │ 26.1626    │
│ 12.7403    │ 0.169992   │ 27.8716    │
│ 11.7129    │ 0.269192   │ 26.5595    │
└────────────┴────────────┴────────────┘</code></pre><p>Predicting:</p><pre><code class="language-julia hljs">yhat = predict(mach)
compare = zip(yhat, y) |&gt; collect</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">150-element Vector{Tuple{CategoricalValue{Int64, UInt32}, CategoricalValue{String, UInt32}}}:
 (2, &quot;setosa&quot;)
 (2, &quot;setosa&quot;)
 (2, &quot;setosa&quot;)
 (2, &quot;setosa&quot;)
 (2, &quot;setosa&quot;)
 (2, &quot;setosa&quot;)
 (2, &quot;setosa&quot;)
 (2, &quot;setosa&quot;)
 (2, &quot;setosa&quot;)
 (2, &quot;setosa&quot;)
 ⋮
 (3, &quot;virginica&quot;)
 (1, &quot;virginica&quot;)
 (3, &quot;virginica&quot;)
 (3, &quot;virginica&quot;)
 (3, &quot;virginica&quot;)
 (1, &quot;virginica&quot;)
 (3, &quot;virginica&quot;)
 (3, &quot;virginica&quot;)
 (1, &quot;virginica&quot;)</code></pre><pre><code class="language-julia hljs">compare[1:8]</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">8-element Vector{Tuple{CategoricalValue{Int64, UInt32}, CategoricalValue{String, UInt32}}}:
 (2, &quot;setosa&quot;)
 (2, &quot;setosa&quot;)
 (2, &quot;setosa&quot;)
 (2, &quot;setosa&quot;)
 (2, &quot;setosa&quot;)
 (2, &quot;setosa&quot;)
 (2, &quot;setosa&quot;)
 (2, &quot;setosa&quot;)</code></pre><pre><code class="language-julia hljs">compare[51:58]</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">8-element Vector{Tuple{CategoricalValue{Int64, UInt32}, CategoricalValue{String, UInt32}}}:
 (1, &quot;versicolor&quot;)
 (1, &quot;versicolor&quot;)
 (3, &quot;versicolor&quot;)
 (1, &quot;versicolor&quot;)
 (1, &quot;versicolor&quot;)
 (1, &quot;versicolor&quot;)
 (1, &quot;versicolor&quot;)
 (1, &quot;versicolor&quot;)</code></pre><pre><code class="language-julia hljs">compare[101:108]</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">8-element Vector{Tuple{CategoricalValue{Int64, UInt32}, CategoricalValue{String, UInt32}}}:
 (3, &quot;virginica&quot;)
 (1, &quot;virginica&quot;)
 (3, &quot;virginica&quot;)
 (3, &quot;virginica&quot;)
 (3, &quot;virginica&quot;)
 (3, &quot;virginica&quot;)
 (1, &quot;virginica&quot;)
 (3, &quot;virginica&quot;)</code></pre><h2 id="Reference"><a class="docs-heading-anchor" href="#Reference">Reference</a><a id="Reference-1"></a><a class="docs-heading-anchor-permalink" href="#Reference" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="MLJTransforms.Standardizer" href="#MLJTransforms.Standardizer"><code>MLJTransforms.Standardizer</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">Standardizer</code></pre><p>A model type for constructing a standardizer, based on <a href="unknown">unknown.jl</a>, and implementing the MLJ model interface.</p><p>From MLJ, the type can be imported using</p><pre><code class="nohighlight hljs">Standardizer = @load Standardizer pkg=unknown</code></pre><p>Do <code>model = Standardizer()</code> to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in <code>Standardizer(features=...)</code>.</p><p>Use this model to standardize (whiten) a <code>Continuous</code> vector, or relevant columns of a table. The rescalings applied by this transformer to new data are always those learned during the training phase, which are generally different from what would actually standardize the new data.</p><p><strong>Training data</strong></p><p>In MLJ or MLJBase, bind an instance <code>model</code> to data with</p><pre><code class="nohighlight hljs">mach = machine(model, X)</code></pre><p>where</p><ul><li><code>X</code>: any Tables.jl compatible table or any abstract vector with <code>Continuous</code> element scitype (any abstract float vector). Only features in a table with <code>Continuous</code> scitype can be standardized; check column scitypes with <code>schema(X)</code>.</li></ul><p>Train the machine using <code>fit!(mach, rows=...)</code>.</p><p><strong>Hyper-parameters</strong></p><ul><li><p><code>features</code>: one of the following, with the behavior indicated below:</p><ul><li><p><code>[]</code> (empty, the default): standardize all features (columns) having <code>Continuous</code> element scitype</p></li><li><p>non-empty vector of feature names (symbols): standardize only the <code>Continuous</code> features in the vector (if <code>ignore=false</code>) or <code>Continuous</code> features <em>not</em> named in the vector (<code>ignore=true</code>).</p></li><li><p>function or other callable: standardize a feature if the callable returns <code>true</code> on its name. For example, <code>Standardizer(features = name -&gt; name in [:x1, :x3], ignore = true, count=true)</code> has the same effect as <code>Standardizer(features = [:x1, :x3], ignore = true, count=true)</code>, namely to standardize all <code>Continuous</code> and <code>Count</code> features, with the exception of <code>:x1</code> and <code>:x3</code>.</p></li></ul><p>Note this behavior is further modified if the <code>ordered_factor</code> or <code>count</code> flags are set to <code>true</code>; see below</p></li><li><p><code>ignore=false</code>: whether to ignore or standardize specified <code>features</code>, as explained above</p></li><li><p><code>ordered_factor=false</code>: if <code>true</code>, standardize any <code>OrderedFactor</code> feature wherever a <code>Continuous</code> feature would be standardized, as described above</p></li><li><p><code>count=false</code>: if <code>true</code>, standardize any <code>Count</code> feature wherever a <code>Continuous</code> feature would be standardized, as described above</p></li></ul><p><strong>Operations</strong></p><ul><li><p><code>transform(mach, Xnew)</code>: return <code>Xnew</code> with relevant features standardized according to the rescalings learned during fitting of <code>mach</code>.</p></li><li><p><code>inverse_transform(mach, Z)</code>: apply the inverse transformation to <code>Z</code>, so that <code>inverse_transform(mach, transform(mach, Xnew))</code> is approximately the same as <code>Xnew</code>; unavailable if <code>ordered_factor</code> or <code>count</code> flags were set to <code>true</code>.</p></li></ul><p><strong>Fitted parameters</strong></p><p>The fields of <code>fitted_params(mach)</code> are:</p><ul><li><p><code>features_fit</code> - the names of features that will be standardized</p></li><li><p><code>means</code> - the corresponding untransformed mean values</p></li><li><p><code>stds</code> - the corresponding untransformed standard deviations</p></li></ul><p><strong>Report</strong></p><p>The fields of <code>report(mach)</code> are:</p><ul><li><code>features_fit</code>: the names of features that will be standardized</li></ul><p><strong>Examples</strong></p><pre><code class="nohighlight hljs">using MLJ

X = (ordinal1 = [1, 2, 3],
     ordinal2 = coerce([:x, :y, :x], OrderedFactor),
     ordinal3 = [10.0, 20.0, 30.0],
     ordinal4 = [-20.0, -30.0, -40.0],
     nominal = coerce([&quot;Your father&quot;, &quot;he&quot;, &quot;is&quot;], Multiclass));

julia&gt; schema(X)
┌──────────┬──────────────────┐
│ names    │ scitypes         │
├──────────┼──────────────────┤
│ ordinal1 │ Count            │
│ ordinal2 │ OrderedFactor{2} │
│ ordinal3 │ Continuous       │
│ ordinal4 │ Continuous       │
│ nominal  │ Multiclass{3}    │
└──────────┴──────────────────┘

stand1 = Standardizer();

julia&gt; transform(fit!(machine(stand1, X)), X)
(ordinal1 = [1, 2, 3],
 ordinal2 = CategoricalValue{Symbol,UInt32}[:x, :y, :x],
 ordinal3 = [-1.0, 0.0, 1.0],
 ordinal4 = [1.0, 0.0, -1.0],
 nominal = CategoricalValue{String,UInt32}[&quot;Your father&quot;, &quot;he&quot;, &quot;is&quot;],)

stand2 = Standardizer(features=[:ordinal3, ], ignore=true, count=true);

julia&gt; transform(fit!(machine(stand2, X)), X)
(ordinal1 = [-1.0, 0.0, 1.0],
 ordinal2 = CategoricalValue{Symbol,UInt32}[:x, :y, :x],
 ordinal3 = [10.0, 20.0, 30.0],
 ordinal4 = [1.0, 0.0, -1.0],
 nominal = CategoricalValue{String,UInt32}[&quot;Your father&quot;, &quot;he&quot;, &quot;is&quot;],)</code></pre><p>See also <a href="#MLJTransforms.OneHotEncoder"><code>OneHotEncoder</code></a>, <a href="#MLJTransforms.ContinuousEncoder"><code>ContinuousEncoder</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJTransforms.jl/blob/v0.1.1/src/transformers/other_transformers/standardizer.jl#L239-L254">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="MLJTransforms.UnivariateBoxCoxTransformer" href="#MLJTransforms.UnivariateBoxCoxTransformer"><code>MLJTransforms.UnivariateBoxCoxTransformer</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">UnivariateBoxCoxTransformer</code></pre><p>A model type for constructing a single variable Box-Cox transformer, based on <a href="unknown">unknown.jl</a>, and implementing the MLJ model interface.</p><p>From MLJ, the type can be imported using</p><pre><code class="nohighlight hljs">UnivariateBoxCoxTransformer = @load UnivariateBoxCoxTransformer pkg=unknown</code></pre><p>Do <code>model = UnivariateBoxCoxTransformer()</code> to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in <code>UnivariateBoxCoxTransformer(n=...)</code>.</p><p>Box-Cox transformations attempt to make data look more normally distributed. This can improve performance and assist in the interpretation of models which suppose that data is generated by a normal distribution.</p><p>A Box-Cox transformation (with shift) is of the form</p><pre><code class="nohighlight hljs">x -&gt; ((x + c)^λ - 1)/λ</code></pre><p>for some constant <code>c</code> and real <code>λ</code>, unless <code>λ = 0</code>, in which case the above is replaced with</p><pre><code class="nohighlight hljs">x -&gt; log(x + c)</code></pre><p>Given user-specified hyper-parameters <code>n::Integer</code> and <code>shift::Bool</code>, the present implementation learns the parameters <code>c</code> and <code>λ</code> from the training data as follows: If <code>shift=true</code> and zeros are encountered in the data, then <code>c</code> is set to <code>0.2</code> times the data mean.  If there are no zeros, then no shift is applied. Finally, <code>n</code> different values of <code>λ</code> between <code>-0.4</code> and <code>3</code> are considered, with <code>λ</code> fixed to the value maximizing normality of the transformed data.</p><p><em>Reference:</em> <a href="https://en.wikipedia.org/wiki/Power_transform">Wikipedia entry for power  transform</a>.</p><p><strong>Training data</strong></p><p>In MLJ or MLJBase, bind an instance <code>model</code> to data with</p><pre><code class="nohighlight hljs">mach = machine(model, x)</code></pre><p>where</p><ul><li><code>x</code>: any abstract vector with element scitype <code>Continuous</code>; check the scitype with <code>scitype(x)</code></li></ul><p>Train the machine using <code>fit!(mach, rows=...)</code>.</p><p><strong>Hyper-parameters</strong></p><ul><li><p><code>n=171</code>: number of values of the exponent <code>λ</code> to try</p></li><li><p><code>shift=false</code>: whether to include a preliminary constant translation in transformations, in the presence of zeros</p></li></ul><p><strong>Operations</strong></p><ul><li><p><code>transform(mach, xnew)</code>: apply the Box-Cox transformation learned when fitting <code>mach</code></p></li><li><p><code>inverse_transform(mach, z)</code>: reconstruct the vector <code>z</code> whose transformation learned by <code>mach</code> is <code>z</code></p></li></ul><p><strong>Fitted parameters</strong></p><p>The fields of <code>fitted_params(mach)</code> are:</p><ul><li><p><code>λ</code>: the learned Box-Cox exponent</p></li><li><p><code>c</code>: the learned shift</p></li></ul><p><strong>Examples</strong></p><pre><code class="nohighlight hljs">using MLJ
using UnicodePlots
using Random
Random.seed!(123)

transf = UnivariateBoxCoxTransformer()

x = randn(1000).^2

mach = machine(transf, x)
fit!(mach)

z = transform(mach, x)

julia&gt; histogram(x)
                ┌                                        ┐
   [ 0.0,  2.0) ┤███████████████████████████████████  848
   [ 2.0,  4.0) ┤████▌ 109
   [ 4.0,  6.0) ┤█▍ 33
   [ 6.0,  8.0) ┤▍ 7
   [ 8.0, 10.0) ┤▏ 2
   [10.0, 12.0) ┤  0
   [12.0, 14.0) ┤▏ 1
                └                                        ┘
                                 Frequency

julia&gt; histogram(z)
                ┌                                        ┐
   [-5.0, -4.0) ┤█▎ 8
   [-4.0, -3.0) ┤████████▊ 64
   [-3.0, -2.0) ┤█████████████████████▊ 159
   [-2.0, -1.0) ┤█████████████████████████████▊ 216
   [-1.0,  0.0) ┤███████████████████████████████████  254
   [ 0.0,  1.0) ┤█████████████████████████▊ 188
   [ 1.0,  2.0) ┤████████████▍ 90
   [ 2.0,  3.0) ┤██▊ 20
   [ 3.0,  4.0) ┤▎ 1
                └                                        ┘
                                 Frequency
</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJTransforms.jl/blob/v0.1.1/src/transformers/other_transformers/univariate_boxcox_transformer.jl#L96-L111">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="MLJTransforms.InteractionTransformer" href="#MLJTransforms.InteractionTransformer"><code>MLJTransforms.InteractionTransformer</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">InteractionTransformer</code></pre><p>A model type for constructing a interaction transformer, based on <a href="unknown">unknown.jl</a>, and implementing the MLJ model interface.</p><p>From MLJ, the type can be imported using</p><pre><code class="nohighlight hljs">InteractionTransformer = @load InteractionTransformer pkg=unknown</code></pre><p>Do <code>model = InteractionTransformer()</code> to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in <code>InteractionTransformer(order=...)</code>.</p><p>Generates all polynomial interaction terms up to the given order for the subset of chosen columns.  Any column that contains elements with scitype <code>&lt;:Infinite</code> is a valid basis to generate interactions.  If <code>features</code> is not specified, all such columns with scitype <code>&lt;:Infinite</code> in the table are used as a basis.</p><p>In MLJ or MLJBase, you can transform features <code>X</code> with the single call</p><pre><code class="nohighlight hljs">transform(machine(model), X)</code></pre><p>See also the example below.</p><p><strong>Hyper-parameters</strong></p><ul><li><code>order</code>: Maximum order of interactions to be generated.</li><li><code>features</code>: Restricts interations generation to those columns</li></ul><p><strong>Operations</strong></p><ul><li><code>transform(machine(model), X)</code>: Generates polynomial interaction terms out of table <code>X</code> using the hyper-parameters specified in <code>model</code>.</li></ul><p><strong>Example</strong></p><pre><code class="nohighlight hljs">using MLJ

X = (
    A = [1, 2, 3],
    B = [4, 5, 6],
    C = [7, 8, 9],
    D = [&quot;x₁&quot;, &quot;x₂&quot;, &quot;x₃&quot;]
)
it = InteractionTransformer(order=3)
mach = machine(it)

julia&gt; transform(mach, X)
(A = [1, 2, 3],
 B = [4, 5, 6],
 C = [7, 8, 9],
 D = [&quot;x₁&quot;, &quot;x₂&quot;, &quot;x₃&quot;],
 A_B = [4, 10, 18],
 A_C = [7, 16, 27],
 B_C = [28, 40, 54],
 A_B_C = [28, 80, 162],)

it = InteractionTransformer(order=2, features=[:A, :B])
mach = machine(it)

julia&gt; transform(mach, X)
(A = [1, 2, 3],
 B = [4, 5, 6],
 C = [7, 8, 9],
 D = [&quot;x₁&quot;, &quot;x₂&quot;, &quot;x₃&quot;],
 A_B = [4, 10, 18],)
</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJTransforms.jl/blob/v0.1.1/src/transformers/other_transformers/interaction_transformer.jl#L43-L58">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="MLJTransforms.UnivariateDiscretizer" href="#MLJTransforms.UnivariateDiscretizer"><code>MLJTransforms.UnivariateDiscretizer</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">UnivariateDiscretizer</code></pre><p>A model type for constructing a single variable discretizer, based on <a href="unknown">unknown.jl</a>, and implementing the MLJ model interface.</p><p>From MLJ, the type can be imported using</p><pre><code class="nohighlight hljs">UnivariateDiscretizer = @load UnivariateDiscretizer pkg=unknown</code></pre><p>Do <code>model = UnivariateDiscretizer()</code> to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in <code>UnivariateDiscretizer(n_classes=...)</code>.</p><p>Discretization converts a <code>Continuous</code> vector into an <code>OrderedFactor</code> vector. In particular, the output is a <code>CategoricalVector</code> (whose reference type is optimized).</p><p>The transformation is chosen so that the vector on which the transformer is fit has, in transformed form, an approximately uniform distribution of values. Specifically, if <code>n_classes</code> is the level of discretization, then <code>2*n_classes - 1</code> ordered quantiles are computed, the odd quantiles being used for transforming (discretization) and the even quantiles for inverse transforming.</p><p><strong>Training data</strong></p><p>In MLJ or MLJBase, bind an instance <code>model</code> to data with</p><pre><code class="nohighlight hljs">mach = machine(model, x)</code></pre><p>where</p><ul><li><code>x</code>: any abstract vector with <code>Continuous</code> element scitype; check scitype with <code>scitype(x)</code>.</li></ul><p>Train the machine using <code>fit!(mach, rows=...)</code>.</p><p><strong>Hyper-parameters</strong></p><ul><li><code>n_classes</code>: number of discrete classes in the output</li></ul><p><strong>Operations</strong></p><ul><li><p><code>transform(mach, xnew)</code>: discretize <code>xnew</code> according to the discretization learned when fitting <code>mach</code></p></li><li><p><code>inverse_transform(mach, z)</code>: attempt to reconstruct from <code>z</code> a vector that transforms to give <code>z</code></p></li></ul><p><strong>Fitted parameters</strong></p><p>The fields of <code>fitted_params(mach).fitesult</code> include:</p><ul><li><p><code>odd_quantiles</code>: quantiles used for transforming (length is <code>n_classes - 1</code>)</p></li><li><p><code>even_quantiles</code>: quantiles used for inverse transforming (length is <code>n_classes</code>)</p></li></ul><p><strong>Example</strong></p><pre><code class="nohighlight hljs">using MLJ
using Random
Random.seed!(123)

discretizer = UnivariateDiscretizer(n_classes=100)
mach = machine(discretizer, randn(1000))
fit!(mach)

julia&gt; x = rand(5)
5-element Vector{Float64}:
 0.8585244609846809
 0.37541692370451396
 0.6767070590395461
 0.9208844241267105
 0.7064611415680901

julia&gt; z = transform(mach, x)
5-element CategoricalArrays.CategoricalArray{UInt8,1,UInt8}:
 0x52
 0x42
 0x4d
 0x54
 0x4e

x_approx = inverse_transform(mach, z)
julia&gt; x - x_approx
5-element Vector{Float64}:
 0.008224506144777322
 0.012731354778359405
 0.0056265330571125816
 0.005738175684445124
 0.006835652575801987</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJTransforms.jl/blob/v0.1.1/src/transformers/other_transformers/univariate_discretizer.jl#L100-L115">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="MLJTransforms.FillImputer" href="#MLJTransforms.FillImputer"><code>MLJTransforms.FillImputer</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">FillImputer</code></pre><p>A model type for constructing a fill imputer, based on <a href="unknown">unknown.jl</a>, and implementing the MLJ model interface.</p><p>From MLJ, the type can be imported using</p><pre><code class="nohighlight hljs">FillImputer = @load FillImputer pkg=unknown</code></pre><p>Do <code>model = FillImputer()</code> to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in <code>FillImputer(features=...)</code>.</p><p>Use this model to impute <code>missing</code> values in tabular data. A fixed &quot;filler&quot; value is learned from the training data, one for each column of the table.</p><p>For imputing missing values in a vector, use <a href="#MLJTransforms.UnivariateFillImputer"><code>UnivariateFillImputer</code></a> instead.</p><p><strong>Training data</strong></p><p>In MLJ or MLJBase, bind an instance <code>model</code> to data with</p><pre><code class="nohighlight hljs">mach = machine(model, X)</code></pre><p>where</p><ul><li><code>X</code>: any table of input features (eg, a <code>DataFrame</code>) whose features each have element scitypes <code>Union{Missing, T}</code>, where <code>T</code> is a subtype of <code>Continuous</code>, <code>Multiclass</code>, <code>OrderedFactor</code> or <code>Count</code>. Check scitypes with <code>schema(X)</code>.</li></ul><p>Train the machine using <code>fit!(mach, rows=...)</code>.</p><p><strong>Hyper-parameters</strong></p><ul><li><p><code>features</code>: a vector of names of features (symbols) for which imputation is to be attempted; default is empty, which is interpreted as &quot;impute all&quot;.</p></li><li><p><code>continuous_fill</code>: function or other callable to determine value to be imputed in the case of <code>Continuous</code> (abstract float) data; default is to apply <code>median</code> after skipping <code>missing</code> values</p></li><li><p><code>count_fill</code>: function or other callable to determine value to be imputed in the case of <code>Count</code> (integer) data; default is to apply rounded <code>median</code> after skipping <code>missing</code> values</p></li><li><p><code>finite_fill</code>: function or other callable to determine value to be imputed in the case of <code>Multiclass</code> or <code>OrderedFactor</code> data (categorical vectors); default is to apply <code>mode</code> after skipping <code>missing</code> values</p></li></ul><p><strong>Operations</strong></p><ul><li><code>transform(mach, Xnew)</code>: return <code>Xnew</code> with missing values imputed with the fill values learned when fitting <code>mach</code></li></ul><p><strong>Fitted parameters</strong></p><p>The fields of <code>fitted_params(mach)</code> are:</p><ul><li><p><code>features_seen_in_fit</code>: the names of features (features) encountered during training</p></li><li><p><code>univariate_transformer</code>: the univariate model applied to determine   the fillers (it&#39;s fields contain the functions defining the filler computations)</p></li><li><p><code>filler_given_feature</code>: dictionary of filler values, keyed on feature (column) names</p></li></ul><p><strong>Examples</strong></p><pre><code class="nohighlight hljs">using MLJ
imputer = FillImputer()

X = (a = [1.0, 2.0, missing, 3.0, missing],
     b = coerce([&quot;y&quot;, &quot;n&quot;, &quot;y&quot;, missing, &quot;y&quot;], Multiclass),
     c = [1, 1, 2, missing, 3])

schema(X)
julia&gt; schema(X)
┌───────┬───────────────────────────────┐
│ names │ scitypes                      │
├───────┼───────────────────────────────┤
│ a     │ Union{Missing, Continuous}    │
│ b     │ Union{Missing, Multiclass{2}} │
│ c     │ Union{Missing, Count}         │
└───────┴───────────────────────────────┘

mach = machine(imputer, X)
fit!(mach)

julia&gt; fitted_params(mach).filler_given_feature
(filler = 2.0,)

julia&gt; fitted_params(mach).filler_given_feature
Dict{Symbol, Any} with 3 entries:
  :a =&gt; 2.0
  :b =&gt; &quot;y&quot;
  :c =&gt; 2

julia&gt; transform(mach, X)
(a = [1.0, 2.0, 2.0, 3.0, 2.0],
 b = CategoricalValue{String, UInt32}[&quot;y&quot;, &quot;n&quot;, &quot;y&quot;, &quot;y&quot;, &quot;y&quot;],
 c = [1, 1, 2, 2, 3],)</code></pre><p>See also <a href="#MLJTransforms.UnivariateFillImputer"><code>UnivariateFillImputer</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJTransforms.jl/blob/v0.1.1/src/transformers/other_transformers/fill_imputer.jl#L299-L314">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="MLJTransforms.UnivariateFillImputer" href="#MLJTransforms.UnivariateFillImputer"><code>MLJTransforms.UnivariateFillImputer</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">UnivariateFillImputer</code></pre><p>A model type for constructing a single variable fill imputer, based on <a href="unknown">unknown.jl</a>, and implementing the MLJ model interface.</p><p>From MLJ, the type can be imported using</p><pre><code class="nohighlight hljs">UnivariateFillImputer = @load UnivariateFillImputer pkg=unknown</code></pre><p>Do <code>model = UnivariateFillImputer()</code> to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in <code>UnivariateFillImputer(continuous_fill=...)</code>.</p><p>Use this model to imputing <code>missing</code> values in a vector with a fixed value learned from the non-missing values of training vector.</p><p>For imputing missing values in tabular data, use <a href="#MLJTransforms.FillImputer"><code>FillImputer</code></a> instead.</p><p><strong>Training data</strong></p><p>In MLJ or MLJBase, bind an instance <code>model</code> to data with</p><pre><code class="nohighlight hljs">mach = machine(model, x)</code></pre><p>where</p><ul><li><code>x</code>: any abstract vector with element scitype <code>Union{Missing, T}</code> where <code>T</code> is a subtype of <code>Continuous</code>, <code>Multiclass</code>, <code>OrderedFactor</code> or <code>Count</code>; check scitype using <code>scitype(x)</code></li></ul><p>Train the machine using <code>fit!(mach, rows=...)</code>.</p><p><strong>Hyper-parameters</strong></p><ul><li><p><code>continuous_fill</code>: function or other callable to determine value to be imputed in the case of <code>Continuous</code> (abstract float) data; default is to apply <code>median</code> after skipping <code>missing</code> values</p></li><li><p><code>count_fill</code>: function or other callable to determine value to be imputed in the case of <code>Count</code> (integer) data; default is to apply rounded <code>median</code> after skipping <code>missing</code> values</p></li><li><p><code>finite_fill</code>: function or other callable to determine value to be imputed in the case of <code>Multiclass</code> or <code>OrderedFactor</code> data (categorical vectors); default is to apply <code>mode</code> after skipping <code>missing</code> values</p></li></ul><p><strong>Operations</strong></p><ul><li><code>transform(mach, xnew)</code>: return <code>xnew</code> with missing values imputed with the fill values learned when fitting <code>mach</code></li></ul><p><strong>Fitted parameters</strong></p><p>The fields of <code>fitted_params(mach)</code> are:</p><ul><li><code>filler</code>: the fill value to be imputed in all new data</li></ul><p><strong>Examples</strong></p><pre><code class="nohighlight hljs">using MLJ
imputer = UnivariateFillImputer()

x_continuous = [1.0, 2.0, missing, 3.0]
x_multiclass = coerce([&quot;y&quot;, &quot;n&quot;, &quot;y&quot;, missing, &quot;y&quot;], Multiclass)
x_count = [1, 1, 1, 2, missing, 3, 3]

mach = machine(imputer, x_continuous)
fit!(mach)

julia&gt; fitted_params(mach)
(filler = 2.0,)

julia&gt; transform(mach, [missing, missing, 101.0])
3-element Vector{Float64}:
 2.0
 2.0
 101.0

mach2 = machine(imputer, x_multiclass) |&gt; fit!

julia&gt; transform(mach2, x_multiclass)
5-element CategoricalArray{String,1,UInt32}:
 &quot;y&quot;
 &quot;n&quot;
 &quot;y&quot;
 &quot;y&quot;
 &quot;y&quot;

mach3 = machine(imputer, x_count) |&gt; fit!

julia&gt; transform(mach3, [missing, missing, 5])
3-element Vector{Int64}:
 2
 2
 5</code></pre><p>For imputing tabular data, use <a href="#MLJTransforms.FillImputer"><code>FillImputer</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJTransforms.jl/blob/v0.1.1/src/transformers/other_transformers/fill_imputer.jl#L197-L212">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="MLJTransforms.UnivariateTimeTypeToContinuous" href="#MLJTransforms.UnivariateTimeTypeToContinuous"><code>MLJTransforms.UnivariateTimeTypeToContinuous</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">UnivariateTimeTypeToContinuous</code></pre><p>A model type for constructing a single variable transformer that creates continuous representations of temporally typed data, based on <a href="unknown">unknown.jl</a>, and implementing the MLJ model interface.</p><p>From MLJ, the type can be imported using</p><pre><code class="nohighlight hljs">UnivariateTimeTypeToContinuous = @load UnivariateTimeTypeToContinuous pkg=unknown</code></pre><p>Do <code>model = UnivariateTimeTypeToContinuous()</code> to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in <code>UnivariateTimeTypeToContinuous(zero_time=...)</code>.</p><p>Use this model to convert vectors with a <code>TimeType</code> element type to vectors of <code>Float64</code> type (<code>Continuous</code> element scitype).</p><p><strong>Training data</strong></p><p>In MLJ or MLJBase, bind an instance <code>model</code> to data with</p><pre><code class="nohighlight hljs">mach = machine(model, x)</code></pre><p>where</p><ul><li><code>x</code>: any abstract vector whose element type is a subtype of <code>Dates.TimeType</code></li></ul><p>Train the machine using <code>fit!(mach, rows=...)</code>.</p><p><strong>Hyper-parameters</strong></p><ul><li><p><code>zero_time</code>: the time that is to correspond to 0.0 under transformations, with the type coinciding with the training data element type. If unspecified, the earliest time encountered in training is used.</p></li><li><p><code>step::Period=Hour(24)</code>: time interval to correspond to one unit under transformation</p></li></ul><p><strong>Operations</strong></p><ul><li><code>transform(mach, xnew)</code>: apply the encoding inferred when <code>mach</code> was fit</li></ul><p><strong>Fitted parameters</strong></p><p><code>fitted_params(mach).fitresult</code> is the tuple <code>(zero_time, step)</code> actually used in transformations, which may differ from the user-specified hyper-parameters.</p><p><strong>Example</strong></p><pre><code class="nohighlight hljs">using MLJ
using Dates

x = [Date(2001, 1, 1) + Day(i) for i in 0:4]

encoder = UnivariateTimeTypeToContinuous(zero_time=Date(2000, 1, 1),
                                         step=Week(1))

mach = machine(encoder, x)
fit!(mach)
julia&gt; transform(mach, x)
5-element Vector{Float64}:
 52.285714285714285
 52.42857142857143
 52.57142857142857
 52.714285714285715
 52.857142</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJTransforms.jl/blob/v0.1.1/src/transformers/other_transformers/univariate_time_type_to_continuous.jl#L134-L149">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="MLJTransforms.OneHotEncoder" href="#MLJTransforms.OneHotEncoder"><code>MLJTransforms.OneHotEncoder</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">OneHotEncoder</code></pre><p>A model type for constructing a one-hot encoder, based on <a href="unknown">unknown.jl</a>, and implementing the MLJ model interface.</p><p>From MLJ, the type can be imported using</p><pre><code class="nohighlight hljs">OneHotEncoder = @load OneHotEncoder pkg=unknown</code></pre><p>Do <code>model = OneHotEncoder()</code> to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in <code>OneHotEncoder(features=...)</code>.</p><p>Use this model to one-hot encode the <code>Multiclass</code> and <code>OrderedFactor</code> features (columns) of some table, leaving other columns unchanged.</p><p>New data to be transformed may lack features present in the fit data, but no <em>new</em> features can be present.</p><p><strong>Warning:</strong> This transformer assumes that <code>levels(col)</code> for any <code>Multiclass</code> or <code>OrderedFactor</code> column, <code>col</code>, is the same for training data and new data to be transformed.</p><p>To ensure <em>all</em> features are transformed into <code>Continuous</code> features, or dropped, use <a href="#MLJTransforms.ContinuousEncoder"><code>ContinuousEncoder</code></a> instead.</p><p><strong>Training data</strong></p><p>In MLJ or MLJBase, bind an instance <code>model</code> to data with</p><pre><code class="nohighlight hljs">mach = machine(model, X)</code></pre><p>where</p><ul><li><code>X</code>: any Tables.jl compatible table. Columns can be of mixed type but only those with element scitype <code>Multiclass</code> or <code>OrderedFactor</code> can be encoded. Check column scitypes with <code>schema(X)</code>.</li></ul><p>Train the machine using <code>fit!(mach, rows=...)</code>.</p><p><strong>Hyper-parameters</strong></p><ul><li><p><code>features</code>: a vector of symbols (feature names). If empty (default) then all <code>Multiclass</code> and <code>OrderedFactor</code> features are encoded. Otherwise, encoding is further restricted to the specified features (<code>ignore=false</code>) or the unspecified features (<code>ignore=true</code>). This default behavior can be modified by the <code>ordered_factor</code> flag.</p></li><li><p><code>ordered_factor=false</code>: when <code>true</code>, <code>OrderedFactor</code> features are universally excluded</p></li><li><p><code>drop_last=true</code>: whether to drop the column corresponding to the final class of encoded features. For example, a three-class feature is spawned into three new features if <code>drop_last=false</code>, but just two features otherwise.</p></li></ul><p><strong>Fitted parameters</strong></p><p>The fields of <code>fitted_params(mach)</code> are:</p><ul><li><p><code>all_features</code>: names of all features encountered in training</p></li><li><p><code>fitted_levels_given_feature</code>: dictionary of the levels associated with each feature encoded, keyed on the feature name</p></li><li><p><code>ref_name_pairs_given_feature</code>: dictionary of pairs <code>r =&gt; ftr</code> (such as <code>0x00000001 =&gt; :grad__A</code>) where <code>r</code> is a CategoricalArrays.jl reference integer representing a level, and <code>ftr</code> the corresponding new feature name; the dictionary is keyed on the names of features that are encoded</p></li></ul><p><strong>Report</strong></p><p>The fields of <code>report(mach)</code> are:</p><ul><li><p><code>features_to_be_encoded</code>: names of input features to be encoded</p></li><li><p><code>new_features</code>: names of all output features</p></li></ul><p><strong>Example</strong></p><pre><code class="nohighlight hljs">using MLJ

X = (name=categorical([&quot;Danesh&quot;, &quot;Lee&quot;, &quot;Mary&quot;, &quot;John&quot;]),
     grade=categorical([&quot;A&quot;, &quot;B&quot;, &quot;A&quot;, &quot;C&quot;], ordered=true),
     height=[1.85, 1.67, 1.5, 1.67],
     n_devices=[3, 2, 4, 3])

julia&gt; schema(X)
┌───────────┬──────────────────┐
│ names     │ scitypes         │
├───────────┼──────────────────┤
│ name      │ Multiclass{4}    │
│ grade     │ OrderedFactor{3} │
│ height    │ Continuous       │
│ n_devices │ Count            │
└───────────┴──────────────────┘

hot = OneHotEncoder(drop_last=true)
mach = fit!(machine(hot, X))
W = transform(mach, X)

julia&gt; schema(W)
┌──────────────┬────────────┐
│ names        │ scitypes   │
├──────────────┼────────────┤
│ name__Danesh │ Continuous │
│ name__John   │ Continuous │
│ name__Lee    │ Continuous │
│ grade__A     │ Continuous │
│ grade__B     │ Continuous │
│ height       │ Continuous │
│ n_devices    │ Count      │
└──────────────┴────────────┘</code></pre><p>See also <a href="#MLJTransforms.ContinuousEncoder"><code>ContinuousEncoder</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJTransforms.jl/blob/v0.1.1/src/transformers/other_transformers/one_hot_encoder.jl#L168-L183">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="MLJTransforms.ContinuousEncoder" href="#MLJTransforms.ContinuousEncoder"><code>MLJTransforms.ContinuousEncoder</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">ContinuousEncoder</code></pre><p>A model type for constructing a continuous encoder, based on <a href="unknown">unknown.jl</a>, and implementing the MLJ model interface.</p><p>From MLJ, the type can be imported using</p><pre><code class="nohighlight hljs">ContinuousEncoder = @load ContinuousEncoder pkg=unknown</code></pre><p>Do <code>model = ContinuousEncoder()</code> to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in <code>ContinuousEncoder(drop_last=...)</code>.</p><p>Use this model to arrange all features (features) of a table to have <code>Continuous</code> element scitype, by applying the following protocol to each feature <code>ftr</code>:</p><ul><li><p>If <code>ftr</code> is already <code>Continuous</code> retain it.</p></li><li><p>If <code>ftr</code> is <code>Multiclass</code>, one-hot encode it.</p></li><li><p>If <code>ftr</code> is <code>OrderedFactor</code>, replace it with <code>coerce(ftr, Continuous)</code> (vector of floating point integers), unless <code>ordered_factors=false</code> is specified, in which case one-hot encode it.</p></li><li><p>If <code>ftr</code> is <code>Count</code>, replace it with <code>coerce(ftr, Continuous)</code>.</p></li><li><p>If <code>ftr</code> has some other element scitype, or was not observed in fitting the encoder, drop it from the table.</p></li></ul><p><strong>Warning:</strong> This transformer assumes that <code>levels(col)</code> for any <code>Multiclass</code> or <code>OrderedFactor</code> column, <code>col</code>, is the same for training data and new data to be transformed.</p><p>To selectively one-hot-encode categorical features (without dropping features) use <a href="#MLJTransforms.OneHotEncoder"><code>OneHotEncoder</code></a> instead.</p><p><strong>Training data</strong></p><p>In MLJ or MLJBase, bind an instance <code>model</code> to data with</p><pre><code class="nohighlight hljs">mach = machine(model, X)</code></pre><p>where</p><ul><li><code>X</code>: any Tables.jl compatible table. features can be of mixed type but only those with element scitype <code>Multiclass</code> or <code>OrderedFactor</code> can be encoded. Check column scitypes with <code>schema(X)</code>.</li></ul><p>Train the machine using <code>fit!(mach, rows=...)</code>.</p><p><strong>Hyper-parameters</strong></p><ul><li><p><code>drop_last=true</code>: whether to drop the column corresponding to the final class of one-hot encoded features. For example, a three-class feature is spawned into three new features if <code>drop_last=false</code>, but two just features otherwise.</p></li><li><p><code>one_hot_ordered_factors=false</code>: whether to one-hot any feature with <code>OrderedFactor</code> element scitype, or to instead coerce it directly to a (single) <code>Continuous</code> feature using the order</p></li></ul><p><strong>Fitted parameters</strong></p><p>The fields of <code>fitted_params(mach)</code> are:</p><ul><li><p><code>features_to_keep</code>: names of features that will not be dropped from the table</p></li><li><p><code>one_hot_encoder</code>: the <code>OneHotEncoder</code> model instance for handling the one-hot encoding</p></li><li><p><code>one_hot_encoder_fitresult</code>: the fitted parameters of the <code>OneHotEncoder</code> model</p></li></ul><p><strong>Report</strong></p><ul><li><p><code>features_to_keep</code>: names of input features that will not be dropped from the table</p></li><li><p><code>new_features</code>: names of all output features</p></li></ul><p><strong>Example</strong></p><pre><code class="language-julia hljs">X = (name=categorical([&quot;Danesh&quot;, &quot;Lee&quot;, &quot;Mary&quot;, &quot;John&quot;]),
     grade=categorical([&quot;A&quot;, &quot;B&quot;, &quot;A&quot;, &quot;C&quot;], ordered=true),
     height=[1.85, 1.67, 1.5, 1.67],
     n_devices=[3, 2, 4, 3],
     comments=[&quot;the force&quot;, &quot;be&quot;, &quot;with you&quot;, &quot;too&quot;])

julia&gt; schema(X)
┌───────────┬──────────────────┐
│ names     │ scitypes         │
├───────────┼──────────────────┤
│ name      │ Multiclass{4}    │
│ grade     │ OrderedFactor{3} │
│ height    │ Continuous       │
│ n_devices │ Count            │
│ comments  │ Textual          │
└───────────┴──────────────────┘

encoder = ContinuousEncoder(drop_last=true)
mach = fit!(machine(encoder, X))
W = transform(mach, X)

julia&gt; schema(W)
┌──────────────┬────────────┐
│ names        │ scitypes   │
├──────────────┼────────────┤
│ name__Danesh │ Continuous │
│ name__John   │ Continuous │
│ name__Lee    │ Continuous │
│ grade        │ Continuous │
│ height       │ Continuous │
│ n_devices    │ Continuous │
└──────────────┴────────────┘

julia&gt; setdiff(schema(X).names, report(mach).features_to_keep) # dropped features
1-element Vector{Symbol}:
 :comments
</code></pre><p>See also <a href="#MLJTransforms.OneHotEncoder"><code>OneHotEncoder</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJTransforms.jl/blob/v0.1.1/src/transformers/other_transformers/continuous_encoder.jl#L78-L93">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="MLJTransforms.OrdinalEncoder" href="#MLJTransforms.OrdinalEncoder"><code>MLJTransforms.OrdinalEncoder</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">OrdinalEncoder</code></pre><p>A model type for constructing a ordinal encoder, based on <a href="https://github.com/JuliaAI/MLJTransforms.jl">MLJTransforms.jl</a>, and implementing the MLJ model interface.</p><p>From MLJ, the type can be imported using</p><pre><code class="nohighlight hljs">OrdinalEncoder = @load OrdinalEncoder pkg=MLJTransforms</code></pre><p>Do <code>model = OrdinalEncoder()</code> to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in <code>OrdinalEncoder(features=...)</code>.</p><p><code>OrdinalEncoder</code> implements ordinal encoding which replaces the categorical values in the specified     categorical features with integers (ordered arbitrarily). This will create an implicit ordering between     categories which may not be a proper modelling assumption.</p><p><strong>Training data</strong></p><p>In MLJ (or MLJBase) bind an instance unsupervised <code>model</code> to data with</p><pre><code class="nohighlight hljs">mach = machine(model, X)</code></pre><p>Here:</p><ul><li><code>X</code> is any table of input features (eg, a <code>DataFrame</code>). Features to be transformed must  have element scitype <code>Multiclass</code> or <code>OrderedFactor</code>. Use <code>schema(X)</code> to   check scitypes. </li></ul><p>Train the machine using <code>fit!(mach, rows=...)</code>.</p><p><strong>Hyper-parameters</strong></p><ul><li><p>features=[]: A list of names of categorical features given as symbols to exclude or include from encoding, according to the value of <code>ignore</code>, or a single symbol (which is treated as a vector with one symbol), or a callable that returns true for features to be included/excluded</p></li><li><p>ignore=true: Whether to exclude or include the features given in <code>features</code></p></li><li><p>ordered_factor=false: Whether to encode <code>OrderedFactor</code> or ignore them</p></li><li><p><code>output_type</code>: The numerical concrete type of the encoded features. Default is <code>Float32</code>.</p></li></ul><p><strong>Operations</strong></p><ul><li><code>transform(mach, Xnew)</code>: Apply ordinal encoding to selected <code>Multiclass</code> or <code>OrderedFactor features of</code>Xnew<code>specified by hyper-parameters, and   return the new table.   Features that are neither</code>Multiclass<code>nor</code>OrderedFactor`  are always left unchanged.</li></ul><p><strong>Fitted parameters</strong></p><p>The fields of <code>fitted_params(mach)</code> are:</p><ul><li><code>index_given_feat_level</code>: A dictionary that maps each level for each column in a subset of the categorical features of X into an integer. </li></ul><p><strong>Report</strong></p><p>The fields of <code>report(mach)</code> are:</p><ul><li>encoded_features: The subset of the categorical features of <code>X</code> that were encoded</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using MLJ

# Define categorical features
A = [&quot;g&quot;, &quot;b&quot;, &quot;g&quot;, &quot;r&quot;, &quot;r&quot;,]  
B = [1.0, 2.0, 3.0, 4.0, 5.0,]
C = [&quot;f&quot;, &quot;f&quot;, &quot;f&quot;, &quot;m&quot;, &quot;f&quot;,]  
D = [true, false, true, false, true,]
E = [1, 2, 3, 4, 5,]

# Combine into a named tuple
X = (A = A, B = B, C = C, D = D, E = E)

# Coerce A, C, D to multiclass and B to continuous and E to ordinal
X = coerce(X,
:A =&gt; Multiclass,
:B =&gt; Continuous,
:C =&gt; Multiclass,
:D =&gt; Multiclass,
:E =&gt; OrderedFactor,
)

# Check scitype coercion:
schema(X)

encoder = OrdinalEncoder(ordered_factor = false)
mach = fit!(machine(encoder, X))
Xnew = transform(mach, X)

julia &gt; Xnew
    (A = [2, 1, 2, 3, 3],
    B = [1.0, 2.0, 3.0, 4.0, 5.0],
    C = [1, 1, 1, 2, 1],
    D = [2, 1, 2, 1, 2],
    E = CategoricalArrays.CategoricalValue{Int64, UInt32}[1, 2, 3, 4, 5],)</code></pre><p>See also <a href="#MLJTransforms.TargetEncoder"><code>TargetEncoder</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJTransforms.jl/blob/v0.1.1/src/encoders/ordinal_encoding/interface_mlj.jl#L70-L94">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="MLJTransforms.FrequencyEncoder" href="#MLJTransforms.FrequencyEncoder"><code>MLJTransforms.FrequencyEncoder</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">FrequencyEncoder</code></pre><p>A model type for constructing a frequency encoder, based on <a href="https://github.com/JuliaAI/MLJTransforms.jl">MLJTransforms.jl</a>, and implementing the MLJ model interface.</p><p>From MLJ, the type can be imported using</p><pre><code class="nohighlight hljs">FrequencyEncoder = @load FrequencyEncoder pkg=MLJTransforms</code></pre><p>Do <code>model = FrequencyEncoder()</code> to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in <code>FrequencyEncoder(features=...)</code>.</p><p><code>FrequencyEncoder</code> implements frequency encoding which replaces the categorical values in the specified     categorical features with their (normalized or raw) frequencies of occurrence in the dataset. </p><p><strong>Training data</strong></p><p>In MLJ (or MLJBase) bind an instance unsupervised <code>model</code> to data with</p><pre><code class="nohighlight hljs">mach = machine(model, X)</code></pre><p>Here:</p><ul><li><code>X</code> is any table of input features (eg, a <code>DataFrame</code>). Features to be transformed must  have element scitype <code>Multiclass</code> or <code>OrderedFactor</code>. Use <code>schema(X)</code> to   check scitypes. </li></ul><p>Train the machine using <code>fit!(mach, rows=...)</code>.</p><p><strong>Hyper-parameters</strong></p><ul><li><p>features=[]: A list of names of categorical features given as symbols to exclude or include from encoding, according to the value of <code>ignore</code>, or a single symbol (which is treated as a vector with one symbol), or a callable that returns true for features to be included/excluded</p></li><li><p>ignore=true: Whether to exclude or include the features given in <code>features</code></p></li><li><p>ordered_factor=false: Whether to encode <code>OrderedFactor</code> or ignore them</p></li><li><p><code>normalize=false</code>: Whether to use normalized frequencies that sum to 1 over category values or to use raw counts.</p></li><li><p><code>output_type=Float32</code>: The type of the output values. The default is <code>Float32</code>, but you can set it to <code>Float64</code> or any other type that can hold the frequency values.</p></li></ul><p><strong>Operations</strong></p><ul><li><code>transform(mach, Xnew)</code>: Apply frequency encoding to selected <code>Multiclass</code> or <code>OrderedFactor features of</code>Xnew<code>specified by hyper-parameters, and   return the new table.   Features that are neither</code>Multiclass<code>nor</code>OrderedFactor`  are always left unchanged.</li></ul><p><strong>Fitted parameters</strong></p><p>The fields of <code>fitted_params(mach)</code> are:</p><ul><li><code>statistic_given_feat_val</code>: A dictionary that maps each level for each column in a subset of the categorical features of X into its frequency.</li></ul><p><strong>Report</strong></p><p>The fields of <code>report(mach)</code> are:</p><ul><li>encoded_features: The subset of the categorical features of <code>X</code> that were encoded</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using MLJ

# Define categorical features
A = [&quot;g&quot;, &quot;b&quot;, &quot;g&quot;, &quot;r&quot;, &quot;r&quot;,]  
B = [1.0, 2.0, 3.0, 4.0, 5.0,]
C = [&quot;f&quot;, &quot;f&quot;, &quot;f&quot;, &quot;m&quot;, &quot;f&quot;,]  
D = [true, false, true, false, true,]
E = [1, 2, 3, 4, 5,]

# Combine into a named tuple
X = (A = A, B = B, C = C, D = D, E = E)

# Coerce A, C, D to multiclass and B to continuous and E to ordinal
X = coerce(X,
:A =&gt; Multiclass,
:B =&gt; Continuous,
:C =&gt; Multiclass,
:D =&gt; Multiclass,
:E =&gt; OrderedFactor,
)

# Check scitype coercions:
schema(X)

encoder = FrequencyEncoder(ordered_factor = false, normalize=true)
mach = fit!(machine(encoder, X))
Xnew = transform(mach, X)

julia &gt; Xnew
    (A = [2, 1, 2, 2, 2],
    B = [1.0, 2.0, 3.0, 4.0, 5.0],
    C = [4, 4, 4, 1, 4],
    D = [3, 2, 3, 2, 3],
    E = CategoricalArrays.CategoricalValue{Int64, UInt32}[1, 2, 3, 4, 5],)</code></pre><p>See also <a href="#MLJTransforms.TargetEncoder"><code>TargetEncoder</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJTransforms.jl/blob/v0.1.1/src/encoders/frequency_encoding/interface_mlj.jl#L75-L99">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="MLJTransforms.TargetEncoder" href="#MLJTransforms.TargetEncoder"><code>MLJTransforms.TargetEncoder</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">TargetEncoder</code></pre><p>A model type for constructing a target encoder, based on <a href="https://github.com/JuliaAI/MLJTransforms.jl">MLJTransforms.jl</a>, and implementing the MLJ model interface.</p><p>From MLJ, the type can be imported using</p><pre><code class="nohighlight hljs">TargetEncoder = @load TargetEncoder pkg=MLJTransforms</code></pre><p>Do <code>model = TargetEncoder()</code> to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in <code>TargetEncoder(features=...)</code>.</p><p><code>TargetEncoder</code> implements target encoding as defined in [1] to encode categorical variables      into continuous ones using statistics from the target variable.</p><p><strong>Training data</strong></p><p>In MLJ (or MLJBase) bind an instance <code>model</code> to data with</p><pre><code class="nohighlight hljs">mach = machine(model, X, y)</code></pre><p>Here:</p><ul><li><code>X</code> is any table of input features (eg, a <code>DataFrame</code>). Features to be transformed must  have element scitype <code>Multiclass</code> or <code>OrderedFactor</code>. Use <code>schema(X)</code> to   check scitypes. </li></ul><ul><li><code>y</code> is the target, which can be any <code>AbstractVector</code> whose element scitype is <code>Continuous</code> or <code>Count</code> for regression problems and  <code>Multiclass</code> or <code>OrderedFactor</code> for classification problems; check the scitype with <code>schema(y)</code></li></ul><p>Train the machine using <code>fit!(mach, rows=...)</code>.</p><p><strong>Hyper-parameters</strong></p><ul><li><p>features=[]: A list of names of categorical features given as symbols to exclude or include from encoding, according to the value of <code>ignore</code>, or a single symbol (which is treated as a vector with one symbol), or a callable that returns true for features to be included/excluded</p></li><li><p>ignore=true: Whether to exclude or include the features given in <code>features</code></p></li><li><p>ordered_factor=false: Whether to encode <code>OrderedFactor</code> or ignore them</p></li><li><p><code>λ</code>: Shrinkage hyperparameter used to mix between posterior and prior statistics as described in [1]</p></li><li><p><code>m</code>: An integer hyperparameter to compute shrinkage as described in [1]. If <code>m=:auto</code> then m will be computed using</p></li></ul><p>empirical Bayes estimation as described in [1]</p><p><strong>Operations</strong></p><ul><li><code>transform(mach, Xnew)</code>: Apply target encoding to selected <code>Multiclass</code> or <code>OrderedFactor features of</code>Xnew<code>specified by hyper-parameters, and   return the new table.   Features that are neither</code>Multiclass<code>nor</code>OrderedFactor`  are always left unchanged.</li></ul><p><strong>Fitted parameters</strong></p><p>The fields of <code>fitted_params(mach)</code> are:</p><ul><li><code>task</code>: Whether the task is <code>Classification</code> or <code>Regression</code></li><li><code>y_statistic_given_feat_level</code>: A dictionary with the necessary statistics to encode each categorical feature. It maps each    level in each categorical feature to a statistic computed over the target.</li></ul><p><strong>Report</strong></p><p>The fields of <code>report(mach)</code> are:</p><ul><li>encoded_features: The subset of the categorical features of <code>X</code> that were encoded</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using MLJ

# Define categorical features
A = [&quot;g&quot;, &quot;b&quot;, &quot;g&quot;, &quot;r&quot;, &quot;r&quot;,]  
B = [1.0, 2.0, 3.0, 4.0, 5.0,]
C = [&quot;f&quot;, &quot;f&quot;, &quot;f&quot;, &quot;m&quot;, &quot;f&quot;,]  
D = [true, false, true, false, true,]
E = [1, 2, 3, 4, 5,]

# Define the target variable 
y = [&quot;c1&quot;, &quot;c2&quot;, &quot;c3&quot;, &quot;c1&quot;, &quot;c2&quot;,]

# Combine into a named tuple
X = (A = A, B = B, C = C, D = D, E = E)

# Coerce A, C, D to multiclass and B to continuous and E to ordinal
X = coerce(X,
:A =&gt; Multiclass,
:B =&gt; Continuous,
:C =&gt; Multiclass,
:D =&gt; Multiclass,
:E =&gt; OrderedFactor,
)
y = coerce(y, Multiclass)

encoder = TargetEncoder(ordered_factor = false, lambda = 1.0, m = 0,)
mach = fit!(machine(encoder, X, y))
Xnew = transform(mach, X)

julia &gt; schema(Xnew)
┌───────┬──────────────────┬─────────────────────────────────┐
│ names │ scitypes         │ types                           │
├───────┼──────────────────┼─────────────────────────────────┤
│ A_1   │ Continuous       │ Float64                         │
│ A_2   │ Continuous       │ Float64                         │
│ A_3   │ Continuous       │ Float64                         │
│ B     │ Continuous       │ Float64                         │
│ C_1   │ Continuous       │ Float64                         │
│ C_2   │ Continuous       │ Float64                         │
│ C_3   │ Continuous       │ Float64                         │
│ D_1   │ Continuous       │ Float64                         │
│ D_2   │ Continuous       │ Float64                         │
│ D_3   │ Continuous       │ Float64                         │
│ E     │ OrderedFactor{5} │ CategoricalValue{Int64, UInt32} │
└───────┴──────────────────┴─────────────────────────────────┘</code></pre><p><strong>Reference</strong></p><p>[1] Micci-Barreca, Daniele.      “A preprocessing scheme for high-cardinality categorical attributes in classification and prediction problems”      SIGKDD Explor. Newsl. 3, 1 (July 2001), 27–32.</p><p>See also <a href="#MLJTransforms.OneHotEncoder"><code>OneHotEncoder</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJTransforms.jl/blob/v0.1.1/src/encoders/target_encoding/interface_mlj.jl#L120-L144">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="MLJTransforms.ContrastEncoder" href="#MLJTransforms.ContrastEncoder"><code>MLJTransforms.ContrastEncoder</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">ContrastEncoder</code></pre><p>A model type for constructing a contrast encoder, based on <a href="https://github.com/JuliaAI/MLJTransforms.jl">MLJTransforms.jl</a>, and implementing the MLJ model interface.</p><p>From MLJ, the type can be imported using</p><pre><code class="nohighlight hljs">ContrastEncoder = @load ContrastEncoder pkg=MLJTransforms</code></pre><p>Do <code>model = ContrastEncoder()</code> to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in <code>ContrastEncoder(features=...)</code>.</p><p><code>ContrastEncoder</code> implements the following contrast encoding methods for  categorical features: dummy, sum, backward/forward difference, and Helmert coding.  More generally, users can specify a custom contrast or hypothesis matrix, and each feature  can be encoded using a different method.</p><p><strong>Training data</strong></p><p>In MLJ (or MLJBase) bind an instance unsupervised <code>model</code> to data with</p><pre><code class="nohighlight hljs">mach = machine(model, X)</code></pre><p>Here:</p><ul><li><code>X</code> is any table of input features (eg, a <code>DataFrame</code>). Features to be transformed must  have element scitype <code>Multiclass</code> or <code>OrderedFactor</code>. Use <code>schema(X)</code> to   check scitypes. </li></ul><p>Train the machine using <code>fit!(mach, rows=...)</code>.</p><p><strong>Hyper-parameters</strong></p><ul><li><p>features=[]: A list of names of categorical features given as symbols to exclude or include from encoding, according to the value of <code>ignore</code>, or a single symbol (which is treated as a vector with one symbol), or a callable that returns true for features to be included/excluded</p></li><li><p><code>mode=:dummy</code>: The type of encoding to use. Can be one of <code>:contrast</code>, <code>:dummy</code>, <code>:sum</code>, <code>:backward_diff</code>, <code>:forward_diff</code>, <code>:helmert</code> or <code>:hypothesis</code>.</p></li></ul><p>If <code>ignore=false</code> (features to be encoded are listed explictly in <code>features</code>), then this can be a vector of the same length as <code>features</code> to specify a different contrast encoding scheme for each feature</p><ul><li><code>buildmatrix=nothing</code>: A function or other callable with signature <code>buildmatrix(colname, k)</code>, </li></ul><p>where <code>colname</code> is the name of the feature levels and <code>k</code> is it&#39;s length, and which returns contrast or  hypothesis matrix with row/column ordering consistent with the ordering of <code>levels(col)</code>. Only relevant if <code>mode</code> is <code>:contrast</code> or <code>:hypothesis</code>.</p><ul><li><p>ignore=true: Whether to exclude or include the features given in <code>features</code></p></li><li><p>ordered_factor=false: Whether to encode <code>OrderedFactor</code> or ignore them</p></li></ul><p><strong>Operations</strong></p><ul><li><code>transform(mach, Xnew)</code>: Apply contrast encoding to selected <code>Multiclass</code> or <code>OrderedFactor features of</code>Xnew<code>specified by hyper-parameters, and   return the new table. Features that are neither</code>Multiclass<code>nor</code>OrderedFactor`  are always left unchanged.</li></ul><p><strong>Fitted parameters</strong></p><p>The fields of <code>fitted_params(mach)</code> are:</p><ul><li><code>vector_given_value_given_feature</code>: A dictionary that maps each level for each column in a subset of the categorical features of X into its frequency.</li></ul><p><strong>Report</strong></p><p>The fields of <code>report(mach)</code> are:</p><ul><li>encoded_features: The subset of the categorical features of <code>X</code> that were encoded</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using MLJ

# Define categorical dataset
X = (
    name   = categorical([&quot;Ben&quot;, &quot;John&quot;, &quot;Mary&quot;, &quot;John&quot;]),
    height = [1.85, 1.67, 1.5, 1.67],
    favnum = categorical([7, 5, 10, 1]),
    age    = [23, 23, 14, 23],
)

# Check scitype coercions:
schema(X)

encoder =  ContrastEncoder(
    features = [:name, :favnum],
    ignore = false, 
    mode = [:dummy, :helmert],
)
mach = fit!(machine(encoder, X))
Xnew = transform(mach, X)

julia &gt; Xnew
    (name_John = [1.0, 0.0, 0.0, 0.0],
    name_Mary = [0.0, 1.0, 0.0, 1.0],
    height = [1.85, 1.67, 1.5, 1.67],
    favnum_5 = [0.0, 1.0, 0.0, -1.0],
    favnum_7 = [2.0, -1.0, 0.0, -1.0],
    favnum_10 = [-1.0, -1.0, 3.0, -1.0],
    age = [23, 23, 14, 23],)</code></pre><p>See also <a href="#MLJTransforms.OneHotEncoder"><code>OneHotEncoder</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJTransforms.jl/blob/v0.1.1/src/encoders/contrast_encoder/interface_mlj.jl#L73-L97">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="MLJTransforms.CardinalityReducer" href="#MLJTransforms.CardinalityReducer"><code>MLJTransforms.CardinalityReducer</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">CardinalityReducer</code></pre><p>A model type for constructing a cardinality reducer, based on <a href="https://github.com/JuliaAI/MLJTransforms.jl">MLJTransforms.jl</a>, and implementing the MLJ model interface.</p><p>From MLJ, the type can be imported using</p><pre><code class="nohighlight hljs">CardinalityReducer = @load CardinalityReducer pkg=MLJTransforms</code></pre><p>Do <code>model = CardinalityReducer()</code> to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in <code>CardinalityReducer(features=...)</code>.</p><p><code>CardinalityReducer</code> maps any level of a categorical feature that occurs with frequency &lt; <code>min_frequency</code> into a new level (e.g., &quot;Other&quot;). This is useful when some categorical features have high cardinality and many levels are infrequent. This assumes that the categorical features have raw types that are in <code>Union{AbstractString, Char, Number}</code>.</p><p><strong>Training data</strong></p><p>In MLJ (or MLJBase) bind an instance unsupervised <code>model</code> to data with</p><pre><code class="nohighlight hljs">mach = machine(model, X)</code></pre><p>Here:</p><ul><li><code>X</code> is any table of input features (eg, a <code>DataFrame</code>). Features to be transformed must  have element scitype <code>Multiclass</code> or <code>OrderedFactor</code>. Use <code>schema(X)</code> to   check scitypes. </li></ul><p>Train the machine using <code>fit!(mach, rows=...)</code>.</p><p><strong>Hyper-parameters</strong></p><ul><li><p>features=[]: A list of names of categorical features given as symbols to exclude or include from encoding, according to the value of <code>ignore</code>, or a single symbol (which is treated as a vector with one symbol), or a callable that returns true for features to be included/excluded</p></li><li><p>ignore=true: Whether to exclude or include the features given in <code>features</code></p></li><li><p>ordered_factor=false: Whether to encode <code>OrderedFactor</code> or ignore them</p></li><li><p><code>min_frequency::Real=3</code>: Any level of a categorical feature that occurs with frequency &lt; <code>min_frequency</code> will be mapped to a new level. Could be</p></li></ul><p>an integer or a float which decides whether raw counts or normalized frequencies are used.</p><ul><li><code>label_for_infrequent::Dict{&lt;:Type, &lt;:Any}()= Dict( AbstractString =&gt; &quot;Other&quot;, Char =&gt; &#39;O&#39;, )</code>: A</li></ul><p>dictionary where the possible values for keys are the types in <code>Char</code>, <code>AbstractString</code>, and <code>Number</code> and each value signifies the new level to map into given a column raw super type. By default, if the raw type of the column subtypes <code>AbstractString</code> then the new value is <code>&quot;Other&quot;</code> and if the raw type subtypes <code>Char</code> then the new value is <code>&#39;O&#39;</code> and if the raw type subtypes <code>Number</code> then the new value is the lowest value in the column - 1.</p><p><strong>Operations</strong></p><ul><li><code>transform(mach, Xnew)</code>: Apply cardinality reduction to selected <code>Multiclass</code> or <code>OrderedFactor</code> features of <code>Xnew</code> specified by hyper-parameters, and   return the new table.   Features that are neither <code>Multiclass</code> nor <code>OrderedFactor</code>  are always left unchanged.</li></ul><p><strong>Fitted parameters</strong></p><p>The fields of <code>fitted_params(mach)</code> are:</p><ul><li><code>new_cat_given_col_val</code>: A dictionary that maps each level in a   categorical feature to a new level (either itself or the new level specified in <code>label_for_infrequent</code>)</li></ul><p><strong>Report</strong></p><p>The fields of <code>report(mach)</code> are:</p><ul><li>encoded_features: The subset of the categorical features of <code>X</code> that were encoded</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">import StatsBase.proportionmap
using MLJ

# Define categorical features
A = [ [&quot;a&quot; for i in 1:100]..., &quot;b&quot;, &quot;b&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;]
B = [ [0 for i in 1:100]..., 1, 2, 3, 4, 4]

# Combine into a named tuple
X = (A = A, B = B)

# Coerce A, C, D to multiclass and B to continuous and E to ordinal
X = coerce(X,
:A =&gt; Multiclass,
:B =&gt; Multiclass
)

encoder = CardinalityReducer(ordered_factor = false, min_frequency=3)
mach = fit!(machine(encoder, X))
Xnew = transform(mach, X)

julia&gt; proportionmap(Xnew.A)
Dict{CategoricalArrays.CategoricalValue{String, UInt32}, Float64} with 3 entries:
  &quot;Other&quot; =&gt; 0.0190476
  &quot;b&quot;     =&gt; 0.0285714
  &quot;a&quot;     =&gt; 0.952381

julia&gt; proportionmap(Xnew.B)
Dict{CategoricalArrays.CategoricalValue{Int64, UInt32}, Float64} with 2 entries:
  0  =&gt; 0.952381
  -1 =&gt; 0.047619</code></pre><p>See also <a href="#MLJTransforms.FrequencyEncoder"><code>FrequencyEncoder</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJTransforms.jl/blob/v0.1.1/src/transformers/cardinality_reducer/interface_mlj.jl#L88-L112">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="MLJTransforms.MissingnessEncoder" href="#MLJTransforms.MissingnessEncoder"><code>MLJTransforms.MissingnessEncoder</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">MissingnessEncoder</code></pre><p>A model type for constructing a missingness encoder, based on <a href="https://github.com/JuliaAI/MLJTransforms.jl">MLJTransforms.jl</a>, and implementing the MLJ model interface.</p><p>From MLJ, the type can be imported using</p><pre><code class="nohighlight hljs">MissingnessEncoder = @load MissingnessEncoder pkg=MLJTransforms</code></pre><p>Do <code>model = MissingnessEncoder()</code> to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in <code>MissingnessEncoder(features=...)</code>.</p><p><code>MissingnessEncoder</code> maps any missing level of a categorical feature into a new level (e.g., &quot;Missing&quot;).  By this, missingness will be treated as a new level by any subsequent model. This assumes that the categorical features have raw types that are in <code>Char</code>, <code>AbstractString</code>, and <code>Number</code>.</p><p><strong>Training data</strong></p><p>In MLJ (or MLJBase) bind an instance unsupervised <code>model</code> to data with</p><pre><code class="nohighlight hljs">mach = machine(model, X)</code></pre><p>Here:</p><ul><li><code>X</code> is any table of input features (eg, a <code>DataFrame</code>). Features to be transformed must  have element scitype <code>Multiclass</code> or <code>OrderedFactor</code>. Use <code>schema(X)</code> to   check scitypes. </li></ul><p>Train the machine using <code>fit!(mach, rows=...)</code>.</p><p><strong>Hyper-parameters</strong></p><ul><li><p>features=[]: A list of names of categorical features given as symbols to exclude or include from encoding, according to the value of <code>ignore</code>, or a single symbol (which is treated as a vector with one symbol), or a callable that returns true for features to be included/excluded</p></li><li><p>ignore=true: Whether to exclude or include the features given in <code>features</code></p></li><li><p>ordered_factor=false: Whether to encode <code>OrderedFactor</code> or ignore them</p></li><li><p><code>label_for_missing::Dict{&lt;:Type, &lt;:Any}()= Dict( AbstractString =&gt; &quot;missing&quot;, Char =&gt; &#39;m&#39;, )</code>: A</p></li></ul><p>dictionary where the possible values for keys are the types in <code>Char</code>, <code>AbstractString</code>, and <code>Number</code> and where each value signifies the new level to map into given a column raw super type. By default, if the raw type of the column subtypes <code>AbstractString</code> then missing values will be replaced with <code>&quot;missing&quot;</code> and if the raw type subtypes <code>Char</code> then the new value is <code>&#39;m&#39;</code> and if the raw type subtypes <code>Number</code> then the new value is the lowest value in the column - 1.</p><p><strong>Operations</strong></p><ul><li><code>transform(mach, Xnew)</code>: Apply cardinality reduction to selected <code>Multiclass</code> or <code>OrderedFactor</code> features of <code>Xnew</code> specified by hyper-parameters, and   return the new table.   Features that are neither <code>Multiclass</code> nor <code>OrderedFactor</code>  are always left unchanged.</li></ul><p><strong>Fitted parameters</strong></p><p>The fields of <code>fitted_params(mach)</code> are:</p><ul><li><code>label_for_missing_given_feature</code>: A dictionary that for each column, maps <code>missing</code> into some value according to <code>label_for_missing</code></li></ul><p><strong>Report</strong></p><p>The fields of <code>report(mach)</code> are:</p><ul><li>encoded_features: The subset of the categorical features of <code>X</code> that were encoded</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">import StatsBase.proportionmap
using MLJ

# Define a table with missing values
Xm = (
    A = categorical([&quot;Ben&quot;, &quot;John&quot;, missing, missing, &quot;Mary&quot;, &quot;John&quot;, missing]),
    B = [1.85, 1.67, missing, missing, 1.5, 1.67, missing],
    C= categorical([7, 5, missing, missing, 10, 0, missing]),
    D = [23, 23, 44, 66, 14, 23, 11],
    E = categorical([missing, &#39;g&#39;, &#39;r&#39;, missing, &#39;r&#39;, &#39;g&#39;, &#39;p&#39;])
)

encoder = MissingnessEncoder()
mach = fit!(machine(encoder, Xm))
Xnew = transform(mach, Xm)

julia&gt; Xnew
(A = [&quot;Ben&quot;, &quot;John&quot;, &quot;missing&quot;, &quot;missing&quot;, &quot;Mary&quot;, &quot;John&quot;, &quot;missing&quot;],
 B = Union{Missing, Float64}[1.85, 1.67, missing, missing, 1.5, 1.67, missing],
 C = [7, 5, -1, -1, 10, 0, -1],
 D = [23, 23, 44, 66, 14, 23, 11],
 E = [&#39;m&#39;, &#39;g&#39;, &#39;r&#39;, &#39;m&#39;, &#39;r&#39;, &#39;g&#39;, &#39;p&#39;],)
</code></pre><p>See also <a href="#MLJTransforms.CardinalityReducer"><code>CardinalityReducer</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJTransforms.jl/blob/v0.1.1/src/encoders/missingness_encoding/interface_mlj.jl#L77-L101">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../loading_model_code/">« Loading Model Code</a><a class="docs-footer-nextpage" href="../feature_selection/">Feature Selection »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.14.1 on <span class="colophon-date" title="Wednesday 10 September 2025 07:28">Wednesday 10 September 2025</span>. Using Julia version 1.11.6.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
