<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>NeuralNetworkClassifier · MLJ</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.svg" alt="MLJ logo"/></a><div class="docs-package-name"><span class="docs-autofit">MLJ</span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><a class="tocitem" href="../../model_browser/">Model Browser</a></li><li><a class="tocitem" href="../../about_mlj/">About MLJ</a></li><li><a class="tocitem" href="../../learning_mlj/">Learning MLJ</a></li><li><a class="tocitem" href="../../getting_started/">Getting Started</a></li><li><a class="tocitem" href="../../common_mlj_workflows/">Common MLJ Workflows</a></li><li><a class="tocitem" href="../../working_with_categorical_data/">Working with Categorical Data</a></li><li><a class="tocitem" href="../../model_search/">Model Search</a></li><li><a class="tocitem" href="../../loading_model_code/">Loading Model Code</a></li><li><a class="tocitem" href="../../machines/">Machines</a></li><li><a class="tocitem" href="../../evaluating_model_performance/">Evaluating Model Performance</a></li><li><a class="tocitem" href="../../performance_measures/">Performance Measures</a></li><li><a class="tocitem" href="../../weights/">Weights</a></li><li><a class="tocitem" href="../../tuning_models/">Tuning Models</a></li><li><a class="tocitem" href="../../learning_curves/">Learning Curves</a></li><li><a class="tocitem" href="../../preparing_data/">Preparing Data</a></li><li><a class="tocitem" href="../../transformers/">Transformers and Other Unsupervised models</a></li><li><a class="tocitem" href="../../more_on_probabilistic_predictors/">More on Probabilistic Predictors</a></li><li><a class="tocitem" href="../../composing_models/">Composing Models</a></li><li><a class="tocitem" href="../../linear_pipelines/">Linear Pipelines</a></li><li><a class="tocitem" href="../../target_transformations/">Target Transformations</a></li><li><a class="tocitem" href="../../homogeneous_ensembles/">Homogeneous Ensembles</a></li><li><a class="tocitem" href="../../model_stacking/">Model Stacking</a></li><li><a class="tocitem" href="../../learning_networks/">Learning Networks</a></li><li><a class="tocitem" href="../../controlling_iterative_models/">Controlling Iterative Models</a></li><li><a class="tocitem" href="../../generating_synthetic_data/">Generating Synthetic Data</a></li><li><a class="tocitem" href="../../openml_integration/">OpenML Integration</a></li><li><a class="tocitem" href="../../acceleration_and_parallelism/">Acceleration and Parallelism</a></li><li><a class="tocitem" href="../../simple_user_defined_models/">Simple User Defined Models</a></li><li><a class="tocitem" href="../../quick_start_guide_to_adding_models/">Quick-Start Guide to Adding Models</a></li><li><a class="tocitem" href="../../adding_models_for_general_use/">Adding Models for General Use</a></li><li><a class="tocitem" href="../../modifying_behavior/">Modifying Behavior</a></li><li><a class="tocitem" href="../../internals/">Internals</a></li><li><a class="tocitem" href="../../list_of_supported_models/">List of Supported Models</a></li><li><a class="tocitem" href="../../third_party_packages/">Third Party Packages</a></li><li><a class="tocitem" href="../../glossary/">Glossary</a></li><li><a class="tocitem" href="../../mlj_cheatsheet/">MLJ Cheatsheet</a></li><li><a class="tocitem" href="../../known_issues/">Known Issues</a></li><li><a class="tocitem" href="../../frequently_asked_questions/">FAQ</a></li><li><a class="tocitem" href="../../julia_blogpost/">Julia BlogPost</a></li><li><a class="tocitem" href="../../api/">Index of Methods</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>NeuralNetworkClassifier</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>NeuralNetworkClassifier</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/alan-turing-institute/MLJ.jl/blob/master/docs/src/models/NeuralNetworkClassifier_MLJFlux.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="NeuralNetworkClassifier_MLJFlux"><a class="docs-heading-anchor" href="#NeuralNetworkClassifier_MLJFlux">NeuralNetworkClassifier</a><a id="NeuralNetworkClassifier_MLJFlux-1"></a><a class="docs-heading-anchor-permalink" href="#NeuralNetworkClassifier_MLJFlux" title="Permalink"></a></h1><pre><code class="language-none">NeuralNetworkClassifier</code></pre><p>A model type for constructing a neural network classifier, based on <a href="https://github.com/alan-turing-institute/MLJFlux.jl">MLJFlux.jl</a>, and implementing the MLJ model interface.</p><p>From MLJ, the type can be imported using</p><pre><code class="language-none">NeuralNetworkClassifier = @load NeuralNetworkClassifier pkg=MLJFlux</code></pre><p>Do <code>model = NeuralNetworkClassifier()</code> to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in <code>NeuralNetworkClassifier(builder=...)</code>.</p><p><code>NeuralNetworkClassifier</code> is for training a data-dependent Flux.jl neural network for making probabilistic predictions of a <code>Multiclass</code> or <code>OrderedFactor</code> target, given a table of <code>Continuous</code> features. Users provide a recipe for constructing  the network, based on properties of the data that is encountered, by specifying  an appropriate <code>builder</code>. See MLJFlux documentation for more on builders.</p><h2 id="Training-data"><a class="docs-heading-anchor" href="#Training-data">Training data</a><a id="Training-data-1"></a><a class="docs-heading-anchor-permalink" href="#Training-data" title="Permalink"></a></h2><p>In MLJ or MLJBase, bind an instance <code>model</code> to data with</p><pre><code class="language-none">mach = machine(model, X, y)</code></pre><p>Here:</p><ul><li><code>X</code> is any table of input features (eg, a <code>DataFrame</code>) whose columns are of scitype <code>Continuous</code>; check column scitypes with <code>schema(X)</code>.</li><li><code>y</code> is the target, which can be any <code>AbstractVector</code> whose element scitype is <code>Multiclass</code> or <code>OrderedFactor</code>; check the scitype with <code>scitype(y)</code></li></ul><p>Train the machine with <code>fit!(mach, rows=...)</code>.</p><h2 id="Hyper-parameters"><a class="docs-heading-anchor" href="#Hyper-parameters">Hyper-parameters</a><a id="Hyper-parameters-1"></a><a class="docs-heading-anchor-permalink" href="#Hyper-parameters" title="Permalink"></a></h2><ul><li><p><code>builder=MLJFlux.Short()</code>: An MLJFlux builder that constructs a neural network. Possible  <code>builders</code> include: <code>MLJFlux.Linear</code>, <code>MLJFlux.Short</code>, and <code>MLJFlux.MLP</code>. See  MLJFlux.jl documentation for examples of user-defined builders. See also <code>finaliser</code>  below.</p></li><li><p><code>optimiser::Flux.Adam()</code>: A <code>Flux.Optimise</code> optimiser. The optimiser performs the updating of the weights of the network. For further reference, see <a href="https://fluxml.ai/Flux.jl/stable/training/optimisers/">the Flux optimiser documentation</a>. To choose a learning rate (the update rate of the optimizer), a good rule of thumb is to start out at <code>10e-3</code>, and tune using powers of 10 between <code>1</code> and <code>1e-7</code>.</p></li><li><p><code>loss=Flux.crossentropy</code>: The loss function which the network will optimize. Should be a function which can be called in the form <code>loss(yhat, y)</code>.  Possible loss functions are listed in <a href="https://fluxml.ai/Flux.jl/stable/models/losses/">the Flux loss function documentation</a>. For a classification task, the most natural loss functions are:</p><ul><li><code>Flux.crossentropy</code>: Standard multiclass classification loss, also known as the log loss.</li><li><code>Flux.logitcrossentopy</code>: Mathematically equal to crossentropy, but numerically more stable than finalising the outputs with <code>softmax</code> and then calculating crossentropy. You will need to specify <code>finaliser=identity</code> to remove MLJFlux&#39;s default softmax finaliser, and understand that the output of <code>predict</code> is then unnormalized (no longer probabilistic).</li><li><code>Flux.tversky_loss</code>: Used with imbalanced data to give more weight to false negatives.</li><li><code>Flux.focal_loss</code>: Used with highly imbalanced data. Weights harder examples more than easier examples.</li></ul><p>Currently MLJ measures are not supported values of <code>loss</code>.</p></li><li><p><code>epochs::Int=10</code>: The duration of training, in epochs. Typically, one epoch represents one pass through the complete the training dataset.</p></li><li><p><code>batch_size::int=1</code>: the batch size to be used for training, representing the number of samples per update of the network weights. Typically, batch size is between 8 and</p><ol><li>Increassing batch size may accelerate training if <code>acceleration=CUDALibs()</code> and a</li></ol><p>GPU is available.</p></li><li><p><code>lambda::Float64=0</code>: The strength of the weight regularization penalty. Can be any value in the range <code>[0, ∞)</code>.</p></li><li><p><code>alpha::Float64=0</code>: The L2/L1 mix of regularization, in the range <code>[0, 1]</code>. A value of 0 represents L2 regularization, and a value of 1 represents L1 regularization.</p></li><li><p><code>rng::Union{AbstractRNG, Int64}</code>: The random number generator or seed used during training.</p></li><li><p><code>optimizer_changes_trigger_retraining::Bool=false</code>: Defines what happens when re-fitting a machine if the associated optimiser has changed. If <code>true</code>, the associated machine will retrain from scratch on <code>fit!</code> call, otherwise it will not.</p></li><li><p><code>acceleration::AbstractResource=CPU1()</code>: Defines on what hardware training is done. For Training on GPU, use <code>CUDALibs()</code>.</p></li><li><p><code>finaliser=Flux.softmax</code>: The final activation function of the neural network (applied after the network defined by <code>builder</code>). Defaults to <code>Flux.softmax</code>.</p></li></ul><h2 id="Operations"><a class="docs-heading-anchor" href="#Operations">Operations</a><a id="Operations-1"></a><a class="docs-heading-anchor-permalink" href="#Operations" title="Permalink"></a></h2><ul><li><code>predict(mach, Xnew)</code>: return predictions of the target given new features <code>Xnew</code>, which should have the same scitype as <code>X</code> above. Predictions are probabilistic but uncalibrated.</li><li><code>predict_mode(mach, Xnew)</code>: Return the modes of the probabilistic predictions returned above.</li></ul><h2 id="Fitted-parameters"><a class="docs-heading-anchor" href="#Fitted-parameters">Fitted parameters</a><a id="Fitted-parameters-1"></a><a class="docs-heading-anchor-permalink" href="#Fitted-parameters" title="Permalink"></a></h2><p>The fields of <code>fitted_params(mach)</code> are:</p><ul><li><code>chain</code>: The trained &quot;chain&quot; (Flux.jl model), namely the series of layers,  functions, and activations which make up the neural network. This includes  the final layer specified by <code>finaliser</code> (eg, <code>softmax</code>).</li></ul><h2 id="Report"><a class="docs-heading-anchor" href="#Report">Report</a><a id="Report-1"></a><a class="docs-heading-anchor-permalink" href="#Report" title="Permalink"></a></h2><p>The fields of <code>report(mach)</code> are:</p><ul><li><code>training_losses</code>: A vector of training losses (penalised if <code>lambda != 0</code>) in  historical order, of length <code>epochs + 1</code>.  The first element is the pre-training loss.</li></ul><h2 id="Examples"><a class="docs-heading-anchor" href="#Examples">Examples</a><a id="Examples-1"></a><a class="docs-heading-anchor-permalink" href="#Examples" title="Permalink"></a></h2><p>In this example we build a classification model using the Iris dataset. This is a very basic example, using a default builder and no standardization.  For a more advanced illustration, see <a href="models/@ref"><code>NeuralNetworkRegressor</code></a> or <a href="models/@ref"><code>ImageClassifier</code></a>, and examples in the MLJFlux.jl documentation.</p><pre><code class="language-julia">using MLJ
using Flux
import RDatasets</code></pre><p>First, we can load the data:</p><pre><code class="language-julia">iris = RDatasets.dataset(&quot;datasets&quot;, &quot;iris&quot;);
y, X = unpack(iris, ==(:Species), rng=123); ## a vector and a table
NeuralNetworkClassifier = @load NeuralNetworkClassifier pkg=MLJFlux
clf = NeuralNetworkClassifier()</code></pre><p>Next, we can train the model:</p><pre><code class="language-julia">mach = machine(clf, X, y)
fit!(mach)</code></pre><p>We can train the model in an incremental fashion, altering the learning rate as we go, provided <code>optimizer_changes_trigger_retraining</code> is <code>false</code> (the default). Here, we also change the number of (total) iterations:</p><pre><code class="language-julia">clf.optimiser.eta = clf.optimiser.eta * 2
clf.epochs = clf.epochs + 5

fit!(mach, verbosity=2) ## trains 5 more epochs</code></pre><p>We can inspect the mean training loss using the <code>cross_entropy</code> function:</p><pre><code class="language-julia">training_loss = cross_entropy(predict(mach, X), y) |&gt; mean</code></pre><p>And we can access the Flux chain (model) using <code>fitted_params</code>:</p><pre><code class="language-julia">chain = fitted_params(mach).chain</code></pre><p>Finally, we can see how the out-of-sample performance changes over time, using MLJ&#39;s <code>learning_curve</code> function:</p><pre><code class="language-julia">r = range(clf, :epochs, lower=1, upper=200, scale=:log10)
curve = learning_curve(clf, X, y,
                     range=r,
                     resampling=Holdout(fraction_train=0.7),
                     measure=cross_entropy)
using Plots
plot(curve.parameter_values,
     curve.measurements,
     xlab=curve.parameter_name,
     xscale=curve.parameter_scale,
     ylab = &quot;Cross Entropy&quot;)
</code></pre><p>See also <a href="models/@ref"><code>ImageClassifier</code></a>.</p></article><nav class="docs-footer"><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Tuesday 14 February 2023 21:14">Tuesday 14 February 2023</span>. Using Julia version 1.8.5.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
