<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>NeuralNetworkRegressor · MLJ</title><meta name="title" content="NeuralNetworkRegressor · MLJ"/><meta property="og:title" content="NeuralNetworkRegressor · MLJ"/><meta property="twitter:title" content="NeuralNetworkRegressor · MLJ"/><meta name="description" content="Documentation for MLJ."/><meta property="og:description" content="Documentation for MLJ."/><meta property="twitter:description" content="Documentation for MLJ."/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.svg" alt="MLJ logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">MLJ</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><a class="tocitem" href="../../model_browser/">Model Browser</a></li><li><a class="tocitem" href="../../about_mlj/">About MLJ</a></li><li><a class="tocitem" href="../../learning_mlj/">Learning MLJ</a></li><li><a class="tocitem" href="../../getting_started/">Getting Started</a></li><li><a class="tocitem" href="../../common_mlj_workflows/">Common MLJ Workflows</a></li><li><a class="tocitem" href="../../working_with_categorical_data/">Working with Categorical Data</a></li><li><a class="tocitem" href="../../model_search/">Model Search</a></li><li><a class="tocitem" href="../../loading_model_code/">Loading Model Code</a></li><li><a class="tocitem" href="../../machines/">Machines</a></li><li><a class="tocitem" href="../../evaluating_model_performance/">Evaluating Model Performance</a></li><li><a class="tocitem" href="../../performance_measures/">Performance Measures</a></li><li><a class="tocitem" href="../../weights/">Weights</a></li><li><a class="tocitem" href="../../tuning_models/">Tuning Models</a></li><li><a class="tocitem" href="../../learning_curves/">Learning Curves</a></li><li><a class="tocitem" href="../../preparing_data/">Preparing Data</a></li><li><a class="tocitem" href="../../transformers/">Transformers and Other Unsupervised models</a></li><li><a class="tocitem" href="../../more_on_probabilistic_predictors/">More on Probabilistic Predictors</a></li><li><a class="tocitem" href="../../composing_models/">Composing Models</a></li><li><a class="tocitem" href="../../linear_pipelines/">Linear Pipelines</a></li><li><a class="tocitem" href="../../target_transformations/">Target Transformations</a></li><li><a class="tocitem" href="../../homogeneous_ensembles/">Homogeneous Ensembles</a></li><li><a class="tocitem" href="../../correcting_class_imbalance/">Correcting Class Imbalance</a></li><li><a class="tocitem" href="../../model_stacking/">Model Stacking</a></li><li><a class="tocitem" href="../../learning_networks/">Learning Networks</a></li><li><a class="tocitem" href="../../controlling_iterative_models/">Controlling Iterative Models</a></li><li><a class="tocitem" href="../../generating_synthetic_data/">Generating Synthetic Data</a></li><li><a class="tocitem" href="../../logging_workflows/">Logging Workflows</a></li><li><a class="tocitem" href="../../openml_integration/">OpenML Integration</a></li><li><a class="tocitem" href="../../acceleration_and_parallelism/">Acceleration and Parallelism</a></li><li><a class="tocitem" href="../../simple_user_defined_models/">Simple User Defined Models</a></li><li><a class="tocitem" href="../../quick_start_guide_to_adding_models/">Quick-Start Guide to Adding Models</a></li><li><a class="tocitem" href="../../adding_models_for_general_use/">Adding Models for General Use</a></li><li><a class="tocitem" href="../../modifying_behavior/">Modifying Behavior</a></li><li><a class="tocitem" href="../../internals/">Internals</a></li><li><a class="tocitem" href="../../list_of_supported_models/">List of Supported Models</a></li><li><a class="tocitem" href="../../third_party_packages/">Third Party Packages</a></li><li><a class="tocitem" href="../../glossary/">Glossary</a></li><li><a class="tocitem" href="../../mlj_cheatsheet/">MLJ Cheatsheet</a></li><li><a class="tocitem" href="../../frequently_asked_questions/">FAQ</a></li><li><a class="tocitem" href="../../api/">Index of Methods</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>NeuralNetworkRegressor</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>NeuralNetworkRegressor</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/alan-turing-institute/MLJ.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/alan-turing-institute/MLJ.jl/blob/dev/docs/src/models/NeuralNetworkRegressor_MLJFlux.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="NeuralNetworkRegressor_MLJFlux"><a class="docs-heading-anchor" href="#NeuralNetworkRegressor_MLJFlux">NeuralNetworkRegressor</a><a id="NeuralNetworkRegressor_MLJFlux-1"></a><a class="docs-heading-anchor-permalink" href="#NeuralNetworkRegressor_MLJFlux" title="Permalink"></a></h1><pre><code class="nohighlight hljs">NeuralNetworkRegressor</code></pre><p>A model type for constructing a neural network regressor, based on <a href="https://github.com/alan-turing-institute/MLJFlux.jl">MLJFlux.jl</a>, and implementing the MLJ model interface.</p><p>From MLJ, the type can be imported using</p><pre><code class="nohighlight hljs">NeuralNetworkRegressor = @load NeuralNetworkRegressor pkg=MLJFlux</code></pre><p>Do <code>model = NeuralNetworkRegressor()</code> to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in <code>NeuralNetworkRegressor(builder=...)</code>.</p><p><code>NeuralNetworkRegressor</code> is for training a data-dependent Flux.jl neural network to predict a <code>Continuous</code> target, given a table of <code>Continuous</code> features. Users provide a recipe for constructing the network, based on properties of the data that is encountered, by specifying an appropriate <code>builder</code>. See MLJFlux documentation for more on builders.</p><h2 id="Training-data"><a class="docs-heading-anchor" href="#Training-data">Training data</a><a id="Training-data-1"></a><a class="docs-heading-anchor-permalink" href="#Training-data" title="Permalink"></a></h2><p>In MLJ or MLJBase, bind an instance <code>model</code> to data with</p><pre><code class="nohighlight hljs">mach = machine(model, X, y)</code></pre><p>Here:</p><ul><li><code>X</code> is either a <code>Matrix</code> or any table of input features (eg, a <code>DataFrame</code>) whose columns are of scitype <code>Continuous</code>; check column scitypes with <code>schema(X)</code>. If <code>X</code> is a <code>Matrix</code>, it is assumed to have columns corresponding to features and rows corresponding to observations.</li><li><code>y</code> is the target, which can be any <code>AbstractVector</code> whose element scitype is <code>Continuous</code>; check the scitype with <code>scitype(y)</code></li></ul><p>Train the machine with <code>fit!(mach, rows=...)</code>.</p><h2 id="Hyper-parameters"><a class="docs-heading-anchor" href="#Hyper-parameters">Hyper-parameters</a><a id="Hyper-parameters-1"></a><a class="docs-heading-anchor-permalink" href="#Hyper-parameters" title="Permalink"></a></h2><ul><li><p><code>builder=MLJFlux.Linear(σ=Flux.relu)</code>: An MLJFlux builder that constructs a neural  network. Possible <code>builders</code> include: <code>MLJFlux.Linear</code>, <code>MLJFlux.Short</code>, and  <code>MLJFlux.MLP</code>. See MLJFlux documentation for more on builders, and the example below  for using the <code>@builder</code> convenience macro.</p></li><li><p><code>optimiser::Flux.Adam()</code>: A <code>Flux.Optimise</code> optimiser. The optimiser performs the updating of the weights of the network. For further reference, see <a href="https://fluxml.ai/Flux.jl/stable/training/optimisers/">the Flux optimiser documentation</a>. To choose a learning rate (the update rate of the optimizer), a good rule of thumb is to start out at <code>10e-3</code>, and tune using powers of 10 between <code>1</code> and <code>1e-7</code>.</p></li><li><p><code>loss=Flux.mse</code>: The loss function which the network will optimize. Should be a function which can be called in the form <code>loss(yhat, y)</code>.  Possible loss functions are listed in <a href="https://fluxml.ai/Flux.jl/stable/models/losses/">the Flux loss function documentation</a>. For a regression task, natural loss functions are:</p><ul><li><code>Flux.mse</code></li><li><code>Flux.mae</code></li><li><code>Flux.msle</code></li><li><code>Flux.huber_loss</code></li></ul><p>Currently MLJ measures are not supported as loss functions here.</p></li><li><p><code>epochs::Int=10</code>: The duration of training, in epochs. Typically, one epoch represents one pass through the complete the training dataset.</p></li><li><p><code>batch_size::int=1</code>: the batch size to be used for training, representing the number of samples per update of the network weights. Typically, batch size is between 8 and</p><ol><li>Increasing batch size may accelerate training if <code>acceleration=CUDALibs()</code> and a</li></ol><p>GPU is available.</p></li><li><p><code>lambda::Float64=0</code>: The strength of the weight regularization penalty. Can be any value in the range <code>[0, ∞)</code>.</p></li><li><p><code>alpha::Float64=0</code>: The L2/L1 mix of regularization, in the range <code>[0, 1]</code>. A value of 0 represents L2 regularization, and a value of 1 represents L1 regularization.</p></li><li><p><code>rng::Union{AbstractRNG, Int64}</code>: The random number generator or seed used during training.</p></li><li><p><code>optimizer_changes_trigger_retraining::Bool=false</code>: Defines what happens when re-fitting a machine if the associated optimiser has changed. If <code>true</code>, the associated machine will retrain from scratch on <code>fit!</code> call, otherwise it will not.</p></li><li><p><code>acceleration::AbstractResource=CPU1()</code>: Defines on what hardware training is done. For Training on GPU, use <code>CUDALibs()</code>.</p></li></ul><h2 id="Operations"><a class="docs-heading-anchor" href="#Operations">Operations</a><a id="Operations-1"></a><a class="docs-heading-anchor-permalink" href="#Operations" title="Permalink"></a></h2><ul><li><code>predict(mach, Xnew)</code>: return predictions of the target given new features <code>Xnew</code>, which should have the same scitype as <code>X</code> above.</li></ul><h2 id="Fitted-parameters"><a class="docs-heading-anchor" href="#Fitted-parameters">Fitted parameters</a><a id="Fitted-parameters-1"></a><a class="docs-heading-anchor-permalink" href="#Fitted-parameters" title="Permalink"></a></h2><p>The fields of <code>fitted_params(mach)</code> are:</p><ul><li><code>chain</code>: The trained &quot;chain&quot; (Flux.jl model), namely the series of layers, functions,  and activations which make up the neural network.</li></ul><h2 id="Report"><a class="docs-heading-anchor" href="#Report">Report</a><a id="Report-1"></a><a class="docs-heading-anchor-permalink" href="#Report" title="Permalink"></a></h2><p>The fields of <code>report(mach)</code> are:</p><ul><li><code>training_losses</code>: A vector of training losses (penalized if <code>lambda != 0</code>) in  historical order, of length <code>epochs + 1</code>.  The first element is the pre-training loss.</li></ul><h2 id="Examples"><a class="docs-heading-anchor" href="#Examples">Examples</a><a id="Examples-1"></a><a class="docs-heading-anchor-permalink" href="#Examples" title="Permalink"></a></h2><p>In this example we build a regression model for the Boston house price dataset.</p><pre><code class="language-julia hljs">using MLJ
import MLJFlux
using Flux</code></pre><p>First, we load in the data: The <code>:MEDV</code> column becomes the target vector <code>y</code>, and all remaining columns go into a table <code>X</code>, with the exception of <code>:CHAS</code>:</p><pre><code class="language-julia hljs">data = OpenML.load(531); ## Loads from https://www.openml.org/d/531
y, X = unpack(data, ==(:MEDV), !=(:CHAS); rng=123);

scitype(y)
schema(X)</code></pre><p>Since MLJFlux models do not handle ordered factors, we&#39;ll treat <code>:RAD</code> as <code>Continuous</code>:</p><pre><code class="language-julia hljs">X = coerce(X, :RAD=&gt;Continuous)</code></pre><p>Splitting off a test set:</p><pre><code class="language-julia hljs">(X, Xtest), (y, ytest) = partition((X, y), 0.7, multi=true);</code></pre><p>Next, we can define a <code>builder</code>, making use of a convenience macro to do so.  In the following <code>@builder</code> call, <code>n_in</code> is a proxy for the number input features (which will be known at <code>fit!</code> time) and <code>rng</code> is a proxy for a RNG (which will be passed from the <code>rng</code> field of <code>model</code> defined below). We also have the parameter <code>n_out</code> which is the number of output features. As we are doing single target regression, the value passed will always be <code>1</code>, but the builder we define will also work for <code>MultitargetNeuralRegressor</code>.</p><pre><code class="language-julia hljs">builder = MLJFlux.@builder begin
    init=Flux.glorot_uniform(rng)
    Chain(
        Dense(n_in, 64, relu, init=init),
        Dense(64, 32, relu, init=init),
        Dense(32, n_out, init=init),
    )
end</code></pre><p>Instantiating a model:</p><pre><code class="language-julia hljs">NeuralNetworkRegressor = @load NeuralNetworkRegressor pkg=MLJFlux
model = NeuralNetworkRegressor(
    builder=builder,
    rng=123,
    epochs=20
)</code></pre><p>We arrange for standardization of the the target by wrapping our model in <code>TransformedTargetModel</code>, and standardization of the features by inserting the wrapped model in a pipeline:</p><pre><code class="language-julia hljs">pipe = Standardizer |&gt; TransformedTargetModel(model, target=Standardizer)</code></pre><p>If we fit with a high verbosity (&gt;1), we will see the losses during training. We can also see the losses in the output of <code>report(mach)</code>.</p><pre><code class="language-julia hljs">mach = machine(pipe, X, y)
fit!(mach, verbosity=2)

## first element initial loss, 2:end per epoch training losses
report(mach).transformed_target_model_deterministic.model.training_losses</code></pre><h3 id="Experimenting-with-learning-rate"><a class="docs-heading-anchor" href="#Experimenting-with-learning-rate">Experimenting with learning rate</a><a id="Experimenting-with-learning-rate-1"></a><a class="docs-heading-anchor-permalink" href="#Experimenting-with-learning-rate" title="Permalink"></a></h3><p>We can visually compare how the learning rate affects the predictions:</p><pre><code class="language-julia hljs">using Plots

rates = rates = [5e-5, 1e-4, 0.005, 0.001, 0.05]
plt=plot()

foreach(rates) do η
  pipe.transformed_target_model_deterministic.model.optimiser.eta = η
  fit!(mach, force=true, verbosity=0)
  losses =
      report(mach).transformed_target_model_deterministic.model.training_losses[3:end]
  plot!(1:length(losses), losses, label=η)
end

plt

pipe.transformed_target_model_deterministic.model.optimiser.eta = 0.0001</code></pre><p>With the learning rate fixed, we compute a CV estimate of the performance (using all data bound to <code>mach</code>) and compare this with performance on the test set:</p><pre><code class="language-julia hljs">## CV estimate, based on `(X, y)`:
evaluate!(mach, resampling=CV(nfolds=5), measure=l2)

## loss for `(Xtest, test)`:
fit!(mach) ## train on `(X, y)`
yhat = predict(mach, Xtest)
l2(yhat, ytest)  |&gt; mean</code></pre><p>These losses, for the pipeline model, refer to the target on the original, unstandardized, scale.</p><p>For implementing stopping criterion and other iteration controls, refer to examples linked from the MLJFlux documentation.</p><p>See also <code>MultitargetNeuralNetworkRegressor</code></p></article><nav class="docs-footer"><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="auto">Automatic (OS)</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.1.2 on <span class="colophon-date" title="Tuesday 21 November 2023 03:25">Tuesday 21 November 2023</span>. Using Julia version 1.9.4.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
