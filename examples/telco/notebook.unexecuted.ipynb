{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLJ for Data Scientists in Two Hours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An application of the [MLJ\n",
    "toolbox](https://juliaai.github.io/MLJ.jl/dev/) to the\n",
    "Telco Customer Churn dataset, aimed at practicing data scientists\n",
    "new to MLJ (Machine Learning in Julia). This tutorial does not\n",
    "cover exploratory data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLJ is a *multi-paradigm* machine learning toolbox (i.e., not just\n",
    "deep-learning)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For other MLJ learning resources see the [Learning\n",
    "MLJ](https://juliaai.github.io/MLJ.jl/dev/learning_mlj/)\n",
    "section of the\n",
    "[manual](https://juliaai.github.io/MLJ.jl/dev/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Topics covered**: Grabbing and preparing a dataset, basic\n",
    "fit/predict workflow, constructing a pipeline to include data\n",
    "pre-processing, estimating performance metrics, ROC curves, confusion\n",
    "matrices, feature importance, basic feature selection, controlling iterative\n",
    "models, hyper-parameter optimization (tuning)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prerequisites for this tutorial.** Previous experience building,\n",
    "evaluating, and optimizing machine learning models using\n",
    "scikit-learn, caret, MLR, weka, or similar tool. No previous\n",
    "experience with MLJ. Only fairly basic familiarity with Julia is\n",
    "required. Uses\n",
    "[DataFrames.jl](https://dataframes.juliadata.org/stable/) but in a\n",
    "minimal way ([this\n",
    "cheatsheet](https://ahsmart.com/pub/data-wrangling-with-data-frames-jl-cheat-sheet/index.html)\n",
    "may help)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Time.** Between two and three hours, first time through."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of methods and types introduced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|code   | purpose|\n",
    "|:-------|:-------------------------------------------------------|\n",
    "| `OpenML.load(id)` | grab a dataset from [OpenML.org](https://www.openml.org)|\n",
    "| `scitype(X)`      | inspect the scientific type (scitype) of object `X`|\n",
    "| `schema(X)`       | inspect the column scitypes (scientific types) of a table `X`|\n",
    "| `coerce(X, ...)`   | fix column encodings to get appropriate scitypes|\n",
    "| `partition(data, frac1, frac2, ...; rng=...)` | vertically split `data`, which can be a table, vector or matrix|\n",
    "| `unpack(table, f1, f2, ...)` | horizontally split `table` based on conditions `f1`, `f2`, ..., applied to column names|\n",
    "| `@load ModelType pkg=...`           | load code defining a model type|\n",
    "| `input_scitype(model)` | inspect the scitype that a model requires for features (inputs)|\n",
    "| `target_scitype(model)`| inspect the scitype that a model requires for the target (labels)|\n",
    "| `ContinuousEncoder`   | built-in model type for re-encoding all features as `Continuous`|\n",
    "| `model1 ∣> model2 ∣> ...` | combine multiple models into a pipeline|\n",
    "| `measures(\"under curve\")` | list all measures (metrics) with string \"under curve\" in documentation|\n",
    "| `accuracy(yhat, y)` | compute accuracy of predictions `yhat` against ground truth observations `y`|\n",
    "| `auc(yhat, y)`, `brier_loss(yhat, y)` | evaluate two probabilistic measures (`yhat` a vector of probability distributions)|\n",
    "| `machine(model, X, y)` | bind `model` to training data `X` (features) and `y` (target)|\n",
    "| `fit!(mach, rows=...)` | train machine using specified rows (observation indices)|\n",
    "| `predict(mach, rows=...)`, | make in-sample model predictions given specified rows|\n",
    "| `predict(mach, Xnew)` | make predictions given new features `Xnew`|\n",
    "| `fitted_params(mach)` | inspect learned parameters|\n",
    "| `report(mach)`        | inspect other outcomes of training|\n",
    "| `confmat(yhat, y)`    | confusion matrix for predictions `yhat` and ground truth `y`|\n",
    "| `roc(yhat, y)` | compute points on the receiver-operator Characteristic|\n",
    "| `StratifiedCV(nfolds=6)` | 6-fold stratified cross-validation resampling strategy|\n",
    "| `Holdout(fraction_train=0.7)` | holdout resampling strategy|\n",
    "| `evaluate(model, X, y; resampling=..., options...)` | estimate performance metrics `model` using the data `X`, `y`|\n",
    "| `FeatureSelector()` | transformer for selecting features|\n",
    "| `Step(3)` | iteration control for stepping 3 iterations|\n",
    "| `NumberSinceBest(6)`, `TimeLimit(60/5), InvalidValue()` | iteration control stopping criteria|\n",
    "| `IteratedModel(model=..., controls=..., options...)` | wrap an iterative `model` in control strategies|\n",
    "| `range(model,  :some_hyperparam, lower=..., upper=...)` | define a numeric range|\n",
    "| `RandomSearch()` | random search tuning strategy|\n",
    "| `TunedModel(model=..., tuning=..., options...)` | wrap the supervised `model` in specified `tuning` strategy|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate a Julia environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code replicates precisely the set of Julia packages\n",
    "used to develop this tutorial. If this is your first time running\n",
    "the notebook, package instantiation and pre-compilation may take a\n",
    "minute or so to complete. **This step will fail** if the [correct\n",
    "Manifest.toml and Project.toml\n",
    "files](https://github.com/JuliaAI/MLJ.jl/tree/dev/examples/telco)\n",
    "are not in the same directory as this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Pkg\n",
    "Pkg.activate(@__DIR__) # get env from TOML files in same directory as this notebook\n",
    "Pkg.instantiate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warm up: Building a model for the iris dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before turning to the Telco Customer Churn dataset, we very quickly\n",
    "build a predictive model for Fisher's well-known iris data set, as way of\n",
    "introducing the main actors in any MLJ workflow. Details that you\n",
    "don't fully grasp should become clearer in the Telco study."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section is a condensed adaption of the [Getting Started\n",
    "example](https://juliaai.github.io/MLJ.jl/dev/getting_started/#Fit-and-predict)\n",
    "in the MLJ documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, using the built-in iris dataset, we load and inspect the features\n",
    "`X_iris` (a table) and target variable `y_iris` (a vector):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using MLJ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "const X_iris, y_iris = @load_iris;\n",
    "schema(X_iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_iris[1:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "levels(y_iris)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load a decision tree model, from the package DecisionTree.jl:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DecisionTree = @load DecisionTreeClassifier pkg=DecisionTree # model type\n",
    "model = DecisionTree(min_samples_split=5)                    # model instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In MLJ, a *model* is just a container for hyper-parameters of\n",
    "some learning algorithm. It does not store learned parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we bind the model together with the available data in what's\n",
    "called a *machine*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mach = machine(model, X_iris, y_iris)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A machine is essentially just a model (ie, hyper-parameters) plus data, but\n",
    "it additionally stores *learned parameters* (the tree) once it is\n",
    "trained on some view of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rows = vcat(1:60, 91:150); # some row indices (observations are rows not columns)\n",
    "fit!(mach, rows=train_rows)\n",
    "fitted_params(mach)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A machine stores some other information enabling [warm\n",
    "restart](https://juliaai.github.io/MLJ.jl/dev/machines/#Warm-restarts)\n",
    "for some models, but we won't go into that here. You are allowed to\n",
    "access and mutate the `model` parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mach.model.min_samples_split  = 10\n",
    "fit!(mach, rows=train_rows) # re-train with new hyper-parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can make predictions on some other view of the data, as in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(mach, rows=71:73)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or on completely new data, as in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xnew = (sepal_length = [5.1, 6.3],\n",
    "        sepal_width = [3.0, 2.5],\n",
    "        petal_length = [1.4, 4.9],\n",
    "        petal_width = [0.3, 1.5])\n",
    "yhat = predict(mach, Xnew)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are probabilistic predictions which can be manipulated using a\n",
    "widely adopted interface defined in the Distributions.jl\n",
    "package. For example, we can get raw probabilities like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf.(yhat, \"virginica\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now turn to the Telco dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the Telco data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = OpenML.load(42178) # data set from OpenML.org\n",
    "df0 = DataFrames.DataFrame(data)\n",
    "first(df0, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The object of this tutorial is to build and evaluate supervised\n",
    "learning models to predict the `:Churn` variable, a binary variable\n",
    "measuring customer retention, based on other variables that are\n",
    "relevant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the table, observations correspond to rows, and features to\n",
    "columns, which is the convention for representing all\n",
    "two-dimensional data in MLJ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Type coercion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Introduces: `scitype`, `schema`, `coerce`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A [\"scientific\n",
    "type\"](https://juliaai.github.io/ScientificTypes.jl/dev/) or\n",
    "*scitype* indicates how MLJ will *interpret* data. For example,\n",
    "`typeof(3.14) == Float64`, while `scitype(3.14) == Continuous` and\n",
    "also `scitype(3.14f0) == Continuous`. In MLJ, model data\n",
    "requirements are articulated using scitypes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are common \"scalar\" scitypes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](assets/scitypes.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also container scitypes. For example, the scitype of any\n",
    "`N`-dimensional array is `AbstractArray{S, N}`, where `S` is the scitype of the\n",
    "elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scitype([\"cat\", \"mouse\", \"dog\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `schema` operator summarizes the column scitypes of a table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema(df0) |> DataFrames.DataFrame  # converted to DataFrame for better display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the fields being interpreted as `Textual` are really\n",
    "something else, either `Multiclass` or, in the case of\n",
    "`:TotalCharges`, `Continuous`. In fact, `:TotalCharges` is\n",
    "mostly floats wrapped as strings. However, it needs special\n",
    "treatment because some elements consist of a single space, \" \",\n",
    "which we'll treat as \"0.0\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_blanks(v) = map(v) do x\n",
    "    if x == \" \"\n",
    "        return \"0.0\"\n",
    "    else\n",
    "        return x\n",
    "    end\n",
    "end\n",
    "\n",
    "df0.TotalCharges = fix_blanks(df0.TotalCharges);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coercing the `:TotalCharges` type to ensure a `Continuous` scitype:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coerce!(df0, :TotalCharges => Continuous);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coercing all remaining `Textual` data to `Multiclass`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coerce!(df0, Textual => Multiclass);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll coerce our target variable `:Churn` to be\n",
    "`OrderedFactor`, rather than `Multiclass`, to enable a reliable\n",
    "interpretation of metrics like \"true positive rate\".  By convention,\n",
    "the first class is the negative one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coerce!(df0, :Churn => OrderedFactor)\n",
    "levels(df0.Churn) # to check order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-inspecting the scitypes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema(df0) |> DataFrames.DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing a holdout set for final testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Introduces: `partition`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reduce training times for the purposes of this tutorial, we're\n",
    "going to dump 90% of observations (after shuffling) and split off\n",
    "30% of the remainder for use as a lock-and-throw-away-the-key\n",
    "holdout set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, df_test, df_dumped = partition(df0, 0.07, 0.03, # in ratios 7:3:90\n",
    "                                   stratify=df0.Churn,\n",
    "                                   rng=123);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reader interested in including all data can instead do\n",
    "`df, df_test = partition(df0, 0.7, stratify=df0.Churn, rng=123)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting data into target and features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Introduces: `unpack`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following call, the column with name `:Churn` is copied over\n",
    "to a vector `y`, and every remaining column, except `:customerID`\n",
    "(which contains no useful information) goes into a table `X`. Here\n",
    "`:Churn` is the target variable for which we seek predictions, given\n",
    "new versions of the features `X`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "const y, X = unpack(df, ==(:Churn), !=(:customerID));\n",
    "schema(X).names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intersect([:Churn, :customerID], schema(X).names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll do the same for the holdout data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "const ytest, Xtest = unpack(df_test, ==(:Churn), !=(:customerID));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a model and checking type requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Introduces: `@load`, `input_scitype`, `target_scitype`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For tools helping us to identify suitable models, see the [Model\n",
    "Search](https://juliaai.github.io/MLJ.jl/dev/model_search/#model_search)\n",
    "section of the manual. We will build a gradient tree-boosting model,\n",
    "a popular first choice for structured data like we have here. Model\n",
    "code is contained in a third-party package called\n",
    "[EvoTrees.jl](https://github.com/Evovest/EvoTrees.jl) which is\n",
    "loaded as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Booster = @load EvoTreeClassifier pkg=EvoTrees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that a *model* is just a container for some algorithm's\n",
    "hyper-parameters. Let's create a `Booster` with default values for\n",
    "the hyper-parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "booster = Booster()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is appropriate for the kind of target variable we have because of\n",
    "the following passing test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scitype(y) <: target_scitype(booster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, our features `X` cannot be directly used with `booster`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scitype(X) <: input_scitype(booster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it turns out, this is because `booster`, like the majority of MLJ\n",
    "supervised models, expects the features to be `Continuous`. (With\n",
    "some experience, this can be gleaned from `input_scitype(booster)`.)\n",
    "So we need categorical feature encoding, discussed next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a model pipeline to incorporate feature encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Introduces: `ContinuousEncoder`, pipeline operator `|>`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The built-in `ContinuousEncoder` model transforms an arbitrary table\n",
    "to a table whose features are all `Continuous` (dropping any fields\n",
    "it does not know how to encode). In particular, all `Multiclass`\n",
    "features are one-hot encoded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A *pipeline* is a stand-alone model that internally combines one or\n",
    "more models in a linear (non-branching) pipeline. Here's a pipeline\n",
    "that adds the `ContinuousEncoder` as a pre-processor to the\n",
    "gradient tree-boosting model above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = ContinuousEncoder() |> booster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the component models appear as hyper-parameters of\n",
    "`pipe`. Pipelines are an implementation of a more general [model\n",
    "composition](https://juliaai.github.io/MLJ.jl/dev/composing_models/#Composing-Models)\n",
    "interface provided by MLJ that advanced users may want to learn about."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above display, we see that component model hyper-parameters\n",
    "are now *nested*, but they are still accessible (important in hyper-parameter\n",
    "optimization):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.evo_tree_classifier.max_depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the pipeline model's performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Introduces: `measures` (function), **measures:** `brier_loss`, `auc`, `accuracy`;\n",
    "> `machine`, `fit!`, `predict`, `fitted_params`, `report`, `roc`, **resampling strategy** `StratifiedCV`, `evaluate`, `FeatureSelector`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without touching our test set `Xtest`, `ytest`, we will estimate the\n",
    "performance of our pipeline model, with default hyper-parameters, in\n",
    "two different ways:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluating by hand.** First, we'll do this \"by hand\" using the `fit!` and `predict`\n",
    "workflow illustrated for the iris data set above, using a\n",
    "holdout resampling strategy. At the same time we'll see how to\n",
    "generate a **confusion matrix**, **ROC curve**, and inspect\n",
    "**feature importances**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Automated performance evaluation.** Next we'll apply the more\n",
    "typical and convenient `evaluate` workflow, but using `StratifiedCV`\n",
    "(stratified cross-validation) which is more informative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In any case, we need to choose some measures (metrics) to quantify\n",
    "the performance of our model. For a complete list of measures, one\n",
    "does `measures()`. Or we also can do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measures(\"Brier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be primarily using `brier_loss`, but also `auc` (area under\n",
    "the ROC curve) and `accuracy`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating by hand (with a holdout set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our pipeline model can be trained just like the decision tree model\n",
    "we built for the iris data set. Binding all non-test data to the\n",
    "pipeline model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mach_pipe = machine(pipe, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already encountered the `partition` method above. Here we apply\n",
    "it to row indices, instead of data containers, as `fit!` and\n",
    "`predict` only need a *view* of the data to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, validation = partition(1:length(y), 0.7)\n",
    "fit!(mach_pipe, rows=train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note in passing that we can access two kinds of information from a trained machine:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The **learned parameters** (eg, coefficients of a linear model): We use `fitted_params(mach_pipe)`\n",
    "- Other **by-products of training** (eg, feature importances): We use `report(mach_pipe)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = fitted_params(mach_pipe);\n",
    "keys(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, we can check that the encoder did not actually drop any features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Set(fp.continuous_encoder.features_to_keep) == Set(schema(X).names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, from the report, extract feature importances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rpt = report(mach_pipe)\n",
    "keys(rpt.evo_tree_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fi = rpt.evo_tree_classifier.feature_importances\n",
    "feature_importance_table =\n",
    "    (feature=Symbol.(first.(fi)), importance=last.(fi)) |> DataFrames.DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For models not reporting feature importances, we recommend the\n",
    "[Shapley.jl](https://expandingman.gitlab.io/Shapley.jl/) package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returning to predictions and evaluations of our measures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ŷ = predict(mach_pipe, rows=validation);\n",
    "@info(\"Measurements\",\n",
    "      brier_loss(ŷ, y[validation]) |> mean,\n",
    "      auc(ŷ, y[validation]),\n",
    "      accuracy(mode.(ŷ), y[validation])\n",
    "      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we need `mode` in the last case because `accuracy` expects\n",
    "point predictions, not probabilistic ones. (One can alternatively\n",
    "use `predict_mode` to generate the predictions.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we're here, lets also generate a **confusion matrix** and\n",
    "[receiver-operator\n",
    "characteristic](https://en.wikipedia.org/wiki/Receiver_operating_characteristic)\n",
    "(ROC):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confmat(mode.(ŷ), y[validation])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Importing the plotting package and calling the plotting\n",
    "functions for the first time can take a minute or so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_curve = roc(ŷ, y[validation])\n",
    "plt = scatter(roc_curve, legend=false)\n",
    "plot!(plt, xlab=\"false positive rate\", ylab=\"true positive rate\")\n",
    "plot!([0, 1], [0, 1], linewidth=2, linestyle=:dash, color=:black)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automated performance evaluation (more typical workflow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get performance estimates with a single call to the\n",
    "`evaluate` function, which also allows for more complicated\n",
    "resampling - in this case stratified cross-validation. To make this\n",
    "more comprehensive, we set `repeats=3` below to make our\n",
    "cross-validation \"Monte Carlo\" (3 random size-6 partitions of the\n",
    "observation space, for a total of 18 folds) and set\n",
    "`acceleration=CPUThreads()` to parallelize the computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose a `StratifiedCV` resampling strategy; the complete list of options is\n",
    "[here](https://juliaai.github.io/MLJ.jl/dev/evaluating_model_performance/#Built-in-resampling-strategies)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_pipe = evaluate(pipe, X, y,\n",
    "                  resampling=StratifiedCV(nfolds=6, rng=123),\n",
    "                  measures=[brier_loss, auc, accuracy],\n",
    "                  repeats=3,\n",
    "                  acceleration=CPUThreads())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(There is also a version of `evaluate` for machines. Query the\n",
    "`evaluate` and `evaluate!` doc-strings to learn more about these\n",
    "functions and what the `PerformanceEvaluation` object `e_pipe` records.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While [less than ideal](https://arxiv.org/abs/2104.00673), let's\n",
    "adopt the common practice of using the standard error of a\n",
    "cross-validation score as an estimate of the uncertainty of a\n",
    "performance measure's expected value. Here's a utility function to\n",
    "calculate 95% confidence intervals for our performance estimates based\n",
    "on this practice, and it's application to the current evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function confidence_intervals(e)\n",
    "    factor = 2.0 # to get level of 95%\n",
    "    measure = e.measure\n",
    "    nfolds = length(e.per_fold[1])\n",
    "    measurement = [e.measurement[j] ± factor*std(e.per_fold[j])/sqrt(nfolds - 1)\n",
    "                   for j in eachindex(measure)]\n",
    "    table = (measure=measure, measurement=measurement)\n",
    "    return DataFrames.DataFrame(table)\n",
    "end\n",
    "\n",
    "const confidence_intervals_basic_model = confidence_intervals(e_pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering out unimportant features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Introduces: `FeatureSelector`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before continuing, we'll modify our pipeline to drop those features\n",
    "with low feature importance, to speed up later optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unimportant_features = filter(:importance => <(0.005), feature_importance_table).feature\n",
    "\n",
    "pipe2 = ContinuousEncoder() |>\n",
    "    FeatureSelector(features=unimportant_features, ignore=true) |> booster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapping our iterative model in control strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Introduces: **control strategies:** `Step`, `NumberSinceBest`, `TimeLimit`, `InvalidValue`, **model wrapper** `IteratedModel`, **resampling strategy:** `Holdout`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to optimize the hyper-parameters of our model. Since our\n",
    "model is iterative, these parameters include the (nested) iteration\n",
    "parameter `pipe.evo_tree_classifier.nrounds`. Sometimes this\n",
    "parameter is optimized first, fixed, and then maybe optimized again\n",
    "after the other parameters. Here we take a more principled approach,\n",
    "**wrapping our model in a control strategy** that makes it\n",
    "\"self-iterating\". The strategy applies a stopping criterion to\n",
    "*out-of-sample* estimates of the model performance, constructed\n",
    "using an internally constructed holdout set. In this way, we avoid\n",
    "some data hygiene issues, and, when we subsequently optimize other\n",
    "parameters, we will always being using an optimal number of\n",
    "iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this approach can be applied to any iterative MLJ model,\n",
    "eg, the neural network models provided by\n",
    "[MLJFlux.jl](https://github.com/FluxML/MLJFlux.jl)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we select appropriate controls from [this\n",
    "list](https://juliaai.github.io/MLJ.jl/dev/controlling_iterative_models/#Controls-provided):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "controls = [\n",
    "    Step(1),              # to increment iteration parameter (`pipe.nrounds`)\n",
    "    NumberSinceBest(4),   # main stopping criterion\n",
    "    TimeLimit(2/3600),    # never train more than 2 sec\n",
    "    InvalidValue()        # stop if NaN or ±Inf encountered\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we wrap our pipeline model using the `IteratedModel` wrapper,\n",
    "being sure to specify the `measure` on which internal estimates of\n",
    "the out-of-sample performance will be based:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterated_pipe = IteratedModel(model=pipe2,\n",
    "                              controls=controls,\n",
    "                              measure=brier_loss,\n",
    "                              resampling=Holdout(fraction_train=0.7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've set `resampling=Holdout(fraction_train=0.7)` to arrange that\n",
    "data attached to our model should be internally split into a train\n",
    "set (70%) and a holdout set (30%) for determining the out-of-sample\n",
    "estimate of the Brier loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For demonstration purposes, let's bind `iterated_model` to all data\n",
    "not in our don't-touch holdout set, and train on all of that data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mach_iterated_pipe = machine(iterated_pipe, X, y)\n",
    "fit!(mach_iterated_pipe);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To recap, internally this training is split into two separate steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A controlled iteration step, training on the holdout set, with the total number of iterations determined by the specified stopping criteria (based on the out-of-sample performance estimates)\n",
    "- A final step that trains the atomic model on *all* available\n",
    "  data using the number of iterations determined in the first step. Calling `predict` on `mach_iterated_pipe` means using the learned parameters of the second step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameter optimization (model tuning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Introduces: `range`, **model wrapper** `TunedModel`, `RandomSearch`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now turn to hyper-parameter optimization. A tool not discussed\n",
    "here is the `learning_curve` function, which can be useful when\n",
    "wanting to visualize the effect of changes to a *single*\n",
    "hyper-parameter (which could be an iteration parameter). See, for\n",
    "example, [this section of the\n",
    "manual](https://juliaai.github.io/MLJ.jl/dev/learning_curves/)\n",
    "or [this\n",
    "tutorial](https://github.com/ablaom/MLJTutorial.jl/blob/dev/notebooks/04_tuning/notebook.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine tuning the hyper-parameters of a gradient booster can be\n",
    "somewhat involved. Here we settle for simultaneously optimizing two\n",
    "key parameters: `max_depth` and `η` (learning_rate)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like iteration control, **model optimization in MLJ is implemented as\n",
    "a model wrapper**, called `TunedModel`. After wrapping a model in a\n",
    "tuning strategy and binding the wrapped model to data in a machine\n",
    "called `mach`, calling `fit!(mach)` instigates a search for optimal\n",
    "model hyperparameters, within a specified range, and then uses all\n",
    "supplied data to train the best model. To predict using that model,\n",
    "one then calls `predict(mach, Xnew)`. In this way the wrapped model\n",
    "may be viewed as a \"self-tuning\" version of the unwrapped\n",
    "model. That is, wrapping the model simply transforms certain\n",
    "hyper-parameters into learned parameters (just as `IteratedModel`\n",
    "does for an iteration parameter)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start with, we define ranges for the parameters of\n",
    "interest. Since these parameters are nested, let's force a\n",
    "display of our model to a larger depth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(iterated_pipe, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = :(model.evo_tree_classifier.η)\n",
    "p2 = :(model.evo_tree_classifier.max_depth)\n",
    "\n",
    "r1 = range(iterated_pipe, p1, lower=-2, upper=-0.5, scale=x->10^x)\n",
    "r2 = range(iterated_pipe, p2, lower=2, upper=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nominal ranges are defined by specifying `values` instead of `lower`\n",
    "and `upper`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we choose an optimization strategy from [this\n",
    "list](https://juliaai.github.io/MLJ.jl/dev/tuning_models/#Tuning-Models):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning = RandomSearch(rng=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we wrap the model, specifying a `resampling` strategy and a\n",
    "`measure`, as we did for `IteratedModel`.  In fact, we can include a\n",
    "battery of `measures`; by default, optimization is with respect to\n",
    "performance estimates based on the first measure, but estimates for\n",
    "all measures can be accessed from the model's `report`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The keyword `n` specifies the total number of models (sets of\n",
    "hyper-parameters) to evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_iterated_pipe = TunedModel(model=iterated_pipe,\n",
    "                                 range=[r1, r2],\n",
    "                                 tuning=tuning,\n",
    "                                 measures=[brier_loss, auc, accuracy],\n",
    "                                 resampling=StratifiedCV(nfolds=6, rng=123),\n",
    "                                 acceleration=CPUThreads(),\n",
    "                                 n=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To save time, we skip the `repeats` here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binding our final model to data and training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mach_tuned_iterated_pipe = machine(tuned_iterated_pipe, X, y)\n",
    "fit!(mach_tuned_iterated_pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As explained above, the training we have just performed was split\n",
    "internally into two separate steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A step to determine the parameter values that optimize the aggregated cross-validation scores\n",
    "- A final step that trains the optimal model on *all* available data. Future predictions `predict(mach_tuned_iterated_pipe, ...)` are based on this final training step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From `report(mach_tuned_iterated_pipe)` we can extract details about\n",
    "the optimization procedure. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rpt2 = report(mach_tuned_iterated_pipe);\n",
    "best_booster = rpt2.best_model.model.evo_tree_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@info \"Optimal hyper-parameters:\" best_booster.max_depth best_booster.η;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `confidence_intervals` function we defined earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_best = rpt2.best_history_entry\n",
    "confidence_intervals(e_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Digging a little deeper, we can learn what stopping criterion was\n",
    "applied in the case of the optimal model, and how many iterations\n",
    "were required:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rpt2.best_report.controls |> collect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can visualize the optimization results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(mach_tuned_iterated_pipe, size=(600,450))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Introduces: `MLJ.save`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how to serialize our final, trained self-iterating,\n",
    "self-tuning pipeline machine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLJ.save(\"tuned_iterated_pipe.jlso\", mach_tuned_iterated_pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll deserialize this in \"Testing the final model\" below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final performance estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, to get an even more accurate estimate of performance, we\n",
    "can evaluate our model using stratified cross-validation and all the\n",
    "data attached to our machine. Because this evaluation implies\n",
    "[nested\n",
    "resampling](https://mlr.mlr-org.com/articles/tutorial/nested_resampling.html),\n",
    "this computation takes quite a bit longer than the previous one\n",
    "(which is being repeated six times, using 5/6th of the data each\n",
    "time):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_tuned_iterated_pipe = evaluate(tuned_iterated_pipe, X, y,\n",
    "                                 resampling=StratifiedCV(nfolds=6, rng=123),\n",
    "                                 measures=[brier_loss, auc, accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_intervals(e_tuned_iterated_pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For comparison, here are the confidence intervals for the basic\n",
    "pipeline model (no feature selection and default hyperparameters):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_intervals_basic_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As each pair of intervals overlap, it's doubtful the small changes\n",
    "here can be assigned statistical significance. Default `booster`\n",
    "hyper-parameters do a pretty good job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the final model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now determine the performance of our model on our\n",
    "lock-and-throw-away-the-key holdout set. To demonstrate\n",
    "deserialization, we'll pretend we're in a new Julia session (but\n",
    "have called `import`/`using` on the same packages). Then the\n",
    "following should suffice to recover our model trained under\n",
    "\"Hyper-parameter optimization\" above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mach_restored = machine(\"tuned_iterated_pipe.jlso\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute predictions on the holdout set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ŷ_tuned = predict(mach_restored, Xtest);\n",
    "ŷ_tuned[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And can compute the final performance measures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@info(\"Tuned model measurements on test:\",\n",
    "      brier_loss(ŷ_tuned, ytest) |> mean,\n",
    "      auc(ŷ_tuned, ytest),\n",
    "      accuracy(mode.(ŷ_tuned), ytest)\n",
    "      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For comparison, here's the performance for the basic pipeline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mach_basic = machine(pipe, X, y)\n",
    "fit!(mach_basic, verbosity=0)\n",
    "\n",
    "ŷ_basic = predict(mach_basic, Xtest);\n",
    "\n",
    "@info(\"Basic model measurements on test set:\",\n",
    "      brier_loss(ŷ_basic, ytest) |> mean,\n",
    "      auc(ŷ_basic, ytest),\n",
    "      accuracy(mode.(ŷ_basic), ytest)\n",
    "      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.5",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 3
}
