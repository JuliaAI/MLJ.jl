{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Lightning tour of MLJ"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "*For a more elementary introduction to MLJ, see [Getting\n",
    "Started](https://alan-turing-institute.github.io/MLJ.jl/dev/getting_started/).*"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Note.** Be sure this file has not been separated from the\n",
    "accompanying Project.toml and Manifest.toml files, which should not\n",
    "should be altered unless you know what you are doing. Using them,\n",
    "the following code block instantiates a julia environment with a tested\n",
    "bundle of packages known to work with the rest of the script:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Activating environment at `~/Google Drive/Julia/MLJ/MLJ/examples/lightning_tour/Project.toml`\n"
     ]
    }
   ],
   "cell_type": "code",
   "source": [
    "using Pkg\n",
    "Pkg.activate(@__DIR__)\n",
    "Pkg.instantiate()"
   ],
   "metadata": {},
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "Assuming Julia 1.6"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In MLJ a *model* is just a container for hyper-parameters, and that's\n",
    "all. Here we will apply several kinds of model composition before\n",
    "binding the resulting \"meta-model\" to data in a *machine* for\n",
    "evaluation, using cross-validation."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Loading and instantiating a gradient tree-boosting model:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Info: For silent loading, specify `verbosity=0`. \n",
      "import EvoTrees ✔\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "EvoTreeRegressor(\n    loss = EvoTrees.Linear(),\n    nrounds = 10,\n    λ = 0.0,\n    γ = 0.0,\n    η = 0.1,\n    max_depth = 2,\n    min_weight = 1.0,\n    rowsample = 1.0,\n    colsample = 1.0,\n    nbins = 64,\n    α = 0.5,\n    metric = :mse,\n    rng = MersenneTwister(444),\n    device = \"cpu\") @522"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "cell_type": "code",
   "source": [
    "using MLJ\n",
    "MLJ.color_off()\n",
    "\n",
    "Booster = @load EvoTreeRegressor # loads code defining a model type\n",
    "booster = Booster(max_depth=2)   # specify hyper-parameter at construction"
   ],
   "metadata": {},
   "execution_count": 2
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "EvoTreeRegressor(\n    loss = EvoTrees.Linear(),\n    nrounds = 50,\n    λ = 0.0,\n    γ = 0.0,\n    η = 0.1,\n    max_depth = 2,\n    min_weight = 1.0,\n    rowsample = 1.0,\n    colsample = 1.0,\n    nbins = 64,\n    α = 0.5,\n    metric = :mse,\n    rng = MersenneTwister(444),\n    device = \"cpu\") @522"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "cell_type": "code",
   "source": [
    "booster.nrounds=50               # or mutate post facto\n",
    "booster"
   ],
   "metadata": {},
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "This model is an example of an iterative model. As is stands, the\n",
    "number of iterations `nrounds` is fixed."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Composition 1: Wrapping the model to make it \"self-iterating\""
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's create a new model that automatically learns the number of iterations,\n",
    "using the `NumberSinceBest(3)` criterion, as applied to an\n",
    "out-of-sample `l1` loss:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "DeterministicIteratedModel(\n    model = EvoTreeRegressor(\n            loss = EvoTrees.Linear(),\n            nrounds = 50,\n            λ = 0.0,\n            γ = 0.0,\n            η = 0.1,\n            max_depth = 2,\n            min_weight = 1.0,\n            rowsample = 1.0,\n            colsample = 1.0,\n            nbins = 64,\n            α = 0.5,\n            metric = :mse,\n            rng = MersenneTwister(444),\n            device = \"cpu\"),\n    controls = Any[Step(2), NumberSinceBest(3), NumberLimit(300)],\n    resampling = Holdout(\n            fraction_train = 0.8,\n            shuffle = false,\n            rng = Random._GLOBAL_RNG()),\n    measure = LPLoss(\n            p = 1),\n    weights = nothing,\n    class_weights = nothing,\n    operation = MLJModelInterface.predict,\n    retrain = true,\n    check_measure = true,\n    iteration_parameter = nothing,\n    cache = true) @630"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "cell_type": "code",
   "source": [
    "using MLJIteration\n",
    "iterated_booster = IteratedModel(model=booster,\n",
    "                                 resampling=Holdout(fraction_train=0.8),\n",
    "                                 controls=[Step(2), NumberSinceBest(3), NumberLimit(300)],\n",
    "                                 measure=l1,\n",
    "                                 retrain=true)"
   ],
   "metadata": {},
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Composition 2: Preprocess the input features"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Combining the model with categorical feature encoding:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Pipeline290(\n    continuous_encoder = ContinuousEncoder(\n            drop_last = false,\n            one_hot_ordered_factors = false),\n    deterministic_iterated_model = DeterministicIteratedModel(\n            model = EvoTreeRegressor{Float64,…} @522,\n            controls = Any[Step(2), NumberSinceBest(3), NumberLimit(300)],\n            resampling = Holdout @258,\n            measure = LPLoss{Int64} @628,\n            weights = nothing,\n            class_weights = nothing,\n            operation = MLJModelInterface.predict,\n            retrain = true,\n            check_measure = true,\n            iteration_parameter = nothing,\n            cache = true)) @585"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "cell_type": "code",
   "source": [
    "pipe = @pipeline ContinuousEncoder iterated_booster"
   ],
   "metadata": {},
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Composition 3: Wrapping the model to make it \"self-tuning\""
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "First, we define a hyper-parameter range for optimization of a\n",
    "(nested) hyper-parameter:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "typename(MLJBase.NumericRange)(Int64, :(deterministic_iterated_model.model.max_depth), ... )"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "cell_type": "code",
   "source": [
    "max_depth_range = range(pipe,\n",
    "                        :(deterministic_iterated_model.model.max_depth),\n",
    "                        lower = 1,\n",
    "                        upper = 10)"
   ],
   "metadata": {},
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we can wrap the pipeline model in an optimization strategy to make\n",
    "it \"self-tuning\":"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "DeterministicTunedModel(\n    model = Pipeline290(\n            continuous_encoder = ContinuousEncoder @294,\n            deterministic_iterated_model = DeterministicIteratedModel{EvoTreeRegressor{Float64,…}} @630),\n    tuning = RandomSearch(\n            bounded = Distributions.Uniform,\n            positive_unbounded = Distributions.Gamma,\n            other = Distributions.Normal,\n            rng = Random._GLOBAL_RNG()),\n    resampling = CV(\n            nfolds = 3,\n            shuffle = true,\n            rng = MersenneTwister(456)),\n    measure = LPLoss(\n            p = 1),\n    weights = nothing,\n    operation = MLJModelInterface.predict,\n    range = NumericRange(\n            field = :(deterministic_iterated_model.model.max_depth),\n            lower = 1,\n            upper = 10,\n            origin = 5.5,\n            unit = 4.5,\n            scale = :linear),\n    selection_heuristic = MLJTuning.NaiveSelection(nothing),\n    train_best = true,\n    repeats = 1,\n    n = 50,\n    acceleration = CPUThreads{Int64}(5),\n    acceleration_resampling = CPU1{Nothing}(nothing),\n    check_measure = true,\n    cache = true) @132"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "cell_type": "code",
   "source": [
    "self_tuning_pipe = TunedModel(model=pipe,\n",
    "                              tuning=RandomSearch(),\n",
    "                              ranges = max_depth_range,\n",
    "                              resampling=CV(nfolds=3, rng=456),\n",
    "                              measure=l1,\n",
    "                              acceleration=CPUThreads(),\n",
    "                              n=50)"
   ],
   "metadata": {},
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Binding to data and evaluating performance"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Loading a selection of features and labels from the Ames\n",
    "House Price dataset:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "X, y = @load_reduced_ames;"
   ],
   "metadata": {},
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "Binding the \"self-tuning\" pipeline model to data in a *machine* (which\n",
    "will additionally store *learned* parameters):"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Machine{DeterministicTunedModel{RandomSearch,…},…} @769 trained 0 times; caches data\n  args: \n    1:\tSource @557 ⏎ `Table{Union{AbstractVector{Continuous}, AbstractVector{Count}, AbstractVector{Multiclass{15}}, AbstractVector{Multiclass{25}}, AbstractVector{OrderedFactor{10}}}}`\n    2:\tSource @538 ⏎ `AbstractVector{Continuous}`\n"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "cell_type": "code",
   "source": [
    "mach = machine(self_tuning_pipe, X, y)"
   ],
   "metadata": {},
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "Evaluating the \"self-tuning\" pipeline model's performance using 5-fold\n",
    "cross-validation (implies multiple layers of nested resampling):"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Info: Performing evaluations using 5 threads.\n",
      "\rEvaluating over 5 folds:  40%[==========>              ]  ETA: 0:08:13\u001b[K\rEvaluating over 5 folds:  60%[===============>         ]  ETA: 0:03:44\u001b[K\rEvaluating over 5 folds:  80%[====================>    ]  ETA: 0:01:24\u001b[K\rEvaluating over 5 folds: 100%[=========================] Time: 0:06:01\u001b[K\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "┌────────────────────┬───────────────┬──────────────────────────────────────────\n│\u001b[22m _.measure          \u001b[0m│\u001b[22m _.measurement \u001b[0m│\u001b[22m _.per_fold                             \u001b[0m ⋯\n├────────────────────┼───────────────┼──────────────────────────────────────────\n│ LPLoss{Int64} @628 │ 17000.0       │ [16500.0, 16200.0, 16600.0, 16600.0, 19 ⋯\n│ LPLoss{Int64} @406 │ 6.86e8        │ [6.18e8, 6.16e8, 6.08e8, 6.21e8, 9.66e8 ⋯\n└────────────────────┴───────────────┴──────────────────────────────────────────\n\u001b[36m                                                                1 column omitted\u001b[0m\n_.per_observation = [[[24800.0, 29400.0, ..., 5360.0], [4300.0, 31900.0, ..., 12600.0], [22400.0, 51600.0, ..., 35700.0], [1940.0, 22200.0, ..., 1920.0], [8920.0, 17900.0, ..., 6750.0]], [[6.15e8, 8.67e8, ..., 2.88e7], [1.85e7, 1.02e9, ..., 1.59e8], [5.03e8, 2.66e9, ..., 1.27e9], [3.76e6, 4.91e8, ..., 3.7e6], [7.96e7, 3.19e8, ..., 4.55e7]]]\n_.fitted_params_per_fold = [ … ]\n_.report_per_fold = [ … ]\n_.train_test_rows = [ … ]\n"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "cell_type": "code",
   "source": [
    "evaluate!(mach,\n",
    "          measures=[l1, l2],\n",
    "          resampling=CV(nfolds=5, rng=123),\n",
    "          acceleration=CPUThreads())"
   ],
   "metadata": {},
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.0"
  },
  "kernelspec": {
   "name": "julia-1.6",
   "display_name": "Julia 1.6.0",
   "language": "julia"
  }
 },
 "nbformat": 4
}
