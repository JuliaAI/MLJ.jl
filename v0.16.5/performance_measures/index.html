<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Performance Measures · MLJ</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.svg" alt="MLJ logo"/></a><div class="docs-package-name"><span class="docs-autofit">MLJ</span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Introduction</a></li><li><a class="tocitem" href="../getting_started/">Getting Started</a></li><li><a class="tocitem" href="../common_mlj_workflows/">Common MLJ Workflows</a></li><li><a class="tocitem" href="../working_with_categorical_data/">Working with Categorical Data</a></li><li><a class="tocitem" href="../model_search/">Model Search</a></li><li><a class="tocitem" href="../loading_model_code/">Loading Model Code</a></li><li><a class="tocitem" href="../machines/">Machines</a></li><li><a class="tocitem" href="../evaluating_model_performance/">Evaluating Model Performance</a></li><li class="is-active"><a class="tocitem" href>Performance Measures</a><ul class="internal"><li><a class="tocitem" href="#Using-built-in-measures"><span>Using built-in measures</span></a></li><li><a class="tocitem" href="#Traits-and-custom-measures"><span>Traits and custom measures</span></a></li><li><a class="tocitem" href="#Using-measures-from-LossFunctions.jl"><span>Using measures from LossFunctions.jl</span></a></li><li><a class="tocitem" href="#List-of-measures"><span>List of measures</span></a></li><li><a class="tocitem" href="#Other-performance-related-tools"><span>Other performance related tools</span></a></li></ul></li><li><a class="tocitem" href="../weights/">Weights</a></li><li><a class="tocitem" href="../tuning_models/">Tuning Models</a></li><li><a class="tocitem" href="../learning_curves/">Learning Curves</a></li><li><a class="tocitem" href="../transformers/">Transformers and other unsupervised models</a></li><li><a class="tocitem" href="../composing_models/">Composing Models</a></li><li><a class="tocitem" href="../controlling_iterative_models/">Controlling Iterative Models</a></li><li><a class="tocitem" href="../homogeneous_ensembles/">Homogeneous Ensembles</a></li><li><a class="tocitem" href="../generating_synthetic_data/">Generating Synthetic Data</a></li><li><a class="tocitem" href="../openml_integration/">OpenML Integration</a></li><li><a class="tocitem" href="../acceleration_and_parallelism/">Acceleration and Parallelism</a></li><li><a class="tocitem" href="../simple_user_defined_models/">Simple User Defined Models</a></li><li><a class="tocitem" href="../quick_start_guide_to_adding_models/">Quick-Start Guide to Adding Models</a></li><li><a class="tocitem" href="../adding_models_for_general_use/">Adding Models for General Use</a></li><li><a class="tocitem" href="../benchmarking/">Benchmarking</a></li><li><a class="tocitem" href="../internals/">Internals</a></li><li><a class="tocitem" href="../list_of_supported_models/">List of Supported Models</a></li><li><a class="tocitem" href="../third_party_packages/">Third Party Packages</a></li><li><a class="tocitem" href="../glossary/">Glossary</a></li><li><a class="tocitem" href="../mlj_cheatsheet/">MLJ Cheatsheet</a></li><li><a class="tocitem" href="../frequently_asked_questions/">FAQ</a></li><li><a class="tocitem" href="../julia_blogpost/">Julia BlogPost</a></li><li><a class="tocitem" href="../api/">Index of Methods</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Performance Measures</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Performance Measures</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/alan-turing-institute/MLJ.jl/blob/master/docs/src/performance_measures.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Performance-Measures"><a class="docs-heading-anchor" href="#Performance-Measures">Performance Measures</a><a id="Performance-Measures-1"></a><a class="docs-heading-anchor-permalink" href="#Performance-Measures" title="Permalink"></a></h1><p>In MLJ loss functions, scoring rules, sensitivities, and so on, are collectively referred to as <em>measures</em>. These include re-exported loss functions from the <a href="https://github.com/JuliaML/LossFunctions.jl">LossFunctions.jl</a> library, overloaded to behave the same way as the built-in measures.</p><p>To see list all measures, run <code>measures()</code>.  Further measures for probabilistic predictors, such as proper scoring rules, and for constructing multi-target product measures, are planned.  If you&#39;d like to see measure added to MLJ, post a comment <a href="https://github.com/alan-turing-institute/MLJBase.jl/issues/299">here</a></p><p><em>Note for developers:</em> The measures interface and the built-in measures described here are defined in MLJBase, but will ultimately live in a separate package.</p><h2 id="Using-built-in-measures"><a class="docs-heading-anchor" href="#Using-built-in-measures">Using built-in measures</a><a id="Using-built-in-measures-1"></a><a class="docs-heading-anchor-permalink" href="#Using-built-in-measures" title="Permalink"></a></h2><p>These measures all have the common calling syntax</p><pre><code class="language-julia">measure(ŷ, y)</code></pre><p>or</p><pre><code class="language-julia">measure(ŷ, y, w)</code></pre><p>where <code>y</code> iterates over observations of some target variable, and <code>ŷ</code> iterates over predictions (<code>Distribution</code> or <code>Sampler</code> objects in the probabilistic case). Here <code>w</code> is an optional vector of sample weights, or a dictionary of class weights, when these are supported by the measure.</p><pre><code class="language-julia-repl">julia&gt; using MLJ

julia&gt; y = [1, 2, 3, 4];

julia&gt; ŷ = [2, 3, 3, 3];

julia&gt; w = [1, 2, 2, 1];

julia&gt; rms(ŷ, y) # reports an aggregrate loss
0.8660254037844386

julia&gt; l2(ŷ, y, w) # reports per observation losses
4-element Vector{Int64}:
 1
 2
 0
 1

julia&gt; y = coerce([&quot;male&quot;, &quot;female&quot;, &quot;female&quot;], Multiclass)
3-element CategoricalArray{String,1,UInt32}:
 &quot;male&quot;
 &quot;female&quot;
 &quot;female&quot;

julia&gt; d = UnivariateFinite([&quot;male&quot;, &quot;female&quot;], [0.55, 0.45], pool=y);

julia&gt; ŷ = [d, d, d];

julia&gt; log_loss(ŷ, y)
3-element Vector{Float64}:
 0.7985076962177716
 0.5978370007556204
 0.5978370007556204</code></pre><p>The measures <code>rms</code>, <code>l2</code> and <code>log_loss</code> illustrated here are actually 	instances of measure <em>types</em>. For, example, <code>l2 = LPLoss(p=2)</code> and <code>log_loss = LogLoss() = LogLoss(tol=eps())</code>. Common aliases are provided:</p><pre><code class="language-julia-repl">julia&gt; cross_entropy
LogLoss(
    tol = 2.220446049250313e-16) @987</code></pre><h2 id="Traits-and-custom-measures"><a class="docs-heading-anchor" href="#Traits-and-custom-measures">Traits and custom measures</a><a id="Traits-and-custom-measures-1"></a><a class="docs-heading-anchor-permalink" href="#Traits-and-custom-measures" title="Permalink"></a></h2><p>Notice that <code>l1</code> reports per-sample evaluations, while <code>rms</code> only reports an aggregated result. This and other behavior can be gleaned from measure <em>traits</em> which are summarized by the <code>info</code> method:</p><pre><code class="language-julia-repl">julia&gt; info(l1)
`LPLoss` - lp loss type with instances `l1`, `l2`.
(name = &quot;LPLoss&quot;,
 instances = [&quot;l1&quot;, &quot;l2&quot;],
 human_name = &quot;lp loss&quot;,
 target_scitype = Union{AbstractVector{Continuous}, AbstractVector{Count}},
 supports_weights = true,
 supports_class_weights = false,
 prediction_type = :deterministic,
 orientation = :loss,
 reports_each_observation = true,
 aggregation = StatisticalTraits.Mean(),
 is_feature_dependent = false,
 docstring = &quot;`LPLoss` - lp loss type with instances `l1`, `l2`. &quot;,
 distribution_type = missing,)</code></pre><p>Query the doc-string for a measure using the name of its type:</p><pre><code class="language-julia-repl">julia&gt; rms
RootMeanSquaredError() @647

julia&gt; @doc RootMeanSquaredError # same as `?RootMeanSqauredError
  MLJBase.RootMeanSquaredError

  A measure type for root mean squared error, which includes the instance(s):
  rms, rmse, root_mean_squared_error.

  RootMeanSquaredError()(ŷ, y)
  RootMeanSquaredError()(ŷ, y, w)

  Evaluate the root mean squared error on observations ŷ, given ground truth
  values y. Optionally specify per-sample weights, w.

  \text{root mean squared error} = \sqrt{n^{-1}∑ᵢ|yᵢ-ŷᵢ|^2} or \text{root
  mean squared error} = \sqrt{\frac{∑ᵢwᵢ|yᵢ-ŷᵢ|^2}{∑ᵢwᵢ}}

  Requires scitype(y) to be a subtype of
  Union{AbstractVector{ScientificTypes.Continuous},
  AbstractVector{ScientificTypes.Count}}; ŷ must be a deterministic
  prediction.

  For more information, run info(RootMeanSquaredError).</code></pre><p>Use <code>measures()</code> to list all measures, and <code>measures(conditions...)</code> to search for measures with given traits (as you would <a href="../model_search/">query models</a>). The trait <code>instances</code> list the actual callable instances of a given measure type (typically aliases for the default instance).</p><article class="docstring"><header><a class="docstring-binding" id="MLJBase.measures-Tuple" href="#MLJBase.measures-Tuple"><code>MLJBase.measures</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">measures()</code></pre><p>List all measures as named-tuples keyed on measure traits.</p><pre><code class="language-none">measures(filters...)</code></pre><p>List all measures compatible with the target <code>y</code>.</p><pre><code class="language-none">measures(needle::Union{AbstractString,Regex}</code></pre><p>List all measures with <code>needle</code> in a measure&#39;s <code>name</code> or <code>docstring</code>.</p><p><strong>Example</strong></p><p>Find all classification measures supporting sample weights:</p><pre><code class="language-none">measures(m -&gt; m.target_scitype &lt;: AbstractVector{&lt;:Finite} &amp;&amp;
              m.supports_weights)</code></pre><p>Find all measures in the <code>rms</code> family:</p><pre><code class="language-none">measures(&quot;rms&quot;)</code></pre></div></section></article><p>A user-defined measure in MLJ can be passed to the <code>evaluate!</code> method, and elsewhere in MLJ, provided it is a function or callable object conforming to the above syntactic conventions. By default, a custom measure is understood to:</p><ul><li><p>be a loss function (rather than a score)</p></li><li><p>report an aggregated value (rather than per-sample evaluations)</p></li><li><p>be feature-independent</p></li></ul><p>To override this behaviour one simply overloads the appropriate trait, as shown in the following examples:</p><pre><code class="language-julia-repl">julia&gt; y = [1, 2, 3, 4];

julia&gt; ŷ = [2, 3, 3, 3];

julia&gt; w = [1, 2, 2, 1];

julia&gt; my_loss(ŷ, y) = maximum((ŷ - y).^2);

julia&gt; my_loss(ŷ, y)
1

julia&gt; my_per_sample_loss(ŷ, y) = abs.(ŷ - y);

julia&gt; MLJ.reports_each_observation(::typeof(my_per_sample_loss)) = true;

julia&gt; my_per_sample_loss(ŷ, y)
4-element Vector{Int64}:
 1
 1
 0
 1

julia&gt; my_weighted_score(ŷ, y) = 1/mean(abs.(ŷ - y));

julia&gt; my_weighted_score(ŷ, y, w) = 1/mean(abs.((ŷ - y).^w));

julia&gt; MLJ.supports_weights(::typeof(my_weighted_score)) = true;

julia&gt; MLJ.orientation(::typeof(my_weighted_score)) = :score;

julia&gt; my_weighted_score(ŷ, y)
1.3333333333333333

julia&gt; X = (x=rand(4), penalty=[1, 2, 3, 4]);

julia&gt; my_feature_dependent_loss(ŷ, X, y) = sum(abs.(ŷ - y) .* X.penalty)/sum(X.penalty);

julia&gt; MLJ.is_feature_dependent(::typeof(my_feature_dependent_loss)) = true

julia&gt; my_feature_dependent_loss(ŷ, X, y)
0.7</code></pre><p>The possible signatures for custom measures are: <code>measure(ŷ, y)</code>, <code>measure(ŷ, y, w)</code>, <code>measure(ŷ, X, y)</code> and <code>measure(ŷ, X, y, w)</code>, each measure implementing one non-weighted version, and possibly a second weighted version.</p><p><em>Implementation detail:</em> Internally, every measure is evaluated using the syntax</p><pre><code class="language-julia">MLJ.value(measure, ŷ, X, y, w)</code></pre><p>and the traits determine what can be ignored and how <code>measure</code> is actually called. If <code>w=nothing</code> then the non-weighted form of <code>measure</code> is dispatched.</p><h2 id="Using-measures-from-LossFunctions.jl"><a class="docs-heading-anchor" href="#Using-measures-from-LossFunctions.jl">Using measures from LossFunctions.jl</a><a id="Using-measures-from-LossFunctions.jl-1"></a><a class="docs-heading-anchor-permalink" href="#Using-measures-from-LossFunctions.jl" title="Permalink"></a></h2><p>The <a href="https://github.com/JuliaML/LossFunctions.jl">LossFunctions.jl</a> package includes &quot;distance loss&quot; functions for <code>Continuous</code> targets, and &quot;marginal loss&quot; functions for <code>Finite{2}</code> (binary) targets. While the LossFunctions.jl interface differs from the present one (for, example binary observations must be +1 or -1), MLJ has overloaded instances of the LossFunctions.jl types to behave the same as the built-in types.</p><p>Note that the &quot;distance losses&quot; in the package apply to deterministic predictions, while the &quot;marginal losses&quot; apply to probabilistic predictions.</p><h2 id="List-of-measures"><a class="docs-heading-anchor" href="#List-of-measures">List of measures</a><a id="List-of-measures-1"></a><a class="docs-heading-anchor-permalink" href="#List-of-measures" title="Permalink"></a></h2><pre><code class="language-julia">ms = measures()
types = map(ms) do m m.name end
instance = map(ms) do m m.instances end
t = (type=types, instances=instance)
DataFrame(t)</code></pre><table class="data-frame"><thead><tr><th></th><th>type</th><th>instances</th></tr><tr><th></th><th>String</th><th>Array…</th></tr></thead><tbody><p>59 rows × 2 columns</p><tr><th>1</th><td>Accuracy</td><td>["accuracy"]</td></tr><tr><th>2</th><td>AreaUnderCurve</td><td>["area_under_curve", "auc"]</td></tr><tr><th>3</th><td>BalancedAccuracy</td><td>["balanced_accuracy", "bacc", "bac"]</td></tr><tr><th>4</th><td>BrierLoss</td><td>["brier_loss"]</td></tr><tr><th>5</th><td>BrierScore</td><td>["brier_score"]</td></tr><tr><th>6</th><td>ConfusionMatrix</td><td>["confusion_matrix", "confmat"]</td></tr><tr><th>7</th><td>FScore</td><td>["f1score"]</td></tr><tr><th>8</th><td>FalseDiscoveryRate</td><td>["false_discovery_rate", "falsediscovery_rate", "fdr"]</td></tr><tr><th>9</th><td>FalseNegative</td><td>["false_negative", "falsenegative"]</td></tr><tr><th>10</th><td>FalseNegativeRate</td><td>["false_negative_rate", "falsenegative_rate", "fnr", "miss_rate"]</td></tr><tr><th>11</th><td>FalsePositive</td><td>["false_positive", "falsepositive"]</td></tr><tr><th>12</th><td>FalsePositiveRate</td><td>["false_positive_rate", "falsepositive_rate", "fpr", "fallout"]</td></tr><tr><th>13</th><td>LPLoss</td><td>["l1", "l2"]</td></tr><tr><th>14</th><td>LogCoshLoss</td><td>["log_cosh", "log_cosh_loss"]</td></tr><tr><th>15</th><td>LogLoss</td><td>["log_loss", "cross_entropy"]</td></tr><tr><th>16</th><td>MatthewsCorrelation</td><td>["matthews_correlation", "mcc"]</td></tr><tr><th>17</th><td>MeanAbsoluteError</td><td>["mae", "mav", "mean_absolute_error", "mean_absolute_value"]</td></tr><tr><th>18</th><td>MeanAbsoluteProportionalError</td><td>["mape"]</td></tr><tr><th>19</th><td>MisclassificationRate</td><td>["misclassification_rate", "mcr"]</td></tr><tr><th>20</th><td>MulticlassFScore</td><td>["macro_f1score", "micro_f1score", "multiclass_f1score"]</td></tr><tr><th>21</th><td>MulticlassFalseDiscoveryRate</td><td>["multiclass_falsediscovery_rate", "multiclass_fdr"]</td></tr><tr><th>22</th><td>MulticlassFalseNegative</td><td>["multiclass_false_negative", "multiclass_falsenegative"]</td></tr><tr><th>23</th><td>MulticlassFalseNegativeRate</td><td>["multiclass_false_negative_rate", "multiclass_fnr", "multiclass_miss_rate", "multiclass_falsenegative_rate"]</td></tr><tr><th>24</th><td>MulticlassFalsePositive</td><td>["multiclass_false_positive", "multiclass_falsepositive"]</td></tr><tr><th>25</th><td>MulticlassFalsePositiveRate</td><td>["multiclass_false_positive_rate", "multiclass_fpr", "multiclass_fallout", "multiclass_falsepositive_rate"]</td></tr><tr><th>26</th><td>MulticlassNegativePredictiveValue</td><td>["multiclass_negative_predictive_value", "multiclass_negativepredictive_value", "multiclass_npv"]</td></tr><tr><th>27</th><td>MulticlassPrecision</td><td>["multiclass_positive_predictive_value", "multiclass_ppv", "multiclass_positivepredictive_value", "multiclass_recall"]</td></tr><tr><th>28</th><td>MulticlassTrueNegative</td><td>["multiclass_true_negative", "multiclass_truenegative"]</td></tr><tr><th>29</th><td>MulticlassTrueNegativeRate</td><td>["multiclass_true_negative_rate", "multiclass_tnr", "multiclass_specificity", "multiclass_selectivity", "multiclass_truenegative_rate"]</td></tr><tr><th>30</th><td>MulticlassTruePositive</td><td>["multiclass_true_positive", "multiclass_truepositive"]</td></tr><tr><th>31</th><td>MulticlassTruePositiveRate</td><td>["multiclass_true_positive_rate", "multiclass_tpr", "multiclass_sensitivity", "multiclass_recall", "multiclass_hit_rate", "multiclass_truepositive_rate"]</td></tr><tr><th>32</th><td>NegativePredictiveValue</td><td>["negative_predictive_value", "negativepredictive_value", "npv"]</td></tr><tr><th>33</th><td>Precision</td><td>["positive_predictive_value", "ppv", "positivepredictive_value", "precision"]</td></tr><tr><th>34</th><td>RootMeanSquaredError</td><td>["rms", "rmse", "root_mean_squared_error"]</td></tr><tr><th>35</th><td>RootMeanSquaredLogError</td><td>["rmsl", "rmsle", "root_mean_squared_log_error"]</td></tr><tr><th>36</th><td>RootMeanSquaredLogProportionalError</td><td>["rmslp1"]</td></tr><tr><th>37</th><td>RootMeanSquaredProportionalError</td><td>["rmsp"]</td></tr><tr><th>38</th><td>TrueNegative</td><td>["true_negative", "truenegative"]</td></tr><tr><th>39</th><td>TrueNegativeRate</td><td>["true_negative_rate", "truenegative_rate", "tnr", "specificity", "selectivity"]</td></tr><tr><th>40</th><td>TruePositive</td><td>["true_positive", "truepositive"]</td></tr><tr><th>41</th><td>TruePositiveRate</td><td>["true_positive_rate", "truepositive_rate", "tpr", "sensitivity", "recall", "hit_rate"]</td></tr><tr><th>42</th><td>DWDMarginLoss</td><td>["dwd_margin_loss"]</td></tr><tr><th>43</th><td>ExpLoss</td><td>["exp_loss"]</td></tr><tr><th>44</th><td>L1HingeLoss</td><td>["l1_hinge_loss"]</td></tr><tr><th>45</th><td>L2HingeLoss</td><td>["l2_hinge_loss"]</td></tr><tr><th>46</th><td>L2MarginLoss</td><td>["l2_margin_loss"]</td></tr><tr><th>47</th><td>LogitMarginLoss</td><td>["logit_margin_loss"]</td></tr><tr><th>48</th><td>ModifiedHuberLoss</td><td>["modified_huber_loss"]</td></tr><tr><th>49</th><td>PerceptronLoss</td><td>["perceptron_loss"]</td></tr><tr><th>50</th><td>SigmoidLoss</td><td>["sigmoid_loss"]</td></tr><tr><th>51</th><td>SmoothedL1HingeLoss</td><td>["smoothed_l1_hinge_loss"]</td></tr><tr><th>52</th><td>ZeroOneLoss</td><td>["zero_one_loss"]</td></tr><tr><th>53</th><td>HuberLoss</td><td>["huber_loss"]</td></tr><tr><th>54</th><td>L1EpsilonInsLoss</td><td>["l1_epsilon_ins_loss"]</td></tr><tr><th>55</th><td>L2EpsilonInsLoss</td><td>["l2_epsilon_ins_loss"]</td></tr><tr><th>56</th><td>LPDistLoss</td><td>["lp_dist_loss"]</td></tr><tr><th>57</th><td>LogitDistLoss</td><td>["logit_dist_loss"]</td></tr><tr><th>58</th><td>PeriodicLoss</td><td>["periodic_loss"]</td></tr><tr><th>59</th><td>QuantileLoss</td><td>["quantile_loss"]</td></tr></tbody></table><h2 id="Other-performance-related-tools"><a class="docs-heading-anchor" href="#Other-performance-related-tools">Other performance related tools</a><a id="Other-performance-related-tools-1"></a><a class="docs-heading-anchor-permalink" href="#Other-performance-related-tools" title="Permalink"></a></h2><p>In MLJ one computes a confusion matrix by calling an instance of the <code>ConfusionMatrix</code> measure type on the data:</p><article class="docstring"><header><a class="docstring-binding" id="MLJBase.ConfusionMatrix" href="#MLJBase.ConfusionMatrix"><code>MLJBase.ConfusionMatrix</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">MLJBase.ConfusionMatrix</code></pre><p>A measure type for confusion matrix, which includes the instance(s): <code>confusion_matrix</code>, <code>confmat</code>.</p><pre><code class="language-none">ConfusionMatrix()(ŷ, y)</code></pre><p>Evaluate the default instance of ConfusionMatrix on observations <code>ŷ</code>, given ground truth values <code>y</code>. </p><p>If <code>r</code> is the return value, then the raw confusion matrix is <code>r.mat</code>, whose rows correspond to predictions, and columns to ground truth. The ordering follows that of <code>levels(y)</code>.</p><p>Use <code>ConfusionMatrix(perm=[2, 1])</code> to reverse the class order for binary data. For more than two classes, specify an appropriate permutation, as in <code>ConfusionMatrix(perm=[2, 3, 1])</code>.</p><p>Requires <code>scitype(y)</code> to be a subtype of <code>AbstractArray{&lt;:OrderedFactor{2}}</code> (binary classification where choice of &quot;true&quot; effects the measure); <code>ŷ</code> must be a deterministic prediction. </p><p>For more information, run <code>info(ConfusionMatrix)</code>. </p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.roc_curve" href="#MLJBase.roc_curve"><code>MLJBase.roc_curve</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">fprs, tprs, ts = roc_curve(ŷ, y) = roc(ŷ, y)</code></pre><p>Return the ROC curve for a two-class probabilistic prediction <code>ŷ</code> given the ground  truth <code>y</code>. The true positive rates, false positive rates over a range of thresholds <code>ts</code> are returned. Note that if there are <code>k</code> unique scores, there are correspondingly  <code>k</code> thresholds and <code>k+1</code> &quot;bins&quot; over which the FPR and TPR are constant:</p><ul><li><code>[0.0 - thresh[1]]</code></li><li><code>[thresh[1] - thresh[2]]</code></li><li>...</li><li><code>[thresh[k] - 1]</code></li></ul><p>consequently, <code>tprs</code> and <code>fprs</code> are of length <code>k+1</code> if <code>ts</code> is of length <code>k</code>.</p><p>To draw the curve using your favorite plotting backend, do <code>plot(fprs, tprs)</code>.</p></div></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../evaluating_model_performance/">« Evaluating Model Performance</a><a class="docs-footer-nextpage" href="../weights/">Weights »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Wednesday 16 June 2021 08:31">Wednesday 16 June 2021</span>. Using Julia version 1.6.1.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
