<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Evaluating model performance Â· MLJ</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link href="../assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><h1>MLJ</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="../search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="../">Getting Started</a></li><li class="current"><a class="toctext" href>Evaluating model performance</a><ul class="internal"></ul></li><li><a class="toctext" href="../measures/">Measures</a></li><li><a class="toctext" href="../tuning_models/">Tuning models</a></li><li><a class="toctext" href="../built_in_transformers/">Built-in Transformers</a></li><li><a class="toctext" href="../learning_networks/">Learning Networks</a></li><li><a class="toctext" href="../simple_user_defined_models/">Simple User Defined Models</a></li><li><a class="toctext" href="../adding_models_for_general_use/">Adding Models for General Use</a></li><li><a class="toctext" href="../working_with_tasks/">Working with Tasks</a></li><li><a class="toctext" href="../benchmarking/">Benchmarking</a></li><li><a class="toctext" href="../internals/">Internals</a></li><li><a class="toctext" href="../glossary/">Glossary</a></li><li><a class="toctext" href="../api/">API</a></li><li><a class="toctext" href="../mlj_cheatsheet/">MLJ Cheatsheet</a></li><li><a class="toctext" href="../NEWS/">MLJ News</a></li><li><a class="toctext" href="../frequently_asked_questions/">FAQ</a></li><li><a class="toctext" href="../julia_blogpost/">Julia BlogPost</a></li></ul></nav><article id="docs"><header><nav><ul><li><a href>Evaluating model performance</a></li></ul><a class="edit-page" href="https://github.com/alan-turing-institute/MLJ.jl/blob/master/docs/src/evaluating_model_performance.md"><span class="fa">ï‚›</span> Edit on GitHub</a></nav><hr/><div id="topbar"><span>Evaluating model performance</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="Evaluation-of-supervised-models-1" href="#Evaluation-of-supervised-models-1">Evaluation of supervised models</a></h1><p>MLJ allows quick evaluation of a model&#39;s performance against a battery of selected losses or scores. For more on available performance measures, see <a href="../measures/">Measures</a>.</p><p>In addition to hold-out and cross-validation, the user can specify their own list of train/evaluation pairs of row indices for resampling, or define their own re-usable resampling strategies.</p><p>For simultaneously evaluating <em>multiple</em> models and/or data sets, see <a href="../benchmarking/">Benchmarking</a>.</p><h3><a class="nav-anchor" id="Evaluating-against-a-single-measure-1" href="#Evaluating-against-a-single-measure-1">Evaluating against a single measure</a></h3><pre><code class="language-julia-repl">julia&gt; using MLJ


julia&gt; X = (a=rand(12), b=rand(12), c=rand(12));

julia&gt; y = X.a + 2X.b + 0.05*rand(12);

julia&gt; @load RidgeRegressor

julia&gt; model = RidgeRegressor()
MLJModels.MultivariateStats_.RidgeRegressor(lambda = 0.0,) @ 1â€¦65

julia&gt; cv=CV(nfolds=3)
CV(nfolds = 3,
   shuffle = false,
   rng = MersenneTwister(UInt32[0xf59573d4, 0x13d39fe7, 0xcca310ea, 0x3351a36c]),) @ 1â€¦35

julia&gt; evaluate(model, X, y, resampling=cv, measure=l2)
Evaluating over 3 folds:  25%[======&gt;                  ]  ETA: 0:00:00[KEvaluating over 3 folds:  50%[============&gt;            ]  ETA: 0:00:03[KEvaluating over 3 folds:  75%[==================&gt;      ]  ETA: 0:00:01[KEvaluating over 3 folds: 100%[=========================] Time: 0:00:03[K
(measure = MLJ.L2[l2],
 measurement = [0.000242358],
 per_fold = Array{Float64,1}[[7.78885e-5, 0.000255661, 0.000393524]],
 per_observation = Array{Array{Float64,1},1}[[[8.55455e-6, 2.13485e-7, 0.000162514, 0.000140271], [4.40792e-5, 0.000470012, 0.000217537, 0.000291018], [0.000278847, 4.48783e-7, 0.0010294, 0.000265401]]],)</code></pre><p>Alternatively, instead of applying <code>evaluate</code> to a model + data, one may call <code>evaluate!</code> on an existing machine wrapping the model in data:</p><pre><code class="language-julia-repl">julia&gt; mach = machine(model, X, y)
Machine{RidgeRegressor} @ 1â€¦09

julia&gt; evaluate!(mach, resampling=cv, measure=l2)
Evaluating over 3 folds:  25%[======&gt;                  ]  ETA: 0:00:00[KEvaluating over 3 folds:  50%[============&gt;            ]  ETA: 0:00:00[KEvaluating over 3 folds:  75%[==================&gt;      ]  ETA: 0:00:00[KEvaluating over 3 folds: 100%[=========================] Time: 0:00:00[K
(measure = MLJ.L2[l2],
 measurement = [0.000242358],
 per_fold = Array{Float64,1}[[7.78885e-5, 0.000255661, 0.000393524]],
 per_observation = Array{Array{Float64,1},1}[[[8.55455e-6, 2.13485e-7, 0.000162514, 0.000140271], [4.40792e-5, 0.000470012, 0.000217537, 0.000291018], [0.000278847, 4.48783e-7, 0.0010294, 0.000265401]]],)</code></pre><p>(The latter call is a mutating call as the learned parameters stored in the machine potentially change. )</p><h3><a class="nav-anchor" id="Multiple-measures-1" href="#Multiple-measures-1">Multiple measures</a></h3><pre><code class="language-julia-repl">julia&gt; evaluate!(mach,
                 resampling=cv,
                 measure=[l1, rms, rmslp1])
Evaluating over 3 folds:  25%[======&gt;                  ]  ETA: 0:00:00[KEvaluating over 3 folds:  50%[============&gt;            ]  ETA: 0:00:00[KEvaluating over 3 folds:  75%[==================&gt;      ]  ETA: 0:00:00[KEvaluating over 3 folds: 100%[=========================] Time: 0:00:00[K
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ measure â”‚ measurement           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ l1      â”‚ 0.012820827982845793  â”‚
â”‚ rms     â”‚ 0.01488409683990373   â”‚
â”‚ rmslp1  â”‚ 0.0065726140101975625 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
(measure = MLJ.Measure[l1, rms, rmslp1],
 measurement = [0.0128208, 0.0148841, 0.00657261],
 per_fold = Array{Float64,1}[[0.00699465, 0.0150318, 0.016436], [0.00882544, 0.0159894, 0.0198374], [0.0031453, 0.00770373, 0.00886882]],
 per_observation = Union{Missing, Array{Array{Float64,1},1}}[Array{Float64,1}[[0.00292482, 0.000462045, 0.0127481, 0.0118436], [0.00663921, 0.0216798, 0.0147491, 0.0170593], [0.0166987, 0.000669912, 0.0320842, 0.0162911]], missing, missing],)</code></pre><h3><a class="nav-anchor" id="Custom-measures-and-weighted-measures-1" href="#Custom-measures-and-weighted-measures-1">Custom measures and weighted measures</a></h3><pre><code class="language-julia-repl">julia&gt; my_loss(yhat, y) = maximum((yhat - y).^2);

julia&gt; my_per_observation_loss(yhat, y) = abs.(yhat - y);

julia&gt; MLJ.reports_each_observation(::typeof(my_per_observation_loss)) = true;

julia&gt; my_weighted_score(yhat, y) = 1/mean(abs.(yhat - y));

julia&gt; my_weighted_score(yhat, y, w) = 1/mean(abs.((yhat - y).^w));

julia&gt; MLJ.supports_weights(::typeof(my_weighted_score)) = true;

julia&gt; MLJ.orientation(::typeof(my_weighted_score)) = :score;

julia&gt; holdout = Holdout(fraction_train=0.8)
Holdout(fraction_train = 0.8,
        shuffle = false,
        rng = MersenneTwister(UInt32[0xf59573d4, 0x13d39fe7, 0xcca310ea, 0x3351a36c]),) @ 5â€¦76

julia&gt; weights = [1, 1, 2, 1, 1, 2, 3, 1, 1, 2, 3, 1];

julia&gt; evaluate!(mach,
                 resampling=CV(nfolds=3),
                 measure=[my_loss, my_per_observation_loss, my_weighted_score, l1],
                 weights=weights)
â”Œ Warning: weights ignored in evaluations of the following measures, as unsupported: 
â”‚ Main.ex-evaluation_of_supervised_models.my_loss, Main.ex-evaluation_of_supervised_models.my_per_observation_loss 
â”” @ MLJ ~/build/alan-turing-institute/MLJ.jl/src/resampling.jl:265
Evaluating over 3 folds:  25%[======&gt;                  ]  ETA: 0:00:00[KEvaluating over 3 folds:  50%[============&gt;            ]  ETA: 0:00:01[KEvaluating over 3 folds:  75%[==================&gt;      ]  ETA: 0:00:00[KEvaluating over 3 folds: 100%[=========================] Time: 0:00:00[K
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ measure                                                         â”‚ measurement           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Main.ex-evaluation_of_supervised_models.my_loss                 â”‚ 0.0005539745230079967 â”‚
â”‚ Main.ex-evaluation_of_supervised_models.my_per_observation_loss â”‚ 0.012820827982845793  â”‚
â”‚ Main.ex-evaluation_of_supervised_models.my_weighted_score       â”‚ 182.15591176275527    â”‚
â”‚ l1                                                              â”‚ 0.01423357600586587   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
(measure = Any[my_loss, my_per_observation_loss, my_weighted_score, l1],
 measurement = [0.000553975, 0.0128208, 182.156, 0.0142336],
 per_fold = Array{Float64,1}[[0.000162514, 0.000470012, 0.0010294], [0.00699465, 0.0150318, 0.016436], [259.858, 165.483, 121.126], [0.00814534, 0.0159008, 0.0186546]],
 per_observation = Union{Missing, Array{Array{Float64,1},1}}[missing, Array{Float64,1}[[0.00292482, 0.000462045, 0.0127481, 0.0118436], [0.00663921, 0.0216798, 0.0147491, 0.0170593], [0.0166987, 0.000669912, 0.0320842, 0.0162911]], missing, Array{Float64,1}[[0.00233985, 0.000369636, 0.020397, 0.0094749], [0.00379384, 0.0247769, 0.0252842, 0.00974815], [0.00954212, 0.000765614, 0.0550015, 0.00930923]]],)</code></pre><h3><a class="nav-anchor" id="User-specified-train/evaluation-sets-1" href="#User-specified-train/evaluation-sets-1">User-specified train/evaluation sets</a></h3><p>Users can either provide their own list of train/evaluation pairs of row indices for resampling, as in this example:</p><pre><code class="language-julia-repl">julia&gt; fold1 = 1:6; fold2 = 7:12;

julia&gt; evaluate!(mach,
                 resampling = [(fold1, fold2), (fold2, fold1)],
                 measure=[l1, l2])
Evaluating over 2 folds:  33%[========&gt;                ]  ETA: 0:00:00[KEvaluating over 2 folds:  67%[================&gt;        ]  ETA: 0:00:00[KEvaluating over 2 folds: 100%[=========================] Time: 0:00:00[K
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ measure â”‚ measurement           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ l1      â”‚ 0.011483315319115228  â”‚
â”‚ l2      â”‚ 0.0001917137023135988 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
(measure = MLJ.Measure[l1, l2],
 measurement = [0.0114833, 0.000191714],
 per_fold = Array{Float64,1}[[0.0111406, 0.0118261], [0.000217862, 0.000165565]],
 per_observation = Array{Array{Float64,1},1}[[[0.00992116, 0.000874664, 0.0133362, 0.00209603, 0.0303308, 0.0102846], [0.0119291, 0.00870139, 0.00838681, 0.0127127, 0.00698648, 0.0222399]], [[9.84294e-5, 7.65036e-7, 0.000177853, 4.39334e-6, 0.000919959, 0.000105772], [0.000142304, 7.57142e-5, 7.03386e-5, 0.000161614, 4.88109e-5, 0.000494612]]],)</code></pre><p>Or define their own re-usable <code>ResamplingStrategy</code> objects, - see <a href="#Custom-resampling-strategies-1">Custom resampling strategies</a> below.</p><h3><a class="nav-anchor" id="Resampling-strategies-1" href="#Resampling-strategies-1">Resampling strategies</a></h3><p><code>Holdout</code> and <code>CV</code> (cross-validation) resampling strategies are available:</p><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MLJ.Holdout" href="#MLJ.Holdout"><code>MLJ.Holdout</code></a> â€” <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-julia">Holdout(; fraction_train=0.7,
          shuffle=false,
          rng=Random.GLOBAL_RNG)</code></pre><p>Single train-test split with a (randomly selected) portion of the data being selected for training and the rest for testing.</p><p>If <code>rng</code> is an integer, then <code>MersenneTwister(rng)</code> is the random number generator used for shuffling rows. Otherwise some <code>AbstractRNG</code> object is expected.</p></div></div><a class="source-link" target="_blank" href="https://github.com/alan-turing-institute/MLJ.jl/blob/b64d2b0403b3e75db0824fbcb2cc1a1c8b1c6c0d/src/resampling.jl#L12-L24">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MLJ.CV" href="#MLJ.CV"><code>MLJ.CV</code></a> â€” <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-julia">CV(; nfolds=6,  shuffle=false, rng=Random.GLOBAL_RNG)</code></pre><p>Cross validation resampling where the data is (randomly) partitioned in <code>nfolds</code> folds and the model is evaluated <code>nfolds</code> times, each time taking one fold for testing and the remaining folds for training.  </p><p>For instance, if <code>nfolds=3</code> then the data will be partitioned in three folds A, B and C and the model will be trained three times, first with A and B and tested on C, then on A, C and tested on B and finally on B, C and tested on A.</p><p>If <code>rng</code> is an integer, then <code>MersenneTwister(rng)</code> is the random number generator used for shuffling rows. Otherwise some <code>AbstractRNG</code> object is expected.</p></div></div><a class="source-link" target="_blank" href="https://github.com/alan-turing-institute/MLJ.jl/blob/b64d2b0403b3e75db0824fbcb2cc1a1c8b1c6c0d/src/resampling.jl#L55-L71">source</a></section><h3><a class="nav-anchor" id="Custom-resampling-strategies-1" href="#Custom-resampling-strategies-1">Custom resampling strategies</a></h3><p>To define your own resampling strategy, make relevant parameters of your strategy the fields of a new type <code>MyResamplingStrategy &lt;: MLJ.ResamplingStrategy</code>, and implement <code>MLJ.train_eval_pairs(my_strategy::MyStragegy, rows)</code>, a method which will take a vector of indices <code>rows</code> and return a vector <code>[(t1, e1), (t2, e2), ... (tk, ek)]</code> of train/evaluation pairs of row indices selected from <code>rows</code>. Here is the code for the <code>Holdout</code> strategy as an example:</p><pre><code class="language-julia">struct Holdout &lt;: ResamplingStrategy
    fraction_train::Float64
    shuffle::Bool
    rng::Union{Int,AbstractRNG}
	
    function Holdout(fraction_train, shuffle, rng)
        0 &lt; fraction_train &lt; 1 || 
		error(&quot;`fraction_train` must be between 0 and 1.&quot;)
        return new(fraction_train, shuffle, rng)
    end
end

# Keyword Constructor
function Holdout(; fraction_train::Float64=0.7,
                   shuffle::Bool=false,
                   rng::Union{Int,AbstractRNG}=Random.GLOBAL_RNG)
    Holdout(fraction_train, shuffle, rng)
end

function train_eval_pairs(holdout::Holdout, rows)
    if holdout.rng isa Integer
        rng = MersenneTwister(holdout.rng)
    else
        rng = holdout.rng
    end
    train, evalu = partition(rows, holdout.fraction_train,
                             shuffle=holdout.shuffle, rng=rng)
    return [(train, evalu),]
end</code></pre><h3><a class="nav-anchor" id="API-1" href="#API-1">API</a></h3><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MLJ.evaluate!" href="#MLJ.evaluate!"><code>MLJ.evaluate!</code></a> â€” <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">evaluate!(mach,    
          resampling=CV(), 
          measure=nothing, 
          weights=nothing,
          operation=predict,  
          parallel=true,
          force=false, 
          verbosity=1)</code></pre><p>Estimate the performance of a machine <code>mach</code> wrapping a supervised model in data, using the specified <code>resampling</code> strategy (defaulting to 6-fold cross-validation) and <code>measure</code>, which can be a single measure or vector.</p><p>Do <code>subtypes(MLJ.ResamplingStrategy)</code> to obtain a list of available resampling strategies. If <code>resampling</code> is not an object of type <code>MLJ.ResamplingStrategy</code>, then a vector of pairs (of the form <code>(train_rows, eval_rows)</code> is expected. For example, setting</p><pre><code class="language-none">resampling = [(1:100), (101:200)), 
               (101:200), (1:100)]</code></pre><p>gives two-fold cross-validation using the first 200 rows of data.</p><p>If <code>resampling isa MLJ.ResamplingStrategy</code> then one may optionally restrict the data used in evaluation by specifying <code>rows</code>. </p><p>An optional <code>weights</code> vector may be passed for measures that support sample weights (<code>MLJ.supports_weights(measure) == true</code>), which is ignored by those that don&#39;t. </p><p>User-defined measures are supported; see the manual for details.</p><p>If no measure is specified, then <code>default_measure(mach.model)</code> is used, unless this default is <code>nothing</code> and an error is thrown.</p><p>Although evaluate! is mutating, <code>mach.model</code> and <code>mach.args</code> are untouched.</p></div></div><a class="source-link" target="_blank" href="https://github.com/alan-turing-institute/MLJ.jl/blob/b64d2b0403b3e75db0824fbcb2cc1a1c8b1c6c0d/src/resampling.jl#L122-L162">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MLJBase.evaluate" href="#MLJBase.evaluate"><code>MLJBase.evaluate</code></a> â€” <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">evaluate(model, X, y; measure=nothing, options...)</code></pre><p>Evaluate the performance of a supervised model <code>model</code> on input data <code>X</code> and target <code>y</code>. See the machine version <code>evaluate!</code> for options.</p></div></div><a class="source-link" target="_blank" href="https://github.com/alan-turing-institute/MLJ.jl/blob/b64d2b0403b3e75db0824fbcb2cc1a1c8b1c6c0d/src/resampling.jl#L167-L173">source</a></section><footer><hr/><a class="previous" href="../"><span class="direction">Previous</span><span class="title">Getting Started</span></a><a class="next" href="../measures/"><span class="direction">Next</span><span class="title">Measures</span></a></footer></article></body></html>
