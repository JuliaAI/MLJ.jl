<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Transformers and Other Unsupervised models · MLJ</title><meta name="title" content="Transformers and Other Unsupervised models · MLJ"/><meta property="og:title" content="Transformers and Other Unsupervised models · MLJ"/><meta property="twitter:title" content="Transformers and Other Unsupervised models · MLJ"/><meta name="description" content="Documentation for MLJ."/><meta property="og:description" content="Documentation for MLJ."/><meta property="twitter:description" content="Documentation for MLJ."/><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../search_index.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script><link href="../assets/favicon.ico" rel="icon" type="image/x-icon"/><link href="https://fonts.googleapis.com/css2?family=Lato:ital,wght@0,100;0,300;0,400;0,700;0,900;1,100;1,300;1,400;1,700;1,900&amp;family=Montserrat:ital,wght@0,100..900;1,100..900&amp;display=swap" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.svg" alt="MLJ logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../">MLJ</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../model_browser/">Model Browser</a></li><li><a class="tocitem" href="../about_mlj/">About MLJ</a></li><li><a class="tocitem" href="../learning_mlj/">Learning MLJ</a></li><li><input class="collapse-toggle" id="menuitem-5" type="checkbox"/><label class="tocitem" for="menuitem-5"><span class="docs-label">Basics</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../getting_started/">Getting Started</a></li><li><a class="tocitem" href="../common_mlj_workflows/">Common MLJ Workflows</a></li><li><a class="tocitem" href="../machines/">Machines</a></li><li><a class="tocitem" href="../mlj_cheatsheet/">MLJ Cheatsheet</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-6" type="checkbox"/><label class="tocitem" for="menuitem-6"><span class="docs-label">Data</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../working_with_categorical_data/">Working with Categorical Data</a></li><li><a class="tocitem" href="../preparing_data/">Preparing Data</a></li><li><a class="tocitem" href="../generating_synthetic_data/">Generating Synthetic Data</a></li><li><a class="tocitem" href="../openml_integration/">OpenML Integration</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-7" type="checkbox" checked/><label class="tocitem" for="menuitem-7"><span class="docs-label">Models</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../model_search/">Model Search</a></li><li><a class="tocitem" href="../loading_model_code/">Loading Model Code</a></li><li class="is-active"><a class="tocitem" href>Transformers and Other Unsupervised models</a><ul class="internal"><li><a class="tocitem" href="#Built-in-transformers"><span>Built-in transformers</span></a></li><li><a class="tocitem" href="#Static-transformers"><span>Static transformers</span></a></li><li><a class="tocitem" href="#Transformers-that-also-predict"><span>Transformers that also predict</span></a></li><li><a class="tocitem" href="#Reference"><span>Reference</span></a></li></ul></li><li><a class="tocitem" href="../feature_selection/">Feature Selection</a></li><li><a class="tocitem" href="../list_of_supported_models/">List of Supported Models</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-8" type="checkbox"/><label class="tocitem" for="menuitem-8"><span class="docs-label">Meta-algorithms</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../evaluating_model_performance/">Evaluating Model Performance</a></li><li><a class="tocitem" href="../tuning_models/">Tuning Models</a></li><li><a class="tocitem" href="../learning_curves/">Learning Curves</a></li><li><a class="tocitem" href="../controlling_iterative_models/">Controlling Iterative Models</a></li><li><a class="tocitem" href="../correcting_class_imbalance/">Correcting Class Imbalance</a></li><li><a class="tocitem" href="../thresholding_probabilistic_predictors/">Thresholding Probabilistic Predictors</a></li><li><a class="tocitem" href="../target_transformations/">Target Transformations</a></li><li><a class="tocitem" href="../homogeneous_ensembles/">Homogeneous Ensembles</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-9" type="checkbox"/><label class="tocitem" for="menuitem-9"><span class="docs-label">Model Composition</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../composing_models/">Composing Models</a></li><li><a class="tocitem" href="../linear_pipelines/">Linear Pipelines</a></li><li><a class="tocitem" href="../model_stacking/">Model Stacking</a></li><li><a class="tocitem" href="../learning_networks/">Learning Networks</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-10" type="checkbox"/><label class="tocitem" for="menuitem-10"><span class="docs-label">Third Party Tools</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../logging_workflows/">Logging Workflows</a></li><li><a class="tocitem" href="../third_party_packages/">Third Party Packages</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-11" type="checkbox"/><label class="tocitem" for="menuitem-11"><span class="docs-label">Customization and Extension</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../simple_user_defined_models/">Simple User Defined Models</a></li><li><a class="tocitem" href="../quick_start_guide_to_adding_models/">Quick-Start Guide to Adding Models</a></li><li><a class="tocitem" href="../adding_models_for_general_use/">Adding Models for General Use</a></li><li><a class="tocitem" href="../modifying_behavior/">Modifying Behavior</a></li><li><a class="tocitem" href="../internals/">Internals</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-12" type="checkbox"/><label class="tocitem" for="menuitem-12"><span class="docs-label">Miscellaneous</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../performance_measures/">Performance Measures</a></li><li><a class="tocitem" href="../weights/">Weights</a></li><li><a class="tocitem" href="../acceleration_and_parallelism/">Acceleration and Parallelism</a></li><li><a class="tocitem" href="../glossary/">Glossary</a></li><li><a class="tocitem" href="../frequently_asked_questions/">FAQ</a></li></ul></li><li><a class="tocitem" href="../api/">Index of Methods</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Models</a></li><li class="is-active"><a href>Transformers and Other Unsupervised models</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Transformers and Other Unsupervised models</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/JuliaAI/MLJ.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/JuliaAI/MLJ.jl/blob/dev/docs/src/transformers.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Transformers-and-Other-Unsupervised-Models"><a class="docs-heading-anchor" href="#Transformers-and-Other-Unsupervised-Models">Transformers and Other Unsupervised Models</a><a id="Transformers-and-Other-Unsupervised-Models-1"></a><a class="docs-heading-anchor-permalink" href="#Transformers-and-Other-Unsupervised-Models" title="Permalink"></a></h1><p>Several unsupervised models used to perform common transformations, such as one-hot encoding, are available in MLJ out-of-the-box. These are detailed in <a href="#Built-in-transformers">Built-in transformers</a> below.</p><p>A transformer is <em>static</em> if it has no learned parameters. While such a transformer is tantamount to an ordinary function, realizing it as an MLJ static transformer (a subtype of <code>Static &lt;: Unsupervised</code>) can be useful, especially if the function depends on parameters the user would like to manipulate (which become <em>hyper-parameters</em> of the model). The necessary syntax for defining your own static transformers is described in <a href="#Static-transformers">Static transformers</a> below.</p><p>Some unsupervised models, such as clustering algorithms, have a <code>predict</code> method in addition to a <code>transform</code> method. We give an example of this in <a href="#Transformers-that-also-predict">Transformers that also predict</a></p><p>Finally, we note that models that fit a distribution, or more generally a sampler object, to some data, which are sometimes viewed as unsupervised, are treated in MLJ as <em>supervised</em> models. See <a href="@ref">Models that learn a probability distribution</a> for an example.</p><h2 id="Built-in-transformers"><a class="docs-heading-anchor" href="#Built-in-transformers">Built-in transformers</a><a id="Built-in-transformers-1"></a><a class="docs-heading-anchor-permalink" href="#Built-in-transformers" title="Permalink"></a></h2><ul><li><a href="#MLJModels.Standardizer"><code>Standardizer</code></a></li><li><a href="#MLJModels.OneHotEncoder"><code>OneHotEncoder</code></a></li><li><a href="#MLJModels.ContinuousEncoder"><code>ContinuousEncoder</code></a></li><li><a href="#MLJModels.FillImputer"><code>FillImputer</code></a></li><li><a href="@ref"><code>UnivariateFillImputer</code></a></li><li><a href="#MLJModels.UnivariateBoxCoxTransformer"><code>UnivariateBoxCoxTransformer</code></a></li><li><a href="#MLJModels.InteractionTransformer"><code>InteractionTransformer</code></a></li><li><a href="#MLJModels.UnivariateDiscretizer"><code>UnivariateDiscretizer</code></a></li><li><a href="#MLJModels.UnivariateTimeTypeToContinuous"><code>UnivariateTimeTypeToContinuous</code></a></li><li><a href="../feature_selection/#FeatureSelection.FeatureSelector"><code>FeatureSelector</code></a>.</li></ul><h2 id="Static-transformers"><a class="docs-heading-anchor" href="#Static-transformers">Static transformers</a><a id="Static-transformers-1"></a><a class="docs-heading-anchor-permalink" href="#Static-transformers" title="Permalink"></a></h2><p>A <em>static transformer</em> is a model for transforming data that does not generalize to new data (does not &quot;learn&quot;) but which nevertheless has hyperparameters. For example, the <code>DBSAN</code> clustering model from Clustering.jl can assign labels to some collection of observations, cannot directly assign a label to some new observation.</p><p>The general user may define their own static models. The main use-case is insertion into a <a href="../linear_pipelines/#Linear-Pipelines">Linear Pipelines</a> some parameter-dependent transformation. (If a static transformer has no hyper-parameters, it is tantamount to an ordinary function. An ordinary function can be inserted directly into a pipeline; the situation for learning networks is only <a href="../learning_networks/#node_overloading">slightly more complicated</a>.</p><p>The following example defines a new model type <code>Averager</code> to perform the weighted average of two vectors (target predictions, for example). We suppose the weighting is normalized, and therefore controlled by a single hyper-parameter, <code>mix</code>.</p><pre><code class="language-julia hljs">mutable struct Averager &lt;: Static
    mix::Float64
end

MLJ.transform(a::Averager, _, y1, y2) = (1 - a.mix)*y1 + a.mix*y2</code></pre><p><em>Important.</em> Note the sub-typing <code>&lt;: Static</code>.</p><p>Such static transformers with (unlearned) parameters can have arbitrarily many inputs, but only one output. In the single input case, an <code>inverse_transform</code> can also be defined. Since they have no real learned parameters, you bind a static transformer to a machine without specifying training arguments; there is no need to <code>fit!</code> the machine:</p><pre><code class="language-julia hljs">mach = machine(Averager(0.5))
transform(mach, [1, 2, 3], [3, 2, 1])</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">3-element Vector{Float64}:
 2.0
 2.0
 2.0</code></pre><p>Let&#39;s see how we can include our <code>Averager</code> in a <a href="../learning_networks/#Learning-Networks">learning network</a> to mix the predictions of two regressors, with one-hot encoding of the inputs. Here&#39;s two regressors for mixing, and some dummy data for testing our learning network:</p><pre><code class="language-julia hljs">ridge = (@load RidgeRegressor pkg=MultivariateStats)()
knn = (@load KNNRegressor)()

import Random.seed!
seed!(112)
X = (
    x1=coerce(rand(&quot;ab&quot;, 100), Multiclass),
    x2=rand(100),
)
y = X.x2 + 0.05*rand(100)
schema(X)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">┌───────┬───────────────┬────────────────────────────────┐
│ names │ scitypes      │ types                          │
├───────┼───────────────┼────────────────────────────────┤
│ x1    │ Multiclass{2} │ CategoricalValue{Char, UInt32} │
│ x2    │ Continuous    │ Float64                        │
└───────┴───────────────┴────────────────────────────────┘
</code></pre><p>And the learning network:</p><pre><code class="language-julia hljs">Xs = source(X)
ys = source(y)

averager = Averager(0.5)

mach0 = machine(OneHotEncoder(), Xs)
W = transform(mach0, Xs) # one-hot encode the input

mach1 = machine(ridge, W, ys)
y1 = predict(mach1, W)

mach2 = machine(knn, W, ys)
y2 = predict(mach2, W)

mach4= machine(averager)
yhat = transform(mach4, y1, y2)

# test:
fit!(yhat)
Xnew = selectrows(X, 1:3)
yhat(Xnew)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">3-element Vector{Float64}:
 0.6403223210037916
 0.9607694439597683
 0.8159225346205365</code></pre><p>We next &quot;export&quot; the learning network as a standalone composite model type. First we need a struct for the composite model. Since we are restricting to <code>Deterministic</code> component regressors, the composite will also make deterministic predictions, and so gets the supertype <code>DeterministicNetworkComposite</code>:</p><pre><code class="language-julia hljs">mutable struct DoubleRegressor &lt;: DeterministicNetworkComposite
    regressor1
    regressor2
    averager
end</code></pre><p>As described in <a href="../learning_networks/#Learning-Networks">Learning Networks</a>, we next paste the learning network into a <code>prefit</code> declaration, replace the component models with symbolic placeholders, and add a learning network &quot;interface&quot;:</p><pre><code class="language-julia hljs">import MLJBase
function MLJBase.prefit(composite::DoubleRegressor, verbosity, X, y)
    Xs = source(X)
    ys = source(y)

    mach0 = machine(OneHotEncoder(), Xs)
    W = transform(mach0, Xs) # one-hot encode the input

    mach1 = machine(:regressor1, W, ys)
    y1 = predict(mach1, W)

    mach2 = machine(:regressor2, W, ys)
    y2 = predict(mach2, W)

    mach4= machine(:averager)
    yhat = transform(mach4, y1, y2)

    # learning network interface:
    (; predict=yhat)
end</code></pre><p>The new model type can be evaluated like any other supervised model:</p><pre><code class="language-julia hljs">X, y = @load_reduced_ames;
composite = DoubleRegressor(ridge, knn, Averager(0.5))</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">DoubleRegressor(
  regressor1 = RidgeRegressor(
        lambda = 1.0, 
        bias = true), 
  regressor2 = KNNRegressor(
        K = 5, 
        algorithm = :kdtree, 
        metric = Distances.Euclidean(0.0), 
        leafsize = 10, 
        reorder = true, 
        weights = NearestNeighborModels.Uniform()), 
  averager = Averager(
        mix = 0.5))</code></pre><pre><code class="language-julia hljs">composite.averager.mix = 0.25 # adjust mix from default of 0.5
evaluate(composite, X, y, measure=l1)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">PerformanceEvaluation object with these fields:
  model, measure, operation,
  measurement, per_fold, per_observation,
  fitted_params_per_fold, report_per_fold,
  train_test_rows, resampling, repeats
Extract:
┌──────────┬───────────┬─────────────┐
│ measure  │ operation │ measurement │
├──────────┼───────────┼─────────────┤
│ LPLoss(  │ predict   │ 17200.0     │
│   p = 1) │           │             │
└──────────┴───────────┴─────────────┘
┌────────────────────────────────────────────────────────┬─────────┐
│ per_fold                                               │ 1.96*SE │
├────────────────────────────────────────────────────────┼─────────┤
│ [15200.0, 15800.0, 18500.0, 16400.0, 18600.0, 18500.0] │ 1350.0  │
└────────────────────────────────────────────────────────┴─────────┘
</code></pre><p>A static transformer can also expose byproducts of the transform computation in the report of any associated machine. See <a href="@ref">Static models (models that do not generalize)</a> for details.</p><h2 id="Transformers-that-also-predict"><a class="docs-heading-anchor" href="#Transformers-that-also-predict">Transformers that also predict</a><a id="Transformers-that-also-predict-1"></a><a class="docs-heading-anchor-permalink" href="#Transformers-that-also-predict" title="Permalink"></a></h2><p>Some clustering algorithms learn to label data by identifying a collection of &quot;centroids&quot; in the training data. Any new input observation is labeled with the cluster to which it is closest (this is the output of <code>predict</code>) while the vector of all distances from the centroids defines a lower-dimensional representation of the observation (the output of <code>transform</code>). In the following example a K-means clustering algorithm assigns one of three labels 1, 2, 3 to the input features of the iris data set and compares them with the actual species recorded in the target (not seen by the algorithm).</p><pre><code class="language-julia hljs">import Random.seed!
seed!(123)

X, y = @load_iris
KMeans = @load KMeans pkg=Clustering
kmeans = KMeans()
mach = machine(kmeans, X) |&gt; fit!</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">[ Info: For silent loading, specify `verbosity=0`.
import MLJClusteringInterface ✔
[ Info: Training machine(KMeans(k = 3, …), …).</code></pre><p>Transforming:</p><pre><code class="language-julia hljs">Xsmall = transform(mach)
selectrows(Xsmall, 1:4) |&gt; pretty</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">┌────────────┬────────────┬────────────┐
│ x1         │ x2         │ x3         │
│ Float64    │ Float64    │ Float64    │
│ Continuous │ Continuous │ Continuous │
├────────────┼────────────┼────────────┤
│ 11.6913    │ 0.021592   │ 25.599     │
│ 11.5503    │ 0.191992   │ 26.1626    │
│ 12.7403    │ 0.169992   │ 27.8716    │
│ 11.7129    │ 0.269192   │ 26.5595    │
└────────────┴────────────┴────────────┘</code></pre><p>Predicting:</p><pre><code class="language-julia hljs">yhat = predict(mach)
compare = zip(yhat, y) |&gt; collect</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">150-element Vector{Tuple{CategoricalValue{Int64, UInt32}, CategoricalValue{String, UInt32}}}:
 (2, &quot;setosa&quot;)
 (2, &quot;setosa&quot;)
 (2, &quot;setosa&quot;)
 (2, &quot;setosa&quot;)
 (2, &quot;setosa&quot;)
 (2, &quot;setosa&quot;)
 (2, &quot;setosa&quot;)
 (2, &quot;setosa&quot;)
 (2, &quot;setosa&quot;)
 (2, &quot;setosa&quot;)
 ⋮
 (3, &quot;virginica&quot;)
 (1, &quot;virginica&quot;)
 (3, &quot;virginica&quot;)
 (3, &quot;virginica&quot;)
 (3, &quot;virginica&quot;)
 (1, &quot;virginica&quot;)
 (3, &quot;virginica&quot;)
 (3, &quot;virginica&quot;)
 (1, &quot;virginica&quot;)</code></pre><pre><code class="language-julia hljs">compare[1:8]</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">8-element Vector{Tuple{CategoricalValue{Int64, UInt32}, CategoricalValue{String, UInt32}}}:
 (2, &quot;setosa&quot;)
 (2, &quot;setosa&quot;)
 (2, &quot;setosa&quot;)
 (2, &quot;setosa&quot;)
 (2, &quot;setosa&quot;)
 (2, &quot;setosa&quot;)
 (2, &quot;setosa&quot;)
 (2, &quot;setosa&quot;)</code></pre><pre><code class="language-julia hljs">compare[51:58]</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">8-element Vector{Tuple{CategoricalValue{Int64, UInt32}, CategoricalValue{String, UInt32}}}:
 (1, &quot;versicolor&quot;)
 (1, &quot;versicolor&quot;)
 (3, &quot;versicolor&quot;)
 (1, &quot;versicolor&quot;)
 (1, &quot;versicolor&quot;)
 (1, &quot;versicolor&quot;)
 (1, &quot;versicolor&quot;)
 (1, &quot;versicolor&quot;)</code></pre><pre><code class="language-julia hljs">compare[101:108]</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">8-element Vector{Tuple{CategoricalValue{Int64, UInt32}, CategoricalValue{String, UInt32}}}:
 (3, &quot;virginica&quot;)
 (1, &quot;virginica&quot;)
 (3, &quot;virginica&quot;)
 (3, &quot;virginica&quot;)
 (3, &quot;virginica&quot;)
 (3, &quot;virginica&quot;)
 (1, &quot;virginica&quot;)
 (3, &quot;virginica&quot;)</code></pre><h2 id="Reference"><a class="docs-heading-anchor" href="#Reference">Reference</a><a id="Reference-1"></a><a class="docs-heading-anchor-permalink" href="#Reference" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="MLJModels.Standardizer" href="#MLJModels.Standardizer"><code>MLJModels.Standardizer</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">Standardizer</code></pre><p>A model type for constructing a standardizer, based on <a href="https://github.com/JuliaAI/MLJModels.jl">MLJModels.jl</a>, and implementing the MLJ model interface.</p><p>From MLJ, the type can be imported using</p><pre><code class="nohighlight hljs">Standardizer = @load Standardizer pkg=MLJModels</code></pre><p>Do <code>model = Standardizer()</code> to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in <code>Standardizer(features=...)</code>.</p><p>Use this model to standardize (whiten) a <code>Continuous</code> vector, or relevant columns of a table. The rescalings applied by this transformer to new data are always those learned during the training phase, which are generally different from what would actually standardize the new data.</p><p><strong>Training data</strong></p><p>In MLJ or MLJBase, bind an instance <code>model</code> to data with</p><pre><code class="nohighlight hljs">mach = machine(model, X)</code></pre><p>where</p><ul><li><code>X</code>: any Tables.jl compatible table or any abstract vector with <code>Continuous</code> element scitype (any abstract float vector). Only features in a table with <code>Continuous</code> scitype can be standardized; check column scitypes with <code>schema(X)</code>.</li></ul><p>Train the machine using <code>fit!(mach, rows=...)</code>.</p><p><strong>Hyper-parameters</strong></p><ul><li><p><code>features</code>: one of the following, with the behavior indicated below:</p><ul><li><p><code>[]</code> (empty, the default): standardize all features (columns) having <code>Continuous</code> element scitype</p></li><li><p>non-empty vector of feature names (symbols): standardize only the <code>Continuous</code> features in the vector (if <code>ignore=false</code>) or <code>Continuous</code> features <em>not</em> named in the vector (<code>ignore=true</code>).</p></li><li><p>function or other callable: standardize a feature if the callable returns <code>true</code> on its name. For example, <code>Standardizer(features = name -&gt; name in [:x1, :x3], ignore = true, count=true)</code> has the same effect as <code>Standardizer(features = [:x1, :x3], ignore = true, count=true)</code>, namely to standardize all <code>Continuous</code> and <code>Count</code> features, with the exception of <code>:x1</code> and <code>:x3</code>.</p></li></ul><p>Note this behavior is further modified if the <code>ordered_factor</code> or <code>count</code> flags are set to <code>true</code>; see below</p></li><li><p><code>ignore=false</code>: whether to ignore or standardize specified <code>features</code>, as explained above</p></li><li><p><code>ordered_factor=false</code>: if <code>true</code>, standardize any <code>OrderedFactor</code> feature wherever a <code>Continuous</code> feature would be standardized, as described above</p></li><li><p><code>count=false</code>: if <code>true</code>, standardize any <code>Count</code> feature wherever a <code>Continuous</code> feature would be standardized, as described above</p></li></ul><p><strong>Operations</strong></p><ul><li><p><code>transform(mach, Xnew)</code>: return <code>Xnew</code> with relevant features standardized according to the rescalings learned during fitting of <code>mach</code>.</p></li><li><p><code>inverse_transform(mach, Z)</code>: apply the inverse transformation to <code>Z</code>, so that <code>inverse_transform(mach, transform(mach, Xnew))</code> is approximately the same as <code>Xnew</code>; unavailable if <code>ordered_factor</code> or <code>count</code> flags were set to <code>true</code>.</p></li></ul><p><strong>Fitted parameters</strong></p><p>The fields of <code>fitted_params(mach)</code> are:</p><ul><li><p><code>features_fit</code> - the names of features that will be standardized</p></li><li><p><code>means</code> - the corresponding untransformed mean values</p></li><li><p><code>stds</code> - the corresponding untransformed standard deviations</p></li></ul><p><strong>Report</strong></p><p>The fields of <code>report(mach)</code> are:</p><ul><li><code>features_fit</code>: the names of features that will be standardized</li></ul><p><strong>Examples</strong></p><pre><code class="nohighlight hljs">using MLJ

X = (ordinal1 = [1, 2, 3],
     ordinal2 = coerce([:x, :y, :x], OrderedFactor),
     ordinal3 = [10.0, 20.0, 30.0],
     ordinal4 = [-20.0, -30.0, -40.0],
     nominal = coerce([&quot;Your father&quot;, &quot;he&quot;, &quot;is&quot;], Multiclass));

julia&gt; schema(X)
┌──────────┬──────────────────┐
│ names    │ scitypes         │
├──────────┼──────────────────┤
│ ordinal1 │ Count            │
│ ordinal2 │ OrderedFactor{2} │
│ ordinal3 │ Continuous       │
│ ordinal4 │ Continuous       │
│ nominal  │ Multiclass{3}    │
└──────────┴──────────────────┘

stand1 = Standardizer();

julia&gt; transform(fit!(machine(stand1, X)), X)
(ordinal1 = [1, 2, 3],
 ordinal2 = CategoricalValue{Symbol,UInt32}[:x, :y, :x],
 ordinal3 = [-1.0, 0.0, 1.0],
 ordinal4 = [1.0, 0.0, -1.0],
 nominal = CategoricalValue{String,UInt32}[&quot;Your father&quot;, &quot;he&quot;, &quot;is&quot;],)

stand2 = Standardizer(features=[:ordinal3, ], ignore=true, count=true);

julia&gt; transform(fit!(machine(stand2, X)), X)
(ordinal1 = [-1.0, 0.0, 1.0],
 ordinal2 = CategoricalValue{Symbol,UInt32}[:x, :y, :x],
 ordinal3 = [10.0, 20.0, 30.0],
 ordinal4 = [1.0, 0.0, -1.0],
 nominal = CategoricalValue{String,UInt32}[&quot;Your father&quot;, &quot;he&quot;, &quot;is&quot;],)</code></pre><p>See also <a href="#MLJModels.OneHotEncoder"><code>OneHotEncoder</code></a>, <a href="#MLJModels.ContinuousEncoder"><code>ContinuousEncoder</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJModels.jl/blob/v0.17.9/src/builtins/Transformers.jl#L1298-L1313">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="MLJModels.OneHotEncoder" href="#MLJModels.OneHotEncoder"><code>MLJModels.OneHotEncoder</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">OneHotEncoder</code></pre><p>A model type for constructing a one-hot encoder, based on <a href="https://github.com/JuliaAI/MLJModels.jl">MLJModels.jl</a>, and implementing the MLJ model interface.</p><p>From MLJ, the type can be imported using</p><pre><code class="nohighlight hljs">OneHotEncoder = @load OneHotEncoder pkg=MLJModels</code></pre><p>Do <code>model = OneHotEncoder()</code> to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in <code>OneHotEncoder(features=...)</code>.</p><p>Use this model to one-hot encode the <code>Multiclass</code> and <code>OrderedFactor</code> features (columns) of some table, leaving other columns unchanged.</p><p>New data to be transformed may lack features present in the fit data, but no <em>new</em> features can be present.</p><p><strong>Warning:</strong> This transformer assumes that <code>levels(col)</code> for any <code>Multiclass</code> or <code>OrderedFactor</code> column, <code>col</code>, is the same for training data and new data to be transformed.</p><p>To ensure <em>all</em> features are transformed into <code>Continuous</code> features, or dropped, use <a href="#MLJModels.ContinuousEncoder"><code>ContinuousEncoder</code></a> instead.</p><p><strong>Training data</strong></p><p>In MLJ or MLJBase, bind an instance <code>model</code> to data with</p><pre><code class="nohighlight hljs">mach = machine(model, X)</code></pre><p>where</p><ul><li><code>X</code>: any Tables.jl compatible table. Columns can be of mixed type but only those with element scitype <code>Multiclass</code> or <code>OrderedFactor</code> can be encoded. Check column scitypes with <code>schema(X)</code>.</li></ul><p>Train the machine using <code>fit!(mach, rows=...)</code>.</p><p><strong>Hyper-parameters</strong></p><ul><li><p><code>features</code>: a vector of symbols (column names). If empty (default) then all <code>Multiclass</code> and <code>OrderedFactor</code> features are encoded. Otherwise, encoding is further restricted to the specified features (<code>ignore=false</code>) or the unspecified features (<code>ignore=true</code>). This default behavior can be modified by the <code>ordered_factor</code> flag.</p></li><li><p><code>ordered_factor=false</code>: when <code>true</code>, <code>OrderedFactor</code> features are universally excluded</p></li><li><p><code>drop_last=true</code>: whether to drop the column corresponding to the final class of encoded features. For example, a three-class feature is spawned into three new features if <code>drop_last=false</code>, but just two features otherwise.</p></li></ul><p><strong>Fitted parameters</strong></p><p>The fields of <code>fitted_params(mach)</code> are:</p><ul><li><p><code>all_features</code>: names of all features encountered in training</p></li><li><p><code>fitted_levels_given_feature</code>: dictionary of the levels associated with each feature encoded, keyed on the feature name</p></li><li><p><code>ref_name_pairs_given_feature</code>: dictionary of pairs <code>r =&gt; ftr</code> (such as <code>0x00000001 =&gt; :grad__A</code>) where <code>r</code> is a CategoricalArrays.jl reference integer representing a level, and <code>ftr</code> the corresponding new feature name; the dictionary is keyed on the names of features that are encoded</p></li></ul><p><strong>Report</strong></p><p>The fields of <code>report(mach)</code> are:</p><ul><li><p><code>features_to_be_encoded</code>: names of input features to be encoded</p></li><li><p><code>new_features</code>: names of all output features</p></li></ul><p><strong>Example</strong></p><pre><code class="nohighlight hljs">using MLJ

X = (name=categorical([&quot;Danesh&quot;, &quot;Lee&quot;, &quot;Mary&quot;, &quot;John&quot;]),
     grade=categorical([&quot;A&quot;, &quot;B&quot;, &quot;A&quot;, &quot;C&quot;], ordered=true),
     height=[1.85, 1.67, 1.5, 1.67],
     n_devices=[3, 2, 4, 3])

julia&gt; schema(X)
┌───────────┬──────────────────┐
│ names     │ scitypes         │
├───────────┼──────────────────┤
│ name      │ Multiclass{4}    │
│ grade     │ OrderedFactor{3} │
│ height    │ Continuous       │
│ n_devices │ Count            │
└───────────┴──────────────────┘

hot = OneHotEncoder(drop_last=true)
mach = fit!(machine(hot, X))
W = transform(mach, X)

julia&gt; schema(W)
┌──────────────┬────────────┐
│ names        │ scitypes   │
├──────────────┼────────────┤
│ name__Danesh │ Continuous │
│ name__John   │ Continuous │
│ name__Lee    │ Continuous │
│ grade__A     │ Continuous │
│ grade__B     │ Continuous │
│ height       │ Continuous │
│ n_devices    │ Count      │
└──────────────┴────────────┘</code></pre><p>See also <a href="#MLJModels.ContinuousEncoder"><code>ContinuousEncoder</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJModels.jl/blob/v0.17.9/src/builtins/Transformers.jl#L1642-L1657">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="MLJModels.ContinuousEncoder" href="#MLJModels.ContinuousEncoder"><code>MLJModels.ContinuousEncoder</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">ContinuousEncoder</code></pre><p>A model type for constructing a continuous encoder, based on <a href="https://github.com/JuliaAI/MLJModels.jl">MLJModels.jl</a>, and implementing the MLJ model interface.</p><p>From MLJ, the type can be imported using</p><pre><code class="nohighlight hljs">ContinuousEncoder = @load ContinuousEncoder pkg=MLJModels</code></pre><p>Do <code>model = ContinuousEncoder()</code> to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in <code>ContinuousEncoder(drop_last=...)</code>.</p><p>Use this model to arrange all features (columns) of a table to have <code>Continuous</code> element scitype, by applying the following protocol to each feature <code>ftr</code>:</p><ul><li><p>If <code>ftr</code> is already <code>Continuous</code> retain it.</p></li><li><p>If <code>ftr</code> is <code>Multiclass</code>, one-hot encode it.</p></li><li><p>If <code>ftr</code> is <code>OrderedFactor</code>, replace it with <code>coerce(ftr, Continuous)</code> (vector of floating point integers), unless <code>ordered_factors=false</code> is specified, in which case one-hot encode it.</p></li><li><p>If <code>ftr</code> is <code>Count</code>, replace it with <code>coerce(ftr, Continuous)</code>.</p></li><li><p>If <code>ftr</code> has some other element scitype, or was not observed in fitting the encoder, drop it from the table.</p></li></ul><p><strong>Warning:</strong> This transformer assumes that <code>levels(col)</code> for any <code>Multiclass</code> or <code>OrderedFactor</code> column, <code>col</code>, is the same for training data and new data to be transformed.</p><p>To selectively one-hot-encode categorical features (without dropping columns) use <a href="#MLJModels.OneHotEncoder"><code>OneHotEncoder</code></a> instead.</p><p><strong>Training data</strong></p><p>In MLJ or MLJBase, bind an instance <code>model</code> to data with</p><pre><code class="nohighlight hljs">mach = machine(model, X)</code></pre><p>where</p><ul><li><code>X</code>: any Tables.jl compatible table. Columns can be of mixed type but only those with element scitype <code>Multiclass</code> or <code>OrderedFactor</code> can be encoded. Check column scitypes with <code>schema(X)</code>.</li></ul><p>Train the machine using <code>fit!(mach, rows=...)</code>.</p><p><strong>Hyper-parameters</strong></p><ul><li><p><code>drop_last=true</code>: whether to drop the column corresponding to the final class of one-hot encoded features. For example, a three-class feature is spawned into three new features if <code>drop_last=false</code>, but two just features otherwise.</p></li><li><p><code>one_hot_ordered_factors=false</code>: whether to one-hot any feature with <code>OrderedFactor</code> element scitype, or to instead coerce it directly to a (single) <code>Continuous</code> feature using the order</p></li></ul><p><strong>Fitted parameters</strong></p><p>The fields of <code>fitted_params(mach)</code> are:</p><ul><li><p><code>features_to_keep</code>: names of features that will not be dropped from the table</p></li><li><p><code>one_hot_encoder</code>: the <code>OneHotEncoder</code> model instance for handling the one-hot encoding</p></li><li><p><code>one_hot_encoder_fitresult</code>: the fitted parameters of the <code>OneHotEncoder</code> model</p></li></ul><p><strong>Report</strong></p><ul><li><p><code>features_to_keep</code>: names of input features that will not be dropped from the table</p></li><li><p><code>new_features</code>: names of all output features</p></li></ul><p><strong>Example</strong></p><pre><code class="language-julia hljs">X = (name=categorical([&quot;Danesh&quot;, &quot;Lee&quot;, &quot;Mary&quot;, &quot;John&quot;]),
     grade=categorical([&quot;A&quot;, &quot;B&quot;, &quot;A&quot;, &quot;C&quot;], ordered=true),
     height=[1.85, 1.67, 1.5, 1.67],
     n_devices=[3, 2, 4, 3],
     comments=[&quot;the force&quot;, &quot;be&quot;, &quot;with you&quot;, &quot;too&quot;])

julia&gt; schema(X)
┌───────────┬──────────────────┐
│ names     │ scitypes         │
├───────────┼──────────────────┤
│ name      │ Multiclass{4}    │
│ grade     │ OrderedFactor{3} │
│ height    │ Continuous       │
│ n_devices │ Count            │
│ comments  │ Textual          │
└───────────┴──────────────────┘

encoder = ContinuousEncoder(drop_last=true)
mach = fit!(machine(encoder, X))
W = transform(mach, X)

julia&gt; schema(W)
┌──────────────┬────────────┐
│ names        │ scitypes   │
├──────────────┼────────────┤
│ name__Danesh │ Continuous │
│ name__John   │ Continuous │
│ name__Lee    │ Continuous │
│ grade        │ Continuous │
│ height       │ Continuous │
│ n_devices    │ Continuous │
└──────────────┴────────────┘

julia&gt; setdiff(schema(X).names, report(mach).features_to_keep) # dropped features
1-element Vector{Symbol}:
 :comments
</code></pre><p>See also <a href="#MLJModels.OneHotEncoder"><code>OneHotEncoder</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJModels.jl/blob/v0.17.9/src/builtins/Transformers.jl#L1761-L1776">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="MLJModels.FillImputer" href="#MLJModels.FillImputer"><code>MLJModels.FillImputer</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">FillImputer</code></pre><p>A model type for constructing a fill imputer, based on <a href="https://github.com/JuliaAI/MLJModels.jl">MLJModels.jl</a>, and implementing the MLJ model interface.</p><p>From MLJ, the type can be imported using</p><pre><code class="nohighlight hljs">FillImputer = @load FillImputer pkg=MLJModels</code></pre><p>Do <code>model = FillImputer()</code> to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in <code>FillImputer(features=...)</code>.</p><p>Use this model to impute <code>missing</code> values in tabular data. A fixed &quot;filler&quot; value is learned from the training data, one for each column of the table.</p><p>For imputing missing values in a vector, use <a href="#MLJModels.UnivariateFillImputer"><code>UnivariateFillImputer</code></a> instead.</p><p><strong>Training data</strong></p><p>In MLJ or MLJBase, bind an instance <code>model</code> to data with</p><pre><code class="nohighlight hljs">mach = machine(model, X)</code></pre><p>where</p><ul><li><code>X</code>: any table of input features (eg, a <code>DataFrame</code>) whose columns each have element scitypes <code>Union{Missing, T}</code>, where <code>T</code> is a subtype of <code>Continuous</code>, <code>Multiclass</code>, <code>OrderedFactor</code> or <code>Count</code>. Check scitypes with <code>schema(X)</code>.</li></ul><p>Train the machine using <code>fit!(mach, rows=...)</code>.</p><p><strong>Hyper-parameters</strong></p><ul><li><p><code>features</code>: a vector of names of features (symbols) for which imputation is to be attempted; default is empty, which is interpreted as &quot;impute all&quot;.</p></li><li><p><code>continuous_fill</code>: function or other callable to determine value to be imputed in the case of <code>Continuous</code> (abstract float) data; default is to apply <code>median</code> after skipping <code>missing</code> values</p></li><li><p><code>count_fill</code>: function or other callable to determine value to be imputed in the case of <code>Count</code> (integer) data; default is to apply rounded <code>median</code> after skipping <code>missing</code> values</p></li><li><p><code>finite_fill</code>: function or other callable to determine value to be imputed in the case of <code>Multiclass</code> or <code>OrderedFactor</code> data (categorical vectors); default is to apply <code>mode</code> after skipping <code>missing</code> values</p></li></ul><p><strong>Operations</strong></p><ul><li><code>transform(mach, Xnew)</code>: return <code>Xnew</code> with missing values imputed with the fill values learned when fitting <code>mach</code></li></ul><p><strong>Fitted parameters</strong></p><p>The fields of <code>fitted_params(mach)</code> are:</p><ul><li><p><code>features_seen_in_fit</code>: the names of features (columns) encountered during training</p></li><li><p><code>univariate_transformer</code>: the univariate model applied to determine   the fillers (it&#39;s fields contain the functions defining the filler computations)</p></li><li><p><code>filler_given_feature</code>: dictionary of filler values, keyed on feature (column) names</p></li></ul><p><strong>Examples</strong></p><pre><code class="nohighlight hljs">using MLJ
imputer = FillImputer()

X = (a = [1.0, 2.0, missing, 3.0, missing],
     b = coerce([&quot;y&quot;, &quot;n&quot;, &quot;y&quot;, missing, &quot;y&quot;], Multiclass),
     c = [1, 1, 2, missing, 3])

schema(X)
julia&gt; schema(X)
┌───────┬───────────────────────────────┐
│ names │ scitypes                      │
├───────┼───────────────────────────────┤
│ a     │ Union{Missing, Continuous}    │
│ b     │ Union{Missing, Multiclass{2}} │
│ c     │ Union{Missing, Count}         │
└───────┴───────────────────────────────┘

mach = machine(imputer, X)
fit!(mach)

julia&gt; fitted_params(mach).filler_given_feature
(filler = 2.0,)

julia&gt; fitted_params(mach).filler_given_feature
Dict{Symbol, Any} with 3 entries:
  :a =&gt; 2.0
  :b =&gt; &quot;y&quot;
  :c =&gt; 2

julia&gt; transform(mach, X)
(a = [1.0, 2.0, 2.0, 3.0, 2.0],
 b = CategoricalValue{String, UInt32}[&quot;y&quot;, &quot;n&quot;, &quot;y&quot;, &quot;y&quot;, &quot;y&quot;],
 c = [1, 1, 2, 2, 3],)</code></pre><p>See also <a href="#MLJModels.UnivariateFillImputer"><code>UnivariateFillImputer</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJModels.jl/blob/v0.17.9/src/builtins/Transformers.jl#L1189-L1204">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="MLJModels.UnivariateFillImputer" href="#MLJModels.UnivariateFillImputer"><code>MLJModels.UnivariateFillImputer</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">UnivariateFillImputer</code></pre><p>A model type for constructing a single variable fill imputer, based on <a href="https://github.com/JuliaAI/MLJModels.jl">MLJModels.jl</a>, and implementing the MLJ model interface.</p><p>From MLJ, the type can be imported using</p><pre><code class="nohighlight hljs">UnivariateFillImputer = @load UnivariateFillImputer pkg=MLJModels</code></pre><p>Do <code>model = UnivariateFillImputer()</code> to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in <code>UnivariateFillImputer(continuous_fill=...)</code>.</p><p>Use this model to imputing <code>missing</code> values in a vector with a fixed value learned from the non-missing values of training vector.</p><p>For imputing missing values in tabular data, use <a href="#MLJModels.FillImputer"><code>FillImputer</code></a> instead.</p><p><strong>Training data</strong></p><p>In MLJ or MLJBase, bind an instance <code>model</code> to data with</p><pre><code class="nohighlight hljs">mach = machine(model, x)</code></pre><p>where</p><ul><li><code>x</code>: any abstract vector with element scitype <code>Union{Missing, T}</code> where <code>T</code> is a subtype of <code>Continuous</code>, <code>Multiclass</code>, <code>OrderedFactor</code> or <code>Count</code>; check scitype using <code>scitype(x)</code></li></ul><p>Train the machine using <code>fit!(mach, rows=...)</code>.</p><p><strong>Hyper-parameters</strong></p><ul><li><p><code>continuous_fill</code>: function or other callable to determine value to be imputed in the case of <code>Continuous</code> (abstract float) data; default is to apply <code>median</code> after skipping <code>missing</code> values</p></li><li><p><code>count_fill</code>: function or other callable to determine value to be imputed in the case of <code>Count</code> (integer) data; default is to apply rounded <code>median</code> after skipping <code>missing</code> values</p></li><li><p><code>finite_fill</code>: function or other callable to determine value to be imputed in the case of <code>Multiclass</code> or <code>OrderedFactor</code> data (categorical vectors); default is to apply <code>mode</code> after skipping <code>missing</code> values</p></li></ul><p><strong>Operations</strong></p><ul><li><code>transform(mach, xnew)</code>: return <code>xnew</code> with missing values imputed with the fill values learned when fitting <code>mach</code></li></ul><p><strong>Fitted parameters</strong></p><p>The fields of <code>fitted_params(mach)</code> are:</p><ul><li><code>filler</code>: the fill value to be imputed in all new data</li></ul><p><strong>Examples</strong></p><pre><code class="nohighlight hljs">using MLJ
imputer = UnivariateFillImputer()

x_continuous = [1.0, 2.0, missing, 3.0]
x_multiclass = coerce([&quot;y&quot;, &quot;n&quot;, &quot;y&quot;, missing, &quot;y&quot;], Multiclass)
x_count = [1, 1, 1, 2, missing, 3, 3]

mach = machine(imputer, x_continuous)
fit!(mach)

julia&gt; fitted_params(mach)
(filler = 2.0,)

julia&gt; transform(mach, [missing, missing, 101.0])
3-element Vector{Float64}:
 2.0
 2.0
 101.0

mach2 = machine(imputer, x_multiclass) |&gt; fit!

julia&gt; transform(mach2, x_multiclass)
5-element CategoricalArray{String,1,UInt32}:
 &quot;y&quot;
 &quot;n&quot;
 &quot;y&quot;
 &quot;y&quot;
 &quot;y&quot;

mach3 = machine(imputer, x_count) |&gt; fit!

julia&gt; transform(mach3, [missing, missing, 5])
3-element Vector{Int64}:
 2
 2
 5</code></pre><p>For imputing tabular data, use <a href="#MLJModels.FillImputer"><code>FillImputer</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJModels.jl/blob/v0.17.9/src/builtins/Transformers.jl#L1089-L1104">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="MLJModels.UnivariateBoxCoxTransformer" href="#MLJModels.UnivariateBoxCoxTransformer"><code>MLJModels.UnivariateBoxCoxTransformer</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">UnivariateBoxCoxTransformer</code></pre><p>A model type for constructing a single variable Box-Cox transformer, based on <a href="https://github.com/JuliaAI/MLJModels.jl">MLJModels.jl</a>, and implementing the MLJ model interface.</p><p>From MLJ, the type can be imported using</p><pre><code class="nohighlight hljs">UnivariateBoxCoxTransformer = @load UnivariateBoxCoxTransformer pkg=MLJModels</code></pre><p>Do <code>model = UnivariateBoxCoxTransformer()</code> to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in <code>UnivariateBoxCoxTransformer(n=...)</code>.</p><p>Box-Cox transformations attempt to make data look more normally distributed. This can improve performance and assist in the interpretation of models which suppose that data is generated by a normal distribution.</p><p>A Box-Cox transformation (with shift) is of the form</p><pre><code class="nohighlight hljs">x -&gt; ((x + c)^λ - 1)/λ</code></pre><p>for some constant <code>c</code> and real <code>λ</code>, unless <code>λ = 0</code>, in which case the above is replaced with</p><pre><code class="nohighlight hljs">x -&gt; log(x + c)</code></pre><p>Given user-specified hyper-parameters <code>n::Integer</code> and <code>shift::Bool</code>, the present implementation learns the parameters <code>c</code> and <code>λ</code> from the training data as follows: If <code>shift=true</code> and zeros are encountered in the data, then <code>c</code> is set to <code>0.2</code> times the data mean.  If there are no zeros, then no shift is applied. Finally, <code>n</code> different values of <code>λ</code> between <code>-0.4</code> and <code>3</code> are considered, with <code>λ</code> fixed to the value maximizing normality of the transformed data.</p><p><em>Reference:</em> <a href="https://en.wikipedia.org/wiki/Power_transform">Wikipedia entry for power  transform</a>.</p><p><strong>Training data</strong></p><p>In MLJ or MLJBase, bind an instance <code>model</code> to data with</p><pre><code class="nohighlight hljs">mach = machine(model, x)</code></pre><p>where</p><ul><li><code>x</code>: any abstract vector with element scitype <code>Continuous</code>; check the scitype with <code>scitype(x)</code></li></ul><p>Train the machine using <code>fit!(mach, rows=...)</code>.</p><p><strong>Hyper-parameters</strong></p><ul><li><p><code>n=171</code>: number of values of the exponent <code>λ</code> to try</p></li><li><p><code>shift=false</code>: whether to include a preliminary constant translation in transformations, in the presence of zeros</p></li></ul><p><strong>Operations</strong></p><ul><li><p><code>transform(mach, xnew)</code>: apply the Box-Cox transformation learned when fitting <code>mach</code></p></li><li><p><code>inverse_transform(mach, z)</code>: reconstruct the vector <code>z</code> whose transformation learned by <code>mach</code> is <code>z</code></p></li></ul><p><strong>Fitted parameters</strong></p><p>The fields of <code>fitted_params(mach)</code> are:</p><ul><li><p><code>λ</code>: the learned Box-Cox exponent</p></li><li><p><code>c</code>: the learned shift</p></li></ul><p><strong>Examples</strong></p><pre><code class="nohighlight hljs">using MLJ
using UnicodePlots
using Random
Random.seed!(123)

transf = UnivariateBoxCoxTransformer()

x = randn(1000).^2

mach = machine(transf, x)
fit!(mach)

z = transform(mach, x)

julia&gt; histogram(x)
                ┌                                        ┐
   [ 0.0,  2.0) ┤███████████████████████████████████  848
   [ 2.0,  4.0) ┤████▌ 109
   [ 4.0,  6.0) ┤█▍ 33
   [ 6.0,  8.0) ┤▍ 7
   [ 8.0, 10.0) ┤▏ 2
   [10.0, 12.0) ┤  0
   [12.0, 14.0) ┤▏ 1
                └                                        ┘
                                 Frequency

julia&gt; histogram(z)
                ┌                                        ┐
   [-5.0, -4.0) ┤█▎ 8
   [-4.0, -3.0) ┤████████▊ 64
   [-3.0, -2.0) ┤█████████████████████▊ 159
   [-2.0, -1.0) ┤█████████████████████████████▊ 216
   [-1.0,  0.0) ┤███████████████████████████████████  254
   [ 0.0,  1.0) ┤█████████████████████████▊ 188
   [ 1.0,  2.0) ┤████████████▍ 90
   [ 2.0,  3.0) ┤██▊ 20
   [ 3.0,  4.0) ┤▎ 1
                └                                        ┘
                                 Frequency
</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJModels.jl/blob/v0.17.9/src/builtins/Transformers.jl#L1525-L1540">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="MLJModels.InteractionTransformer" href="#MLJModels.InteractionTransformer"><code>MLJModels.InteractionTransformer</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">InteractionTransformer</code></pre><p>A model type for constructing a interaction transformer, based on <a href="https://github.com/JuliaAI/MLJModels.jl">MLJModels.jl</a>, and implementing the MLJ model interface.</p><p>From MLJ, the type can be imported using</p><pre><code class="nohighlight hljs">InteractionTransformer = @load InteractionTransformer pkg=MLJModels</code></pre><p>Do <code>model = InteractionTransformer()</code> to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in <code>InteractionTransformer(order=...)</code>.</p><p>Generates all polynomial interaction terms up to the given order for the subset of chosen columns.  Any column that contains elements with scitype <code>&lt;:Infinite</code> is a valid basis to generate interactions.  If <code>features</code> is not specified, all such columns with scitype <code>&lt;:Infinite</code> in the table are used as a basis.</p><p>In MLJ or MLJBase, you can transform features <code>X</code> with the single call</p><pre><code class="nohighlight hljs">transform(machine(model), X)</code></pre><p>See also the example below.</p><p><strong>Hyper-parameters</strong></p><ul><li><code>order</code>: Maximum order of interactions to be generated.</li><li><code>features</code>: Restricts interations generation to those columns</li></ul><p><strong>Operations</strong></p><ul><li><code>transform(machine(model), X)</code>: Generates polynomial interaction terms out of table <code>X</code> using the hyper-parameters specified in <code>model</code>.</li></ul><p><strong>Example</strong></p><pre><code class="nohighlight hljs">using MLJ

X = (
    A = [1, 2, 3],
    B = [4, 5, 6],
    C = [7, 8, 9],
    D = [&quot;x₁&quot;, &quot;x₂&quot;, &quot;x₃&quot;]
)
it = InteractionTransformer(order=3)
mach = machine(it)

julia&gt; transform(mach, X)
(A = [1, 2, 3],
 B = [4, 5, 6],
 C = [7, 8, 9],
 D = [&quot;x₁&quot;, &quot;x₂&quot;, &quot;x₃&quot;],
 A_B = [4, 10, 18],
 A_C = [7, 16, 27],
 B_C = [28, 40, 54],
 A_B_C = [28, 80, 162],)

it = InteractionTransformer(order=2, features=[:A, :B])
mach = machine(it)

julia&gt; transform(mach, X)
(A = [1, 2, 3],
 B = [4, 5, 6],
 C = [7, 8, 9],
 D = [&quot;x₁&quot;, &quot;x₂&quot;, &quot;x₃&quot;],
 A_B = [4, 10, 18],)
</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJModels.jl/blob/v0.17.9/src/builtins/Transformers.jl#L1955-L1970">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="MLJModels.UnivariateDiscretizer" href="#MLJModels.UnivariateDiscretizer"><code>MLJModels.UnivariateDiscretizer</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">UnivariateDiscretizer</code></pre><p>A model type for constructing a single variable discretizer, based on <a href="https://github.com/JuliaAI/MLJModels.jl">MLJModels.jl</a>, and implementing the MLJ model interface.</p><p>From MLJ, the type can be imported using</p><pre><code class="nohighlight hljs">UnivariateDiscretizer = @load UnivariateDiscretizer pkg=MLJModels</code></pre><p>Do <code>model = UnivariateDiscretizer()</code> to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in <code>UnivariateDiscretizer(n_classes=...)</code>.</p><p>Discretization converts a <code>Continuous</code> vector into an <code>OrderedFactor</code> vector. In particular, the output is a <code>CategoricalVector</code> (whose reference type is optimized).</p><p>The transformation is chosen so that the vector on which the transformer is fit has, in transformed form, an approximately uniform distribution of values. Specifically, if <code>n_classes</code> is the level of discretization, then <code>2*n_classes - 1</code> ordered quantiles are computed, the odd quantiles being used for transforming (discretization) and the even quantiles for inverse transforming.</p><p><strong>Training data</strong></p><p>In MLJ or MLJBase, bind an instance <code>model</code> to data with</p><pre><code class="nohighlight hljs">mach = machine(model, x)</code></pre><p>where</p><ul><li><code>x</code>: any abstract vector with <code>Continuous</code> element scitype; check scitype with <code>scitype(x)</code>.</li></ul><p>Train the machine using <code>fit!(mach, rows=...)</code>.</p><p><strong>Hyper-parameters</strong></p><ul><li><code>n_classes</code>: number of discrete classes in the output</li></ul><p><strong>Operations</strong></p><ul><li><p><code>transform(mach, xnew)</code>: discretize <code>xnew</code> according to the discretization learned when fitting <code>mach</code></p></li><li><p><code>inverse_transform(mach, z)</code>: attempt to reconstruct from <code>z</code> a vector that transforms to give <code>z</code></p></li></ul><p><strong>Fitted parameters</strong></p><p>The fields of <code>fitted_params(mach).fitesult</code> include:</p><ul><li><p><code>odd_quantiles</code>: quantiles used for transforming (length is <code>n_classes - 1</code>)</p></li><li><p><code>even_quantiles</code>: quantiles used for inverse transforming (length is <code>n_classes</code>)</p></li></ul><p><strong>Example</strong></p><pre><code class="nohighlight hljs">using MLJ
using Random
Random.seed!(123)

discretizer = UnivariateDiscretizer(n_classes=100)
mach = machine(discretizer, randn(1000))
fit!(mach)

julia&gt; x = rand(5)
5-element Vector{Float64}:
 0.8585244609846809
 0.37541692370451396
 0.6767070590395461
 0.9208844241267105
 0.7064611415680901

julia&gt; z = transform(mach, x)
5-element CategoricalArrays.CategoricalArray{UInt8,1,UInt8}:
 0x52
 0x42
 0x4d
 0x54
 0x4e

x_approx = inverse_transform(mach, z)
julia&gt; x - x_approx
5-element Vector{Float64}:
 0.008224506144777322
 0.012731354778359405
 0.0056265330571125816
 0.005738175684445124
 0.006835652575801987</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJModels.jl/blob/v0.17.9/src/builtins/Transformers.jl#L1432-L1447">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="MLJModels.UnivariateTimeTypeToContinuous" href="#MLJModels.UnivariateTimeTypeToContinuous"><code>MLJModels.UnivariateTimeTypeToContinuous</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">UnivariateTimeTypeToContinuous</code></pre><p>A model type for constructing a single variable transformer that creates continuous representations of temporally typed data, based on <a href="https://github.com/JuliaAI/MLJModels.jl">MLJModels.jl</a>, and implementing the MLJ model interface.</p><p>From MLJ, the type can be imported using</p><pre><code class="nohighlight hljs">UnivariateTimeTypeToContinuous = @load UnivariateTimeTypeToContinuous pkg=MLJModels</code></pre><p>Do <code>model = UnivariateTimeTypeToContinuous()</code> to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in <code>UnivariateTimeTypeToContinuous(zero_time=...)</code>.</p><p>Use this model to convert vectors with a <code>TimeType</code> element type to vectors of <code>Float64</code> type (<code>Continuous</code> element scitype).</p><p><strong>Training data</strong></p><p>In MLJ or MLJBase, bind an instance <code>model</code> to data with</p><pre><code class="nohighlight hljs">mach = machine(model, x)</code></pre><p>where</p><ul><li><code>x</code>: any abstract vector whose element type is a subtype of <code>Dates.TimeType</code></li></ul><p>Train the machine using <code>fit!(mach, rows=...)</code>.</p><p><strong>Hyper-parameters</strong></p><ul><li><p><code>zero_time</code>: the time that is to correspond to 0.0 under transformations, with the type coinciding with the training data element type. If unspecified, the earliest time encountered in training is used.</p></li><li><p><code>step::Period=Hour(24)</code>: time interval to correspond to one unit under transformation</p></li></ul><p><strong>Operations</strong></p><ul><li><code>transform(mach, xnew)</code>: apply the encoding inferred when <code>mach</code> was fit</li></ul><p><strong>Fitted parameters</strong></p><p><code>fitted_params(mach).fitresult</code> is the tuple <code>(zero_time, step)</code> actually used in transformations, which may differ from the user-specified hyper-parameters.</p><p><strong>Example</strong></p><pre><code class="nohighlight hljs">using MLJ
using Dates

x = [Date(2001, 1, 1) + Day(i) for i in 0:4]

encoder = UnivariateTimeTypeToContinuous(zero_time=Date(2000, 1, 1),
                                         step=Week(1))

mach = machine(encoder, x)
fit!(mach)
julia&gt; transform(mach, x)
5-element Vector{Float64}:
 52.285714285714285
 52.42857142857143
 52.57142857142857
 52.714285714285715
 52.857142</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJModels.jl/blob/v0.17.9/src/builtins/Transformers.jl#L1886-L1901">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../loading_model_code/">« Loading Model Code</a><a class="docs-footer-nextpage" href="../feature_selection/">Feature Selection »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.11.4 on <span class="colophon-date" title="Tuesday 3 June 2025 10:32">Tuesday 3 June 2025</span>. Using Julia version 1.11.5.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
