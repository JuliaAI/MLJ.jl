@inproceedings{beygelzimer2005weighted,
  title={Weighted one-against-all},
  author={Beygelzimer, Alina and Langford, John and Zadrozny, Bianca},
  booktitle={American Association for Artificial Intelligence (AAAI)},
  pages={720--725},
  year={2005}
}
@article{burman1992data,
  title={Data-Dependent Estimation Of Prediction Functions},
  author={Burman, Prabir and Nolan, Deborah},
  journal={Journal of Time Series Analysis},
  volume={13},
  number={3},
  pages={189--207},
  year={1992},
  publisher={Wiley Online Library}
}

@book{mcquarrie1998regression,
  title={Regression and time series model selection},
  author={McQuarrie, Allan DR and Tsai, Chih-Ling},
  year={1998},
  publisher={World Scientific}
}
@article{bergmeir2018note,
  title={A note on the validity of cross-validation for evaluating autoregressive time series prediction},
  author={Bergmeir, Christoph and Hyndman, Rob J and Koo, Bonsoo},
  journal={Computational Statistics \& Data Analysis},
  volume={120},
  pages={70--83},
  year={2018},
  publisher={Elsevier}
}
@article{kunst2008cross,
  title={Cross validation of prediction models for seasonal time series by parametric bootstrapping},
  author={Kunst, Robert M},
  journal={Austrian Journal of Statistics},
  volume={37},
  number={3\&4},
  pages={271--284},
  year={2008}
}
@article{racine2000consistent,
  title={Consistent cross-validatory model-selection for dependent data: hv-block cross-validation},
  author={Racine, Jeff},
  journal={Journal of econometrics},
  volume={99},
  number={1},
  pages={39--61},
  year={2000},
  publisher={Elsevier}
}
@article{burman1994cross,
  title={A cross-validatory method for dependent data},
  author={Burman, Prabir and Chow, Edmond and Nolan, Deborah},
  journal={Biometrika},
  volume={81},
  number={2},
  pages={351--358},
  year={1994},
  publisher={Oxford University Press}
}
@book{gyorfi2013nonparametric,
  title={Nonparametric curve estimation from time series},
  author={Gy{\"o}rfi, L{\'a}zl{\'o} and H{\"a}rdle, Wolfgang and Sarda, Pascal and Vieu, Philippe},
  volume={60},
  year={2013},
  publisher={Springer}
}
@incollection{beygelzimer2008machine,
  title={Machine learning techniques—reductions between prediction quality metrics},
  author={Beygelzimer, Alina and Langford, John and Zadrozny, Bianca},
  booktitle={Performance Modeling and Engineering},
  pages={3--28},
  year={2008},
  publisher={Springer}
}
@article{deng2013time,
  title={A time series forest for classification and feature extraction},
  author={Deng, Houtao and Runger, George and Tuv, Eugene and Vladimir, Martyanov},
  journal={Information Sciences},
  volume={239},
  pages={142--153},
  year={2013},
  publisher={Elsevier}
}
@article{terpilowski2019scikit,
  title={scikit-posthocs: Pairwise multiple comparison tests in Python},
  author={Terpilowski, M},
  journal={Journal of Open Source Software},
  volume={4},
  pages={1169},
  year={2019}
}
@book{Lutkepohl2005,
author = {L{\"{u}}tkepohl, Helmut},
publisher = {Springer Science {\&} Business Media},
title = {{New introduction to multiple time series analysis}},
year = {2005}
}
@incollection{graves2012supervised,
  title={Supervised sequence labelling},
  author={Graves, Alex},
  booktitle={Supervised sequence labelling with recurrent neural networks},
  pages={5--13},
  year={2012},
  publisher={Springer}
}
@article{Hyndman25years,
  title={25 years of time series forecasting},
  author={De Gooijer, Jan G and Hyndman, Rob J},
  journal={International journal of forecasting},
  volume={22},
  number={3},
  pages={443--473},
  year={2006},
  publisher={Elsevier}
}
@book{Hastie2009,
abstract = {During the past decade there has been an explosion in computation and information technology. With it has come vast amounts of data in a variety of fields such as medicine, biology, finance, and marketing. The challenge of understanding these data has led to the development of new tools in the field of statistics, and spawned new areas such as data mining, machine learning, and bioinformatics. Many of these tools have common underpinnings but are often expressed with different terminology. This book describes the important ideas in these areas in a common conceptual framework. While the approach is statistical, the emphasis is on concepts rather than mathematics. Many examples are given, with a liberal use of color graphics. It should be a valuable resource for statisticians and anyone interested in data mining in science or industry. The book's coverage is broad, from supervised learning (prediction) to unsupervised learning. The many topics include neural networks, support vector machines, classification trees and boosting-the first comprehensive treatment of this topic in any book. Trevor Hastie, Robert Tibshirani, and Jerome Friedman are professors of statistics at Stanford University. They are prominent researchers in this area: Hastie and Tibshirani developed generalized additive models and wrote a popular book of that title. Hastie wrote much of the statistical modeling software in S-PLUS and invented principal curves and surfaces. Tibshirani proposed the Lasso and is co-author of the very successful An Introduction to the Bootstrap. Friedman is the co-inventor of many data-mining tools including CART, MARS, and projection pursuit. FROM THE REVIEWS: TECHNOMETRICS "This is a vast and complex book. Generally, it concentrates on explaining why and how the methods work, rather than how to use them. Examples and especially the visualizations are principle features...As a source for the methods of statistical learning...it will probably be a long time before there is a competitor to this book."},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Hastie, Trevor and Tibshirani, Robert T. and Friedman, Jerome},
booktitle = {The Elements of Statistical Learning},
doi = {10.1007/b94608},
edition = {2},
eprint = {arXiv:1011.1669v3},
file = {:Users/mloning/Library/Application Support/Mendeley Desktop/Downloaded/Hastie, Tibshirani, Friedman - 2009 - The Elements of Statistical Learning.pdf:pdf},
isbn = {978-0-387-84857-0},
issn = {0172-7397},
mendeley-groups = {Data Science {\&} Machine Learning},
pages = {1--694},
pmid = {12377617},
publisher = {Springer},
title = {{The Elements of Statistical Learning}},
Adsurl = {http://www.springerlink.com/index/10.1007/b94608},
volume = {1},
year = {2009}
}
@article{Perez2007,
author = {Perez, Fernando and Granger, Brian E.},
doi = {10.1109/MCSE.2007.53},
issn = {1521-9615},
journal = {Computing in Science {\&} Engineering},
keywords = {Python,computer languages,scientific computing,scientific programming},
number = {3},
pages = {21--29},
publisher = {IEEE Educational Activities Department},
title = {{IPython: A System for Interactive Scientific Computing}},
Adsurl = {http://ieeexplore.ieee.org/document/4160251/},
volume = {9},
year = {2007}
}
@article{beygelzimer2015learning,
  title={Learning reductions that really work},
  author={Beygelzimer, Alina and Daum{\'e}, Hal and Langford, John and Mineiro, Paul},
  journal={Proceedings of the IEEE},
  volume={104},
  number={1},
  pages={136--147},
  year={2015},
  publisher={IEEE}
}
@article{Fulcher2017,
abstract = {Phenotype measurements frequently take the form of time series, but we currently lack a systematic method for relating these complex data streams to scientifically meaningful outcomes, such as relating the movement dynamics of organisms to their genotype or measurements of brain dynamics of a patient to their disease diagnosis. Previous work addressed this problem by comparing implementations of thousands of diverse scientific time-series analysis methods in an approach termed highly comparative time-series analysis. Here, we introduce hctsa, a software tool for applying this methodological approach to data. hctsa includes an architecture for computing over 7,700 time-series features and a suite of analysis and visualization algorithms to automatically select useful and interpretable time-series features for a given application. Using exemplar applications to high-throughput phenotyping experiments, we show how hctsa allows researchers to leverage decades of time-series research to quantify and understand informative structure in time-series data.},
author = {Fulcher, Ben D and Jones, Nick S},
doi = {10.1016/j.cels.2017.10.001},
issn = {2405-4712},
journal = {Cell systems},
keywords = {high-throughput phenotyping,time-series analysis},
mendeley-groups = {Data Science {\&} Machine Learning},
month = {nov},
number = {5},
pages = {527--531.e3},
pmid = {29102608},
publisher = {Elsevier},
title = {{hctsa: A Computational Framework for Automated Time-Series Phenotyping Using Massive Feature Extraction.}},
Adsurl = {http://www.ncbi.nlm.nih.gov/pubmed/29102608},
volume = {5},
year = {2017}
}

@article{Christ2018,
abstract = {Time series feature engineering is a time-consuming process because scientists and engineers have to consider the multifarious algorithms of signal processing and time series analysis for identifying and extracting meaningful features from time series. The Python package tsfresh (Time Series FeatuRe Extraction on basis of Scalable Hypothesis tests) accelerates this process by combining 63 time series characterization methods, which by default compute a total of 794 time series features, with feature selection on basis automatically configured hypothesis tests. By identifying statistically significant time series characteristics in an early stage of the data science process, tsfresh closes feedback loops with domain experts and fosters the development of domain specific features early on. The package implements standard APIs of time series and machine learning libraries (e.g. pandas and scikit-learn) and is designed for both exploratory analyses as well as straightforward integration into operational data science applications.},
author = {Christ, Maximilian and Braun, Nils and Neuffer, Julius and Kempa-Liehr, Andreas W.},
doi = {10.1016/J.NEUCOM.2018.03.067},
issn = {0925-2312},
journal = {Neurocomputing},
mendeley-groups = {Data Science {\&} Machine Learning},
month = {sep},
pages = {72--77},
publisher = {Elsevier},
title = {{Time Series FeatuRe Extraction on basis of Scalable Hypothesis tests (tsfresh – A Python package)}},
Adsurl = {https://www.sciencedirect.com/science/article/pii/S0925231218304843?via{\%}3Dihub},
volume = {307},
year = {2018}
}
@article{takeuchi2006unifying,
  title={A unifying framework for detecting outliers and change points from time series},
  author={Takeuchi, Jun-ichi and Yamanishi, Kenji},
  journal={IEEE transactions on Knowledge and Data Engineering},
  volume={18},
  number={4},
  pages={482--492},
  year={2006},
  publisher={IEEE}
}
@inproceedings{guralnik1999event,
  title={Event detection from time series data},
  author={Guralnik, Valery and Srivastava, Jaideep},
  booktitle={Proceedings of the fifth ACM SIGKDD international conference on Knowledge discovery and data mining},
  pages={33--42},
  year={1999},
  organization={ACM}
}
@article{Hoyer2017,
author = {Hoyer, Stephan and Hamman, Joseph J.},
doi = {10.5334/jors.148},
journal = {Journal of Open Research Software},
mendeley-groups = {Data Science {\&} Machine Learning},
month = {apr},
number = {1},
publisher = {Ubiquity Press},
title = {{xarray: N-D labeled Arrays and Datasets in Python}},
Adsurl = {http://openresearchsoftware.metajnl.com/articles/10.5334/jors.148/},
volume = {5},
year = {2017}
}
@misc{xarray_v0_8_0,
      author = {Stephan Hoyer and Clark Fitzgerald and Joe Hamman and others},
      title  = {xarray: v0.8.0},
      month  = aug,
      year   = 2016,
      doi    = {10.5281/zenodo.59499},
      url    = {https://doi.org/10.5281/zenodo.59499}
     }
@article{davydov2018xpandas,
author = {Davydov, Vitaly and Kir{\'{a}}ly, Franz J},
journal = {openreview.net},
title = {{xpandas: Python data containers for structured types and structured machine learning tasks}},
year = {2018}
}
@book{hyndman2018forecasting,
  title={Forecasting: principles and practice},
  author={Hyndman, Rob J and Athanasopoulos, George},
  year={2018},
  publisher={OTexts}
}
@inproceedings{Bontempi2012,
author = {Bontempi, Gianluca and Taieb, Souhaib Ben and {Le Borgne}, Yann-A{\"{e}}l},
booktitle = {Business Intelligence},
organization = {Springer, Berlin, Heidelberg},
pages = {62--77},
title = {{Machine Learning Strategies for Time Series Forecasting}},
year = {2013}
}
@article{Demsar2006,
abstract = {While methods for comparing two learning algorithms on a single data set have been scrutinized for quite some time already, the issue of statistical tests for comparisons of more algorithms on multiple data sets, which is even more essential to typical machine learning studies, has been all but ignored. This article reviews the current practice and then theoretically and empirically examines several suitable tests. Based on that, we recommend a set of simple, yet safe and robust non-parametric tests for statistical comparisons of classifiers: the Wilcoxon signed ranks test for comparison of two classifiers and the Friedman test with the corresponding post-hoc tests for comparison of more classifiers over multiple data sets. Results of the latter can also be neatly presented with the newly introduced CD (critical difference) diagrams.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Dem{\v{s}}ar, Janez},
doi = {10.1016/j.jecp.2010.03.005},
eprint = {arXiv:1011.1669v3},
isbn = {9781424450404},
issn = {1532-4435},
journal = {Journal of Machine Learning Research},
keywords = {Friedman test,Wilcoxon signed ranks test,comparative studies,multiple comparisons tests,statistical methods},
pages = {1--30},
pmid = {20451214},
title = {{Statistical Comparisons of Classifiers over Multiple Data Sets}},
volume = {7},
year = {2006}
}
@article{taylor2018forecasting,
  title={Forecasting at scale},
  author={Taylor, Sean J and Letham, Benjamin},
  journal={The American Statistician},
  volume={72},
  number={1},
  pages={37--45},
  year={2018},
  publisher={Taylor \& Francis}
}
@misc{taylor2016pyflux,
  title={Pyflux: An open source time series library for Python},
  author={Taylor, R},
  year={2016}
}
@article{burns2018seglearn,
  title={Seglearn: a python package for learning sequences and time series},
  author={Burns, David M and Whyne, Cari M},
  journal={The Journal of Machine Learning Research},
  volume={19},
  number={1},
  pages={3238--3244},
  year={2018},
  publisher={JMLR. org}
}
@misc{tavenard2017tslearn,
  title={tslearn: A machine learning toolkit dedicated to time-series data},
  author={Tavenard, Romain},
  year={2017}
}
@inproceedings{Kanter2015,
author = {Kanter, James Max and Veeramachaneni, Kalyan},
booktitle = {2015 IEEE International Conference on Data Science and Advanced Analytics, DSAA 2015, Paris, France, October 19-21, 2015},
organization = {IEEE},
pages = {1--10},
title = {{Deep feature synthesis: Towards automating data science endeavors}},
year = {2015}
}
@article{Hall2009,
author = {Hall, Mark and Frank, Eibe and Holmes, Geoffrey and Pfahringer, Bernhard and Reutemann, Peter and Witten, Ian H.},
doi = {10.1145/1656274.1656278},
issn = {19310145},
journal = {ACM SIGKDD Explorations Newsletter},
month = {nov},
number = {1},
pages = {10},
publisher = {ACM},
title = {{The WEKA data mining software}},
Adsurl = {http://portal.acm.org/citation.cfm?doid=1656274.1656278},
volume = {11},
year = {2009}
}
@misc{Jones,
annote = {[Online; accessed ]},
author = {Jones, Eric and Oliphant, Travis E. and Peterson, Pearu and Others},
title = {{SciPy: Open source scientific tools for Python}},
Adsurl = {http://www.scipy.org/},
year = {2001}
}
@misc{Guecioueur2018,
author = {Guecioueur, Ahmed},
title = {{pysf: Supervised forecasting of sequential data in Python}},
Adsurl = {https://pypi.org/project/pysf/},
year = {2018}
}
@inproceedings{Holmes1994,
author = {Holmes, G. and Donkin, A. and Witten, I.H.},
booktitle = {Proceedings of ANZIIS '94 - Australian New Zealnd Intelligent Information Systems Conference},
doi = {10.1109/ANZIIS.1994.396988},
isbn = {0-7803-2404-8},
pages = {357--361},
publisher = {IEEE},
title = {{WEKA: a machine learning workbench}},
Adsurl = {http://ieeexplore.ieee.org/document/396988/},
year = {1994}
}
@article{VanderWalt2011,
abstract = {In the Python world, NumPy arrays are the standard representation for numerical data and enable efficient implementation of numerical computations in a high-level language. As this effort shows, NumPy performance can be improved through three techniques: vectorizing calculations, avoiding copying data in memory, and minimizing operation counts.},
author = {van der Walt, St{\'{e}}fan and Colbert, S Chris and Varoquaux, Ga{\"{e}}l},
doi = {10.1109/MCSE.2011.37},
issn = {1521-9615},
journal = {Computing in Science {\&} Engineering},
keywords = {data structures,high level languages,mathematics computing},
mendeley-groups = {Data Science {\&} Machine Learning},
month = {mar},
number = {2},
pages = {22--30},
publisher = {IEEE Computer Society},
title = {{The NumPy Array: A Structure for Efficient Numerical Computation}},
Adsurl = {http://ieeexplore.ieee.org/document/5725236/},
volume = {13},
year = {2011}
}

@inproceedings{Perktold2010,
abstract = {Statsmodels is a library for statistical and econometric analysis in Python. This paper discusses the current relationship between statistics and Python and open source more generally, outlining how the statsmodels package fills a gap in this relationship. An overview of statsmodels is provided, including a discussion of the overarching design and philosophy, what can be found in the package, and some usage examples. The paper concludes with a look at what the future holds.},
author = {Perktold, Josef and Seabold, Skipper},
booktitle = {Proceedings of the 9th Python in Science Conference},
mendeley-groups = {Data Science {\&} Machine Learning},
title = {{Statsmodels: Econometric and Statistical Modeling with Python Quantitative histology of aorta View project Statsmodels: Econometric and Statistical Modeling with Python}},
Adsurl = {https://www.researchgate.net/publication/264891066},
year = {2010}
}
@inproceedings{McKinney2011,
author = {McKinney, Wes},
booktitle = {Python for High Performance and Scientific Computing},
title = {{pandas: a Foundational Python Library for Data Analysis and Statistics}},
year = {2011}
}
@article{taleoftwotoolkits2019,
  title={A tale of two toolkits, report the first: benchmarking time series classification algorithms for correctness and efficiency},
  author={Bagnall, Anthony and Löning, Markus and Middlehurst, Matthew and Oastler, George},
  journal={arXiv preprint arXiv:1909.05738},
  year={2019}
}
@article{kazakov2019machine,
  title={Machine Learning Automation Toolbox (MLaut)},
  author={Kazakov, Viktor and Kir{\'a}ly, Franz J},
  journal={arXiv preprint arXiv:1901.03678},
  year={2019}
}
@book{Daley2003,
abstract = {Point processes and random measures find wide applicability in telecommunications, earthquakes, image analysis, spatial point patterns, and stereology, to name but a few. The authors have made a major reshaping of their classic work published in 1988. It will be a standard reference for researchs and graduate students in pure and applied probability.},
address = {New York},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Daley, Daryl J. and Vere-Jones, David},
booktitle = {Book},
doi = {10.1007/b97277},
edition = {2nd},
eprint = {arXiv:1011.1669v3},
file = {:Users/mloning/Library/Application Support/Mendeley Desktop/Downloaded/Daley, Vere-Jones - 2003 - An Introduction to the Theory of Point Processes.pdf:pdf},
isbn = {0-387-95541-0},
issn = {01727397},
keywords = {point{\_}processes reference},
mendeley-groups = {Probability Theory},
pages = {xviii+573},
pmid = {15772297},
publisher = {Springer},
title = {{An Introduction to the Theory of Point Processes}},
Adsurl = {http://link.springer.com/10.1007/b97277},
year = {2003}
}

@misc{chollet2015keras,
  title={Keras},
  author={Chollet, Fran\c{c}ois and others},
  year={2015},
  howpublished={\url{https://keras.io}},
}
@techreport{Heckbert1997,
author = {Heckbert, Paul S and Garland, Michael},
institution = {Carnegie-Mellon Univ Pittsburgh PA School of Computer Science},
title = {{Survey of polygonal surface simplification algorithms}},
year = {1997}
}
@book{Cox1984,
author = {Cox, D. R. and Oakes, David},
isbn = {9780412244902},
pages = {201},
publisher = {Chapman and Hall},
title = {{Analysis of Survival Data}},
year = {1984}
}
@book{Diggle2013a,
author = {Diggle, Peter and Heagerty, Patrick and Liang, Kung-Yee and Zeger, Scott},
edition = {2nd},
isbn = {0199676755},
mendeley-groups = {Economics},
publisher = {Oxford University Press},
title = {{Analysis of longitudinal data}},
Adsurl = {https://books.google.co.uk/books?id=ur0BlXPuOukC{\&}printsec=frontcover{\&}dq=Diggle,+P.,+P.+J.+Diggle,+P.+Heagerty,+P.+J.+Heagerty,+K.-Y.+Liang,+S.+Zeger,+et+al.+(2013).+Analysis+of+Longitudinal+Data+(2+ed.).+Oxford+University+Press.{\&}hl=en{\&}sa=X{\&}ved=0ahUKEwisyM},
year = {2013}
}
@book{Baltagi2008,
abstract = {4th ed. This new edition of an established textbook reflects the rapid developments in the field, covering the vast research that has been conducted on panel data since its initial publication. Preface. 1. Introduction. 1.1 Panel Data: Some Examples. 1.2 Why Should We Use Panel Data? Their Benefits and Limitations. Note. 2. The One-way Error Component Regression Model. 2.1 Introduction. 2.2 The Fixed Effects Model. 2.3 The Random Effects Model. 2.4 Maximum Likelihood Estimation. 2.5 Prediction. 2.6 Examples. 2.7 Selected Applications. 2.8 Computational Note. Notes. Problems. 3. The Two-way Error Component Regression Model. 3.1 Introduction. 3.2 The Fixed Effects Model. 3.3 The Random Effects Model. 3.4 Maximum Likelihood Estimation. 3.5 Prediction. 3.6 Examples. 3.7 Selected Applications. Notes. Problems. 4. Test of Hypotheses with Panel Data. 4.1 Tests for Poolability of the Data. 4.2 Tests for Individual and Time Effects. 4.3 Hausman?s Specification Test. 4.4 Further Reading. Notes. Problems. 5. Heteroskedasticity and Serial Correlation in the Error Component Model. 5.1 Heteroskedasticity. 5.2 Serial Correlation. Notes. Problems. 6. Seemingly Unrelated Regressions with Error Components. 6.1 The One-way Model. 6.2 The Two-way Model. 6.3 Applications and Extensions. Problems. 7. Simultaneous Equations with Error Components. 7.1 Single Equation Estimation. 7.2 Empirical Example: Crime in North Carolina. 7.3 System Estimation. 7.4 The Hausman and Taylor Estimator. 7.5 Empirical Example: Earnings Equation Using PSID Data. 7.6 Further Reading and Extensions. Notes. Problems. 8. Dynamic Panel Data Models. 8.1 Introduction. 8.2 The Arellano and Bond Estimator. 8.3 The Arellano and Bover Estimator. 8.4 The Ahn and Schmidt Moment Conditions. 8.5 The Blundell and Bond System GMM Estimator. 8.6 The Keane and Runkle Estimator. 8.7 Further Developments. 8.8 Empirical Examples. 8.9 Further Reading. Notes. Problems. 9. Unbalanced Panel Data Models. 9.1 Introduction. 9.2 The Unbalanced One-way Error Component Model. 9.3 Empirical Example: Hedonic Housing. 9.4 The Unbalanced Two-way Error Component Model. 9.5 Testing for Individual and Time Effects Using Unbalanced Panel Data. 9.6 The Unbalanced Nested Error Component Model. Notes. Problems. 10. Special Topics. 10.1 Measurement Error and Panel Data. 10.2 Rotating Panels. 10.3 Pseudo-panels. 10.4 Alternative Methods of Pooling Time Series of Cross-section Data. 10.5 Spatial Panels. 10.6 Short-run vs Long-run Estimates in Pooled Models. 10.7 Heterogeneous Panels. 10.8 Count Panel Data. Notes. Problems. 11. Limited Dependent Variables and Panel Data. 11.1 Fixed and Random Logit and Probit Models. 11.2 Simulation Estimation of Limited Dependent Variable Models with Panel Data. 11.3 Dynamic Panel Data Limited Dependent Variable Models. 11.4 Selection Bias in Panel Data. 11.5 Censored and Truncated Panel Data Models. 11.6 Empirical Applications. 11.7 Empirical Example: Nurses Labor Supply. 11.8 Further Reading. Notes. Problems. 12. Nonstationary Panels. 12.1 Introduction. 12.2 Panel Unit Root Tests Assuming Cross-Sectional Independence. 12.3 Panel Unit Root Tests Allowing for Cross-Sectional Dependence. 12.4 Spurious Regression in Panel Data. 12.5 Panel Cointegration Tests. 12.6 Estimation and Inference in Panel Cointegration Models. 12.7 Empirical Example: Purchasing Power Parity. 12.8 Further Reading. Notes. Problems. References. Index.},
author = {Baltagi, Badi H.},
edition = {4},
isbn = {9780470518861},
mendeley-groups = {Economics},
pages = {351},
publisher = {John Wiley {\&} Sons},
title = {{Econometric Analysis of Panel Data}},
year = {2008}
}
@book{Wooldridge2010,
abstract = {2nd ed. "The second edition of this acclaimed graduate text provides a unified treatment of two methods used in contemporary econometric research, cross section and data panel methods. By focusing on assumptions that can be given behavioral content, the book maintains an appropriate level of rigor while emphasizing intuitive thinking. The analysis covers both linear and nonlinear models, including models with dynamics and/or individual heterogeneity. In addition to general estimation frameworks (particular methods of moments and maximum likelihood), specific linear and nonlinear methods are covered in detail, including probit and logit models and their multivariate, Tobit models, models for count data, censored and missing data schemes, causal (or treatment) effects, and duration analysis.Econometric Analysis of Cross Section and Panel Data was the first graduate econometrics text to focus on microeconomic data structures, allowing assumptions to be separated into population and sampling assumptions. This second edition has been substantially updated and revised. Improvements include a broader class of models for missing data problems; more detailed treatment of cluster problems, an important topic for empirical researchers; expanded discussion of "generalized instrumental variables" (GIV) estimation; new coverage (based on the author's own recent research) of inverse probability weighting; a more complete framework for estimating treatment effects with panel data, and a firmly established link between econometric approaches to nonlinear panel data and the "generalized estimating equation" literature popular in statistics and other fields. New attention is given to explaining when particular econometric methods can be applied; the goal is not only to tell readers what does work, but why certain "obvious" procedures do not. The numerous included exercises, both theoretical and computer-based, allow the reader to extend methods covered in the text and discover new insights."--Back cover. Introduction -- Conditional expectations and related concepts in econometrics -- Basic asymptotic theory -- Single-equation linear model and ordinary least squares estimation -- Instrumental variables estimation of single-equation linear models -- Additional single-equation topics -- Estimating systems of equations by ordinary least squares and generalized least squares -- System estimation by instrumental variables -- Simultaneous equations models -- Basic linear unobserved effects panel data models -- More topics in linear unobserved effects models -- M-estimation, nonlinear regression, and quantile regression -- Maximum likelihood methods -- Generalized method of moments and minimum distance estimation -- Binary response models -- Multinomial and ordered response models -- Corner solution responses -- Count, fractional, and other nonnegative responses -- Censored data, sample selection, and attrition -- Stratified sampling and cluster sampling -- Estimating average treatment effects -- Duration analysis.},
author = {Wooldridge, Jeffrey M},
isbn = {0262232588},
mendeley-groups = {Economics},
pages = {1064},
publisher = {MIT Press},
title = {{Econometric analysis of cross section and panel data}},
Adsurl = {https://books.google.co.uk/books?id=yov6AQAAQBAJ{\&}printsec=frontcover{\&}dq=wooldridge+panel+data{\&}hl=en{\&}sa=X{\&}ved=0ahUKEwie5JH8u7PeAhVFTcAKHR3qA70Q6AEIKzAA{\#}v=onepage{\&}q=wooldridge panel data{\&}f=false},
year = {2010}
}

@book{Brockwell2016,
author = {Brockwell, Peter J. and Davis, Richard A.},
doi = {10.1007/978-3-319-29854-2},
isbn = {978-3-319-29852-8},
mendeley-groups = {Data Science {\&} Machine Learning},
publisher = {Springer International Publishing},
series = {Springer Texts in Statistics},
title = {{Introduction to Time Series and Forecasting}},
Adsurl = {http://link.springer.com/10.1007/978-3-319-29854-2},
year = {2016}
}
@book{Box2015,
abstract = {Fifth edition / George E.P. Box, Gwilym M. Jenkins, Gregory C. Reinsel, Greta M. Ljung. Praise for the Fourth Edition "The book follows faithfully the style of the original edition. The approach is heavily motivated by real-world time series, and by developing a complete approach to model building, estimation, forecasting and control.""--Mathematical Reviews Bridging classical models and modern topics, the Fifth Edition of Time Series Analysis: Forecasting and Control maintains a balanced presentation of the tools for modeling and analyzing time series. Also describing the latest developments that have occurred in the field over the past decade through applicati. Cover; Wiley Series in Probability and Statistics; Title Page; Copyright; Dedication; Preface to the Fifth Edition; Preface to the Fourth Edition; Preface to the Third Edition; 1.1 Five Important Practical Problems; 1.2 Stochastic and Deterministic Dynamic Mathematical Models; 1.3 Basic Ideas in Model Building; Appendix A1.1 Use Of The R Software; Exercises; Chapter 1: Introduction; Part One: Stochastic Models and Their Forecasting; Chapter 2: Autocorrelation Function and Spectrum of Stationary Processes; 2.1 Autocorrelation Properties of Stationary Models. 2.2 Spectral Properties of Stationary ModelsAppendix A2.1 Link Between the Sample Spectrum and Autocovariance Function Estimate; Exercises; Chapter 3: Linear Stationary Models; 3.1 General Linear Process; 3.2 Autoregressive Processes; 3.3 Moving Average Processes; 3.4 Mixed Autoregressive-Moving Average Processes; Appendix A3.1 Autocovariances, Autocovariance Generating Function, and Stationarity Conditions for a General Linear Process; Appendix A3.2 Recursive Method for Calculating Estimates of Autoregressive Parameters; Exercises; Chapter 4: Linear Nonstationary Models. 4.1 Autoregressive Integrated Moving Average Processes4.2 Three Explicit Forms for the Arima Model; 4.3 Integrated Moving Average Processes; Appendix A4.1 Linear Difference Equations; Appendix A4.2 IMA(0, 1, 1) Process with Deterministic Drift; Appendix A4.3 Arima Processes with Added Noise; Exercises; Chapter 5: Forecasting; 5.1 Minimum Mean Square Error Forecasts and Their Properties; 5.2 Calculating Forecasts and Probability Limits; 5.3 Forecast Function and Forecast Weights; 5.4 Examples of Forecast Functions and Their Updating. 5.5 Use of State-Space Model Formulation for Exact Forecasting5.6 Summary; Appendix A5.1 Correlation Between Forecast Errors; Appendix A5.2 Forecast Weights for Any Lead Time; Appendix A5.3 Forecasting in Terms of the General Integrated Form; Exercises; Part Two: Stochastic Model Building; Chapter 6: Model Identification; 6.1 Objectives of Identification; 6.2 Identification Techniques; 6.3 Initial Estimates for the Parameters; 6.4 Model Multiplicity; Appendix A6.1 Expected Behavior of the Estimated Autocorrelation Function for a Nonstationary Process; Exercises. Chapter 7: Parameter Estimation7.1 Study of the Likelihood and Sum-of-Squares Functions; 7.2 Nonlinear Estimation; 7.3 Some Estimation Results for Specific Models; 7.4 Likelihood Function Based on the State-Space Model; 7.5 Estimation Using Bayes' Theorem; Appendix A7.1 Review of Normal Distribution Theory; Appendix A7.2 Review of Linear Least-Squares Theory; Appendix A7.3 Exact Likelihood Function for Moving Average and Mixed Processes; Appendix A7.4 Exact Likelihood Function for an Autoregressive Process; Appendix A7.5 Asymptotic Distribution of Estimators for Autoregressive Models.},
author = {Box, George E. P. and Jenkins, Gwilym M. and Reinsel, Gregory C. and Ljung, Greta M.},
isbn = {9781118674925},
mendeley-groups = {Data Science {\&} Machine Learning},
publisher = {John Wiley {\&} Sons},
title = {{Time series analysis: forecasting and control}},
year = {2015}
}

@article{Gressmann2018,
abstract = {Predictive modelling and supervised learning are central to modern data science. With predictions from an ever-expanding number of supervised black-box strategies - e.g., kernel methods, random forests, deep learning aka neural networks - being employed as a basis for decision making processes, it is crucial to understand the statistical uncertainty associated with these predictions. As a general means to approach the issue, we present an overarching framework for black-box prediction strategies that not only predict the target but also their own predictions' uncertainty. Moreover, the framework allows for fair assessment and comparison of disparate prediction strategies. For this, we formally consider strategies capable of predicting full distributions from feature variables, so-called probabilistic supervised learning strategies. Our work draws from prior work including Bayesian statistics, information theory, and modern supervised machine learning, and in a novel synthesis leads to (a) new theoretical insights such as a probabilistic bias-variance decomposition and an entropic formulation of prediction, as well as to (b) new algorithms and meta-algorithms, such as composite prediction strategies, probabilistic boosting and bagging, and a probabilistic predictive independence test. Our black-box formulation also leads (c) to a new modular interface view on probabilistic supervised learning and a modelling workflow API design, which we have implemented in the newly released skpro machine learning toolbox, extending the familiar modelling interface and meta-modelling functionality of sklearn. The skpro package provides interfaces for construction, composition, and tuning of probabilistic supervised learning strategies, together with orchestration features for validation and comparison of any such strategy - be it frequentist, Bayesian, or other.},
archivePrefix = {arXiv},
arxivId = {1801.00753},
author = {Gressmann, Frithjof and Kir{\'{a}}ly, Franz J. and Mateen, Bilal and Oberhauser, Harald},
eprint = {1801.00753},
file = {:Users/mloning/Library/Application Support/Mendeley Desktop/Downloaded/Gressmann et al. - 2018 - Probabilistic supervised learning.pdf:pdf},
journal = {arXiv preprint arXiv:1801.00753},
title = {{Probabilistic supervised learning}},
Adsurl = {http://arxiv.org/abs/1801.00753},
year = {2018}
}
@book{Kleinbaum2012,
abstract = {3rd ed. This book is available on the World Wide Web. Access is restricted to computers located within Mount Sinai or to those users eligible for remote access services.},
author = {Kleinbaum, David G. and Klein, Mitchel.},
isbn = {9781441966469},
pages = {700},
publisher = {Springer},
title = {{Survival analysis: A Self-Learning Text}},
year = {2012}
}
@book{Kalbfleisch2002,
abstract = {2nd ed. Contains additional discussion and examples on left truncation as well as material on more general censoring and truncation patterns.Introduces the martingale and counting process formulation swil lbe in a new chapter.Develops multivariate failure time data in a separate chapter and extends the material on Markov and semi Markov formulations.Presents new examples and applications of data analysis. Front Matter -- Introduction -- Failure Time Models -- Inference in Parametric Models and Related Topics -- Relative Risk (Cox) Regression Models -- Counting Processes and Asymptotic Theory -- Likelihood Construction and Further Results -- Rank Regression and the Accelerated Failure Time Model -- Competing Risks and Multistate Models -- Modeling and Analysis of Recurrent Event Data -- Analysis of Correlated Failure Time Data -- Additional Failure Time Data Topics -- Glossary of Notation -- Appendix A: Some Sets of Data -- Appendix B: Supporting Technical Material -- Bibliography -- Author Index -- Subject Index -- Wiley Series in Probability and Statistics. Failure time models -- Inference in parametric models and related topics -- Relative risk (Cox)regression models -- Counting processes and asymptotic theory -- Likelihood construction and further results -- Rank regression and the accelerated failure time model -- Competing risks and multi-state models -- Modeling and analysis of recurrent event data -- Analysis of correlated failure time data -- Additional failure time data topics -- Appendix A: Some sets of data -- Appendix B: Some supporting technical material.},
author = {Kalbfleisch, J. D. and Prentice, Ross L.},
isbn = {9781118031230},
pages = {439},
publisher = {J. Wiley},
title = {{The statistical analysis of failure time data}},
Adsurl = {https://www.wiley.com/en-us/The+Statistical+Analysis+of+Failure+Time+Data{\%}2C+2nd+Edition-p-9781118031230},
year = {2002}
}
@article{alexandrov2019gluonts,
  title={GluonTS: Probabilistic Time Series Models in Python},
  author={Alexandrov, Alexander and Benidis, Konstantinos and Bohlke-Schneider, Michael and Flunkert, Valentin and Gasthaus, Jan and Januschowski, Tim and Maddix, Danielle C and Rangapuram, Syama and Salinas, David and Schulz, Jasper and others},
  journal={arXiv preprint arXiv:1906.05264},
  year={2019}
}
@incollection{Dietterich2002,
author = {Dietterich, Thomas G.},
booktitle = {Structural, Syntactic, and Statistical Pattern Recognition},
doi = {10.1007/3-540-70659-3_2},
editor = {{Caelli T.} and {Amin A.} and {Duin R.P.W.} and {de Ridder D.} and {Kamel M.}},
pages = {15--30},
publisher = {Springer, Berlin, Heidelberg},
title = {{Machine Learning for Sequential Data: A Review}},
Adsurl = {http://link.springer.com/10.1007/3-540-70659-3{\_}2},
year = {2002}
}
@incollection{keogh2004segmenting,
  title={Segmenting time series: A survey and novel approach},
  author={Keogh, Eamonn and Chu, Selina and Hart, David and Pazzani, Michael},
  booktitle={Data mining in time series databases},
  pages={1--21},
  year={2004},
  publisher={World Scientific}
}
@article{makridakis2019m4,
  title={The M4 Competition: 100,000 time series and 61 forecasting methods},
  author={Makridakis, Spyros and Spiliotis, Evangelos and Assimakopoulos, Vassilios},
  journal={International Journal of Forecasting},
  year={2019},
  publisher={Elsevier}
}
@article{makridakis2018m4,
  title={The M4 Competition: Results, findings, conclusion and way forward},
  author={Makridakis, Spyros and Spiliotis, Evangelos and Assimakopoulos, Vassilios},
  journal={International Journal of Forecasting},
  volume={34},
  number={4},
  pages={802--808},
  year={2018},
  publisher={Elsevier}
}
@article{makridakis2018statistical,
  title={Statistical and Machine Learning forecasting methods: Concerns and ways forward},
  author={Makridakis, Spyros and Spiliotis, Evangelos and Assimakopoulos, Vassilios},
  journal={PloS one},
  volume={13},
  number={3},
  pages={e0194889},
  year={2018},
  publisher={Public Library of Science}
}
@article{Varoquaux2015,
author = {Varoquaux, Ga{\"{e}}l and Buitinck, L. and Louppe, Gilles and Grisel, Olivier and Pedregosa, F. and Mueller, A.},
doi = {10.1145/2786984.2786995},
journal = {GetMobile: Mobile Computing and Communications},
month = {jun},
number = {1},
pages = {29--33},
title = {{Scikit-learn: Machine Learning Without Learning the Machinery}},
Adsurl = {http://dl.acm.org/citation.cfm?doid=2786984.2786995},
volume = {19},
year = {2015}
}
@article{Pedregosa2001,
author = {Pedregosa, Fabian and Varoquaux, Ga{\"{e}}l and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and Vanderplas, Jake and Passos, Alexandre and Cournapeau, David and Brucher, Matthieu and Perrot, Matthieu and Duchesnay, {\'{E}}douard},
file = {:Users/mloning/Library/Application Support/Mendeley Desktop/Downloaded/Pedregosa et al. - 2001 - Scikit-learn Machine Learning in Python.pdf:pdf},
journal = {The Journal of Machine Learning Research},
pages = {2825--2830},
publisher = {MIT Press},
title = {{Scikit-learn: Machine Learning in Python}},
Adsurl = {https://dl.acm.org/citation.cfm?id=2078195},
volume = {12},
year = {2011}
}
@article{Buitinck2013,abstract = {scikit-learn is an increasingly popular machine learning library. Written in Python, it is designed to be simple and efficient, accessible to non-experts, and reusable in various contexts. In this paper, we present and discuss our design choices for the application programming interface (API) of the project. In particular, we describe the simple and elegant interface shared by all learning and processing units in the library and then discuss its advantages in terms of composition and reusabil-ity. The paper also comments on implementation details specific to the Python ecosystem and analyzes obstacles faced by users and developers of the library.},
author = {Buitinck, Lars and Louppe, Gilles and Blondel, Mathieu and Pedregosa, Fabian and M{\"{u}}ller, Andreas C and Grisel, Olivier and Niculae, Vlad and Prettenhofer, Peter and Gramfort, Alexandre and Grobler, Jaques and Layton, Robert and Vanderplas, Jake and Joly, Arnaud and Holt, Brian and Varoquaux, Ga{\"{e}}l},
file = {:Users/mloning/Library/Application Support/Mendeley Desktop/Downloaded/Buitinck et al. - 2013 - API design for machine learning software experiences from the scikit-learn project.pdf:pdf},
journal = {ArXiv e-prints},
title = {{API design for machine learning software: experiences from the scikit-learn project}},
Adsurl = {https://github.com/scikit-learn},
year = {2013}
}

@article{behnel2011cython, 
  title={Cython: The best of both worlds}, 
  author={Behnel, Stefan and Bradshaw, Robert and Citro, Craig and Dalcin, Lisandro and Seljebotn, Dag Sverre and Smith, Kurt}, 
  journal={Computing in Science \& Engineering}, 
  volume={13}, 
  number={2}, 
  pages={31--39}, 
  year={2011}, 
  publisher={IEEE} 
}
@article{balcan2008robust,
  title={Robust reductions from ranking to classification},
  author={Balcan, Maria-Florina and Bansal, Nikhil and Beygelzimer, Alina and Coppersmith, Don and Langford, John and Sorkin, Gregory B},
  journal={Machine learning},
  volume={72},
  number={1-2},
  pages={139--153},
  year={2008},
  publisher={Springer}
}
@inproceedings{beygelzimer2005error,
  title={Error limiting reductions between classification tasks},
  author={Beygelzimer, Alina and Dani, Varsha and Hayes, Tom and Langford, John and Zadrozny, Bianca},
  booktitle={Proceedings of the 22nd international conference on Machine learning},
  pages={49--56},
  year={2005},
  organization={ACM}
}

%%% Added by Tony

@string{DAMI          = "Data Mining and Knowledge Discovery"}
@string{ICDM          = "Proc. {IEEE} International Conference on Data Mining"}
@string{JMLR          = "Journal of Machine Learning Research"}
@string{ML            = "Machine Learning"}
@string{PAMI     = "{IEEE} Transactions on Pattern Analysis and Machine
                         Intelligence"}
@string{PR            = "Pattern Recognition"}
@string{PRL           = "Pattern Recognition Letters"}



@article{abanda19distance,
  title={A review on distance based time series classification},
  author={A. Abanda and U. Mori and J. Lozano},
  journal={Data Mining and Knowledge Discovery},
  volume={33},
  number={2},
  pages={378--412},
  year={2019}
  }



@article{lucas19proximity,
  title={Proximity Forest: an effective and scalable distance-based classifier for time series},
  author={B. Lucas and A. Shifaz and C. Pelletier and L. O’Neill and N. Zaidi and B. Goethals and F. Petitjean and G. Webb},
  journal={Data Mining and Knowledge Discovery},
  volume={33},
  number={3},
  pages={607--635},
  year={2019}
  }
@article{shifaz19ts-chief,
  author    = {A. Shifaz and C. Pelletier and F. Petitjean and G. Webb},
  title     = {{TS-CHIEF}: A Scalable and Accurate Forest Algorithm
for Time Series Classification},
  volume    = {	arXiv:1906.10329},
 journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1906.10329},  
   url       = {http://arxiv.org/abs/1906.10329},
   year={2019}
}


@article{large19dictionary,
  author = {J. Large and A. Bagnall and S. Malinowski and R. Tavenard},
  title = {On Time Series Classification with Dictionary-Based Classifiers},
 journal = {Intelligent Data Analysis},
    volume    = {23},
    number    = {5},
  year = {2019}
}

@inproceedings{gharghabi18matrixprofile12,
	Author = {S. Gharghabi and S. Imani and A. Bagnall and A. Darvishzadeh and E. Keogh},
	Title ="{Matrix Profile XII: MPDist}: A Novel Time Series Distance Measure to allow Data Mining in more Challenging Scenarios",
        BOOKTITLE = {Proc. 18th {IEEE} International Conference on Data Mining},
        	Year = {2018}
}

@article{lampert18constrained,
  author={ T. Lampert and  T. Dau, T. and B. Lafabregue and N. Serrette and G. Forestier and B. Cr{\'e}milleux and C. Vrain and P. Gancarski},
  title={Constrained distance based clustering for time-series: a comparative and experimental study},
  journal={Data Mining and Knowledge Discovery},
  volume={32},
  number={6},
  pages={1663--1707},
  year={2018}
}


    
@article{yeh18matrixprofile,
  author={M. Yeh and Y. Zhu and L. Ulanova and N. Begum and Y. Ding and  H. Dau and D. Silva and A. Mueen and E. Keogh},
  title={Time series joins, motifs, discords and shapelets: a unifying view that exploits the matrix profile},
  journal={Data Mining and Knowledge Discovery},
  volume={32},
  number={1},
  pages={83--123},
  year={2018}
}


@article{dau18archive,
  author    = {H. Dau and A. Bagnall and K. Kamgar and M. Yeh and Y. Zhu and S. Gharghabi and C. Ratanamahatana},
  title     = {The {UCR} Time Series Archive},
  volume    = {	arXiv:1810.07758},
 journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1810.07758},  
   url       = {http://arxiv.org/abs/1810.07758},
   year={2018}
}

@article{fawaz18deep,
  title={Deep learning for time series classification: a review},
  author={H. Fawaz and G. Forestier and J. Weber and L. Idoumghar and P. Muller},
journal="Data Mining and Knowledge Discovery",
volume="33",
number="4",
pages="917--963",
year="2019"
}
@Article{dau18optimizing,
author="H. Dau and D. Silva and F. Petitjean and G. Forestier and
A. Bagnall and E. Keogh",
title="Optimizing Dynamic Time Warping’s Window Width for Time Series Data Mining Applications",
journal="Data Mining and Knowledge Discovery",
volume="32",
number="4",
pages="1074--1120",
year="2018"
}

 
 
 @inproceedings{tan18dtwcv,
author="C. Tan and M. Herrmann and G. Forestierand G. Webb and F. Petitjean",
title="Efficient search of the best warping window for Dynamic Time Warping",
booktitle    ="Proc. 18th {SIAM} International Conference on Data Mining  {(SDM)}",
year="2018"
}

@inproceedings{tan18efficientDTW,
  title={Efficient search of the best warping window for Dynamic Time Warping},
  author={C. Tan and M. Herrman and G. Forestier and G. Webb and F. Petitjean},
booktitle = {Proc. 18th SIAM International Conference on Data Mining},
  year={2018}
}
@article{bagnall18mtsc,
  author    = {A. Bagnall and H. Dau and J. Lines and M. Flynn and J. Large and A. Bostrom and P. Southam and  E. Keogh},
  title     = {The {UEA} multivariate time series classification archive, 2018},
   journal = {ArXiv e-prints},
archivePrefix = "arXiv",
volume    = {arXiv:1811.00075},
  year      = {2018},
  url       = {http://arxiv.org/abs/1809.06705}
}



@article{lines18hive,
author = {J. Lines and S. Taylor and Bagnall, A.},
title = {Time Series Classification with {HIVE-COTE}: The Hierarchical Vote Collective of Transformation-based Ensembles},
journal = {ACM Trans. Knowledge Discovery from Data},
volume="12",
number="5",
year = {2018},
publisher = {ACM},
}

@article{bagnall17simulators,
  author = {A. Bagnall and A. Bostrom and J. Large and J.Lines},
  title = {Simulated Data Experiments for Time Series Classification Part 1: Accuracy 		Comparison with Default Settings},
 journal = {ArXiv e-prints},
    volume    = {arXiv:1703.09480},
archivePrefix = "arXiv",
   eprint = {1703.09480},  
   url       = {http://arxiv.org/abs/1703.09480},
  year = {2017}
}


@article{large17hesca,
  author = {J. Large and J. Lines and A. Bagnall},
  title = {The Heterogeneous Ensembles of Standard Classification Algorithms {(HESCA)}: the Whole is Greater than the 
  Sum of its Parts},
 journal = {ArXiv e-prints},
    volume    = {arXiv:1710.09220},
archivePrefix = "arXiv",
   eprint = {1710.09220},  
   url       = {http://arxiv.org/abs/1710.09220},
  year = {2017}
}






@Article{bagnall17bakeoff,
author="Bagnall, A. 
and Lines, J.
and Bostrom, A.
and Large, J.
and Keogh, E.",
title="The great time series classification bake off: a review and experimental evaluation of recent algorithmic advances",
journal="Data Mining and Knowledge Discovery",
volume="31",
number="3",
pages="606--660",
year="2017",
}


@inproceedings{gao17motifs,
  title={Efficient Discovery of Time Series Motifs with Large Length Range in Million Scale Time Series},
  author={Y. Gao and J. Lin},
booktitle = {Proc. 17th {IEEE} International Conference on Data Mining},
  year={2017}
}




@inproceedings{yeh16matrixiprofile1,
  title={{Matrix Profile I}: All Pairs Similarity Joins for Time Series: A Unifying View that Includes Motifs, Discords and Shapelets},
  author={M. Yeh and Y. Zhu and L. Ulanova and N. Begum and Y. Ding and  H. Dau and D. Silva and A. Mueen and E. Keogh},
booktitle = {Proc. 16th {IEEE} International Conference on Data Mining},
  year={2016}
}




@Article{aghabozorgi15clustering,
author={S. Aghabozorgi 
and A. Shirkhorshidi
and T. Wah},

title="Time-series clustering – A decade review",
journal="Information Systems",
volume="53",
pages="606--660",
year="2015",
}


@inproceedings{lines16hive,
	Author = {J. Lines and S. Taylor and A. Bagnall},
	Title ="{HIVE-COTE}: The Hierarchical Vote Collective of
Transformation-based Ensembles for Time Series
Classification",
        BOOKTITLE = {Proc. 16th {IEEE} International Conference on Data Mining},
        	Year = {2016}
}


@inproceedings{hamooni14phoneme,
  title={Dual-domain hierarchical classification of phonetic time series},
  author={H. Hamooni and A. Mueen},
booktitle = {Proc. 14th {IEEE} International Conference on Data Mining},
  year={2014}
}


@article{chapelle02choosing,
author = {O. Chapelle and V. Vapnik and O. Bousquet and S. Mukherjee},
journal = {Machine Learning},
number = {46},
pages = {131-159},
title = {Choosing multiple parameters for support vector machines},
volume = {1-3},
year = {2002}
}

@inproceedings{hu13realistic,
  title={Time Series Classification under More Realistic Assumption},
  author={B. Hu, Y. Chen and E. Keogh},
booktitle = {Proc. 13th SIAM International Conference on Data Mining},
  year={2013}
}

@inproceedings{schafer12sfa,
  title={{SFA: a symbolic {Fourier} approximation and index for similarity search in high dimensional datasets}},
  author={P. Sch{\"a}fer and M. H{\"o}gqvist},
  booktitle={Proceedings of the 15th International Conference on Extending Database Technology},
  pages={516--527},
  year={2012},
}


@article{lin07sax,
  title={Experiencing {SAX}: a novel symbolic representation of time series},
  author={J. Lin and E. Keogh and L. Wei and S. Lonardi},
  journal={Data Mining and Knowledge Discovery},
  volume={15},
  number={2},
  year={2007}
}




% Paper reading 2/8/17
@inbook{giusti15ensembles,
 title={Time Series Classification with Representation Ensembles},
 author={R. Giusti and D. Silva and G. Batista},
bookTitle="Proc. 14th International Symposium on advances in Intelligent Data Analysis",
year="2015",
pages="108--119"
}
@inproceedings{giusti16representation,
 title={Improved Time Series Classification with
Representation Diversity and SVM},
 author={R. Giusti and D. Silva and G. Batista},
 booktitle={proc. 16th {IEEE} International Conference on Machine Learning and Applications},
 year={2016}
}

%Added by Tony: Jan 2017
@article{zhao16descriptors,
  author="J. Zhao and L. Itti",
  title="Classifying Time Series Using Local Descriptors with Hybrid Sampling",
  journal="KDE",
  year="2016",
  volume="28",
  number="3",
  pages="623--637"
}



@Inbook{bailly16botsw,
author={A. Bailly and S. Malinowski and R. Tavenard and L. Chapel  and T. Guyet},
title="Dense Bag-of-Temporal-{SIFT}-Words for Time Series Classification",
booktitle="First Advanced Analysis and Learning on Temporal Data Workshop:  Selected papers in Lecture Notes on Artificial Intelligence",
year="2016",
pages="17--30"
}
@article{bailly15sift,
	author = {A. Bailly and S. Malinowski and R. Tavenard and T. Guyet and L. Chapel},
	journal = {ECML/PKDD Workshop on Advanced Analytics and Learning on Temporal Data},
	title = {{Bag-of-Temporal-SIFT-Words for Time Series Classification}},
	year = {2015}
}



@article{schafer16boss-vs,
  author={P. Sch{\"a}fer},
  title="Scalable time series classification",
  journal=DAMI,
volume="30",
number="5",
pages="1273--1298",
  year={2016}
}


@ARTICLE{Weka,     AUTHOR = "M. Hall and E. Frank and G. Holmes and B. Pfahringer and P. Reutemann and I. Witten",
    TITLE = "The {WEKA} Data Mining Software: An Update",
    JOURNAL = "SIGKDD Explorations",
    VOLUME = "11",
    NUMBER = "1 ",
    pages="10--18",
    YEAR = "2009"
}

@inproceedings{keogh05hotsax,
  title={{HOT SAX}: efficiently finding the most unusual time series subsequence},
  author={E. Keogh and J. Lin and A. Fu},
booktitle = {Proc. 5th {IEEE} International Conference on Data Mining},
  year={2005}
}


@article{keogh03benchmarks,
  author={E. Keogh and S. Kasetty},
  title={On the Need for Time Series Data Mining Benchmarks: A Survey and Empirical Demonstration},
  journal={Data Mining and Knowledge Discovery},
  volume={7},
  number={4},
  pages={349--371},
  year={2003}
}
@ARTICLE{rakthanmanon13trillions,
        AUTHOR = "T. Rakthanmanon and J. Bilson and L. Campana and A. Mueen and G. Batista and B. Westover and Q. Zhu and J. Zakaria and E. Keogh",
        TITLE = "Addressing Big Data Time Series: Mining Trillions of Time Series Subsequences Under Dynamic Time Warping",
        JOURNAL = "{ACM} Transactions on Knowledge Discovery from Data",
        YEAR = "2013",
        VOLUME = "7",
        NUMBER = "3"
}

@inproceedings{keogh04segmenting,
  title={An online algorithm for segmenting time series},
  author={E. Keogh and S. Chu and D. Hart and M. Pazzani},
booktitle = {Proc. 1st {IEEE} International Conference on Data Mining},
  year={2001}
}

@inproceedings{ge00deformable,
author="X. Ge and P. Smyth",
title="Deformable Markov model templates for time-series pattern matching",
booktitle    ="Proc. 6th {ACM SIGKDD} International Conference on Knowledge Discovery and Data Mining",
year="2000"
}


%%%%%%%%%% BAKEOFF REFS %%%%%%%%%%%%%5
%%%%%%%%%%%%%%% ELASTIC DISTANCE: TWE, MSM, WDTW, DTD_C, DD_DTW,



%LCSS
@article{hirschberg77algorithms,
title={Algorithms for the longest common subsequence problem},
author={D. Hirschberg},
journal={Journal of the ACM},
volume={24},
number={4},
pages={664--675},
year={1977}
}

%%ERP
@inproceedings{chen04erd,
title={On the marriage of {L}p-norms and edit distance},
author={L. Chen and R. Ng},
booktitle={Proc. 30th International Conference on Very Large Databases {(VLDB)}},
year={2004}
}



%% ALGORITHM: TWE
@ARTICLE{marteau09stiffness,     AUTHOR = "P. Marteau",
        TITLE = "Time Warp Edit Distance with Stiffness Adjustment for Time Series Matching ",
        JOURNAL = "{IEEE} Transactions on Pattern Analysis and Machine Intelligence",
        YEAR = "2009 ",
        VOLUME = "31",
        NUMBER = "2 ",
        PAGES="306--318 "
        }

%% Algorithm WDTW
@article{jeong11weighted,
author="Y. Jeong and M. Jeong and O. Omitaomu",
title="Weighted dynamic time warping for time series classification",
journal="Pattern Recognition",
volume ="44",
issue ="9",
year="2011",
pages ="2231--2240"
}

%% ALGORITHM: MSM
@ARTICLE{stefan13move-split-merge,     AUTHOR = "A. Stefan and V. Athitsos and G. Das ",
        TITLE = "The {Move-Split-Merge} Metric for Time Series",
        JOURNAL = "{IEEE} Transactions on Knowledge and Data Engineering",
        YEAR = "2013",
        VOLUME = "25 ",
        NUMBER = "6 ",
        PAGES="1425--1438" }

@article{gorecki13derivative,
  title={Using derivatives in time series classification},
  author={T. G\'{o}recki and M. \L{}uczak},
  journal={Data Mining and Knowledge Discovery},
  volume={26},
  number={2},
  pages={310--331},
  year={2013}
}

]
%% ALGORITHM: DTD_C
@article{gorecki14nonisometric,
  title={Non-isometric transforms in time series classification using {DTW}},
  author={T. G\'{o}recki and M.  \L{}uczak},
  journal={Knowledge-Based Systems},
  volume={61},
  pages={98--108},
  year={2014}
}
%% ALGORITHM: DTWF
@article{kate16features,
  title={Using dynamic time warping distances as features for improved time series classification},
  author={R. Kate},
  journal={Data Mining and Knowledge Discovery},
  volume={30},
  number={2},
  pages="283--312",
  year={2016}
}
%% APPLICATION Fifty words
@inproceedings{rath03fiftywords,
author="T. Rath and R. Manamatha ",
title="Word image matching using dynamic time warping",
booktitle    ="Proc. Computer Vision and Pattern Recognition",
year="2003"
}


%%%%%%%%%%%%%%%% DICTIONARY BASED: BOSS, BoP, SAXVSM

%% ALGORITHM BOP
@article{lin12bagofpatterns,
  author    = {J. Lin and R. Khade and Y. Li},
  title     = {Rotation-invariant similarity in time series using bag-of-patterns representation},
  journal   = {Journal of Intelligent Information Systems},
  volume    = {39},
  number    = {2},
  year      = {2012},
  pages     = {287-315}
}


%% ALGORITHM SAXVSM
@inproceedings{senin13sax_vsm,
author="P. Senin and S. Malinchik",
title="{SAX-VSM:} Interpretable Time Series Classification Using SAX and Vector Space Model",
booktitle    ="Proc. 13th {IEEE} International Conference on Data Mining {(ICDM)}",
year="2013"
}

%% ALGORITHM BOSS
@article{schafer15boss,
  title={The {BOSS} is concerned with time series classification in the presence of noise},
  author={P. Sch{\"a}fer},
  journal={Data Mining and Knowledge Discovery},
  volume={29},
  number={6},
  pages={1505--1530},
  year={2015}
}


%%%%%%%%%%%%%%%% SHAPELETS %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
@inproceedings{ye09time,
  title={Time series shapelets: a new primitive for data mining},
  author={Ye, Lexiang and Keogh, Eamonn},
  booktitle={Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining},
  pages={947--956},
  year={2009},
  organization={ACM}
}
@inproceedings{mueen11logicalshapelet,
author="A. Mueen and E. Keogh and N. Young",
title="Logical-Shapelets: An Expressive Primitive for Time Series Classification",
booktitle    ="Proc. 17th {ACM SIGKDD} International Conference on Knowledge Discovery and Data Mining",
year="2011"
}
@article{ye11shapelets,
  author    = {L. Ye and E. Keogh},
  title     = {Time series shapelets: a novel technique that allows accurate, interpretable and fast classification},
  journal   = {Data Mining and Knowledge Discovery},
  volume    = {22},
  number    = {1-2},
  year      = {2011},
  pages     = {149-182}
}


%% ALGORITHM RANDOM SHAPELET TREES
@inproceedings{karlsson15forests,
  title={Forests of randomized shapelet trees},
  author={I. Karlsson and P, Papapetrou and H. Bostr{\"o}m},
  booktitle={International Symposium on Statistical Learning and Data Sciences},
  pages={126--136},
  year={2015},
  organization={Springer}
}

@article{karlsson16generalized,
  title={Generalized random shapelet forests},
  author={I. Karlsson and P, Papapetrou and H. Bostr{\"o}m},
  journal={Data Mining and Knowledge Discovery},
  volume={30},
  number={5},
  pages={1053--1085},
  year={2016},
  publisher={Springer}
}


@inproceedings{karlsson16early,
  title={Early Random Shapelet Forest},
  author={I. Karlsson and P, Papapetrou and H. Bostr{\"o}m},
  booktitle={International Conference on Discovery Science},
  pages={261--276},
  year={2016},
  organization={Springer}
}


@article{gordon12fast,
  title={Fast randomized model generation for shapelet-based time series classification},
  author={D. Gordon and D. Hendler and L. Rokach},
  journal={arXiv preprint arXiv:1209.5038},
  year={2012}
}

@phdthesis{hills15thesis,
  title={Mining Time-series Data using Discriminative Subsequences},
  author={J. Hills},
  school={School of Computing Sciences, University of East Anglia},
  year={2015}
}

%% ALGORITHM ST
%Added by Tony: 17/1/17
@inproceedings{lines12shapelet,
   author    = "J. Lines and L. Davis and J. Hills  and A. Bagnall",
   title     = "A Shapelet Transform for Time Series Classification",
   BOOKTITLE = "Proc. the 18th {ACM SIGKDD} International Conference on Knowledge Discovery and Data Mining",
   year      = "2012"
}
@inproceedings{lines12alternative,
  title={Alternative quality measures for time series shapelets},
  author={J. Lines and A. Bagnall},
  booktitle={International Conference on Intelligent Data Engineering and Automated Learning},
  pages={475--483},
  year={2012},
  organization={Springer}
}
@article{hills14shapelet,
  title={Classification of time series by shapelet transformation},
  author={J. Hills  and  J. Lines and E. Baranauskas and J. Mapp and A. Bagnall},
  journal={Data Mining and Knowledge Discovery},
  volume={28},
  number={4},
  pages={851--881},
  year={2014}
}
@article{bostrom15binary,
	author = {A. Bostrom and A. Bagnall},
	title ={Binary Shapelet Transform for Multiclass Time Series Classification},
	journal ={Proc. 17th International Conference on Big Data Analytics and Knowledge Discovery {(DAWAK)}},
	Year = {2015}
}

@article{bostrom17binary,
  author={A. Bostrom and A. Bagnall},
  title={Binary Shapelet Transform for Multiclass Time Series Classification},
  journal={Transactions on Large-Scale Data and Knowledge Centered Systems},
  volume={32},
  year={2017},
  pages={24--46}
}

@article{bostrom16evaluating,
  title={Evaluating Improvements to the Shapelet Transform},
  author={A. Bostrom and A. Bagnall and J. Lines},
  year  = {2016},
  journal = {Knowledge Discovery and Data Mining, in Workshop on Mining and Learning from Time Series}
}

@article{bostrom17multivariate,
   author = {A. Bostrom and A. Bagnall},
   title = "{A Shapelet Transform for Multivariate Time Series Classification}",
   journal = {ArXiv e-prints},
   archivePrefix = "arXiv",
   eprint = {1712.06428},
   year = 2017,
}



%added by Aaron. RandomShapelet.
@inproceedings{renard15random,
  title={Random-shapelet: an algorithm for fast shapelet discovery},
  author={X. Renard and M. Rifqi and W. Erray and M. Detyniecki},
  booktitle={Proc. IEEE International Conference om Data Science and Advanced Analytics},
  year={2015}
  }

@article{gordon12RandomisedModel,
  author    = {D. Gordon and
               D. Hendler and
               L. Rokach},
  title     = {Fast Randomized Model Generation for Shapelet-Based Time Series Classification},
  journal   = {CoRR},
  volume    = {abs/1209.5038},
  year      = {2012},
  url       = {http://arxiv.org/abs/1209.5038},
  timestamp = {Wed, 10 Oct 2012 21:28:55 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/abs-1209-5038},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@article{raza17ensembles,
  title={Ensembles of Randomized Time Series Shapelets Provide Improved Accuracy while Reducing Computational Costs},
  author={A. Raza and S. Kramer},
  journal={arXiv preprint arXiv:1702.06712},
  year={2017}
}

%% ALGORITHM LS
@inproceedings{grabocka14learning-shapelets,
author="J. Grabocka and N. Schilling and M. Wistuba and L. Schmidt-Thieme",
title="Learning Time-Series Shapelets",
booktitle    ="Proc. 20th {ACM SIGKDD} International Conference on Knowledge Discovery and Data Mining",
year="2014"
}

%FLAG
@inproceedings{hou16efficient,
  title={Efficient learning of timeseries shapelets},
  author={L. Hou and J. Kwok and J. Zurada},
  booktitle={Proc. 30th AAAI Conference on Artificial Intelligence},
  year={2016}
}

@article{grabocka16fast,
  title={Fast classification of univariate and multivariate time series through shapelet discovery},
  author={J. Grabocka and M. Wistuba and L. Schmidt-Thieme},
  journal={Knowledge and Information Systems},
  volume={49},
  number={2},
  pages={429--454},
  year={2016},
  publisher={Springer}
}

%unsupervised shapelets
@article{zakaria16accelerating,
  title={Accelerating the discovery of unsupervised-shapelets},
  author={J. Zakaria and A. Mueen and E. Keogh and N. Young},
  journal={Data mining and knowledge discovery},
  volume={30},
  number={1},
  pages={243--281},
  year={2016},
  publisher={Springer}
}

% ALGORITHM FS
@inproceedings{rakthanmanon13fastshapelets,
author="T. Rakthanmanon and E. Keogh ",
title="Fast-Shapelets: A Fast Algorithm for Discovering Robust Time Series Shapelets",
booktitle    ="Proc. 13th {SIAM} International Conference on Data Mining  {(SDM)}",
year="2013"
}
%%%%%%%%%% INTERVAL BASED
@article{rodriguez05intervalbased,
author="J. Rodr\'{i}guez and C. Alonso and J. Maestro",
title="Support Vector Machines of Interval-based Features for Time Series Classification",
Journal="Knowledge-Based Systems",
volume="18",
issue="4",
pages="171--178",
year="2005"
}

@article{deng13forest,
author = {H. Deng and G. Runger and E. Tuv and M. Vladimir},
 title = {A time series forest for classification and feature extraction},
 journal = {Information Sciences},
 volume = {239},
 pages={142--153},
 year = {2013}
}
@article{baygogan13tsbf,
  author    = {M. Baydogan and G. Runger and E. Tuv},
  title     = {A Bag-of-Features Framework to Classify Time Series},
  journal   = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence},
  volume    = {25},
  number    = {11},
  year      = {2013},
  pages     = {2796--2802}
}
@article{baydogan15lps,
  title={Time series representation and similarity based on local autopatterns},
  author={M. Baydogan and G. Runger},
  journal={Data Mining and Knowledge Discovery},
  volume={30},
  number= {2},
  pages     = {476--509},
  year={2016}
}

%% ENSEMBLES

@inproceedings{bagnall12transformation,
  title={Transformation Based Ensembles for Time Series Classification.},
  author={Bagnall, A. and Davis, L. M. and Hills, J. and Lines, J.},
booktitle = {Proceedings of the 2012 SIAM International Conference on Data Mining},
  volume={12},
  pages={307--318},
  year={2012}
}


@article{bagnall15cote,
  title={Time-Series Classification with {COTE}: The Collective of Transformation-Based Ensembles},
  author={A. Bagnall and J. Lines and J. Hills and A. Bostrom},
  journal={{IEEE} Transactions on Knowledge and Data Engineering},
  volume={27},
  issue={9},
  pages={2522--2535},
  year={2015}
}
@article{lines15elastic,
  title={Time Series Classification with Ensembles of Elastic Distance Measures},
  author={J. Lines and A. Bagnall},
  journal={Data Mining and Knowledge Discovery},
  volume={29},
  issue={3},
  pages={565--592},
  year={2015}
}

%%%%%%%%%%% COMPLEXITY BASED


%% ALGORITHMS CID_ED CID_DTW
@article{batista14cid,
  title={{CID}: an efficient complexity-invariant distance measure for time series},
  author={G. Batista and  E. Keogh and O. Tataw and V. deSouza},
  journal={Data Mining and Knowledge Discovery},
  volume={28},
  number={3},
  pages="634--669",
  year={2014}
}

%%% NOT INCLUDED
@inproceedings{chen13kernel,
author="H. Chen and F. Tang and P. Tino and X. Yao",
title= "Model-based kernel for efficient time series analysis",
booktitle    ="Proc. 19th {ACM SIGKDD} International Conference on Knowledge Discovery and Data Mining",
year="2013"
}
@inproceedings{smyth97hmm,
    author = "P. Smyth",
    title = "Clustering Sequences with Hidden Markov Models",
    booktitle = "Advances in Neural Information Processing Systems",
    volume = "9",
    year = "1997"
}


@ARTICLE{maharaj00clusters,
    author={E. A. Maharaj},
    title= {Clusters of time series},
    journal={Journal of Classification},
    volume= {17},
    pages={297--314},
    year={2000}
    }


@article{corduas08autoregressive,
 author = {M. Corduas and D. Piccolo},
 title = {Time series clustering and classification by the autoregressive metric},
 journal = {Computational Statistics and Data Analysis},
 volume = {52},
 number = {4},
 year = {2008},
 pages = {1860--1872}
 }


@article{bagnall14histogram,
  title={A run length transformation for discriminating between auto regressive time series},
  author={A. Bagnall and G. Janacek},
  journal={Journal of Classification},
  volume={31},
  issue={2},
  pages={154--178},
  year={2014}
}


@article{grabocka14invariant,
  title={Invariant time-series factorization},
  author={J. Grabocka and L. Schmidt-Thieme},
  journal={Data Mining and Knowledge Discovery},
  volume={28},
    number={5},
    pages={1455--1479},
  year={2014}
}


%% GENERAL
@article{fulcher14comparative,
  author={B.  Fulcher and N. Jones},
  title={Highly comparative feature-based time-series classification},
  journal={{IEEE} Transactions on Knowledge and Data Engineering},
  volume={26},
  number={12},
  pages={3026--3037},
  year={2014}
}


@inproceedings{silva13recurrence,
author="D. Silva and V. de Souza and G. Batista",
title="Time Series Classification Using Compression Distance of Recurrence Plots",
booktitle    ="Proc. 13th {IEEE} International Conference on Data Mining",
year="2013"
}

@inproceedings{ratanamahatana05threemyths,
  author    = {C. Ratanamahatana and E. Keogh},
  title     = {Three Myths about Dynamic Time Warping Data Mining},
  booktitle = {Proc. 5th {SIAM} International Conference on Data Mining},
  year      = {2005}
}

@INPROCEEDINGS{sart10accelerating, 
    author={D. Sart and A. Mueen and W. Najjar and E. Keogh and V. Niennattrakul}, 
    title={Accelerating Dynamic Time Warping Subsequence Search with GPUs and FPGAs}, 
    booktitle={Proc. 10th {IEEE} International Conference on Data Mining}, 
    year={2010}
    }

@article{keogh05exact,
  title={Exact indexing of dynamic time warping},
  author={E. Keogh and A. Ratanamahatana},
  journal={Knowledge and information systems},
  volume={7},
  number={3},
  pages={358--386},
  year={2005},
  publisher={Springer}
}


@article{chakrabarti02dimensionality,
     author = {K. Chakrabarti and E. Keogh and S. Mehrotra and M. Pazzani},
     title = {Locally Adaptive Dimensionality Reduction for Indexing Large Time Series Databases},
     journal = {ACM Transactions on Database Systems},
     volume = {27},
     number = {2},
     year = {2002},
     pages = {188--228}
     }
@inproceedings{keogh01derivative,
author="E. Keogh and M. Pazzani",
title="Derivative dynamic time warping",
booktitle    ="Proc. 1st {SIAM} International Conference on Data Mining  {(SDM)}",
year="2001"
}
@inproceedings{ding08comparison,
author="H. Ding and G. Trajcevski and P. Scheuermann and X. Wang and E. Keogh",
title="Querying and Mining of Time Series Data: Experimental Comparison of Representations and Distance Measures",
booktitle    ="Proc. 34th International Conference on Very Large Data Bases {(VLDB)}",
year="2008"
}
%%% ADD PAPER READING PAPERS AT THE END
%17/1/17
@article{wang13comparison,
author="X. Wang and A. Mueen and H. Ding and G. Trajcevski and P. Scheuermann and E. Keogh",
title="Experimental comparison of representation methods and distance measures for time series data",
        journal = "Data Mining and Knowledge Discovery",
        volume = "26",
        number = "2",
        pages = "275--309",
        year = "2013"
}

%24/1/17
@article{wang13bagofwords,
author = "J. Wang and P. Liu and M. She and S. Nahavandi and A. Kouzani",
title = "Bag-of-words representation for biomedical time series classification ",
journal = "Biomedical Signal Processing and Control",
volume = "8",
number = "6",
pages = "634 - 644",
year = "2013"
}
%24/1/17
@inproceedings{wang15imputation,
 author = {Z. Wang and T. Oates},
 title = {Imaging Time-series to Improve Classification and Imputation},
 booktitle = {Proc. 24th International Conference on Artificial Intelligence},
 year = {2015}
} 


@article{villar16epilepsy,
	author={J. Villar and P. Vergara and M. Men{\'e}ndez and E. de la Cal and V. Gonz{\'a}lez and J. Sedano},
	title={Generalized Models for the classification of abnormal movements in daily life and its applicability to epilepsy
	convulsions recognition},
	journal={International Journal of Neural Systems},
  	year={2016}
}

@ARTICLE{caiado06periodogram,
    author = {J. Caiado and N. Crato and D. Pena },
    title = {A periodogram-based metric for time series classification},
    journal = {Computational Statistics and Data Analysis},
    year = {2006},
    volume = {50},
    pages = {2668--2684}
}

% additions of ADB:

@article{BischlEtal2016,
  author  = {Bernd Bischl and Michel Lang and Lars Kotthoff and Julia Schiffner and Jakob Richter and Erich Studerus and Giuseppe Casalicchio and Zachary M. Jones},
  title   = {mlr: Machine Learning in R},
  journal = {Journal of Machine Learning Research},
  year    = {2016},
  volume  = {17},
  number  = {170},
  pages   = {1-5},
  url     = {http://jmlr.org/papers/v17/15-066.html}
}

@article {BezansonEtal2017,
    AUTHOR = {Bezanson, Jeff and Edelman, Alan and Karpinski, Stefan and
              Shah, Viral B.},
     TITLE = {Julia: a fresh approach to numerical computing},
   JOURNAL = {SIAM Rev.},
  FJOURNAL = {SIAM Review},
    VOLUME = {59},
      YEAR = {2017},
    NUMBER = {1},
     PAGES = {65--98},
      ISSN = {0036-1445},
   MRCLASS = {68N15 (65Y05 97P40)},
  MRNUMBER = {3605826},
       DOI = {10.1137/141000671},
       Adsurl = {https://doi.org/10.1137/141000671},
}

@article{Blaom_I,
  author = {Blaom, Anthony},
  title = {Flexible model composition in machine learning and its implementation in MLJ},
  journal = {\normalfont {In preparation}},
  year = {2020}
}

@article{Innes2018,
  doi = {10.21105/joss.00602},
  Adsurl = {https://doi.org/10.21105/joss.00602},
  year = {2018},
  publisher = {The Open Journal},
  volume = {3},
  number = {25},
  pages = {602},
  author = {Mike Innes},
  title = {Flux: Elegant machine learning with Julia},
  journal = {Journal of Open Source Software}
}

@article{Kuhn2008,
   author = {Max Kuhn},
   title = {Building Predictive Models in R Using the caret Package},
   journal = {Journal of Statistical Software, Articles},
   volume = {28},
   number = {5},
   year = {2008},
   keywords = {},
   abstract = {The caret package, short for classification and regression training, contains numerous tools for developing predictive models using the rich set of models available in R. The package focuses on simplifying model training and tuning across a wide variety of modeling techniques. It also includes methods for pre-processing training data, calculating variable importance, and model visualizations. An example from computational chemistry is used to illustrate the functionality on a real data set and to benchmark the benefits of parallel processing with several types of models.},
   issn = {1548-7660},
   pages = {1--26},
   doi = {10.18637/jss.v028.i05},
   Adsurl = {https://www.jstatsoft.org/v028/i05}
}

@software{LinEtal2020,
  author       = {Dahua Lin and
                  John Myles White and
                  Simon Byrne and
                  Andreas Noack and
                  Mathieu Besançon and
                  Douglas Bates and
                  John Pearson and
                  Alex Arslan and
                  Kevin Squire and
                  David Anthoff and
                  John Zito and
                  Theodore Papamarkou and
                  Moritz Schauer and
                  Jan Drugowitsch and
                  Avik Sengupta and
                  Brian J Smith and
                  Glenn Moynihan and
                  Giuseppe Ragusa and
                  Gord Stephen and
                  Christoph Dann and
                  Mike J Innes and
                  Michael and
                  Martin O'Leary and
                  Tamas K. Papp and
                  Jiahao Chen and
                  Iain Dunning and
                  Gustavo Lacerda and
                  Richard Reeve and
                  Kai Xu and
                  David Widmann},
  title        = {JuliaStats/Distributions.jl: v0.23.2},
  month        = mar,
  year         = 2020,
  publisher    = {Zenodo},
  version      = {v0.23.2},
  doi          = {10.5281/zenodo.3730565},
  url          = {https://doi.org/10.5281/zenodo.3730565}
}

@misc{CategoricalArrays,
    author = {Bouchet-Valat, Milan et al.},
    title = {JuliaData/CategoricalArrays.jl},
    year = {2014},
    publisher = {​GitHub},
    journal = {​GitHub repository},
    Adsurl = {https://github.com/JuliaData/CategoricalArrays.jl}
}

@misc{MLJ,
    author = {Blaom, Anthony et al.},
    title = {MLJ: A machine learning framework for Julia},
    year = {2019},
    publisher = {​GitHub},
    journal = {​GitHub repository},
    Adsurl = {https://github.com/alan-turing-institute/MLJ.jl}
}

@misc{MLJdocs,
    author = {Blaom, Anthony},
    title = {MLJ documentation},
    year = {2020},
    publisher = {​GitHub},
    journal = {​GitHub pages},
    Adsurl = {https://alan-turing-institute.github.io/MLJ.jl/dev/}
}

@misc{MLJTuning,
    author = {Blaom, Anthony and collaborators},
    title = {Hyperparameter optimization algorithms for use in the MLJ machine learning framework},
    year = {2020},
    publisher = {​GitHub},
    journal = {​GitHub repository},
    Adsurl = {https://alan-turing-institute.github.io/MLJTuning/}
}

@misc{MLJtutorials,
    author = {Lienart, Thibaut and Blaom, Anthony and collaborators},
    title = {Data Analysis and Machine Learning in Julia},
    year = {2020},
    publisher = {​GitHub},
    journal = {​GitHub pages},
    Adsurl = {https://alan-turing-institute.github.io/MLJTutorials/}
}

@misc{Rackauckas2017,
    author = {C. Rackauckas},
    title = {A Comparison Between Differential Equation Solver Suites In MATLAB, R, Julia, Python, C, Mathematica, Maple, and Fortran},
    year = {2017},
    publisher = {​C. Rackauckas},
    journal = {​Stochastic Lifestyle (blog)},
    Adsurl = {​https://www.stochasticlifestyle.com/comparison-differential-equation-solver-suites-matlab-r-julia-python-c-fortran/}
}

@article{RackauckasNie2017,
    author = {C. Rackauckas and Q. Nie},
    year = {2017},
    title = {DifferentialEquations.jl – A Performant and Feature-Rich Ecosystem for Solving Differential Equations in Julia},
    journal = {Journal of Open Research Software},
    volume = 5,
    number = 1,
    pages = {15},
    doi = {10.5334/jors.151}
}

@misc{Quinn,
    author = {J. Quinn},
    title = {Tables.jl: An interface for tables in Julia},
    year = {2020},
    publisher = {​GitHub},
    journal = {​GitHub repository},
    Adsurl = {​https://github.com/JuliaData/Tables.jl}
}
